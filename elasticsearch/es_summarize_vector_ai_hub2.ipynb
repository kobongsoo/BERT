{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# - 문장들을  추출 요약해서 요약문장을 만들고, 요약 문장의 평균을 구하여 문장 embedding을 생성하여 ES에 인덱스에 vector 추가하고, 검색하는 예제임\n",
    "# - 말뭉치는 ai_hub에 원천말뭉치인 ts1 말뭉치에 tilte, content를 추출하여, content 요약문과 title에 대해 각각 vector를 만들어서 ES 인덱스로 추가하는 예시임.\n",
    "#\n",
    "# => 대규모 웹데이터 기반 한국어 말뭉치 데이터 \n",
    "# 말뭉치 출처: https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624\n",
    "\n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "# - elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# - 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss\n",
    "\n",
    "# 추출 요약 설치\n",
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "import kss\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "# FutureWarning 제거\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, getListOfFiles\n",
    "device = GPU_info()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# 0. param 설정\n",
    "#------------------------------------------------------------------------------------\n",
    "seed = 111\n",
    "query_num = 500             # 쿼리 최대 갯수: KorQuAD_v1.0_dev.json 최대값은 5533개임, 0이면 모든 5533개 쿼리함.\n",
    "search_k = 5                # FAISS 검색시, 검색 계수(5=쿼리와 가장 근접한 5개 결과값을 반환함)\n",
    "avg_num = 1                 # 쿼리에 대해 sub 문장들중 최대 scorce를 갖는 문장을 몇개 찾고 평균낼지.(3=쿼리에 가장 유사한 sub문장 3개를 찾고 평균을 냄)\n",
    "faiss_index_method = 0      # 0= Cosine Similarity 적용(IndexFlatIP 사용), 1= Euclidean Distance 적용(IndexFlatL2 사용)\n",
    "\n",
    "# 토큰 임베딩 관련 param\n",
    "IS_EMBED_DIVIDE = True      #여기서는 토큰 임베딩은 True고정, True=문단의 여러 문장을, 토큰 단위로 분리후 벡터 구해서 인덱스 만듬/False=문단의 여러문장을 하나의 벡터를 구해서 인덱스 만듬.\n",
    "EMBED_DIVIDE_LEN = [5,7,9]  #5 # 문장을 몇개(토큰)으로 분리할지 (7,8,10) 일때 성능 좋음=>50.8%, (5,7,9) 일때 차원축소 128=>41.80%(81.8%) 성능 좋음\n",
    "MAX_TOKEN_LEN = 40          # 최대 몇개 token까지만 임베딩 할지\n",
    "\n",
    "# 차원 축소 관련 param\n",
    "# 차원 축소 할지, 768 값을 앞에 384 만큼만 배열 resize 시킴.  \n",
    "# - 384로 줄일대 -2% 성능 저하 발생(512: -1.2%, 256: -6%)\n",
    "DIM_RESIZE_METHOD = 2  # 0=차원축소 안함/1=resize 로 차원축소/2=Dense 모델 이용 차원축소\n",
    "DIM_RESIZE_LEN = 128\n",
    "\n",
    "# ONNX 모델 사용\n",
    "IS_ONNX_MODEL = False        # True=onnx 모델 사용\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---bi_encoder---------------------------\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 128, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "---dense param---------------------------\n",
      "*dense_weight:torch.Size([128, 768])\n",
      "*dense_bias:torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 1. 검색모델 로딩\n",
    "# => bi_encoder 모델 로딩, polling_mode 설정\n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import bi_encoder, dense_model\n",
    "\n",
    "# bi_encoder 모델 로딩\n",
    "bi_encoder_path = \"bongsoo/klue-sbert-v1\"\n",
    "pooling_mode = 'mean' # bert면=mean, albert면 = cls\n",
    " # 출력 임베딩 크기 지정 : 0=기본 모델 임베딩크기(768), 예:128=128 츨력임베딩 크기 \n",
    "out_dimension = 128 if DIM_RESIZE_METHOD == 2 else 0 if DIM_RESIZE_METHOD != 2 else None\n",
    "    \n",
    "word_embedding_model1, bi_encoder1 = bi_encoder(model_path=bi_encoder_path, max_seq_len=512, do_lower_case=True, \n",
    "                                              pooling_mode=pooling_mode, out_dimension=out_dimension, device=device)\n",
    "print(f'---bi_encoder---------------------------')   \n",
    "print(bi_encoder1)\n",
    "print(word_embedding_model1)\n",
    "\n",
    "print(f'---dense param---------------------------')   \n",
    "# 출력 값 차원 축소 지정인 경우, token_embeddings 일때는 sentencebert 라이브러리를 이용하여 dense_model 모델 추가할수 없으므로,\n",
    "# 사용자정의 dense_model을 정의해서, 가중치와 bias를 bi_encoder모델에 것을 얻어와서 적용해서 차원 죽소함.\n",
    "# => resize 방식 보다 성능 떨어지지만, 128일때는 더 성능이 좋음\n",
    "if DIM_RESIZE_METHOD == 2:\n",
    "    #-------------------------------------------------------------------------\n",
    "    # 처음에는 아래 코드를 활용하여 해당 모델의 128 weight와 bias를 저장해 두어야 함.\n",
    "    #state_dict = bi_encoder1.state_dict()  # bi_encoder모델이 state_dict 얻어옴\n",
    "    #print(state_dict.keys())\n",
    "    #dense_weight = state_dict['2.linear.weight'] # denser 모델에 bi_encoder모델이 linear.weight 적용\n",
    "    #dense_bias = state_dict['2.linear.bias']     # denser 모델에 bi_encoder모델이 linear.bias 적용\n",
    "    \n",
    "    # 처음에  weigth, bias 파일을 저장해 둠.\n",
    "    #torch.save(dense_weight, 'klue-sbert-v1-weigth.pt')\n",
    "    #torch.save(dense_bias, 'klue-sbert-v1-bias.pt')\n",
    "    #-------------------------------------------------------------------------\n",
    "    # weigth, bias 저장해둔 파일 로딩\n",
    "    dense_weight = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-weight-128.pt')\n",
    "    dense_bias = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-bias-128.pt')\n",
    "\n",
    "    print('*dense_weight:{}'.format(dense_weight.size()))\n",
    "    print(f'*dense_bias:{dense_bias.size()}')\n",
    " \n",
    "# onnx 모델 로딩\n",
    "if IS_ONNX_MODEL == True:\n",
    "    onnx_model_path = \"bongsoo/klue-sbert-v1-onnx\"#\"bongsoo/klue-sbert-v1-onnx\"\n",
    "    onnx_tokenizer, onnx_model = onnx_model(onnx_model_path)\n",
    "    print(f'---onnx_model---------------------------')\n",
    "    print(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b67318-d790-422c-a469-874a1b5b7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import embed_text, onnx_embed_text\n",
    "\n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "def embedding(paragrphs:list):\n",
    "    if IS_ONNX_MODEL == True:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs) \n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs, token_embeddings=False)\n",
    "    else:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, token_embeddings=True, return_tensor=False)\n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, return_tensor=False)  \n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0068d-6e57-430a-8142-19a05fa9e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import kss\n",
    "from myutils import embed_text, divide_arr_avg_exten, clean_text\n",
    "\n",
    "# 인덱싱 함수 \n",
    "def index_data():\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    count = 0\n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "        count += 1\n",
    "        #print(f'{count}:{source}') # 인덱스 구조 출력\n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    # json 파일들이 있는 폴더에 .json 파일 이름들을 얻기\n",
    "    # =>DATA_FOLDER: .JSON파일들이 있는 폴더\n",
    "    files = getListOfFiles(DATA_FOLDER)\n",
    "    assert len(files) > 0 # files가 0이면 assert 발생\n",
    "    print('*file_count: {}, file_list:{}'.format(len(files), files[0:5]))\n",
    " \n",
    "    for idx, file in enumerate(tqdm(files)):\n",
    "        if \".json\" not in file:  #.json 파일이 아니면 합치지 않음\n",
    "            continue\n",
    "            \n",
    "        count = 0\n",
    "        docs = []\n",
    "    \n",
    "        # json 파일 로딩 => [SJML][text] entry만 불러옴\n",
    "        json_data = json.load(open(file, \"r\", encoding=\"utf-8\"))['SJML']['text']\n",
    "        for data in json_data:\n",
    "        #for data in json_data:\n",
    "            count += 1\n",
    "            doc = {} #dict 선언\n",
    "            \n",
    "            doc['title'] = data['title']            # 제목 설정\n",
    "            doc['paragraph'] = data['content']      # 문장 담음.\n",
    "                \n",
    "            docs.append(doc)\n",
    "            #print(f'count:{count}')\n",
    "            #print(doc['title'])\n",
    "            \n",
    "            if count % BATCH_SIZE == 0:\n",
    "                index_batch(docs)\n",
    "                docs = []\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "            # ** 10 개만 보냄\n",
    "            #if count >= 10:\n",
    "            #   break\n",
    "            \n",
    "        if docs:\n",
    "            index_batch(docs)\n",
    "            print(\"Indexed {} documents.\".format(count))   \n",
    "            \n",
    "        es.indices.refresh(index=INDEX_NAME)\n",
    "            \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    #print(\"=== End Done indexing===\")\n",
    "                   \n",
    "\n",
    "def index_batch(docs):\n",
    "    \n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        title = doc['title']\n",
    "        paragraph = doc['paragraph']\n",
    "        #sub_contexts = []\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        paragraph = clean_text(paragraph)  # 전처리 : (한글, 숫자, 영문, (), {}, [], %, ,,.,\",')  등을 제외한 특수문자 제거\n",
    "        # 입력 문단을 여러 문장들로 나눔.\n",
    "        #sentences = [sentence for sentence in paragraph.split('.') if sentence != '' and len(sentence) > 10]  # '.'(마침표) 로 구분해서 sub 문장을 만듬.\n",
    "        sentences = [sentence for sentence in kss.split_sentences(paragraph) if sentence != '' and len(sentence) > 10] # kss 이용해서 sub 문장을 만듬\n",
    "        \n",
    "        # 만약 sentences(sub 문장) 가 하나도 없으면 원본문장을 담고, 10이상이면  10개만 담음.\n",
    "        sub_contexts.append([context] if len(sentences) < 1 else sentences[0:10] if len(sentences) > 10 else sentences)\n",
    "       \n",
    "        if i < 1:\n",
    "            print(sub_contexts)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # 토큰 분할 임베딩 처리\n",
    "        token_embeds = embedding(sub_contexts)\n",
    "        #------------------------------------------------------------------------------------------------------------------------ \n",
    "        # 토큰 분할인 경우 처리 start=>           \n",
    "        token_embed_arr_list = []\n",
    "        tcount = 0\n",
    "        # tensor(250,768) 한문장 토큰 임베딩 얻어와서, 각 ?개 토큰씩 평균을 구함.\n",
    "        for token_embed in token_embeds:\n",
    "            token_embed = token_embed[1:-1] # 맨앞에 [CLS]와 맨뒤에 [SEP] 토큰은 뺌\n",
    "            if tcount >= MAX_TOKEN_LEN: \n",
    "                break\n",
    "                \n",
    "            token_embed_arrs = token_embed.cpu().numpy().astype('float32')\n",
    "            #print(f'token_embed_arrs:{token_embed_arrs.shape}')\n",
    "            # 5,7,10 씩 자르면서 문장 토큰 평균을 구함\n",
    "            token_embed_divide_arrs = divide_arr_avg_exten(embed_arr=token_embed_arrs, divide_arrs=EMBED_DIVIDE_LEN) \n",
    "\n",
    "             # Dense 방식으로 차원 축소 => 평균 구한후 차원 축소하는 방식이 0.6% 정도 성능 좋음\n",
    "            if DIM_RESIZE_METHOD == 2:\n",
    "                tmp1 = torch.Tensor(token_embed_divide_arrs)\n",
    "                #tmp1 = torch.from_numpy(token_embed_divide_arrs)\n",
    "                debug1 = False\n",
    "                tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias, debug=debug1)\n",
    "                arrs = tmp2.detach().numpy().astype('float32')\n",
    "            else:  \n",
    "                arrs = token_embed_divide_arrs\n",
    "                \n",
    "            # 평균 구한 토큰들을 token_embed_arr_list 리스트에 담아둠.(50보다 크면 50개만 담음)           \n",
    "            for idx, arr in enumerate(arrs):\n",
    "                \n",
    "                 # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "                if DIM_RESIZE_METHOD == 1:\n",
    "                    darr = np.resize(arr, (DIM_RESIZE_LEN,))\n",
    "                else:\n",
    "                    darr = arr\n",
    "                    \n",
    "                token_embed_arr_list.append(darr)\n",
    "                tcount +=1\n",
    "                if tcount >=MAX_TOKEN_LEN:\n",
    "                    break\n",
    "\n",
    "        #embeddings = np.array(token_embed_arr_list)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # ES에 문단 인덱싱 처리\n",
    "        request = {}  #dict 정의\n",
    "        request[\"rfile_name\"] = title       # 제목               \n",
    "        request[\"rfile_text\"] = paragraph   # 문장\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"        \n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        # for문 돌면서 벡터 처리\n",
    "        #print(type(token_embed_arr_list))\n",
    "        #print(len(token_embed_arr_list))\n",
    "        \n",
    "        # vector 1~40 까지 값을 0으로 초기화 해줌.\n",
    "        for i in range(MAX_TOKEN_LEN):\n",
    "            if DIM_RESIZE_METHOD > 0:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((DIM_RESIZE_LEN))\n",
    "            else:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((768))\n",
    "            \n",
    "        # vector 값들을 담음.\n",
    "        for i, token_embed_arr in enumerate(token_embed_arr_list):\n",
    "            request[\"vector\"+str(i+1)] = token_embed_arr\n",
    "            \n",
    "        requests.append(request)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'aihub-ts1-test-klue-sbert-v1-mpower10u-stoken-128d'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "INDEX_FILE = './data/mpower10u_128d.json'                 # 인덱스 구조 파일\n",
    "DATA_FOLDER = '../../../data11/ai_hub/ts1/sample1/'     # 인덱스할 파일들이 있는 폴더경로 \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "## 입력 ##\n",
    "# GET korquad/_count\n",
    "\n",
    "## 출력 ###\n",
    "'''\n",
    "{\n",
    "  \"count\" : 1420,\n",
    "  \"_shards\" : {\n",
    "    \"total\" : 2,\n",
    "    \"successful\" : 2,\n",
    "    \"skipped\" : 0,\n",
    "    \"failed\" : 0\n",
    "}\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 하기\n",
    "\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from myutils import embed_text, divide_arr_avg_exten\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    \n",
    "    # 문장으로 비교할때=>쿼리 문장에 대한 벡터 생성해서 비교\n",
    "    #query_vector = embed_text(model=bi_encoder,paragraphs=[query])[0]\n",
    "    #------------------------------------------------------------------\n",
    "    # 토큰 평균으로 비교할때=> 쿼리 문장에 대한 모든 토큰 벡터를 생성해서 비교\n",
    "    # 토큰 분할 임베딩 처리\n",
    "    token_query_embeds = embedding([query])\n",
    "    \n",
    "    token_query_embed_arr_list = []\n",
    "    # 쿼리 문장들의 토큰들의 평균을 구함.\n",
    "    for token_query_embed in token_query_embeds:\n",
    "        \n",
    "        token_query_embed = token_query_embed[1:-1] # 맨앞에 [CLS]와 맨두에 [SEP] 토큰은 뺌\n",
    "        print(f'token_query_embed.shape:{token_query_embed.shape}')\n",
    "        \n",
    "        tmp = token_query_embed.cpu().numpy().astype('float32')\n",
    "        tmp=tmp.mean(axis=0) #평균 구함\n",
    "        \n",
    "         # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "        if DIM_RESIZE_METHOD == 1:\n",
    "            tmp = np.resize(tmp, (DIM_RESIZE_LEN,))\n",
    "         # Dense 방식으로 차원 축소=> 평균 구하기 전에 차원 축소하는것이 성능 +0.6더 좋음\n",
    "        elif DIM_RESIZE_METHOD == 2:\n",
    "            tmp1 = torch.Tensor([tmp]) # 1차원 배열을 -> 2차둰 텐서(1,768)로 변환\n",
    "            tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias)\n",
    "            tmp = tmp2.detach().numpy().astype('float32').ravel(order='C') # 1차원 배열로 변경(128,)        \n",
    "        \n",
    "        token_query_embed_arr_list.append(tmp)\n",
    "        \n",
    "    query_vector = np.array(token_query_embed_arr_list)[0]  # 리스트를 배열로 변환  \n",
    "    \n",
    "    # print(query_vector)\n",
    "    print(f'query_vector.shape:{query_vector.shape}')\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    '''\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector2']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴(doc['summarize_vector'])\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # 문단별 50개의 벡터와 쿼리벡터를 서로 비교하여 최대값 갖는 문단들중 가장 유사한  문단 출력\n",
    "    # => script \"\"\" 안에 코드는 java 임.\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "                \"script\":{\n",
    "                    \"source\": \"\"\"\n",
    "                      float max_score = 0;\n",
    "                      for(int i = 1; i <= params.VectorNum; i++) \n",
    "                      {\n",
    "                          float[] v = doc['vector'+i].vectorValue; \n",
    "                          float vm = doc['vector'+i].magnitude;  \n",
    "                          \n",
    "                          float dotProduct = 0;\n",
    "                          \n",
    "                          for(int j = 0; j < v.length; j++) \n",
    "                          {\n",
    "                              dotProduct += v[j] * params.queryVector[j];\n",
    "                          }\n",
    "                          \n",
    "                          float score = dotProduct / (vm * (float) params.queryVectorMag);\n",
    "                          \n",
    "                          if(score > max_score) \n",
    "                          {\n",
    "                              max_score = score;\n",
    "                          }\n",
    "                      }\n",
    "                      return max_score\n",
    "                    \"\"\",\n",
    "                \"params\": \n",
    "                {\n",
    "                  \"queryVector\": query_vector,\n",
    "                  \"queryVectorMag\": 0.25357,\n",
    "                  \"VectorNum\": 30\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\",\"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "\n",
    "        \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    texts = []\n",
    "    titles = [] \n",
    "    bi_scores = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        '''\n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[요약문]')\n",
    "        print(hit[\"_source\"][\"summarize\"]) \n",
    "        print()\n",
    "                \n",
    "        '''\n",
    "        #print(hit)\n",
    "        \n",
    "        # 리스트에 저장해둠\n",
    "        titles.append(hit[\"_source\"][\"rfile_name\"])\n",
    "        texts.append(hit[\"_source\"][\"rfile_text\"])\n",
    "        bi_scores.append(hit[\"_score\"])\n",
    "        \n",
    "     # 내림 차순으로 정렬\n",
    "    dec_bi_scores = reversed(np.argsort(bi_scores))\n",
    "    print(dec_bi_scores)\n",
    "    \n",
    "    # 내림차순으로 출력\n",
    "    for idx in dec_bi_scores:\n",
    "        print(\"{:.2f}\\t[제목]:{}\\n{}\\n\".format(float(bi_scores[idx]), titles[idx], texts[idx]))\n",
    "    \n",
    "    # 처리 시간들 출력\n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "     #========================================================================================================    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Elasticsearch.info of <Elasticsearch([{'host': '192.168.0.27', 'port': 9200}])>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  갤럭시 s6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_query_embed.shape:torch.Size([3, 768])\n",
      "query_vector.shape:(128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_13540\\1923550877.py:115: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7386 total hits.\n",
      "<reversed object at 0x000001D87EA4E4C0>\n",
      "2.77\t[제목]:lgu+, iot 기술 ddp에 총동원한다\n",
      "lg유플러스가 서울디자인재단과 손잡고 서울의 명소 동대문디자인플라자(ddp)에 '세계가 벤치마킹 할' iot(internet of thing, 사물인터넷) 플랫폼을 구현한다.. . lg유플러스는 iot 선도 기술력과 all-ip 네트워크를 기반으로 세계 최대 3차원 비정형 건물인 ddp에 iot 미래 기술을 6월부터 구축에 착수한다고 7일 밝혔다.. . lg유플러스는 연간 1000만명 국내·외 방문객을 눈앞에 두고 있는 ddp에 iot 솔루션을 도입, ▲비콘(beacon)기반의 고객 서비스 ▲무선 센서 기반의 관제 시스템 ▲비콘 및 결제 시스템을 이용한 지역 상권 확대에 나선다.. . ◇기다리지 않고 마음껏 즐겨라. . lg유플러스는 축구장 크기의 3배에 달하는 ddp에서 고객이 전시시설을 쉽게 찾을 수 있도록 비콘 기반의 위치정보 서비스를 제공할 계획이다. 비콘은 저전력 블루투스를 이용해 사람이나 사물의 위치를 파악하는 근거리 무선 통신 기술이다. nfc와 달리 근접하지 않아도 정보수신이 가능하며, gps로는 불가능했던 실내에서도 정확한 위치확인이 가능한 iot 기술이다.. . ddp 방문객은 비콘 기반의 '스마트 3d 도보 내비게이션' 앱으로 실내 사진을 이용해 전시 공간을 비롯한 ddp 내 목적지를 간편하게 찾을 수 있다. 주차장에서는 비콘으로 내 주차 위치를 스마트폰으로 확인하고 찾을 수 있는 '내차 위치 안내' 서비스, 전시관 바로 앞에서 (이름)용을 확인하고 바로 티켓을 구매할 수 있는 '모바일 발권 시스템', 본인의 스마트폰으로 작품 설명을 들을 수 있는 '스마트 비디오 도슨트', ddp 살림터에서 이벤트 정보를 확인하는 '모바일 카탈로그' 등 다양한 편의 서비스도 이용할 수 있다.. . ◇무선 센서(sensor)로 ddp 실시간 관제. . lg유플러스는 초대형 건축물인 ddp를 위한 무선 센서 기반의 관제 시스템을 전격 도입하기로 했다.. . ddp 곳곳에 작년 국내 상용화한 지웨이브(z-wave) 기반으로 '무선 센서 인프라'를 구축해 이 정보를 클라우드 정보로 변환해 ddp를 빈틈없이 능동적으로 관리할 수 있도록 할 예정이다. 무선 센서는 유선 센서와 달리, 설치비용은 적고 관리가 쉽다. 온도, 습도, 소음 등 기본 건물관리부터 화재, 재난, 미세먼지와 같은 특수 상황까지 세심한 측정과 관리를 할 수 있다.. . 이러한 센서 인프라를 활용해 화재, 지진 등 재난 상황 시, 고객과 가장 가까운 비상구를 실시간으로 지도에 알려주는 '생명을 구하는 재난 방지 시스템'을 운용하고, 온도와 습도, 소음센서를 활용한 '전시작품 모니터링 및 소음관리' 등 최적의 (이름)경을 구축할 계획이다.. . ◇주변 지역 경제 활성화까지!. . lg유플러스는 ddp의 전시관람 고객이 동대문의 다양한 문화 체험 및 쇼핑을 경험할 수 있도록 전시회 티켓의 'qr코드'를 통해 동대문 상권의 다양한 정보와 할인쿠폰 등을 제공할 계획이다. 또 'u+도보 내비'를 통해 매장을 편리하게 찾을 수 있도록 도움을 주기로 했다.. . 이를 통해 ddp의 방문객을 기반으로 세계 최고 수준의 iot 솔루션을 (이름)여 강북 상권 활성화의 중심이 될 것으로 기대된다.. . (이름) lg유플러스 sd(service development) 본부장은 '세계적인 건축물인 ddp에 lg유플러스의 우수한 iot 서비스를 도입해 국내·외 관광객이 직접 체험하고 벤치마킹하도록 이번 mou를 체결했다'며 '다양한 공공시설 및 산업군에서 활용될 수 있는 iot 기술 및 서비스를 확보해, 고객의 상상을 뛰어넘는 새로운 형태의 개인화(me-centric) 서비스를 전략적으로 넓혀나갈 계획'이라고 말했다.. . 이근 서울디자인재단 대표는 '이번 mou 체결로 혁신을 지향하는 ddp와 lg유플러스가 협력해 ddp 방문객들에게 새로운 가치를 창출하는 서비스를 제공하고자 한다'며 'iot 인프라 구축을 통해 ddp와 주변 지역상권이 더욱 활성화될 것으로 기대한다'고 전했다..\n",
      "\n",
      "2.73\t[제목]:kt, mwc서 '농업 iot 빅데이터' 표준 공개\n",
      "kt가 농업 사물인터넷(iot) 빅데이터를 표준화해 전세계에 공개한다. . kt는 mwc 2017 행사기간 중 ‘세계이동통신사업자연합회(gsma) ide(iot data ecosystem) 프로젝트’ 일환으로 iot 빅데이터 공유 플랫폼 ‘gs1 농식품 정보’를 선보인다고 밝혔다. . . kt가 2015년 제안해 시작된 ‘gsma ide 프로젝트’는 iot(사물인터넷) 데이터가 쉽고 빠르게 교환되는 플랫폼 구조를 만들고, iot 데이터가 각종 산업 영역에 바로 적용될 수 있도록 표준화하는 것을 목표로 운영되고 있다. kt를 포함해 오렌지, 텔레포니카, 차이나모바일 등 6개 글로벌 모바일 사업자가 참여 중이다. . . kt는 농업 분야를 맡아 ‘농업 iot 빅데이터’를 표준화하고 누구나 활용할 수 있도록 연결해 주는 ‘gs1 농식품 정보’ 플랫폼을 선보이게 된다. kt의 ‘gs1 농식품 정보’ 플랫폼은 재배 과정, 유통 현황, 농산물 출처 인증 정보 등 농식품을 생산하고 유통하는 과정에서 발생하는 iot 빅데이터를 gs1 코드(국제표준 식별코드)와 결합해 표준화된 정보로 만든다. 표준화된 정보는 포털()을 통해 공유돼 생산자, 소비자, 애플리케이션(앱) 개발자 모두 활용할 수 있게 되는 것. . . 아울러 kt는 mwc 2017서 ‘gs1 농식품 정보’ 플랫폼을 활용해 선택한 농식품의 안전 여부 판단 정보를 소비자에게 전달하는 ‘농산물 이력정보’ 솔루션도 함께 공개한다. ‘농산물 이력정보’ 솔루션은 kt가 개발한 키오스크 또는 스마트폰 앱으로 농식품에 부착된 바코드를 촬영하면 소비자가 선택한 농식품의 생산부터 유통까지 과정을 한 화면에서 연결된 정보로 보여준다. . . 홍경표 kt 융합기술원 컨버전스연구소장은 “iot 생태계 활성화를 위해선 데이터를 누구나 사용할 수 있도록 공개하는 것이 꼭 필요하다”며 “글로벌 통신사들 간의 협력으로 스마트시티, 농업, 커넥티드카 등 다양한 분야의 데이터를 표준화하고 공유할 수 있는 체계를 갖춰 iot 생태계가 더욱 활성화되도록 노력하겠다”고 말했다.\n",
      "\n",
      "2.66\t[제목]:skt, 여럿이 함께 쓰는 't포켓파이' 2종 출시\n",
      "sk텔레콤이 휴대형 라우터 't포켓파이' 2종을 출시한다. 포켓파이는 여러 명이 동시에 데이터를 이용할 수 있고, 스마트폰 등에 비해 상대적으로 비용도 저렴하다. . . 이번에 출시하는 't포켓파이a'에는 라우터 본체에 usb 동글을 내장했다. 동글은 본체에 장착해 그대로 사용해도 되고, 따로 떼어 노트북, 자동차 등에 꽂아 사용할 수 있다. 보조배터리 충전, 외장 sd저장 기능도 탑재했다. 출고가는 12만4300원이고, 색상은 화이트, 쿨블루 2가지다.. . 다른 신제품 't포켓파이s'는 얇은 두께(8.2mm)와 가벼운 무게(67g)가 특징이다. 절전모드 기능을 갖춰 네트워크 사용 환경에 따라 자동으로 모드를 바꿔 배터리 소모를 줄여준다. 고급 파우치와 5000mah 대용량 보조배터리도 제공한다. 출고가는 13만4200원. 색상은 화이트, 민트, 다크그레이 3종이다.. . 두 제품 모두 lte와 와이파이 네트워크를 지원한다. 지난 28일 먼저 출시된 a 모델은 sk텔레콤의 't포켓파이' 요금제에 가입하면 공시지원금 10만원을 지원받을 수 있다. 't포켓파이s'는 11월 3일 출시된다. 공시지원금도 출시일에 공개될 예정이다.. . 요금제는 't포켓파이20(월 2만4750원, 부가세포함)'과 't포켓파이10(월 1만6500원)' 등 2종이다. 20요금제는 월 데이터 20gb(기가바이트), 10요금제는 10gb를 제공한다. .\n",
      "\n",
      "2.62\t[제목]:skt, 영업익 1.7조…전년比 6.4% 감소(2보)\n",
      "sk텔레콤은 지난해 실적을 발표하고, 영업이익 1조7080억원을 기록했다고 밝혔다. 이는 전년 대비 6.4% 감소한 수치다. . . 순이익 역시 전년 대비 15.8% 감소한 1조5159억원을 기록했다..\n",
      "\n",
      "2.60\t[제목]:sk텔레콤, 울릉도·독도에 lte-a망 개통\n",
      "sk텔레콤이 울릉도 주요 관광지와 독도에 lte-a(롱텀에볼루션 어드밴스트)망을 개통했다고 13일 밝혔다. . . 이에따라 울릉도 지역 주민과 매년 울릉도와 독도를 찾는 관광객들도 최대 속도 150mbps의 lte-a 서비스를 이용할 수 있게 된다. 울릉도를 찾는 관광객은 2010년 23만명에서 지난해 40만명을 돌파했다. sk텔레콤은 lte-a 서비스를 위해 울릉도에 있는 800mhz 대역 기지국에 1.8ghz 장비를 추가했다. . . sk텔레콤은 또 광대역 lte-a망 확대를 통해 오는 7월에는 광대역 lte 전국 서비스를 제공할 예정이다. 지난 2월1.8ghz 기지국을 전국 군·읍·면 주요 지역으로 확대했으며 현재 전국 6개 광역시에 서비스를 제공 중이다. .\n",
      "\n",
      "2.58\t[제목]:금융·공공기관 주민번호 암호화 안하면 과태료(종합)\n",
      "금융기관·공공기관은 앞으로 이용자들의 주민등록번호를 보관할 때 반드시 암호화해야 한다. 이를 위반할 경우 3000만원 이하의 과태료가 부과된다. . . 국회는 28일 본회의를 열고 이같은 내용을 담은 개인정보 보호법 일부개정안을 가결했다. 신용카드 개인정보 유출사태에 따른 후속조치법의 하나다. 암호화 적용 대상이나 대상별 적용 시기는 대통령령으로 정하도록 했다.. . 일단 금융당국은 기간을 부여해 단계적으로 금융기관에 적용토록 관계부처와 협의해 나간다는 방침이다.. . 금융위원회 관계자는 '암호화를 해야 하는 대상, 시기 등은 시행령에 위임돼 있는 만큼 담당 부처인 안전행정부와 단계적으로 적용할 수 있도록 협의할 것'이라고 밝혔다.. . 금융사 비용 부담, 정보보안업체 규모 및 인력 등을 감안할 때 금융사 전체가 동시에 데이터 암호화에 나서는 것이 현실적으로 불가능하다는 판단에서다. 대형 시중은행 한 곳이 암호화를 위한 전산투자 비용만 1000억원에 달하는 것으로 알려졌다. . . 금융위 관계자는 '금융사들이 통상 5년 마다 차세대 전산시스템을 구축하는 만큼 이 때 암호화하는 방안을 검토하고 있다'고 말했다. . . 보안업계는 주민번호 암호화 의무화에 대해 '뒤늦은 감이 크지만 반드시 필요한 조치'라고 평가하면서 '다만 실제 암호화된 파일을 사용할 때 권한에 따라 접근하도록 지속적인 관리가 필요하다'고 강조했다.. . 한편 이번에 발의된 개정안 중 일관된 개인정보 보호체계(컨트롤타워)를 구축하는 등의 내용은 추가 심사키로 하면서 결국 국회에서 처리하지 못했다.. . 개인정보보호 정책은 정부 부처별로 쪼개져 있어 그동안 컨트롤타워 성격의 전담기구 설치 요구가 많았다. 개인정보보호법 주무 부처는 안전행정부지만 금융위원회 신용정보법, (이름)신위원회 정보통신망법, 금융감독원 전자금융거래법 등 소관부처별로 개인정보 보호 관련 법률안들을 두고 있다. . . 이 때문에 체계적인 개인정보보호 관리가 어렵고 대규모 개인정보유출 사태가 터질 때마다 혼선을 빚으면서 관리가 부실하다는 지적이 많았다. . . 하지만 개인정보보호 컨트롤타워 설치는 정부조직체계 변동이 필요한 부분이기 때문에 여러 상임위가 함께 다뤄야한다는 판단에 따라 향후 통합 논의키로 했다.. . 보안업계 관계자는 '쪼개져 있는 개인정보 조항을 일반화하는 것이 쉬운 작업은 아니지만 일관되고 체계적인 시스템이 마련돼야 한다'며 '대규모 정보유출 사태가 터질 때마가 컨트롤타워 구축 논의가 나왔지만 부처 (이름)의 속에 또 다시 물건너 가는 것 아닌지 우려된다'고 말했다..\n",
      "\n",
      "2.58\t[제목]:iptv도 애완견 전용 채널 도입…skb 'dogtv' 론칭\n",
      ". sk브로드밴드는 애완견이 시청하는 'dogtv'를 채널 30번의 실시간 유료방송으로 서비스한다고 28일 밝혔다. 애완견 시청 채널이 케이블tv에는 편성돼 있지만 iptv(인터넷tv)에 편성된 것은 이번이 처음이다. . . 월정액은 8000원이나 론칭기념으로 2개월간 6000원으로 낮춰주는 프로모션을 진행하고 있다. . . sk브로드밴드는 '국내 애완견 가구 1000만 시대를 맞아 미국에서 인기를 모으고 있는 dogtv에 대한 고객들의 프로그램 편성 요구가 잇따르고 있어 24시간 실시간 방송을 편성하게 됐다'고 설명했다. . . sk브로드밴드가 dogtv를 도입함에 따라 다른 iptv에서도 애완견 전용 채널이 도입될 전망이다. kt는 당초 사업성이 없다고 판단했으나 최근 들어 애완견 전용 채널 도입을 검토하고 있으며 lg유플러스도 마찬가지로 최근 들어 채널 도입을 긍정적으로 검토하기 시작했다.. . 한편 cj헬로비전, 티브로드, 현대hcn, 씨앤앰 등 케이블방송사들은 dogtv나 해피독tv 등 애완견 전용 채널을 서비스하고 있으며 많은 유료가입자를 확보한 것으로 알려졌다..\n",
      "\n",
      "2.54\t[제목]:23년 만에 수술대 오른 '통신요금 인가제'\n",
      "정부가 1991년 도입한 통신요금 인가제가 23년 만에 수술대에 올랐다.. . 미래창조과학부는 올해 6월까지 요금제와 가계통신비 부담 및 이용자보호 관계 등 제반사항을 종합적으로 고려해 통신요금 제도 개선 로드맵을 마련키로 했다고 밝혔다. 이와 관련, 미래부는 지난해부터 통신요금 제도개선 연구반을 가동 중이다.. . 현재 정부의 통신요금 제도는 통신 사업자 가운데 시장 과점 사업자의 경우, 새로운 요금상품이나 요금인상시 정부 허가(인가)를 받고, 나머지 사업자는 신고사항으로 규정한 '요금인가제'가 핵심이다. . . 특정 사업자가 통신 시장의 50% 이상을 과점한 상태에서 약탈적 요금인상을 막아 시장 왜곡과 이용자 후생침해를 막겠다는 취지였다. 실제 이같은 요금인가제는 정부의 통(이름) 정책의 실질적인 통제수단이자 통신시장의 유효경쟁 체제를 구축하는 기능으로 작용해왔다. . . 그러나 요금 인가제가 전세계 국가 중 유일하게 우리나라만 채택하고 있는 상황에서 통신업계의 자유로운 요금 경쟁을 가로막고 있다는 지적도 제기돼왔다. 실제 많게는 수개월 걸리는 정부의 요금 인가 특성상 시장에서 적기 대응을 할 수 없었던 경우도 적지 않았다.. . 시민단체들의 휴대전화 요금 원가 산정 자료 공개 압박 역시 통신요금 인가제 개선을 검토하게 된 주된 배경으로 꼽히고 있다. 전기통신사업법에 따라 정부는 그동안 통신사들이 원가 산정에 필요한 영업보고서 등 자료 일체를 매년 보고하도록 의무화해왔다. 새로운 요금상품의 적절성 등을 검토할 수 있는 기반 자료로 활용하기 위해서다.. . 그러나 2012년 참여연대 등이 '원가산정 관련 정부 보유자료를 모두 공개하라'며 제기한 소송에서 원고 일부 승소판결이 난데다, 지난 6일 항소심에서도 서울고등법원이 통신요금 인가 자료 일부를 공개하라고 판결하는 등 자료 공개 압박에 시달려왔다. 국정감사 때마다 단골 소재기도 하다. . . 이에 대해 통신사들은 통신요금 인가제로 통신사들이 정부에 제출한 자료에는 기업의 영업상 기밀에 해당하는 핵심 전략들이 모두 공개돼 있다는 (이름) 극렬히 반대해왔다. 만약, 항소심대로 자료 공개가 불가피해질 경우, 영업 기밀보호상 자료 제출을 거부할 수밖에 없다는 하소연이다.. . 결국 기존 통신 인가제에 대한 전면적인 재검토가 이뤄지지 않을 경우, 이같은 딜레마와 소비성 논란 역시 지속될 수밖에 없을 것이라는 지적이다.. . 이에 대해 미래부 관계자는 '시장 과점업체가 50% 이상을 점유하고 있는 국내 통신시장 환경에서 요금 인가제가 공정 경쟁 환경을 (이름)고 이용자를 보호하는 기능을 해왔던 측면이 강했다는 측면에서 이를 전면 폐지하기 위해서는 선행돼야 할 과제들이 적지 않다'며 '당분간 요금 인가제를 유지하면서도 바뀐 시장 경쟁 환경을 수용해 보완해나가는 방안 등 다양한 안들을 도출해 결정될 것'이라고 밝혔다.. .\n",
      "\n",
      "2.53\t[제목]:kt·국내 금융권 힘겨루기, 해외ib만 배불렸다\n",
      "kt와 국내 금융권의 힘겨루기로 해외 ib(투자은행)만 배불렸다. kt의 해외사채 발행은 금융기관이 아닌 일반기업으로는 사상 최대 규모인데 인수 수수료 등은 해외 ib들이 모두 챙긴 상태다. 그 규모도 총 20억원 (이름) 것으로 추정된다. 반면 국내 증권사들은 구조조정을 진행할 정도로 어려움에도 kt와의 자존심 싸움으로 15억원의 수익을 놓쳤다. . . 16일 kt 등에 따르면 kt 해외채권 발행에 참여한 메릴린치, 씨티그룹, 골드만삭스, hsbc, 도이치 등 5개 ib는 총 20억원 (이름) 인수 수수료 등을 챙긴 것으로 추정된다. . . kt는 전날 10억달러(약 1조435억원) 규모의 해외채권 발행을 확정했다. 이는 금융위기 이후 국내 민간기업 해외채권 발행 사상 최대 규모다. . . 이번 해외채권 발행 주관사는 해외 5개 ib다. 보통 해외채권을 발행할 때 주로 해외 ib를 주관사로 쓰지만 국내 금융기관 1곳을 넣기도 한다. 하지만 kt는 모든 주관사를 해외 ib로 채웠다.. . 시장에서는 kt가 6억달러 (이름) 해외사채 발행에 나설 것으로 예상했다. 6월에 만기가 도래하는 해외사채 6억달러가 있어서다. 하지만 kt는 해외사채 발행 규모를 10억달러로 늘리고, 국내 ib는 모두 제외했다. 실제로 이번에 kt가 추가 발행한 4억달러는 kt가 당초 발행하려고 했던 국내 사채 규모인 4000억원과 비슷하다. . . 업계에서는 kt가 국내 금융권과 관계가 소원해지면서 국내가 아닌 해외에서 대부분의 자금을 조달하기로 방향을 선회한 것으로 분석하고 있다. . . 지난달 kt는 성황리에 수요예측까지 마친 5000억원 규모의 사채발행을 돌연 취소했다. kt ens가 법정관리를 (이름)면서 피해가 불가피한 국내 금융권과 kt 사이가 소원해진 결과라는 분석이다. . . 즉, 국내에서 자금 조달이 어려워진 kt는 해외사채 발행 규모를 늘렸고, 결과적으로 국내 증권사들은 인수 수수료 등을 포함해 15억원 내외의 수익을 놓쳤다. 구조조정을 실시할 정도로 어려운 증권업계가 한 폰이 아까운 시기에 자존심 싸움으로 수십억원의 수익을 놓친 셈이다. . . kt 관계자는 '이번 해외채권 발행으로 해외에서 kt에 대한 신뢰가 여전히 두텁다는 사실을 확인했다'고 말했다.. . 보통 인수 수수료가 0.2% 내외임을 고려하면 해외 ib는 인수 수수료로만 20억원을 챙겼을 것으로 추정된다. 실제로 지난달 kt가 추진한 5000억원 규모의 사채의 인수 수수료는 0.2%였다.. . 인수 수수료 외 발행분담금과 신용평가수수료 등 기타 수수료를 포함하면 kt는 이번 해외채권 발행으로 30억원 (이름) 비용을 쓴 것으로 추정된다.. . 한편 이번에 발행한 사채가 외화사채이기 때문에 kt는 환율변동에 따른 위험을 회피하기 위해 파생상품도 추가로 설정했다. 통화스왑 거래는 이번 거래를 담당한 해외 ib가 담당했을 것으로 추정된다..\n",
      "\n",
      "2.52\t[제목]:lgu+ 27일부터 영업정지…기변·홈상품에 집중\n",
      "lg유플러스가 내일(27일)부터 영업정지에 돌입한다. 신규, 번호(이름) 전면 중단된다. 단, 기기 변경은 가능하다. lg유플러스는 기존 가입자에 대한 기기변경에 주력한다는 방침이다.. . (이름)신위원회는 지난 21일 sk텔레콤과 lg유플러스의 추가 영업정지 시기를 추석 연휴 전후로 결정했다. 영업정지 기간을 먼저 선택할 수 있는 lg유플러스는 추석 전 영업정지를 택했다. 8월까지는 기존 시장 분위기가 이어질 것이라고 예상해서다.. . 이에 따라 27일부터 9월2일까지 1주일간 신규, 번호이동 등 신규가입자 모집이 중단된다. 신규가입자 모집에는 유심(usim) 단독 개통도 포함된다. . . 다만 기존 가입자를 대상으로 하는 기기변경은 가능하다. 이는 3월부터 5월까지 지속된 영업정지와 다르다. 당시에는 신규, 번호이동 등 신규가입자 모집은 물론 기기변경도 불가능했다. 게다가 이번에는 1개 사업자만 영업이 중단돼 소비자 불편은 크지 않을 전망이다. . . 기기변경이 가능함에 따라 lg유플러스는 영업정지기간 기기변경 가입자를 중심으로 대응한다는 전략이다. lg유플러스 관계자는 '영업 현장에서 4월과 7월 출시한 요금형·보상형 대박기변 프로그램의 홍보를 강화해 기기변경 혜택을 적극 알릴 계획'이라고 말했다.. . 네트워크 강화에도 힘쓴다. lte 브랜드 강화를 통해 이탈자를 줄이고 영업 재개 후를 고려한 조치다. 특히 주요 (이름)이 영업 및 네트워크 현장을 방문해 임직원들간 소통과 조직 결속력도 강화한다. . . lg유플러스 관계자는 '네트워크 품질 등 본원적 경쟁력을 강화하는데 집중해 가입자 손실을 (이름)하고 향후 가입자 순증을 지속적으로 달성할 수 있는 원동력을 확보하겠다'고 밝혔다.. . 아울러 영업정지와 관련없는 인터넷, iptv(인터넷tv) 등 홈상품 판매에도 적극적으로 나선다는 계획이다. lg유플러스 관계자는 '홈보이, u+tv g 등 홈상품 경쟁력도 제고하는 계기로 삼을 예정'이라고 말했다..\n",
      "\n",
      "embedding time: 31.79 ms\n",
      "search time: 416.91 ms\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  sk텔레콤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_query_embed.shape:torch.Size([4, 768])\n",
      "query_vector.shape:(128,)\n",
      "7386 total hits.\n",
      "<reversed object at 0x000001D87EA4E550>\n",
      "3.22\t[제목]:탄핵 소통에 모바일 데이터 2배 '폭증'\n",
      "(이름) 전 대통령에 대한 헌법재판소의 탄핵 인용이 결정된 10일, 헌재의 선고를 전후해 모바일 데이터 사용량이 두 배 이상 증가한 것으로 확인됐다. 이동통신사들은 탄핵 관련 집회 대비 차원에서 (이름)지국을 최대 5대까지 배치하는 등 준비에 들어갔다. . . 10일 sk텔레콤에 따르면 헌재의 탄핵 선고가 진행됐던 이날 오전 11시 전후 모바일 데이터 사용량이 평소의 2배가량 증가한 것으로 확인됐다.. . 이용자들이 모바일 기기를 이용해 생방송으로 진행된 탄핵 심판 선고 뉴스를 지켜 봤고, 이것이 데이터 사용량을 늘린 가장 큰 (이름) 것으로 추정된다. 아울러 모바일 메신저로 탄핵 관련 이야기를 주고받은 이용자들의 패턴 증가도 모바일 데이터 사용량에 영향을 미친 것으로 보인다. . . 이와 함께 kt와 lg유플러스의 이용자들도 정확한 수치가 공개되진 않았지만 평소보다 많은 데이터 사용량을 보인 것으로 알려졌다. . . 이동통신 3사는 이 날 오후 대통령 탄핵과 관련해 대규모 집회가 예상되는 서울 광화문과 종로, 시청 주변에 (이름)지국을 배치하는 등의 준비로 이용객들의 불편을 (이름)한다는 계산이다. . . 이용자가 가장 많은 sk텔레콤은 5대의 (이름)지국을 배치할 예정이다. 집회 예상지역 트래픽 수용량을 평소보다 4.5배 가량 늘렸고 lte(롱텀에볼루션)와 3g 기지국 400개도 추가 설치했다. . . kt도 3대의 (이름)지국을 운영한다. 기존 대비 운영시설을 2.6배 증설하고 긴급복구용 발전기 6대를 추가로 구성했다. 비상근무 인원 147명이 현재 대기 중인 상황이다. . . lg유플러스는 5대의 (이름)지국을 배치했다. 집회가 진행될 것으로 보이는 지역에 기지국 60대, 와이파이 중계기 13대를 설치했다. 평소 3배 (이름) 트래픽 수용이 가능하도록 하는 대비 태세를 갖췄다.\n",
      "\n",
      "3.15\t[제목]:skt, 아이리버 인수 마무리…앱세서리 등 新사업 탄력\n",
      "sk텔레콤의 아이리버 인수절차가 마무리됐다. sk텔레콤은 이를 계기로 앱세서리, 음원사업 등을 성장동력 사업으로 집중 육성한다는 전략이다.. . 13일 아이리버는 sk텔레콤이 인수 잔금을 납입해 최대 주주(39.27%)가 됐다고 공시했다. sk텔레콤은 지난 6월 아이리버 최대주주였던 보고펀드측과 지분 인수계약을 체결한 바 있다. . . 이 날 아이리버는 임시주주총회를 통해 sk텔레콤 (이름) 경영전략실장과 (이름) 컨버전스본부장,을 사내이사로, (이름) 자금팀장을 감사로 각각 선임했다. 최대 주주가 sk텔레콤으로 변경됐지만, 아이비러의 사명은 그대로 유지될 전망이다. . . sk텔레콤은 아이리버 인수를 마무리됨에 따라 아이리버의 제품과 기술력을 결합해 앱세서리 등 차기 성장동력 사업에서 본격적인 시너지를 내겠다는 기대다..\n",
      "\n",
      "3.15\t[제목]:韓·中·日 20~40대 '사랑의 고수' 심리 들여다보니\n",
      "'애인 위해 지갑 잘 열고, 애인에게 딴 여자 생기면 바로 정리' (서울 女). '배우자 선택할 때 가장 중요한 한 건 '감성코드'' (도쿄 女). '첫눈에 반해도 잠자리는 no' (상하이 女). . 크리스마스를 앞두고 '사랑'을 주제로 서울, 상하이, 도쿄의 20~40대 여성의 심리를 조사한 결과가 나와 눈길을 끈다.. . ◇가장 가꾸고 싶은 곳 '눈(目)', 2위가 '머릿결'. . 제일기획이 23일 공개한 '마이너리티 리포트'에 따르면 서울, 상하이, 도쿄의 20대~40대 여성 9169명을 조사한 결과 81%가 '나는 늘 사랑하며 살기를 갈망한다'고 답해 3개국 여성 모두 사랑에 대한 니즈가 높은 것으로 나타났다. 하지만 ‘사랑은 참 어렵다’, ‘사랑 잘 하는 여자가 제일 부럽다’라는 문항에도 각각 82%, 72.9%의 여성이 답해 마음처럼 쉽게 되지 않는 것도 사랑인 것을 보여줬다. . . 또 '나는 늘 누군가와 사랑하며 살고 싶고, 마음에 드는 남자가 나타나면 먼저 다가가고, 지금까지 사귄 모든 남자들을 진심으로 사랑했다’고 응답한 300명(국가별 100명)을 선발해 세부적으로 조사했다. 이들은 연애 경험 횟수가 평균 14회며 '자신이 원하는 남자라면 누구라도 내 것으로 만들 수 있다'고 100% 확신하는 소위 말하는 '사랑의 고수(高手)'라는 게 제일기획의 설명이다. . . 사랑관(觀)을 조사한 결과 '일 보다 사랑이 훨씬 중요하다'는 질문에 도쿄 여성 70%가 선택했다. 반면 한국은 절반 수준인 52%만 사랑이 훨씬 중요하다고 답해 비교적 일과 사랑의 균형을 추구하는 것으로 나타났다.. . 배우자를 선택하는 기준을 묻는 질문에는 3개국 여성 모두 ‘성격’을 가장 많이 선택했다. 세부적으로는 서울 여성은 배우자 선택시 ‘직업이 중요하다’고 답한 비율이 26%로 상하이(14%), 도쿄(9%) 보다 상대적으로 높게 나타났다. 상하이 여성들은 건강(27%)을, 도쿄 여성들은 서로 교감할 수 있는 감성코드(41%)를 많이 선택했다.. . 아울러 서울 사랑녀의 63%는 애인(남편)에게 새로운 여자가 생기면 당장 관계를 (이름)다고 답했다. 또 애인을 위해 지갑을 여는 성향은 서울 ‘사랑녀’들이 가장 뚜렷했다. ‘돈을 아끼지 않고 쇼핑하는 품목’을 고르는 질문에 ‘애인에게 주는 선물’에 응답한 비율은 서울이 20%로 상하이(8%)와 도쿄(8%)에 비해 가장 높았다.. . 이 밖에도 도쿄 ‘사랑녀’ 86%가 ‘약속에 늦는 남자 친구를 10분 이상 기다린다’고 답했으며 상하이 ‘사랑녀’ 중 단 1%만 첫 눈에 반한 남자와 성관계를 가질 수 있다고 답하는 등 3개국 여성들의 각기 다른 사랑관을 나타냈다.. . '가장 아름답기를 꿈꾸는 신체 부위'를 묻는 질문에 3개국 모두 ‘눈’을 꼽았다. 이어 '머릿결'이 2위를 차지해 주목됐다. 세부적으로는 서울 ‘사랑녀’들은 눈, 머릿결, 가슴, 코, 손 등 신체 전 부위를 고르게 지지한 데 비해, 상하이와 도쿄는 눈과 머릿결에 집중했다.. . ◇외형적 아름다움 넘어 라이프 스타일 꾸미는 데 관심. . 서울, 상하이, 도쿄 일반 여성 20명을 대상으로 심층 인터뷰를 한 결과 요즘 여성들이 꾸미고 싶은 미(美)의 대상이 얼굴, 몸 등 신체를 넘어 라이프 스타일 전체로 확산되는 것으로 나타났다. . . 심층 인터뷰에 참가한 일본인 후지타와(25세)씨는 '이제는 외형적 아름다움 보다는 내가 얼마나 행복하고 만족하는 삶을 사느냐, 그리고 스스로가 정신적으로 얼마나 여유 있는 삶을 즐기느냐가 아름다움에 있어 가장 중요한 기준이라고 생각한다'고 말했다. . . 아오이(32세)씨도 '젊은 여성들이 즐겨 보는 패션 잡지들을 보면, 최근에는 어떻게 예뻐질 수 있느냐는 내용보다는 얼마나 자기 시간을 충실히 보낼 수 있는지 등에 관한 라이프 스타일 이야기가 점점 늘어나고 있다'고 제일기획은 전했다. . . 이에 제일기획측은 '아시아 여성들의 라이프 스타일을 변화시키는 '취향'에 주목하고, 그녀들의 새로운 취향을 적극적으로 이끌어 갈 수 있는 플랫폼 개발이 필요하다'고 제안했다. . . 한편 제일기획은 2011년부터 매년 '마이너리티 리포트'를 통해 특징 있는 소수 집단 분석해 트렌드를 예측해오고 있다.\n",
      "\n",
      "3.10\t[제목]:'응답'pd, 청춘에게 '꿈 꾸되, 실패에 좌절하지 마라'\n",
      "'저는 끝까지 열심히 하면 꿈을 이룬다는 말을 싫어해요. 현실은 그렇지 않아요. 우리나라는 꿈(을 이뤄야 한다는데) 대한 압박감이 강한데, 열심히 해 보고 (현실과) 부딪혀 이루지 못해도 괜찮아요. 그걸로 (자신을) 평가절하하거나 좌절하지 말았으면 좋겠어요.'. . 인기 드라마 '응답하라' 시리즈로 유명한 (이름) pd(cj e&m)가 방송 분야 취업을 꿈꾸며 모인 180여명의 취업준비생들에게 한 말이다. '열심히 하면 된다'는 식의 희망적인 조언이 아니지만, 그 솔직함에 청중들의 얼굴에는 웃음이 번졌다. . . 한국케이블tv방송협회(kcta)가 25일 오전 서울 삼성동 코엑스 그랜드볼룸에서 개최한 취업토크콘서트 '도전하는 청춘! 케이블을 job(잡)자!'에서 특별강연을 맡은 (이름) pd는 이날 취업준비생들의 질문에 직접 '응답'했다. 그는 '영화'만 보고 괴로워하던 자신의 20대를 돌아보며 '꿈을 이루겠다고 자신을 괴롭히며 지내지 말라'고 강조했다.. . 신 pd는 '물론 가만히 앉아서 힘들어서는 방법이 없다'면서 다양한 방법을 뚫어 보는 것도 좋다고 추천했다. 자신이 방송사 입사를 준비하던 2000년과는 취업 상황이 다르다는 점을 설명했다. 그는 '방송사도 많아졌고, 외부업체도 다양해져서 방법은 많다'고 전했다. 다양한 방법을 두드리다보면 또 다른 길이 있을지 모른다는 의미다.. . pd가 되기 위한 자질을 묻는 질문에는 창의성을 꼽았다. pd 일을 시작하면서 습득하게 되는 정형화된 업무 틀을 일종에 업무용 '회로'가 머릿속에 장착된다고 그는 비유했다. . . 신pd는 '연차가 어릴 때는 이 회로를 빨리 습득하는 친구들에게 일을 잘한다고 하지만 6~7년 넘게 일하면 결국 모든 사람이 익힐 수 있다'면서 '나의 경험을 바탕으로 결과가 빠르게 나오는 것을 보고 기존 회로를 의심하지 않고 계속 쓰면 소위 말하는 '꼰대'가 되는 길이라고 생각한다'고 설명했다. 익숙해진 후에는 오히려 그 '회로'를 어느 정도 깨야 색다른 프로그램을 만들 수 있다는 것. . 그는 '저도 굉장히 혁신적인 사람은 못 되지만 그 기본 회로가 지금은 틀린 것은 아닐지 의심하고 10~20% 유연성을 발휘해서 지금까지 나쁘지 않게 해온 것이 아닐까 생각한다'고 말했다.. . 응답하라 시리즈와 관련한 에피소드도 공유했다. 그는 '응답하라 1988을 제작하는 내내 sns(소셜네트워크서비스) 자신소개 글이 '지옥'이었다'면서 정신적, 또 체력적으로 고된 작업이었다고 털어놓았다. 이어 '갈수록 따뜻한 이야기가 하고 싶어져서, 전편보다 멜로 비중을 줄이게 됐다'고 설명했다.. . 예능에서 드라마를 걸쳐, 20대 당시 꿈이었던 영화도 언젠가 도전하게 될 것이라는 포부도 밝혔다. 여러 문을 두드리면서 열심히 달려온 결과 꿈을 다시 만나게 된 셈이다. 신 pd는 '감사하게도 응답시리즈가 끝나고 영화 관계자들의 연락을 받았다'고 밝혔다. 다만 그 세월 동안 영화가 삶의 1순위이던 청년은 딸과 보내는 시간이 0순위인 가장으로 변했다. 그는 '(최근 받은 제안은) 고사했다'면서 '두 딸이 더 크고 나면 그때 하려고 한다'고 덧붙였다..\n",
      "\n",
      "3.00\t[제목]:통신3社 연말 인사·조직개편 키워드…'외풍 속 신사업 강화'\n",
      ". ‘외풍 속 신사업 강화’. sk텔레콤과 kt, lg유플러스 등 통신3사의 연말 정기인사와 조직개편 키워드다. lg유플러스가 지난 1일 임원인사를 단행한데 이어 kt와 sk텔레콤이 이르면 다음주 임원인사 및 조직개편을 단행할 예정이다.. . sk텔레콤의 cj헬로비전 m&a(인수합병) 무산(sk텔레콤)과 ‘최순실 게이트’ 연루 의혹(kt) 등 굵직한 대외변수 등장으로 인해 그 어느 때보다 인사방향을 섣불리 예단하기 어렵다는 관측이 나온다. 탄핵정국으로 내년 더욱 불투명한 상황을 감안할 경우, 인사 폭은 크지 않을 것이란 전망이 지배적이다. 조직 진용 역시 5g(5세대)와 ai(인공지능), 플랫폼 사업 등 미래 신성장 사업 가속화에 무게를 둘 것으로 보인다.. . kt는 지난해 12월 대대적인 인사와 조직 개편을 단행한 만큼 올해는 큰 변화가 없을 것으로 예상된다. kt는 기존 ‘미래융합사업추진실’과 ‘플랫폼사업기획실’을 중심으로 △iot(사물인터넷) △빅데이터 사업화 △헬스케어와 같은 신사업 성과를 강화하고, 2018년 평창 동계올림픽에 대비해 5g 준비 조직을 승격 시키는 선(tf→팀)에서 마무리될 예정이다.. . 무엇보다 통신업계 관심은 (이름) 회장의 연임 여부에 쏠려있다. 황 회장은 내년 3월 임기만료를 앞두고 있다. kt는 늦어도 내년 1월말까지 ceo추천위원회를 열어 연임 여부를 결정해야 한다. 황 회장은 재임 기간 중 실적 개선과 ‘기가토피아’를 위시한 미래 성장전략 등 경영 측면에서 적잖은 성과를 거뒀다. 다만, 최근 최순실 국정농단 사태의 불똥이 kt로 옮겨붙은 데다 내년 조기 대선 가능성 등이 황 회장 연임여부를 결정하는 변수로 대두되고 있다. 황 회장은 아직까지 측근들에게 연임여부에 대해 이렇다 할 입장을 내비치지 않고 있는 것으로 전해졌다.. . sk텔레콤의 조직 개편과 인사 폭 역시 예년보다 크진 않을 것이란 관측이 우세하다. 지난 7월 cj헬로비전과의 인수합병(m&a)이 무산되면서 한때 대대적인 조직 정비가 이뤄질 것이란 예상이 나돌기는 했지만, 정부 심사과정에서 불가항력적인 측면이 컸던 데다 현재 추진되고 있는 각종 신사업들이 탄력을 받고 있다는 이유에서다. sk텔레콤은 (이름) 사장 취임 이후 ‘생활가치’과 ‘iot’, ‘미디어’ 등 3대 플랫폼 사업을 중심으로 조직 재편을 지속적으로 추진해왔다. 다만, sk브로드밴드에 이어 최근 sk커뮤니케이션즈까지 완전 자회사로 편입되면서 자회사들과의 시너지와 연계를 위한 후속 조직 개편과 인사가 단행될 가능성이 나오고 있다.. . 한편, lg유플러스는 지난 1일 (이름) ps본부장(전무)을 부사장으로 승진시키는 등 총 10명의 임원에 대한 승진인사를 단행한 바 있다. 업계의 한 관계자는 “인사는 정말 뚜껑을 열어봐야 알겠지만, 극도의 정국 혼란 상황에서 큰 변화를 주기는 어려울 것”이라며 “다만 통신 사업보다는 신사업을 강화하는 형태로 조직이 재정비되지 않겠느냐”고 말했다.\n",
      "\n",
      "2.91\t[제목]:skt, 클라우드 서비스에 오픈스택 도입\n",
      "sk텔레콤은 글로벌 클라우드 오픈소스 프로젝트 '오픈스택(openstack)'을 적극 지원하고 클라우드 서비스에도 오픈스택을 도입하겠다고 5일 밝혔다. . . 오픈스택은 서버, 스토리지, 네트워크 등 하드웨어를 가상화해 필요에 따라 손쉽게 활용할 수 있게 만드는 오픈소스 플랫폼으로 현재 140여개국 1만7000여명의 개발자들과 엔지니어들이 참여하고 있는 세계 최대 규모의 개방형 프로젝트다. . . 오픈스택은 서비스 차별화를 꾀할 공간이 상용 플랫폼보다 상대적으로 넓고 운영 비용도 절감할 수 있어 클라우드 컴퓨팅 업계의 핫이슈로 부상하고 있다.. . sk텔레콤은 이날 잠실 롯데호텔에서 열린 국내 최대 오픈스택 관련 행사인 'openstack day in korea 2015' 컨퍼런스를 후원한다. . . (이름) sk텔레콤 종합기술원장 겸 cto(최고기술경영자)는 기조 연설을 통해 소프트웨어 중심 경제 시대를 맞아 '오픈스택'의 중요성을 강조하며 국내 활성화에 적극 기여하겠다는 의지를 내비쳤다. . . sk텔레콤은 클라우드 서비스인 't클라우드비즈'의 품질 제고와 가격 경쟁력 강화를 위해 오픈스택을 선도적으로 적용할 예정이다.. . 최 원장은 '클라우드 서비스의 품질과 비용 경쟁력을 제고하기 위해 개방형 기술인 오픈스택을 적극 도입할 것'이라며 '오픈스택 커뮤니티와 지속적 협업을 통해 iot(사물인터넷), 클라우드 등 차세대 기술 개발 분야에서 혁신적인 성과를 만들어 나갈 것'이라고 밝혔다. .\n",
      "\n",
      "2.90\t[제목]:skt, 인도네시아 '텔콤'과 iot사업 협력키로\n",
      "sk텔레콤이 12일 인도네시아 국영통신사 텔콤과 iot(사물인터넷) 등 신사업 부문에서 협력하는 mou(양해각서)를 체결했다고 밝혔다. 텔콤은 지난해 매출 7조4000억원을 기록한 인도네이사 유무선통신 부문 1위 사업자다.. . 이번 협약으로 양사는 2년 안에 iot 사업을 위한 합작회사를 인도네시아 자카르타에 설립하는 방안을 논의한다. sk텔레콤의 iot플랫폼 '씽플러그'와 lpwa 기반의 iot 네트워크를 기반으로 인도네시아에서 스마트시티, iot 융합서비스 등 관련 사업 기회를 양사가 만들어나갈 계획이다. 씽플러그는 최근 국제표준 인증을 획득했다.. . sk텔레콤의 자회사인 엔트릭스의 미디어 솔루션 '클라우드스트리밍'으로 텔콤 가입자 대상 클라우드 tv 서비스도 개발한다. 클라우드스트리밍은 미디어 사업자가 셋톱박스 성능에 관계없이 높은 품질의 서비스를 구현하도록 지원한다.. . 양사는 sk텔레콤의 라이프웨어 기기 'uo브랜드'의 인도네시아 진출에도 협력한다. 텔콤의 유통자회사인 pins(핀스)가 uo스마트빔 레이저, uo스마트빔2, uo링키지 등 기기의 현지 판매를 맡는다.. . 양사는 인도네시아 정부가 2018년 아시안게임 개최를 앞두고 스마트시티 구축을 지원하고 있어 이번 협약을 계기로 신사업 분야에서 성장할 것으로 기대했다. 이응상 sk텔레콤 글로벌사업부문장은 '지속적인 협력으로 인도네시아를 비롯해 동남아시아 전역에서 함께 도약할 수 있도록 노력할 것'이라고 밝혔다..\n",
      "\n",
      "2.85\t[제목]:서강대·세종대, skt 'iot 플랫폼' 정규교육 과정에 도입\n",
      "sk텔레콤이 자사 사물인터넷(iot) 플랫폼 '씽플러그(thingplug)'를 대학교 정규 교육과정에 도입했다고 18일 밝혔다.. . sk텔레콤과 서강대, 세종대는 이날 서울 을지로 sk텔레콤 본사에서 '씽플러그 기반 iot 서비스 개발 커리큘럼 공동 운영' 협약을 체결했다.. . 1학기부터 소프트웨어(sw)중심대학인 서강대와 세종대 컴퓨터공학 전공 학부생들을 대상으로 씽플러그를 교육하고, iot 플랫폼 전반에 대한 이해와 실습 기회를 제공할 계획이다.. . 국제표준 'onem2m' 기반인 씽플러그는 표준을 준수하는 기기·애플리케이션과 연동되는 개방형 플랫폼이다. 개발자가 iot 서비스를 쉽게 이용할 수 있는 '서비스 플랫폼' 기능과 원하는 iot 서비스를 직접 만들 수 있는 'diy 개발 환경'을 제공한다.. . 학생들은 sw 개발 꾸러미인 sdk 기반으로 마더보드, 센서 등을 조합해 자신만의 iot 기기, 서비스를 직접 개발할 수 있다. 개발된 기기와 서비스는 웹 포털에 등록 후 즉시 이용할 수 있다.. . sk텔레콤은 sw 개발 현장에 바로 투입될 수 있는 전문인력을 양성할 계획이다. 과정 종료 시점에 창의적인 아이디어 구현에 대해 시상도 하고, iot 경진대회 '해카톤'에 참가한 경우 가산점도 부여할 예정이다.. . 차인혁 sk텔레콤 플랫폼기술원장은 '이번에 신설된 과정으로 학생들이 경험을 통해 iot 개발에 대한 (이름)을 높일 수 있을 것'이라며 '씽플러그 플랫폼을 기반으로 iot 시장 창출과 생태계 발전을 이끌어가겠다'고 밝혔다.. . 한편 한국사물인터넷협회의 협회장사를 맡고 있는 sk텔레콤은 지난 16일 iot 전국망 구축, iot 통합 관제센터 구축, iot 전용 모듈 개발 등을 포함한 'iot 토탈케어' 프로그램을 발표했다..\n",
      "\n",
      "2.85\t[제목]:통신3사 ceo '탈(脫)통신…새판 짠다'\n",
      ". “플랫폼 사업으로 통신시장 새판 짜겠다”, “사업 혁신으로 1등 신화 만들겠다”. . 정유년 새해를 맞아 sk텔레콤, kt, lg유플러스 등 통신3사 최고경영자(ceo)들이 밝힌 신년 각오다. 공통점은 ‘탈(脫) 통신’. 올해 미디어 등 플랫폼 사업과 iot(사물인터넷), ai(인공지능), 빅데이터 등 미래먹거리 분야를 집중 공략해 새로운 성장동력을 확보하겠다는 구상이다. 이동통신 가입자가 포화된 상태에서 가입자 빼앗기 경쟁으로는 더 이상 성장을 담보할 수 없다는 위기감이 반영된 것으로 풀이된다.. . ◇skt 개방형 플랫폼 내세워 글로벌 정조준=(이름) sk텔레콤 신임 사장은 2일 시무식에 참석한 임직원에게 “4차 산업혁명 시대에 맞게 새 사업 모델을 만들고 글로벌 성장을 이루도록 새로운 ‘판’을 만들자”고 말했다. 인공지능(ai), 자율주행, 로보틱스, 5g 기술 등 전 ict 영역에서 국내 벤처·스타트업, 글로벌 대형 기업 등 다양한 기업과 협업 생태계를 구축, 글로벌 ict 새판짜기를 주도하겠다는 목표다. sk텔레콤의 새로운 사령탑으로 (이름) 사장이 가장 강조하는 사업은 iot(사물인터넷)이다. sk c&c, sk하이닉스 등 그룹 내 모든 ict 역량을 총결집, 커넥티트카, 에너지 관리 솔루션, 스마트홈 등에서 혁신적인 서비스·상품을 발굴하겠다는 것. 가정용 iot 솔루션(홈) 부분에도 과감한 투자와 다양한 사업자들과의 협력으로 ‘종합 홈 솔루션’ 등 신규 사업 모델을 내놓겠다고도 했다. 미디어·홈 부문에서는 과감한 투자로 세계 시장에서 통용되는 콘텐츠 생산자로서의 역할이 필요하다고 거듭 강조했다.. . 박 사장은 “글로벌 성장을 이루기 위해서는 아예 다른 ‘판’을 만들어야 한다”며 “플랫폼 사업 등을 내세워 글로벌 ict 새판 짜기를 주도해야 한다”고 밝혔다. 박 사장은 미국 라스베이거스에서 오는 5일(현지시간) 열리는 세계 최대 가전전시회 ‘ces 2017’에도 참석한다. 다양한 영역에서 새로운 ict 융합을 선도하는 sk텔레콤의 성장 방안을 모색할 예정이다.. . ◇kt, 지능형 네트워크·미디어 플랫폼 강조=(이름) kt 회장 역시 신년 결의식에서 “고정관념의 틀에서 벗어나 새로운 시각으로 차원이 다른 목표인 ‘혁신기술 1등 기업’에 도전하자”고 강조했다. 그는 특히 “kt는 통신시장, iptv 1위 기업이라는 지엽적인 목표가 아닌 지능형 네트워크 기반의 플랫폼 회사, 미디어 시장에서 새로운 트렌드를 만드는 미디어 플랫폼 기업으로 발돋움해야 한다”고 강조했다. 에너지·보안 등 각종 신사업에 대한 철저한 대비도 요구했다. 황 회장은 “에너지나 보안사업은 다양한 고객의 요구에 맞춘 서비스로 질적인 발전이 필요하다”며 “인증·결제사업도 인증방식의 다양화, 비대면 거래 증가 추세에 맞춰 변화와 성장을 이뤄야 한다”고 새로운 자세를 요구했다.. . 일각에서는 황 회장의 이번 신년사에 담긴 내용과 무게를 놓고 볼 때 연임 가능성이 높은 것 아니냐는 해석을 내놓기도 한다. 황 회장의 임기는 오는 3월까지다. 이달 ceo추천위원회를 열고 황 회장을 연임하거나 새로운 ceo를 뽑아야 한다.. . ◇lgu+, iot·ai(인공지능)·빅데이터 등 신사업 ‘1위’ 성장 의지=(이름) lg유플러스 부회장도 이날 시무식에서 iot와 인공지능(ai), 빅데이터 등 신사업을 최우선 과제로 제시했다. 권 부회장은 ‘자승자강’(自勝者强; 자신을 이기는 사람이 강한 사람)의 정신으로 일등이라는 꿈을 이뤄내자“며 “우리 스스로의 한계를 뛰어넘어 그 누구보다 (이름)다면 경쟁사는 따라오지 못할 것”이라고 강조했다. 그는 “통신시장은 우리가 판을 뒤집을 수 있는 신규 사업의 기회가 분명히 있다”며 무엇보다 미래 먹거리로 떠오른 iot과 ai, 빅데이터, iptv 등의 분야에서 1등 사업자가 되자고 주문했다.\n",
      "\n",
      "2.82\t[제목]:(이름) kisdi 원장 취임…'창(이름) 정책지원 강화'\n",
      "(이름) 제11대 정보통신정책연구원(kisdi) 원장이 31일 취임했다. . . 김 신임원장은 이 날 오전 11시 kisdi 대회의실에서 취임식을 갖고 정식 업무에 들어갔다. 김 원장의 임기는 3년이다. . . 김 원장은 취임식에서 '현재 우리나라 국정과제 로드맵의 1순위는 ‘창(이름)’이며 디지털 경제의 새로운 성장 동력으로써 ict(정보통신기술)는 창(이름) 실현의 핵심 수단”이라며 “국가 비전과 국정철학을 공유하고, 정부의 국정과제 수행에 기여하기 위해 kisdi의 정책연구 범위를 확대하고, 정책지원 기능을 강화해 나가겠다”고 밝혔다. . . 김 원장은 이를 위해 ▲창(이름) 실현을 위한 전략연구 및 정책지원 기능 강화 ▲국가(이름) 전략연구 및 정책지원 기능 재개 ▲통신서비스 시장과 방송·미디어 시장의 발전전략 연구 및 정책지원 기능의 강화가 필요하다고 역설했다. . . kisdi는 미래창조과학부가 창(이름)의 컨트롤 타워로서 종합계획을 수립하고, 부처 간 협업을 리드하며, 집단지성의 새로운 가치를 창출할 수 있도록 kisdi가 심도 깊은 전략연구를 수행, 분야별 창(이름)의 청사진을 제시하고, 선도적으로 정책과제를 도출하는 등 정책지원 기능을 강화할 계획이다.. . 또 범정부 차원으로 추진되는 국가(이름) 사업이 ‘정부3.0’의 다양한 과제, ‘비타민 프로젝트’ 등과 연계돼 창(이름)를 견인할 수 있도록 미래창조과학부의 국가(이름) 컨트롤 타워 기능을 적극 지원키로 했다.. . 김 원장은 '통신시장 발전 및 공정경쟁 기반 정착을 위한 규제체계 개선, 미디어시장의 발전전략, 방송 분야의 중장기 비전 및 정책은 전통적으로 kisdi가 선도하는 정책연구 분야'라고 (이름)고, '공정경쟁 환경 속에서 ict 생태계가 발전하고 소비자의 후생이 증진될 수 있도록 통신과 방송·미디어 시장의 전략연구 및 정책지원을 선제적으로 수행하겠다'고 강조했다.. . 김 신임원장은 1993년 kisdi 연구위원으로 8년간 재직했으며, 이후 세종대 경영학과 교수로 옮긴 뒤 정보통신부 imt-2000 사업자 심사평가단 심사위원과 정보통신부 재정사업평가위원장, 기획예산처 기금운용평가위원, 국무총리 소속 공공데이터 전략위원회 민간위원 등을 역임했다. (이름) 정부와는 2012년 대선캠프 국민행복추진위원회 (이름)신추진단 위원으로 활동하며 연을 맺었다.\n",
      "\n",
      "embedding time: 31.04 ms\n",
      "search time: 411.16 ms\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  kisdi 원장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_query_embed.shape:torch.Size([5, 768])\n",
      "query_vector.shape:(128,)\n",
      "7386 total hits.\n",
      "<reversed object at 0x000001D87EA4E4F0>\n",
      "3.12\t[제목]:청학동 어린 선비들, 태블릿 들고 과거시험을…\n",
      "kt가 24일 경남 하동군 청암면 묵계리 청학동 기가창조마을 도서관 앞 마당에서 청학동 거주 어린이들을 대상으로 '청학동 it 과거시험'을 치렀다.. . 이날 행사는 지난 7월 구축한 '청학동 기가 창조마을' 개소 100일을 맞아 마련됐다. 청학동 거주 어린이 총 60여명이 '지필묵' 대신 태블릿 pc와 시험 전용 앱(애플리케이션)을 활용해 과거시험에 참여했다. . . 옛 선비들이 입었던 도포와 유건을 착용하고 참가 어린이들은 앱으로 풀어보는 '한자시험'과 사자소학을 암송하는 '강경대회', 상식을 겨루는 '도전! 청학 골든벨'등의 다양한 시험을 봤다. . . 이날 시험에서는 청학동 서당의 훈장과 kt와 지자체 관계자의 공정한 심사를 거쳐 장원, 방안, 탐화 등 총 10명의 어린이가 수상의 영광을 안았다. .\n",
      "\n",
      "2.88\t[제목]:'리베이트 과다 지급' skt, 1주간 신규모집금지\n",
      "sk텔레콤이 대리점과 판매점이 주는 판매장려금(리베이트)를 과도하게 지급해 불법 보조금으로 사용할 수 있게 했다며 1주일간 신규모집 금지 조치를 받았다. 신규모집금지 시기는 차기 회의에서 결정한다.. . (이름)신위원회는 26일 전체회의를 열고 지난 1월1일부터 31일까지 sk텔레콤 유통점 38개의 가입자 2960건을 조사한 결과 2050명에게 페이백 등의 방법으로 평균 22만8000원의 보조금을 편법 지급했다며 신규모집금지 7일에 과징금 235억원을 부과했다.. . 방통위에 따르면 sk텔레콤은 1월17일부터 19일까지 갤럭시 노트4와 아이폰6 등 최신 단말기에 대해 최대 50만원 정도의 리베이트를 지급해 차별적 지원금을 유도한 것으로 조사됐다. . . 방통위는 1월16일부터 시장 상황이 과열된다고 보고 이통3사 마케팅 담당자를 불러 자제를 권고했으며, 시장이 진정되지 않으면 단독조사를 진행할 수 있다고 경고했다. 하지만 sk텔레콤은 이후에도 리베이트 지급을 멈추지 않아 방통위는 sk텔레콤에 대한 단독조사를 결정했다.. . 조사과정에서 sk텔레콤 판매점은 판매사무실을 폐쇄하거나 pc를 파괴하는 등 조사를 거부했으며, sk텔레콤은 e메일과 sms 등으로 자료 삭제를 요청하기도 했다. 특히 sk ict 기술원에서 개발한 전산프로그램이 리베이트 내역을 숨기는 데 사용된 것으로 조사했다.. . 조사를 방해한 유통점에는 500만원의 과태료를 부과했다.. . 방통위는 오는 30일 회의를 열고 sk텔레콤의 신규모집금지 시기를 결정할 방침이다..\n",
      "\n",
      "2.81\t[제목]:방통위 제재받은 skt, '조사기간 등 매우 유감'\n",
      "(이름)신위원회로부터 과징금 235억원에 1주일간 영업정지 처분을 받은 sk텔레콤이 이번 심결에 '유감스럽다'는 입장을 밝혔다. . . 26일 sk텔레콤은 보도자료를 통해 '정부의 조치 관련 조사 기간의 시장상황을 종합적으로 고려할 때 단독조사에 의한 제재는 매우 유감스럽다'고 밝혔다. . . 그러나 sk텔레콤은 '다만, 이번 심결을 계기로 sk텔레콤은 시장(이름) 및 단말기유통법 안착을 위해 더욱 노력하겠다'고 덧붙였다. . . 한편, (이름)신위원회는 이 날 전체회의를 열고 지난 1월1일부터 31일까지 sk텔레콤 유통점 38개의 가입자 2960건을 조사한 결과 2050명에게 페이백 등의 방법으로 평균 22만8000원의 보조금을 편법 지급했다며 sk텔레콤에게 영업정지(신규모집금지) 7일에 과징금 235억원을 부과했다.\n",
      "\n",
      "2.81\t[제목]:국정기획위, 통신비 인하안 이르면 오늘 발표\n",
      "(이름) 대통령의 인수위원회 역할을 수행 중인 국정기획자문위원회(이하 국정기획위)가 이르면 오늘 통신비 인하 대책을 발표할 예정이다. . . 가계통신비 인하 공약 이행을 점검 중인 국정기획위 경제2분과 (이름) 위원장은 이날 오전 출근길에 머니투데이 기자와 만나 '이르면 오늘 오후 통신비 인하 방안을 발표할 수 있다'고 밝혔다. . . 이 위원장은 '아직 협의할 사항이 남아 있긴 하다'며 '협의가 길어지면 내일(22일) 오전으로 연기될 수도 있다'고 말했다. . . 한편, 국정위와 미래창조과학부는 통신 사업자에게 부담을 줄 수 있는 기본료 폐지 보다는 선택약정할인율을 20%에서 25%로 상향하는 방안과 최소 필요 데이터를 저가로 제공하는 '보편적 요금제' 도입 등에 무게를 두고 통신비 인하안을 마련 중인 것으로 알려진 상황이다.. . 아울러 △사회적 약자에 대한 통신비 경감 △공공와이파이 확대 △단말기 지원금 분리공시제 △알뜰폰 활성화 방안 등도 국정기획위의 가계통신비 인하 방안에 담길 것으로 전망된다.\n",
      "\n",
      "2.70\t[제목]:skt, nsok 지분 sk텔링크로 넘긴다…'子회사서 손자회사로'\n",
      "sk텔레콤이 지난 2014년 인수했던 보안기업 nsok 지분을 또다른 자회사인 sk텔링크로 넘긴다. . . sk텔레콤과 sk텔링크는 지난 22일 각각 이사회를 열고, sk텔레콤이 보유한 자회사 nsok의 지분(83.93%)을 sk텔링크로 현물 출자하고 sk텔링크의 보통주 21만9967주(2.4%)를 취득하기로 결의했다고 23일 밝혔다.. . sk텔레콤은 보유한 nsok 잔여지분(16.07%)에 대한 콜옵션을 sk텔링크로 이관하고, sk텔링크는 이를 행사해 nsok를 100% 자회사화할 예정이다.. . sk텔링크는 보유한 인프라와 경영시스템을 nsok에 접목해 성장 추세에 있는 물리보안 사업을 차세대 성장동력으로 육성할 계획이다. 또 상품과 유통망 결합, 알뜰폰(mvno)를 활용한 안심폰 서비스 도입 등 양사간 시너지 창출에 적극 나설 예정이다.. . sk텔링크는 '알뜰폰 사업처럼 가입자 기반 사업을 단기간 내 성공적으로 안착시킨 경험과 노하우를 보유한 만큼 물리보안 사업에서도 의미있는 성과가 기대된다'고 설명했다.. . 한편, sk텔레콤은 지난 2014년 4월 nsok 인수 이후 연간 매출상승률이 47%에 달하는 등 물리보안 사업을 적극 추진해왔다. 이번 현물출자 이후에도 sk텔링크와 nsok와 적극적인 협력을 통해 비디오 클라우드와 스마트 홈 등 ict(정보통신기술)와 결합한 물리보안 서비스를 고객들에게 제공할 예정이다. 이번 현물출자는 법원인가를 거쳐 10월 중 완료될 예정이다.\n",
      "\n",
      "2.69\t[제목]:미래부, 사물인터넷 활성화 위해 주파수 출력강화\n",
      "미래창조과학부가 사물인터넷(iot) 신산업 창출 및 서비스 활성화를 위한 주파수 출력 조정에 나섰다.. . 미래부는 15일 900㎒ 대역(917~923.5㎒) 출력 기준을 기존 10㎽에서 최대 200㎽로 상향하는 기술기준 개정안을 마련한다고 행정 예고했다. . . 900㎒ 대역은 도달률·회절률 등이 우수하다. 하지만 출력 제한으로 인해 rfid, z-wave(홈 iot용) 등 근거리용 서비스에 이용됐다. 하지만 최근 저전력 장거리 서비스를 위한 iot 주파수 대역으로 부각되고 있다.. . 이에 미래부는 '제9차 무역투자진흥회의' 및 '규제프리존을 통한 지역경제 발전방안' 후속조치로 iot 활성화를 위한 주파수 정책을 마련했다. . . 출력기준이 상향되면 저전력 장거리 서비스와 같은 새로운 형태의 iot 전용 전국망 구축 기반을 마련할 수 있게 된다. . . 예를 들어 iot 전용망 구축시 기존 10㎽ 출력으로 100개의 기지국이 필요했던 반면 200㎽로 상향 시엔 동일서비스 범위를 27개 기지국으로 좁힐 수 있다. 기지국 기준으로 약 70%의 설치비용이 감소되는 셈.. . 특히 △미터링(수도·전기·가스 등 무인 측정) △위치 트래킹(미아 및 애완견 찾기, 물류 관리) △모니터링 및 컨트롤(주차·가로등·폐기물 관리) 등의 iot 서비스를 센서·단말기의 배터리 교체 없이 5년 이상 이용할 수 있다. . . 전성배 미래부 전파정책국장은 '이번 iot 주파수 출력 상향은 2022년까지 약 22조9000억원 규모의 iot 신산업 창출 및 활성화에 기여할 것'이라고 기대했다. .\n",
      "\n",
      "2.68\t[제목]:ebs2 올해 본방송…상업광고·협찬고지도 금지\n",
      "한국교육방송공사(ebs)가 다채널방송(mms) 'ebs-2'를 올해부터 본방송하되, 상업광고와 협찬고지는 금지되는 방안이 추진된다. 광고시장에 대한 영향을 줄이면서도 사교육비 절감 등 무료 보편적 서비스로서 mms 기능에 집중하도록 환경을 (이름)기 위해서다.. . (이름)신위원회는 28일 전체회의를 열고 'mms 도입방안'을 마련하고 ebs-2 채널의 본방송 도입을 위해 방송법 등 관련 법령 개정을 추진키로 의결했다. . . mms는 디지털 압축기술을 활용해 기존 1개 채널 주파수 대역(6mhz)에서 2개 (이름) 다수 채널을 송출하는 서비스다. 지난해 2월부터 진행된 시범사업인 ebs-2는 초중학 교육과 영어교육 콘텐츠 중심으로 편성돼 현재 매일 19시간 방송 중이다.. . 방통위는 시범사업을 통해 ebs-2의 연간 사교육비 절감효과가 약 1750억원으로 추정되고, 화질평가 부문 등 기술적 안정성도 검증됐다고 설명했다. 다만 방송광고시장에 대한 파급력과 방송의 공익적 역할 제고라는 정책목적을 고려해 상업광고는 물론 협찬고지도 금지하는 방안을 추진한다.. . 다만 신규 mms채널 이외에서 방송된 프로그램을 재방송하는 경우 등에는 예외를 두는 방안을 검토키로 했다. 이와 관련 재방송편성비율 제한 방안도 다시 논의할 예정이다.. . 광고 규제와 관련 방통위 상임위원 대부분은 취지에 공감하면서도 재원 마련을 위해 일부 유연성을 발휘해야 한다는 의견을 보였다. 고삼석 방통위 상임위원은 '신규 제작 의무도 부과돼 재원 마련이 필요한데 현재 ebs (재정)사정이 어렵다'며 '협찬 부분은 광고시장에 미치는 영향을 보면서 유연하게 해야 하지 않겠냐'고 의견을 제시했다.. . (이름) 방통위원장도 '(협찬 유형 가운데)제작비 협찬과 일반 프로그램 광고는 큰 차이가 없는 상호 보완적인 수단이라서 어렵지만, 소품 협찬이나 자문 협찬 등은 별도의 영역이니까 적절하게 풀어주는 방안을 검토할 여지가 있다'고 말했다.. . 도입 대상 사업자는 ebs로 제한돼 추진된다. 논란이 있었던 kbs, mbc, sbs 등 지상파 방송사의 mms는 현 단계에서 검토하지 않기로 했다. 다만 일부 상임위원들은 장기적인 관점에서 mms 확대를 고민해야 한다고 강조해 논쟁의 불씨는 남을 것으로 예측된다.. . 편성 규제는 신규 애니메이션 비율 등 기존 편성규제를 완화해 적용하고 mms용 신규콘텐츠 편성은 의무화할 계획이다. 재난보도와 교육뉴스에 국한해 보도편성을 허용하는 방안도 추진된다. ebs-2의 의무재송신은 사업자들간 자율적 협의 사안으로 두되 필요시 방통위가 원활한 재송신 협의를 지원키로 했다.. . 최 위원장은 'mms 도입은 전 국민에게 제공되는 무료보편적 방송서비스 확대를 위한 정책'이라며 '조속한 본방송 도입을 위해 국회와 관련 기관과의 긴밀한 협조를 통해 관련 법령을 개정해 나가겠다'고 말했다..\n",
      "\n",
      "2.63\t[제목]:'공짜폰' 허위마케팅 sk텔링크 과징금 4.8억…피해자 절반 60대\n",
      "(이름)신위원회가 '공짜폰'이라고 속여 알뜰폰 이용자를 모집한 sk텔링크에 대해 과징금 4억8000만원을 부과했다. 특히 조사 결과 피해를 입은 이용자 절반 (이름) 60대 (이름) 것으로 드러났다. . . (이름)신위원회는 21일 전체회의를 열고 sk텔링크에 대해 시정명령과 함께 과징금을 부과했다. 이용자에게 중요사항을 고지하지 않은 행위에 대한 과징금을 4억원으로 정하고, 위반 행위 기간(10개월)을 고려해 20% 가중한 4억8000만원을 최종 과징금으로 결정했다.. . sk텔링크는 가입자 모집 과정에서 sk텔레콤 휴대폰을 판매하는 것처럼 속이거나 '휴대폰을 무료로 교체해 주겠다'며 알뜰폰 가입자를 유치한 사실이 드러나 방통위 조사를 받았다. '기기값이 100% 무료' '나라에서 100% 단말기 대금 지원' 등을 내세워 홍보한 것으로 밝혀졌다. 특히 약정할인금액을 지원금처럼 말해 공짜폰을 받을 수 있는 것처럼 이용자를 속인 사례가 문제가 됐다. . . 지난해 1월부터 10월까지 sk텔링크 접수민원을 분석한 결과 위반 사실 관련 연령별 가입자를 보면 60대 (이름) 59.8%, 50대가 25.9%인 것으로 드러났다. . . 방통위는 피해자가 노년층이 많은 점을 감안하면 일부나마 사업자가 피해회복 조치를 취한 것은 긍정적으로 본다고 밝혔다. sk텔링크는 현재 민원을 제기한 2186건을 포함한 총 2만8000여건에 대해 총 11억원 규모 손해배상을 이행하고 있다. 36개월 약정할인금과 휴대폰 단말기 할부원금의 차액을 이용자에게 지급키로 한 것. . . 지난 7월30일 민원 제기한 2186건 중 차액이 발생한 440건에 대해 3000만원 가량 현금 보상을 실시했고, 나머지 건수에 대해서는 이달 요금 청구분에서 요금을 감액하고 있다.. . 조사결과 드러난 문제점을 개선하기 위해 방통위는 △텔레마케팅 시 서비스 제공 회사명을 명확히 밝힐 것 △요금할인과 단말기 대금 등 이용자가 반드시 알아야 할 중요사항을 고지할 것 △계약 체결에 대한 이용자 동의사실을 입증할 수 있는 녹취록(최초의 전화권유 포함)을 계약 종료시까지 보관할 것 △계약 체결 후 이용자에게 이용계약서를 반드시 교부할 것 등을 개선토록 명령했다. . . 박노익 방통위 이용자정책국장은 과징금 부과 결정과 별도로 '현재 진행 중인 피해보상을 계획대로 이행하는지 확인하겠다'고 밝혔다.. . sk텔링크 관계자는 '민원을 제기하지 않았어도 민원 제기 고객과 동일한 채널을 통해 가입한 고객은 전향적으로 보상계획을 확정해 시행 중'이라며 '이번 결정을 겸허히 수용해 알뜰폰 서비스 질적 향상 계기로 삼고자 한다'고 말했다. 노년층 피해가 많은 점과 관련 '알뜰폰 서비스 초창기 시작부터 사용하던 고객들의 연령 자체가 높은 영향'이라고 설명했다..\n",
      "\n",
      "2.62\t[제목]:정치권 '통신비 인하'…실현가능성은?\n",
      "20대 국회에서 가계통신비 인하 경쟁이 시작된다. 여야 모두 '국민경제'를 내세우며 주요 방안 가운데 하나로 통신요금 인하를 주목하고 있다. 하지만 통신업계 및 정부에서는 정치권의 이같은 주장이 인기에 영합해 현실을 감안하지 않았다고 우려하고 있다. . . 통신비 인하의 선봉은 3선에 성공한 (이름) 더불어민주당 의원이다. 우 의원은 17일 '19대 국회에서 여당의 반대로 기본료 폐지법안이 처리되지 않았다'며 '20대 국회가 시작되면 제일 먼저 이 법안을 다시 발의하겠다'고 포문을 열었다. . . 19대 국회 미래창조과학(이름)신위원회(미방위) 야당 간사인 우 의원은 지난해 2만원대(vat 별도) 무제한 통화를 담은 데이터중심요금제의 산파역할을 했다. 이어 1만원 안팎의 기본요금 폐지 역시 주장하고 있다. 관례상 원내대표, 상임위원장 자격이 주어지는 3선 고지에 오른 우 의원의 발언권은 더욱 강화될 전망이다. . . 새누리당에서는 재선인 (이름) 의원이 통신비 인하 목소리를 높이고 있다. 19대에서도 배 의원은 기본요금 인하를 주장했다. 20대 국회에서도 미방위 활동이 유력해 통신비 관련 정책에 깊숙히 관여할 전망이다. . . 반면 이동통신 기본요금은 가욋돈이 아니라는 것이 통신업계의 주장이다. 우선 이용자 명목상 기본요금 항목 자체가 없는데다 각 요금제 별 통화·sms 무료제공 비용을 기본요금에서 충당한다는 해명이다.. . 아울러 기본요금이 폐지되면 통신3사의 적자(이름) 우려되고 결국 이는 설비투자 및 고용 위축을 가져올 것이라는 지적이다. 한 통신업계 관계자는 '기본요금이 폐지되거나 줄어들면 그 액수만큼되면 고스란히 매출과 영업이익이 줄어든다'고 토로했다. . . 통신3사의 지난해 영업이익은 sk텔레콤 1조7080억원, kt 1조 2929억원, lg유플러스 6323억원이다. 가입자 당 1만원 상당인 기본요금이 폐지되면 매출은 매달 약 2631억원, 연간 3조1572억원이 준다. kt와 lg유플러스 역시 각각 1조8324억원, 1조3860억원 상당의 매출 감소가 예상된다. . . 기본요금 수익이 사라지거나 축소되면 이달 말 주파수 경매와 기지국 설비 의무화에 투입되는 수(이름) 설비투자 비용을 집행하기 어렵다는 설명이다. . . 정부 관계자 역시 '통신사들의 자율경쟁을 통해 가계통신비 부담을 줄인다는 것이 정부의 기본 원칙'이라며 '정부나 정치권이 나서서 사기업의 요금체계를 (이름)는 것은 자칫 자유시장경제를 침해한다는 논란이 될 수 있다'고 우려했다. . . 이에 우 의원은 '기본료는 이동통신 초기 설비투자비를 회수하게 하기 위해 마련된 것'이라며 '이미 초기 투자비 회수가 완료된데다 가계통신비 부담이 큰 상황에서 기본료폐지는 꼭 이뤄져야 한다'고 말했다. . . 한편, 20대 국회에서는 단말기유통법 개정, 혹은 폐지 논의도 이어질 전망이다. 미래부와 방통위는 단통법 이후 20% 요금할인, 데이터중심요금제 등이 도입되고, 제도도 안정적으로 운용되고 있다는 입장이다. 하지만 단통법에 대한 비판여론은 여전히 존재한다. 또한 단통법에 비판적인 더불어민주당이 원내 1당이 됐다. 내년 대선을 앞두고 여당 역시 여론에 민감하게 반응할 수밖에 없다. . . 더불어민주당 관계자는 '단통법의 문제를 개선하는 법안이 다수 발의됐지만 다수인 새누리당이 소극적으로 임하면서 제대로 논의조차 하지 못했다'며 '새누리당이 독점한 미방위 상임위원장 자리 역시 야당이 맡아야 본격적인 통신비 인하를 위한 다양한 입법 논의가 가능하다'고 강조했다. .\n",
      "\n",
      "2.59\t[제목]:세종시 교사들 skt 스마트로봇으로 코딩 교육받아\n",
      "세종창(이름)혁신센터와 sk텔레콤은 지난 8월 6~7일, 13~14일 2회에 걸쳐 세종시 교육청 소속 교사들을 대상으로 '로봇 활용 코딩 교육' 연수를 실시했다고 16일 밝혔다. 앞서 상반기에는 세종시 연동초등학교에서 '스마트로봇 코딩스쿨' 시범사업을 운영한 바 있다.. . sk텔레콤은 '로봇 활용 코딩 교육'에 대한 참여 교사들의 반응이 긍정적이었다고 전했다. 연수 이후 의견 조사 결과 1차 교육에 참가한 교사 10명 (이름) 모든 문항(10개)에서 5점 만점에 5점을 제시했다. . . 교육 교사들은 스마트로봇 '알버트' '아띠' 등 초등학생들도 쉽게 다가갈 수 있는 로봇을 활용한 코딩교육을 통해 학생들의 컴퓨팅적 사고와 문제 해결 능력을 향상시킬 수 있다는 점에 높은 점수를 줬다. . . 세종창(이름)혁신센터와 sk텔레콤은 교육 교사들을 대상으로 향후 동호회(가칭 '로봇 코딩 교사 연구회')를 운영하고, 정기적인 모임과 후속 교육을 지속할 계획이다. . . 최길성 세종창(이름)혁신센터장은 '이번 교원대상 스마트로봇 활용 코딩스쿨 교육을 통해 sw 인재 양성에 대한 뜨거운 열기가 확인됐다'며 '우리 센터는 앞으로도 코딩스쿨 지원을 통해 도농간 교육 격차 해소는 물론 sw 창의 인재 육성에 적극 나설 계획'이라고 설명했다. . . 이번 교육이 실시된 장소인 '박스쿨'은 sk텔레콤 신사업추진단과 kaist 산업디자인학과이 공동으로 만든 공간으로, 최근 디자인 어워드 '레드닷'의 컨셉부문 대상을 받았다. '박스쿨'은 교실공간과 수업을 위한 hw(하드웨어), 다양한 인터랙티브 sw 솔루션들이 결합된 통합형 교실이다..\n",
      "\n",
      "embedding time: 38.45 ms\n",
      "search time: 418.03 ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "INDEX_NAME = 'aihub-ts1-test-klue-sbert-v1-mpower10u-stoken-128d'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "#INDEX_NAME = 'korquad-klue-sbert-v1.0-noavg' # 요약문 평균값 처리 안한경우\n",
    "\n",
    "SEARCH_SIZE = 10\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 추가하가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquad'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "title = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시'\n",
    "        ]\n",
    "\n",
    "paragraph = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "# 2. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(title, paragraph):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8054-dfc6-4289-8309-8f0710b16771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES 데이터 조회하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "###########################################################\n",
    "# 인덱스내 데이터 조회 => query 이용\n",
    "###########################################################\n",
    "def search(index, data=None):\n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    res = es.search(index=index, body=body)\n",
    "    return res\n",
    "###########################################################\n",
    "\n",
    "# 모든 데이터 조회\n",
    "#sr = search(index=INDEX_NAME)\n",
    "#pprint.pprint(sr)\n",
    "\n",
    "# 단일 필드 조회\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7fc52-530f-46f4-ae83-2ccca3fd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 삭제하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 삭제 => query 이용\n",
    "############################################################\n",
    "def delete(index, data):\n",
    "    if data is None:  # data가 없으면 모두 삭제\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    return es.delete_by_query(index, body=body)\n",
    "\n",
    "############################################################\n",
    "## 2: 인덱스 내의 데이터 삭제 => id 이용\n",
    "############################################################\n",
    "def delete_by_id(index, id):\n",
    "    return es.delete(index, id=id)\n",
    "\n",
    "############################################################\n",
    "## 3: 인덱스 자체 삭제\n",
    "############################################################\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# 1: query 이용 데이터 삭제\n",
    "delete(index=INDEX_NAME, data={'title':'한라산'})\n",
    "\n",
    "# 3: 인덱스 자체 삭제\n",
    "#delete_index(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659658-75cf-4339-a88a-db338bddfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 업데이트하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 업데이트=>_id 에 데이터 업데이트\n",
    "############################################################\n",
    "def update(index, id, doc, doc_type):\n",
    "    \n",
    "    body = {\n",
    "        'doc': doc\n",
    "    }\n",
    "    \n",
    "    res=es.update(index=index, id=id, body=body, doc_type=doc_type)\n",
    "    return res\n",
    "############################################################\n",
    "\n",
    "#=====================================================================\n",
    "# 검색해서, _id, _type을 구함\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print('\\n')\n",
    "print(\"===[검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    id = hits[\"_id\"]      # id\n",
    "    type = hits[\"_type\"]  # type\n",
    "    \n",
    "    print(f'id: {id}')\n",
    "    print(f'type: {type}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # update 시킴\n",
    "    print(\"===[업데이트]===\")\n",
    "    doc = {'paragraph': '제주도는 대한민국에 가장 남쪽에 있는 섬으로, 인구는 약 71만명이며, 화산섬으로 관광자원이 많은 천혜의 관광지 이다.'}\n",
    "    print(doc)\n",
    "    print('\\n')\n",
    "    \n",
    "    ur=update(index=INDEX_NAME, id=id, doc=doc, doc_type=type)\n",
    "    print(\"===[업데이트 결과]===\")\n",
    "    print(ur)\n",
    "    print('\\n')\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "# 인덱스 refresh 함\n",
    "# elasticsearch의 자동 새로고침의 시간은 1초 정도 소요\n",
    "# 따라서 코드에 아래 명령어를 입력하지 않았을 경우 검색을 하지 못할 가능성도 존재\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# 제주도로 검색해서 한번더 확인\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print(\"===[재검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    \n",
    "    print(f'id:{hits[\"_id\"]}')\n",
    "    print(f'type: {hits[\"_type\"]}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
