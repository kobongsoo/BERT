{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# - 문장들을  추출 요약해서 요약문장을 만들고, 요약 문장의 평균을 구하여 문장 embedding을 생성하여 ES에 인덱스에 vector 추가하고, 검색하는 예제임\n",
    "# - 말뭉치는 ai_hub에 원천말뭉치인 ts1 말뭉치에 tilte, content를 추출하여, content 요약문과 title에 대해 각각 vector를 만들어서 ES 인덱스로 추가하는 예시임.\n",
    "#\n",
    "# => 대규모 웹데이터 기반 한국어 말뭉치 데이터 \n",
    "# 말뭉치 출처: https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624\n",
    "\n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "# - elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# - 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss\n",
    "\n",
    "# 추출 요약 설치\n",
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "import kss\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "# FutureWarning 제거\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, getListOfFiles\n",
    "device = GPU_info()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# 0. param 설정\n",
    "#------------------------------------------------------------------------------------\n",
    "seed = 111\n",
    "query_num = 500             # 쿼리 최대 갯수: KorQuAD_v1.0_dev.json 최대값은 5533개임, 0이면 모든 5533개 쿼리함.\n",
    "search_k = 5                # FAISS 검색시, 검색 계수(5=쿼리와 가장 근접한 5개 결과값을 반환함)\n",
    "avg_num = 1                 # 쿼리에 대해 sub 문장들중 최대 scorce를 갖는 문장을 몇개 찾고 평균낼지.(3=쿼리에 가장 유사한 sub문장 3개를 찾고 평균을 냄)\n",
    "faiss_index_method = 0      # 0= Cosine Similarity 적용(IndexFlatIP 사용), 1= Euclidean Distance 적용(IndexFlatL2 사용)\n",
    "\n",
    "# 토큰 임베딩 관련 param\n",
    "IS_EMBED_DIVIDE = True      #여기서는 토큰 임베딩은 True고정, True=문단의 여러 문장을, 토큰 단위로 분리후 벡터 구해서 인덱스 만듬/False=문단의 여러문장을 하나의 벡터를 구해서 인덱스 만듬.\n",
    "EMBED_DIVIDE_LEN = [5,7,9]  #5 # 문장을 몇개(토큰)으로 분리할지 (7,8,10) 일때 성능 좋음=>50.8%, (5,7,9) 일때 차원축소 128=>41.80%(81.8%) 성능 좋음\n",
    "MAX_TOKEN_LEN = 40          # 최대 몇개 token까지만 임베딩 할지\n",
    "\n",
    "# 차원 축소 관련 param\n",
    "# 차원 축소 할지, 768 값을 앞에 384 만큼만 배열 resize 시킴.  \n",
    "# - 384로 줄일대 -2% 성능 저하 발생(512: -1.2%, 256: -6%)\n",
    "DIM_RESIZE_METHOD = 2  # 0=차원축소 안함/1=resize 로 차원축소/2=Dense 모델 이용 차원축소\n",
    "DIM_RESIZE_LEN = 128\n",
    "\n",
    "# ONNX 모델 사용\n",
    "IS_ONNX_MODEL = True        # True=onnx 모델 사용\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---bi_encoder---------------------------\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 128, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "---dense param---------------------------\n",
      "*dense_weight:torch.Size([128, 768])\n",
      "*dense_bias:torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4466c581a9466484f4fba40f9c3a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e114f117d64846abb9b951f718701b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07073cb88d4b4aaf9e655751988f6b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/752k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9459a81810ae4b84b8eb6cd160cf6ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c98c43add754a64b1bf65d722294fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/646 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0160d329dde7499c9b66c33703497ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/250M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---onnx_model---------------------------\n",
      "<optimum.onnxruntime.modeling_ort.ORTModelForFeatureExtraction object at 0x000002B59C61DF10>\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 1. 검색모델 로딩\n",
    "# => bi_encoder 모델 로딩, polling_mode 설정\n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import bi_encoder, dense_model, onnx_model\n",
    "\n",
    "# bi_encoder 모델 로딩\n",
    "bi_encoder_path = \"bongsoo/klue-sbert-v1\"\n",
    "pooling_mode = 'mean' # bert면=mean, albert면 = cls\n",
    " # 출력 임베딩 크기 지정 : 0=기본 모델 임베딩크기(768), 예:128=128 츨력임베딩 크기 \n",
    "out_dimension = 128 if DIM_RESIZE_METHOD == 2 else 0 if DIM_RESIZE_METHOD != 2 else None\n",
    "    \n",
    "word_embedding_model1, bi_encoder1 = bi_encoder(model_path=bi_encoder_path, max_seq_len=512, do_lower_case=True, \n",
    "                                              pooling_mode=pooling_mode, out_dimension=out_dimension, device=device)\n",
    "print(f'---bi_encoder---------------------------')   \n",
    "print(bi_encoder1)\n",
    "print(word_embedding_model1)\n",
    "\n",
    "print(f'---dense param---------------------------')   \n",
    "# 출력 값 차원 축소 지정인 경우, token_embeddings 일때는 sentencebert 라이브러리를 이용하여 dense_model 모델 추가할수 없으므로,\n",
    "# 사용자정의 dense_model을 정의해서, 가중치와 bias를 bi_encoder모델에 것을 얻어와서 적용해서 차원 죽소함.\n",
    "# => resize 방식 보다 성능 떨어지지만, 128일때는 더 성능이 좋음\n",
    "if DIM_RESIZE_METHOD == 2:\n",
    "    #-------------------------------------------------------------------------\n",
    "    # 처음에는 아래 코드를 활용하여 해당 모델의 128 weight와 bias를 저장해 두어야 함.\n",
    "    #state_dict = bi_encoder1.state_dict()  # bi_encoder모델이 state_dict 얻어옴\n",
    "    #print(state_dict.keys())\n",
    "    #dense_weight = state_dict['2.linear.weight'] # denser 모델에 bi_encoder모델이 linear.weight 적용\n",
    "    #dense_bias = state_dict['2.linear.bias']     # denser 모델에 bi_encoder모델이 linear.bias 적용\n",
    "    \n",
    "    # 처음에  weigth, bias 파일을 저장해 둠.\n",
    "    #torch.save(dense_weight, 'klue-sbert-v1-weigth.pt')\n",
    "    #torch.save(dense_bias, 'klue-sbert-v1-bias.pt')\n",
    "    #-------------------------------------------------------------------------\n",
    "    # weigth, bias 저장해둔 파일 로딩\n",
    "    dense_weight = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-weight-128.pt')\n",
    "    dense_bias = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-bias-128.pt')\n",
    "\n",
    "    print('*dense_weight:{}'.format(dense_weight.size()))\n",
    "    print(f'*dense_bias:{dense_bias.size()}')\n",
    " \n",
    "# onnx 모델 로딩\n",
    "if IS_ONNX_MODEL == True:\n",
    "    onnx_model_path = \"bongsoo/klue-sbert-v1-onnx\"#\"bongsoo/klue-sbert-v1-onnx\"\n",
    "    onnx_tokenizer, onnx_model = onnx_model(onnx_model_path)\n",
    "    print(f'---onnx_model---------------------------')\n",
    "    print(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b67318-d790-422c-a469-874a1b5b7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 안덱싱 및 검색 조건에 맞게 임베딩 처리 하는 함수 \n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import embed_text, onnx_embed_text\n",
    "\n",
    "def embedding(paragrphs:list):\n",
    "    if IS_ONNX_MODEL == True:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs) \n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs, token_embeddings=False)\n",
    "    else:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, token_embeddings=True, return_tensor=False)\n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, return_tensor=False)  \n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0068d-6e57-430a-8142-19a05fa9e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 안덱싱 처리\n",
    "#-------------------------------------------------------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "import kss\n",
    "from myutils import embed_text, divide_arr_avg_exten, clean_text\n",
    "\n",
    "# 인덱싱 함수 \n",
    "def index_data():\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    count = 0\n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "        count += 1\n",
    "        #print(f'{count}:{source}') # 인덱스 구조 출력\n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    # json 파일들이 있는 폴더에 .json 파일 이름들을 얻기\n",
    "    # =>DATA_FOLDER: .JSON파일들이 있는 폴더\n",
    "    files = getListOfFiles(DATA_FOLDER)\n",
    "    assert len(files) > 0 # files가 0이면 assert 발생\n",
    "    print('*file_count: {}, file_list:{}'.format(len(files), files[0:5]))\n",
    " \n",
    "    for idx, file in enumerate(tqdm(files)):\n",
    "        if \".json\" not in file:  #.json 파일이 아니면 합치지 않음\n",
    "            continue\n",
    "            \n",
    "        count = 0\n",
    "        docs = []\n",
    "    \n",
    "        # json 파일 로딩 => [SJML][text] entry만 불러옴\n",
    "        json_data = json.load(open(file, \"r\", encoding=\"utf-8\"))['SJML']['text']\n",
    "        for data in json_data:\n",
    "        #for data in json_data:\n",
    "            count += 1\n",
    "            doc = {} #dict 선언\n",
    "            \n",
    "            doc['title'] = data['title']            # 제목 설정\n",
    "            doc['paragraph'] = data['content']      # 문장 담음.\n",
    "                \n",
    "            docs.append(doc)\n",
    "            #print(f'count:{count}')\n",
    "            #print(doc['title'])\n",
    "            \n",
    "            if count % BATCH_SIZE == 0:\n",
    "                index_batch(docs)\n",
    "                docs = []\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "            # ** 10 개만 보냄\n",
    "            #if count >= 10:\n",
    "            #   break\n",
    "            \n",
    "        if docs:\n",
    "            index_batch(docs)\n",
    "            print(\"Indexed {} documents.\".format(count))   \n",
    "            \n",
    "        es.indices.refresh(index=INDEX_NAME)\n",
    "            \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    #print(\"=== End Done indexing===\")\n",
    "                   \n",
    "\n",
    "def index_batch(docs):\n",
    "        \n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        title = doc['title']\n",
    "        paragraph = doc['paragraph']\n",
    "\n",
    "        sub_contexts = []\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        paragraph = clean_text(paragraph)  # 전처리 : (한글, 숫자, 영문, (), {}, [], %, ,,.,\",')  등을 제외한 특수문자 제거\n",
    "        # 입력 문단을 여러 문장들로 나눔.\n",
    "        #sentences = [sentence for sentence in paragraph.split('.') if sentence != '' and len(sentence) > 10]  # '.'(마침표) 로 구분해서 sub 문장을 만듬.\n",
    "        sentences = [sentence for sentence in kss.split_sentences(paragraph) if sentence != '' and len(sentence) > 10] # kss 이용해서 sub 문장을 만듬\n",
    "        \n",
    "        # 만약 sentences(sub 문장) 가 하나도 없으면 원본문장을 담고, 10이상이면  10개만 담음.\n",
    "        sub_contexts.append([paragraph] if len(sentences) < 1 else sentences[0:10] if len(sentences) > 10 else sentences)\n",
    "       \n",
    "        if i < 1:\n",
    "            print(sub_contexts[0])\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # 토큰 분할 임베딩 처리\n",
    "        # => sub_contexts은  1차원 리스트 임 (예:['오늘 비가 온다','오늘 눈이 온다','날씨가 좋다'])\n",
    "        token_embeds = embedding(sub_contexts[0])\n",
    "        #------------------------------------------------------------------------------------------------------------------------ \n",
    "        # 토큰 분할인 경우 처리 start=>           \n",
    "        token_embed_arr_list = []\n",
    "        tcount = 0\n",
    "        # tensor(250,768) 한문장 토큰 임베딩 얻어와서, 각 ?개 토큰씩 평균을 구함.\n",
    "        for token_embed in token_embeds:\n",
    "            token_embed = token_embed[1:-1] # 맨앞에 [CLS]와 맨뒤에 [SEP] 토큰은 뺌\n",
    "            if tcount >= MAX_TOKEN_LEN: \n",
    "                break\n",
    "             \n",
    "            #print(f'token_embed.shape:{token_embed.shape}')\n",
    "            \n",
    "            token_embed_arrs = token_embed.cpu().numpy().astype('float32')\n",
    "            #print(f'token_embed_arrs:{token_embed_arrs.shape}')\n",
    "            # 5,7,10 씩 자르면서 문장 토큰 평균을 구함\n",
    "            token_embed_divide_arrs = divide_arr_avg_exten(embed_arr=token_embed_arrs, divide_arrs=EMBED_DIVIDE_LEN) \n",
    "\n",
    "             # Dense 방식으로 차원 축소 => 평균 구한후 차원 축소하는 방식이 0.6% 정도 성능 좋음\n",
    "            if DIM_RESIZE_METHOD == 2:\n",
    "                tmp1 = torch.Tensor(token_embed_divide_arrs)\n",
    "                #tmp1 = torch.from_numpy(token_embed_divide_arrs)\n",
    "                debug1 = False\n",
    "                tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias, debug=debug1)\n",
    "                arrs = tmp2.detach().numpy().astype('float32')\n",
    "            else:  \n",
    "                arrs = token_embed_divide_arrs\n",
    "                \n",
    "            # 평균 구한 토큰들을 token_embed_arr_list 리스트에 담아둠.(50보다 크면 50개만 담음)           \n",
    "            for idx, arr in enumerate(arrs):\n",
    "                \n",
    "                 # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "                if DIM_RESIZE_METHOD == 1:\n",
    "                    darr = np.resize(arr, (DIM_RESIZE_LEN,))\n",
    "                else:\n",
    "                    darr = arr\n",
    "                    \n",
    "                token_embed_arr_list.append(darr)\n",
    "                tcount +=1\n",
    "                if tcount >=MAX_TOKEN_LEN:\n",
    "                    break\n",
    "\n",
    "        #embeddings = np.array(token_embed_arr_list)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # ES에 문단 인덱싱 처리\n",
    "        request = {}  #dict 정의\n",
    "        request[\"rfile_name\"] = title       # 제목               \n",
    "        request[\"rfile_text\"] = paragraph   # 문장\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"        \n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        # for문 돌면서 벡터 처리\n",
    "        #print(type(token_embed_arr_list))\n",
    "        #print(len(token_embed_arr_list))\n",
    "        \n",
    "        # vector 1~40 까지 값을 0으로 초기화 해줌.\n",
    "        for i in range(MAX_TOKEN_LEN):\n",
    "            if DIM_RESIZE_METHOD > 0:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((DIM_RESIZE_LEN))\n",
    "            else:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((768))\n",
    "            \n",
    "        # vector 값들을 담음.\n",
    "        for i, token_embed_arr in enumerate(token_embed_arr_list):\n",
    "            request[\"vector\"+str(i+1)] = token_embed_arr\n",
    "            \n",
    "        requests.append(request)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'aihub-ts1-acsampe-klue-sbert-v1-mpower10u-128d-onnx'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "INDEX_FILE = './data/mpower10u_128d.json'                 # 인덱스 구조 파일\n",
    "DATA_FOLDER = '../../../data11/ai_hub/ts1/acsample/'     # 인덱스할 파일들이 있는 폴더경로 \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "## 입력 ##\n",
    "# GET korquad/_count\n",
    "\n",
    "## 출력 ###\n",
    "'''\n",
    "{\n",
    "  \"count\" : 1420,\n",
    "  \"_shards\" : {\n",
    "    \"total\" : 2,\n",
    "    \"successful\" : 2,\n",
    "    \"skipped\" : 0,\n",
    "    \"failed\" : 0\n",
    "}\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 검색 처리\n",
    "#-------------------------------------------------------------------------------------\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from myutils import embed_text, divide_arr_avg_exten\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    \n",
    "    # 문장으로 비교할때=>쿼리 문장에 대한 벡터 생성해서 비교\n",
    "    #query_vector = embed_text(model=bi_encoder,paragraphs=[query])[0]\n",
    "    #------------------------------------------------------------------\n",
    "    # 토큰 평균으로 비교할때=> 쿼리 문장에 대한 모든 토큰 벡터를 생성해서 비교\n",
    "    # 토큰 분할 임베딩 처리\n",
    "    token_query_embeds = embedding([query])\n",
    "    \n",
    "    token_query_embed_arr_list = []\n",
    "    # 쿼리 문장들의 토큰들의 평균을 구함.\n",
    "    for token_query_embed in token_query_embeds:\n",
    "        \n",
    "        token_query_embed = token_query_embed[1:-1] # 맨앞에 [CLS]와 맨두에 [SEP] 토큰은 뺌\n",
    "        print(f'*token_query_embed.shape:{token_query_embed.shape}')\n",
    "        \n",
    "        tmp = token_query_embed.cpu().numpy().astype('float32')\n",
    "        tmp=tmp.mean(axis=0) #평균 구함\n",
    "        \n",
    "         # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "        if DIM_RESIZE_METHOD == 1:\n",
    "            tmp = np.resize(tmp, (DIM_RESIZE_LEN,))\n",
    "         # Dense 방식으로 차원 축소=> 평균 구하기 전에 차원 축소하는것이 성능 +0.6더 좋음\n",
    "        elif DIM_RESIZE_METHOD == 2:\n",
    "            tmp1 = torch.Tensor([tmp]) # 1차원 배열을 -> 2차둰 텐서(1,768)로 변환\n",
    "            tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias)\n",
    "            tmp = tmp2.detach().numpy().astype('float32').ravel(order='C') # 1차원 배열로 변경(128,)        \n",
    "        \n",
    "        token_query_embed_arr_list.append(tmp)\n",
    "        \n",
    "    query_vector = np.array(token_query_embed_arr_list)[0]  # 리스트를 배열로 변환  \n",
    "    \n",
    "    # print(query_vector)\n",
    "    print(f'*query_vector.shape:{query_vector.shape}\\n')\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    '''\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector2']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴(doc['summarize_vector'])\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # 문단별 40개의 벡터와 쿼리벡터를 서로 비교하여 최대값 갖는 문단들중 가장 유사한  문단 출력\n",
    "    # => script \"\"\" 안에 코드는 java 임.\n",
    "    # => \"queryVectorMag\": 0.1905 일때 100% 일치하는 값은 9.98임(즉 10점 만점임)\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "                \"script\":{\n",
    "                    \"source\": \"\"\"\n",
    "                      float max_score = 0;\n",
    "                      for(int i = 1; i <= params.VectorNum; i++) \n",
    "                      {\n",
    "                          float[] v = doc['vector'+i].vectorValue; \n",
    "                          float vm = doc['vector'+i].magnitude;  \n",
    "                          \n",
    "                          if (v[0] != 0)\n",
    "                          {\n",
    "                              float dotProduct = 0;\n",
    "\n",
    "                              for(int j = 0; j < v.length; j++) \n",
    "                              {\n",
    "                                  dotProduct += v[j] * params.queryVector[j];\n",
    "                              }\n",
    "\n",
    "                              float score = dotProduct / (vm * (float) params.queryVectorMag);\n",
    "\n",
    "                              if(score > max_score) \n",
    "                              {\n",
    "                                  max_score = score;\n",
    "                              }\n",
    "                            }\n",
    "                      }\n",
    "                      return max_score\n",
    "                    \"\"\",\n",
    "                \"params\": \n",
    "                {\n",
    "                  \"queryVector\": query_vector,\n",
    "                  \"queryVectorMag\": 0.1905,\n",
    "                  \"VectorNum\": 40\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\",\"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "\n",
    "        \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    texts = []\n",
    "    titles = [] \n",
    "    bi_scores = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        '''\n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[요약문]')\n",
    "        print(hit[\"_source\"][\"summarize\"]) \n",
    "        print()\n",
    "                \n",
    "        '''\n",
    "        #print(hit)\n",
    "        \n",
    "        # 리스트에 저장해둠\n",
    "        titles.append(hit[\"_source\"][\"rfile_name\"])\n",
    "        texts.append(hit[\"_source\"][\"rfile_text\"])\n",
    "        bi_scores.append(hit[\"_score\"])\n",
    "        \n",
    "     # 내림 차순으로 정렬\n",
    "    dec_bi_scores = reversed(np.argsort(bi_scores))\n",
    "    print(dec_bi_scores)\n",
    "    \n",
    "    # 내림차순으로 출력\n",
    "    for idx in dec_bi_scores:\n",
    "        print(\"{:.2f}\\t[제목]:{}\\n{}\\n\".format(float(bi_scores[idx]), titles[idx], texts[idx]))\n",
    "    \n",
    "    # 처리 시간들 출력\n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "    #========================================================================================================    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Elasticsearch.info of <Elasticsearch([{'host': '192.168.0.27', 'port': 9200}])>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  교과서 편찬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_15464\\3470692988.py:120: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*token_query_embed.shape:torch.Size([2, 768])\n",
      "*query_vector.shape:(128,)\n",
      "\n",
      "397 total hits.\n",
      "<reversed object at 0x000002B5A77F82E0>\n",
      "4.80\t[제목]:한울원전 5호기 재가동, 8일께 전력공급 (이름)\n",
      "지난달 29일 고장으로 가동이 멈췄던 경북 울진군 한울원자력발전소 5호기가 8일 만에 재가동됐다.. . 6일 한국수력원자력(이하 (이름))은 이날 오전 10시 원자력안전위원회(이하 원안위)의 승인에 따라 오후 8시경 설비용량 100만kw 규모의 한울원전 5호기 발전을 재가동했다고 밝혔다. (이름)은 8일 오전쯤 전력공급이 (이름) 될 것으로 내다봤다.. . 한울원전 5호기의 고장 원인은 제어카드 이상으로 밝혀졌다. 원안위에 따르면 제어카드 뒷면의 코팅과정에서 이물질이 유입되면서, 순간적으로 전류가 흐르는 '섬락(flashover)' 현상이 발생했다는 설명이다. 전체 73개 중 2개의 제어카드가 손상된 것으로 확인됐다.. . 원안위는 '(이름)의 정밀점검 등 조치내용이 적절했는지 확인했다'며 '앞으로 코팅과정에서 이물질 검사를 강화하는 방안을 마련할 것을 주문했다'고 밝혔다.\n",
      "\n",
      "4.65\t[제목]:'한국형 원전' nrc 설계인증 법안, 美 연방관보 게재\n",
      "한국수력원자력은 apr1400 원전의 미국 원자력규제위원회(nrc) 설계인증 관련 법안이 22일(현지시각) 미국 연방관보에 게재된다고 22일 밝혔다. 법안은 앞으로 30일간 대중들의 의견을 수렴하는 절차를 거친다. 공청 기간 동안 특별한 반대의견이 없다면 apr1400은 최종 설계인증을 취득하게 된다. . . apr1400은 1400급 국산 신형경수로 원전으로, 신고리 56호기를 비롯해 현재 국내에서 5개 호기가 건설 중이다. 아랍에미리트(uae)에 수출해 4개 호기를 짓고 있는 바라카 원전과도 노형이 같다. . . 앞서 (이름)과 한국전력은 2014년 12월23일 공동으로 미국 nrc에 apr1400 표준설계에 대한 설계인증을 신청했다. 이듬해 3월부터 심사가 착수돼 지난해 9월28일 nrc로부터 표준설계인증서(sda)를 받았다. 이번 연방관보 게재는 법제화 과정의 일부로, 이르면 오는 7월말쯤 설계인증 취득이 마무리될 전망이다. . . nrc의 설계인증은 미 원자력 규제당국이 원전 전체에 적용되는 '표준설계'의 안전성을 입증하는 절차다. 설계인증을 받은 원전 모델에 대해 미국 내 전력사업자가 건설운영허가를 받고자 할 경우 표준설계 승인 부분에 대한 심사를 면제받기 때문에 인허가 기간과 비용이 줄어든다. 이에따라 apr1400의 미국 수출경쟁력이 향상될 것으로 보인다. . . .\n",
      "\n",
      "4.24\t[제목]:바이든도 베팅 '그린뉴딜'…'선제대응 필수'\n",
      "전 세계가 코로나19로 침체된 경제에 활력을 불어넣기 위해 그린뉴딜에 주목하고 있다. 산업 전반을 개조해 탄소 배출을 획기적으로 줄이는 효과까지 노린다는 전략이다. .. . 대한무역투자진흥공사(코트라)가 6일 발간한 '주요국 그린뉴딜 정책의 내용과 시사점'에 따르면 미국, 유럽연합(eu), 중국, 인도 등 세계 주요국이 경기 부양을 위해 그린 모빌리티, 청정에너지 확대 등 그린뉴딜 정책을 본격 도입 중이다.. . 세계 최대 경제대국인 미국은 올해 바이든 행정부 출범에 맞춰 대대적인 친환경 정책 시행이 예상된다. 조 바이든 당선인은 취임 즉시 도널드 트럼프 대통령이 탈퇴한 파리기후협정에 재가입하고 2050년까지 온실가스 배출 0(net zero)을 목표로 연방예산 1조7000억 달러를 투자할 계획을 밝혔다. . . 환경보호와 경제 재건을 목적으로 무공해자동차와 청정에너지 도입, 스마트시티그린시티 등 친환경 도시정책에 집중할 것으로 예상된다. 그린뉴딜 사업에도 바이 아메리칸 정책을 시행해 자국산업 보호에 나설 가능성도 크다.. . eu는 일찌감치 '유럽그린딜'을 발표하며 2050년까지 세계 최초로 탄소중립 대륙이 되겠다는 비전을 제시했다. eu 집행위원회는 가장 환경 친화적인 경기 부양책으로 평가받는 유럽그린딜에 예산 1조 유로 이상을 투자할 방침이다. 유럽그린딜은 그린 모빌리티, 청정에너지, 에너지 효율, 순환경제 등 분야를 포함한다. . . 세계 최대 탄소배출국 중국도 친환경 산업 육성에 뛰어들었다. 그동안 국제사회의 탄소배출 감축 요구에도 선진국개발도상국 간 차별적 책임 원칙을 고수했다. 국제사회의 기후변화 대책 마련 요구가 커지면서 최근 중국 정부는 2060년 탄소중립 달성 목표를 발표하는 등 입장 변화를 보이고 있다. 다른 주요국의 탈탄소 그린뉴딜 정책과 달리 중국은 신인프라 정책을 통해 기후변화 대응에 나서고 있다. 인프라의 범위를 정보융합혁신 분야로 확대하고 5g, 인공지능 등 첨단기술을 활용해 새로운 녹색산업을 집중 육성할 계획이다.. . 인도는 중국, 미국에 이어 온실가스 배출 3위국가이지만 뚜렷한 그린뉴딜 정책을 제시하지 않고 있다. 그러나 글로벌 경쟁력을 보유한 풍력, 태양광 등 재생에너지 개발에 주력하면서 낙후된 인프라 개선을 위해 100대 스마트시티 건설에 역점을 두는 등 친환경 산업화를 적극 추진하고 있다.. . 세계 각국이 그린뉴딜에 적극 뛰어들면서 녹색공공조달제도 및 탄소국경세의 역외국 적용 등 '녹색보호주의'도 eu를 중심으로 확산할 것으로 보이는 만큼 국내기업들의 선제적인 대응이 중요해졌다. 최근 정부가 '한국형 re100(재생에너지 사용 100%)' 시행에 나선 이유다. 정부는 재생에너지로 생산한 전기를 우선구매하는 효과가 있는 '녹색프리미엄제도'를 도입하고 이달 첫 입찰에 들어갔다. . . 손수득 코트라 경제통상협력본부장은 '주요국의 그린뉴딜 정책은 규제인 동시에 새로운 비즈니스 기회로 작용한다'며 '기회를 선점하기 위해 녹색산업에 대한 관심을 바탕으로 녹색보호주의 확산에도 대비해야 한다'고 말했다..\n",
      "\n",
      "4.21\t[제목]:혈당억제 '당조고추' 초고령 일본사회 사로 잡았다\n",
      "식사후 혈당수치 상승을 억제하는 국산 당조고추가 '일본 기능성표시식품'으로 정식 등록됐다.. . 농림축산식품부와 한국농수산식품유통공사(at)는 국산 당조고추가 일본 '기능성표시식품'에 등록돼 지난 6월부터 현지 대형마트에서 본격 판매되고 있다고 25일 밝혔다.. . 일본 기능성표시식품에 등록된 건 국산농산물로는 당조고추가 처음이다. . . 일본은 65세 이상 인구가 전체 30%에 육박하는 초고령화사회에 이미 진입할 정도로 노령인구가 높고 건강식품에 대한 관심이 크다. 일본 정부는 이를 위해 2015년부터 기능성표시식품제도를 시행했다. 또 유통 식품에 대한 능성' 표시 기준을 엄격하게 적용하고 있다. . . . 2008년 국내 연구진에 의해 개발된 당조고추는 이름 그대로 '당을 조절해주는' 기능성이 탁월하다. 당조고추에 함유된 루테오린 성분이 당류의 분해와 흡수를 완만하게 해 식후 혈당치의 상승을 억제한다.. . 농림축산식품부와 at는 그동안 건강기능성식품 시장의 성장 가능성을 주목해 왔다. 2011년부터 당조고추의 단계별 수출가능성을 타진하는 한편 일본 국립대학과 연계한 임상시험을 통해 기능성 검증 등 과학적 근거를 마련했다.. . 또 일본 현지 소비자들을 대상으로 한 테스트판매도 실시해 판매 가능성도 검증했다.. . at는 특히 당조고추에 이른바 '제스프리(zespri) 모델'을 적용했다. 과거 파프리카, 딸기 등의 품목에서 경험했던 수출시장 난립을 방지하기 위해 시작단계부터 생산 및 수출창구를 단일화했다.. . 일본 내 시장질서 유지를 위해 당조고추의 일본 상품명 '토우쵸토가라시(糖調唐辛子)'를 직접 고안해 상표권 등록까지 마쳤다.. . 백진석 at 부사장은 '기능성채소의 인기는 일본뿐만 아니라 중국, 미국 등 세계적인 추세로, 한국산 당조고추의 기능성에 모두 주목하고 있다'며 '지금까지의 성과에 만족하지 않고 제2의 당초고추와 같은 신규 유망품목을 지속적으로 발굴해 나가겠다'고 말했다..\n",
      "\n",
      "4.07\t[제목]:감사원, 공기업 방만경영 내달 고강도 감사\n",
      "감사원이 공기업에 대한 전방위 고강도 감사에 착수한다. 앞서 기획재정부가 방만 경영 논란을 부르고 있는 공기업에 대한 자산매각 압력을 가한 바 있어 '공공부문 개혁'이 (이름)정부 집권 2년차 국정운영의 화두가 될 전망이다.. . 6일 감사원에 따르면 감사원은 지난해 말부터 공공기관감사국을 중심으로 산업금융감사국, 국토해양감사국 등에서 30여 명에 이르는 감사준비팀을 운영 중이다.. . 감사준비팀은 집권 2년차를 맞아 다음 달부터 본격적으로 실시될 공공기관 감사를 위해 현재 자료 및 통계, 언론 보도 등 자료 수집에 집중하고 있는 것으로 알려졌다. 감사원은 이어 다음 달 중순쯤 본격 감사에 착수할 계획이다. . . 감사원은 통상 전체 공공기관 중 일부 감사대상을 선정해 진행해 온 기관운영감사 방식 대신 전체 공공기관을 대상으로 동시에 감사에 착수하는 특정감사 형태로 이번 감사를 진행할 예정이다. 감사에서 비위행위나 부실경영 등에 대한 사실이 드러나면 검찰 고발을 통해 신속히 공공기관 개혁이 추진될 수 있도록 한다는 계획이다.. . 아울러 결과를 공기업 경영평가에 반영하고, 기관장에게도 퇴진 등 엄중 책임을 물을 것으로 보인다.. . 앞서 (이름) 대통령은 지난해 12월 새로 취임한 (이름) 감사원장에게 임명장을 수여하면서 '공기업의 방만한 경영이라든가 부조리, 공직의 기강 해이 등을 확실히 바로잡아야 한다'고 말한 바 있다.. . 황 원장 역시 신년사에서 '반복되는 지적에도 근절되지 않고 있는 공공기관의 도덕적 해이와 불합리한 관행이 더 이상 용인하기 어려운 수준에 이르렀다'며 '공공기관의 방만 경영을 근절하기 위한 강도 높은 점검을 실시하겠다'고 밝혔다..\n",
      "\n",
      "embedding time: 50.95 ms\n",
      "search time: 72.69 ms\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  경찰\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*token_query_embed.shape:torch.Size([1, 768])\n",
      "*query_vector.shape:(128,)\n",
      "\n",
      "397 total hits.\n",
      "<reversed object at 0x000002B59C6380D0>\n",
      "6.41\t[제목]:석유공사 울산지사서 배관폭발 사고… 1명 사망\n",
      ". 한국석유공사 울산지사에서 폭발사고가 발생해 6명의 사상자가 발생했다.. . 국민안전처와 석유공사에 따르면 14일 오후 2시 35분쯤 울산 울주군 온산읍 석유공사 울산지사 내 원유배관 이설공사 현장에서 폭발사고가 발생했다. . . 사고 원인은 조사 중이나 배관 청소 작업을 진행하던 중 배관내 잔여 가스가 폭발한 것으로 추정된다. . . 이 사고로 현재까지 1명이 숨지고 최소 5명이 중경상을 입었다. 현재 추가 사고의 위험은 없는 것으로 확인됐다. . . 국민안전처 관계자는 현재 상황은 종료가 됐다며 정확한 원인에 대해 조사 중이라고 말했다. . . 한편 김정래 석유공사 사장은 이날 오후3시30분쯤 서울 여의도 국회에서 산업통상자원위원회 종합 국정감사를 받던 중 급히 울산으로 떠났다..\n",
      "\n",
      "5.97\t[제목]:공공기관 배당 20.4% 줄어…정부 '혁신성장 재투자하라'\n",
      "정부가 출자한 공공기관 배당금이 지난해보다 20.4% 줄었다. 정부가 정책적으로 배당규모을 줄인 탓이다. 대신 배당금조정을 통해 확보한 자금을 경제활력을 위해 재투자하라는 것이 정부 의지다.. . 기획재정부는 lh 등 38개 정부 출자기관 중 올해 배당이 결정된 21개 출자기관의 배당금 1조4382억원을 수납했다고 21일 밝혔다. 한국방송광고진흥공사, 한국석유공사 등 17개 기관은 당기순손실 및 이월결손 보전 등으로 올해 배당에서 제외했다.. . 출자기관이 납부한 배당금은 일반회계에 6696억원, 특별회계 및 기금에 7685억원이 각각 세입 수입으로 처리된다. 올해 정부 배당금은 지난해와 비교해 3679억원(20.4%) 감소했다. 평균 배당성향도 34.98%에서 32.48%로 2.5%포인트 낮아졌다.. . 이는 기업구조조정, 혁신성장투자, 환경안전투자, 수출경쟁력 강화 등 등 출자기관의 경제활력 투자에 필요한 재원 마련을 지원하기 위해 1.5조원 규모의 배당금을 조(이름)다.. . 주요 공공기관별 배당금 규모를 보면 lh는 지난해보다 29.0% 줄어든 3325억원을 배당했고 인천공항공사도 20.5% 줄어든 3755억원을 배당했다. 중소기업은행은 9.4% 줄어든 1872억원을, 한국산업은행 1.5% 줄어든 1449억원을 수납했다. . . 반면 한국자산관리공사는 59.3% 증가한 146억원을 배당했고 인천항만공사도 12.0% 늘어난 56억원을 배당했다. 지난해 배당을 하지 않았던 한국가스공사와 대한무역투자진흥공사는 각각 328억원과 43억원을 배당했다. . . 지난해 배당했던 한국방송공사(46억원)와 한국농어촌공사(94억원), 한국전력공사(923억원), 한국관광공사(23억원), 한국교육방송공사(20억원) 등은 올해엔 배당을 하지 않았다.. . 기재부 관계자는 '정부는 출자기관이 배당금 조정으로 확보한 재원을 활용해 경제활력 제고에 적극적 역할을 수행할 수 있도록 집행 점검 등 철저한 사후 관리를 실시할 예정'이라고 말했다..\n",
      "\n",
      "5.40\t[제목]:한전, 빅데이터 활용 '에너지 新서비스' 찾는다\n",
      "한국전력공사는 다음달 28일 서울시 서초동 한전 아트센터에서 '전력데이터 활용 신(新)서비스 개발 경진대회'를 개최한다고 11일 밝혔다.. . 산업통상자원부가 주최하고 한전이 주관하는 이번 행사는 한전의 전력 빅데이터를 활용하는 새로운 사업 모델을 발굴하기 위해 열렸다.. . 참가를 희망하는 일반시민과 학생, 벤처기업과 스타트업은 11일부터 24일까지 한전 홈페이지에서 제안서를 다운로드해 작성, 제출하면 된다. . . 1차 서류심사를 통과한 팀 또는 개인은 한전 아트센터에 마련된 데이터 공유센터에서 1개월간 서비스를 개발하게 된다. 서울시를 비롯한 공공기관과 민간데이터도 활용 가능하다. . . 심사는 완성도 기술성 상용화 가능성 확장성 등을 기준으로 사내 직원과 사외 전문위원이 함께 진행한다. 시상은 기업과 학생일반인 부문으로 나눠서 이뤄진다. 최종 선정 팀은 총 1400만원의 상금을 받고, 한전의 전력데이터 서비스 마켓(edsmarket)에 서비스를 등록하는 기회도 갖게 된다. . . 김종갑 한전 사장은 '한전의 전력데이터와 공공기관민간의 데이터를 융합해 보다 가치 있는 대국민 편익서비스를 많이 만들어 내길 기대한다'며 '데이터 공유센터와 전력데이터 서비스 마켓을 통해 데이터 분야 에너지신산업 지원에 최선을 다하겠다'고 밝혔다. . .\n",
      "\n",
      "5.32\t[제목]:남동발전 '영흥발전 사고 깊은 유감…근본대책 마련 하겠다'\n",
      ". 한국남동발전이 지난달 28일 발생한 추락 사망사고와 관련 '깊은 유감을 표한다'며 '다시는 이같은 사고가 발생하지 않도록 근본적 대책을 마련함과 동시에 진행중인 경찰과 고용노동부 조사에도 책임있는 자세로 임할 것을 약속한다'고 밝혔다.. . 남동발전은 이날 설명자료를 내고 이같이 밝혔다. 남동발전에 따르면 화물차 기사 심씨는 지난달 28일 오후 1시1분 인천시 옹진군 영흥화력발전소 현장에서 일하던 도중 높이 3.5m 화물차 적재함 문에서 지상으로 추락했다.. . 5분 뒤인 오후 1시6분 후속 차량 운전자와 설비 운(이름) 사고 현장에 도착해 119에 신고했고, 오후 1시14분부터 자체 소방대와 119 구급대가 도착할 때까지 심폐소생술을 실시했지만 심씨는 숨을 거뒀다.. . 남동발전은 '이번 사고로 유명을 달리하신 재해자 분과 사랑하는 가족을 잃은 유가족 분들에게 진심으로 사과와 위로의 말씀을 드린다'며 '재해자가 사망에 이르게 돼 매우 안타깝게 생각한다'고 밝혔다.. . 남동발전은 '사고현장의 안전시설과 관련 현장에는 안전계단, 안전난간 등이 있었다'며 '하지만 그럼에도 다시 한 번 현장을 면밀히 점검해 사고예방을 위한 철저한 대책을 마련하겠다'고 했다.. . 고용노동부와 경찰은 해당 사고 관련 안전시설 유무 등에 대해 조사하고 있다. . . 유가족과 노조는 진상규명과 사과를 요구하고 있다. 사망한 심씨의 아들은 전일 기자회견에서 '오늘 사고 현장 방문과 cctv 열람을 통해 그동안 아버지가 얼마나 열악한 환경에서 근무하게 됐는지 새삼 확인했다'며 '화력발전소 측은 제대로 된 사고 경위에 대한 설명도 없이, 모든 책임을 안전규칙을 지키지 않고 일한 아버지에게 전가하려 한다'고 밝혔다.. .\n",
      "\n",
      "5.18\t[제목]:'오토바이 법규위반, 보도통행 가장 많아'\n",
      "오토바이 등 이륜차 법규위반 중 보도통행이 가장 많은 것으로 나타났다.. . 21일 한국교통안전공단에 따르면 지난 5월부터 7월까지 3개월간 접수된 이륜차 법규위반 관련 공익제보 중 보도통행이 21%로 가장 많았다. . . 다음으로 신호위반 19.1%, 안전모 미착용 14.6%, 중앙선침범 5.3% 순으로 집계됐다.. . 공단은 국토교통부와 함께 이륜차 교통사고 감소를 위해 교통안전 공익제보단을 운영하고 있다. 시민 2000명으로 구성돼 있으며, 공익제보 활동을 통해 이륜차 안전운행을 유도하고 있다.. . 공단은 이륜차 교통안전 공익제보단 운영을 1개월 연장키로 했다. 신종 코로나바이러스 감염증(코로나19) 재확산으로 사회적 거리두기가 계속되면서 배달이 많아지는 현실을 반영한 것이다.. . (이름) 공단 이사장은 '이륜차의 불법행위 사각지대를 줄여나가는 것도 중요하지만, 무엇보다 이륜차 배달 종사자 스스로가 교통법규를 지켜 안전하게 운행하는 것이 가장 중요하다'고 말했다..\n",
      "\n",
      "embedding time: 55.72 ms\n",
      "search time: 87.38 ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "INDEX_NAME = 'aihub-ts1-acsampe-klue-sbert-v1-mpower10u-128d-onnx'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "#INDEX_NAME = 'korquad-klue-sbert-v1.0-noavg' # 요약문 평균값 처리 안한경우\n",
    "\n",
    "SEARCH_SIZE = 5\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 추가하가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquad'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "title = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시'\n",
    "        ]\n",
    "\n",
    "paragraph = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "# 2. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(title, paragraph):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8054-dfc6-4289-8309-8f0710b16771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES 데이터 조회하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "###########################################################\n",
    "# 인덱스내 데이터 조회 => query 이용\n",
    "###########################################################\n",
    "def search(index, data=None):\n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    res = es.search(index=index, body=body)\n",
    "    return res\n",
    "###########################################################\n",
    "\n",
    "# 모든 데이터 조회\n",
    "#sr = search(index=INDEX_NAME)\n",
    "#pprint.pprint(sr)\n",
    "\n",
    "# 단일 필드 조회\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7fc52-530f-46f4-ae83-2ccca3fd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 삭제하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 삭제 => query 이용\n",
    "############################################################\n",
    "def delete(index, data):\n",
    "    if data is None:  # data가 없으면 모두 삭제\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    return es.delete_by_query(index, body=body)\n",
    "\n",
    "############################################################\n",
    "## 2: 인덱스 내의 데이터 삭제 => id 이용\n",
    "############################################################\n",
    "def delete_by_id(index, id):\n",
    "    return es.delete(index, id=id)\n",
    "\n",
    "############################################################\n",
    "## 3: 인덱스 자체 삭제\n",
    "############################################################\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# 1: query 이용 데이터 삭제\n",
    "delete(index=INDEX_NAME, data={'title':'한라산'})\n",
    "\n",
    "# 3: 인덱스 자체 삭제\n",
    "#delete_index(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659658-75cf-4339-a88a-db338bddfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 업데이트하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 업데이트=>_id 에 데이터 업데이트\n",
    "############################################################\n",
    "def update(index, id, doc, doc_type):\n",
    "    \n",
    "    body = {\n",
    "        'doc': doc\n",
    "    }\n",
    "    \n",
    "    res=es.update(index=index, id=id, body=body, doc_type=doc_type)\n",
    "    return res\n",
    "############################################################\n",
    "\n",
    "#=====================================================================\n",
    "# 검색해서, _id, _type을 구함\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print('\\n')\n",
    "print(\"===[검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    id = hits[\"_id\"]      # id\n",
    "    type = hits[\"_type\"]  # type\n",
    "    \n",
    "    print(f'id: {id}')\n",
    "    print(f'type: {type}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # update 시킴\n",
    "    print(\"===[업데이트]===\")\n",
    "    doc = {'paragraph': '제주도는 대한민국에 가장 남쪽에 있는 섬으로, 인구는 약 71만명이며, 화산섬으로 관광자원이 많은 천혜의 관광지 이다.'}\n",
    "    print(doc)\n",
    "    print('\\n')\n",
    "    \n",
    "    ur=update(index=INDEX_NAME, id=id, doc=doc, doc_type=type)\n",
    "    print(\"===[업데이트 결과]===\")\n",
    "    print(ur)\n",
    "    print('\\n')\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "# 인덱스 refresh 함\n",
    "# elasticsearch의 자동 새로고침의 시간은 1초 정도 소요\n",
    "# 따라서 코드에 아래 명령어를 입력하지 않았을 경우 검색을 하지 못할 가능성도 존재\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# 제주도로 검색해서 한번더 확인\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print(\"===[재검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    \n",
    "    print(f'id:{hits[\"_id\"]}')\n",
    "    print(f'type: {hits[\"_type\"]}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
