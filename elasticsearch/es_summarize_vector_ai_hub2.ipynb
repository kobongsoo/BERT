{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# - 문장들을  추출 요약해서 요약문장을 만들고, 요약 문장의 평균을 구하여 문장 embedding을 생성하여 ES에 인덱스에 vector 추가하고, 검색하는 예제임\n",
    "# - 말뭉치는 ai_hub에 원천말뭉치인 ts1 말뭉치에 tilte, content를 추출하여, content 요약문과 title에 대해 각각 vector를 만들어서 ES 인덱스로 추가하는 예시임.\n",
    "#\n",
    "# => 대규모 웹데이터 기반 한국어 말뭉치 데이터 \n",
    "# 말뭉치 출처: https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624\n",
    "\n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "# - elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# - 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss\n",
    "\n",
    "# 추출 요약 설치\n",
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "import kss\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "# FutureWarning 제거\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, getListOfFiles\n",
    "device = GPU_info()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# 0. param 설정\n",
    "#------------------------------------------------------------------------------------\n",
    "seed = 111\n",
    "query_num = 500             # 쿼리 최대 갯수: KorQuAD_v1.0_dev.json 최대값은 5533개임, 0이면 모든 5533개 쿼리함.\n",
    "search_k = 5                # FAISS 검색시, 검색 계수(5=쿼리와 가장 근접한 5개 결과값을 반환함)\n",
    "avg_num = 1                 # 쿼리에 대해 sub 문장들중 최대 scorce를 갖는 문장을 몇개 찾고 평균낼지.(3=쿼리에 가장 유사한 sub문장 3개를 찾고 평균을 냄)\n",
    "faiss_index_method = 0      # 0= Cosine Similarity 적용(IndexFlatIP 사용), 1= Euclidean Distance 적용(IndexFlatL2 사용)\n",
    "\n",
    "# 토큰 임베딩 관련 param\n",
    "IS_EMBED_DIVIDE = True      #여기서는 토큰 임베딩은 True고정, True=문단의 여러 문장을, 토큰 단위로 분리후 벡터 구해서 인덱스 만듬/False=문단의 여러문장을 하나의 벡터를 구해서 인덱스 만듬.\n",
    "EMBED_DIVIDE_LEN = [5,7,9]  #5 # 문장을 몇개(토큰)으로 분리할지 (7,8,10) 일때 성능 좋음=>50.8%, (5,7,9) 일때 차원축소 128=>41.80%(81.8%) 성능 좋음\n",
    "MAX_TOKEN_LEN = 40          # 최대 몇개 token까지만 임베딩 할지\n",
    "\n",
    "# 차원 축소 관련 param\n",
    "# 차원 축소 할지, 768 값을 앞에 384 만큼만 배열 resize 시킴.  \n",
    "# - 384로 줄일대 -2% 성능 저하 발생(512: -1.2%, 256: -6%)\n",
    "DIM_RESIZE_METHOD = 2  # 0=차원축소 안함/1=resize 로 차원축소/2=Dense 모델 이용 차원축소\n",
    "DIM_RESIZE_LEN = 128\n",
    "\n",
    "# ONNX 모델 사용\n",
    "IS_ONNX_MODEL = True        # True=onnx 모델 사용\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabad516-e286-46a4-8643-09058c504919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n###########################################################\\n# 인덱스 생성/삭제\\n###########################################################\\n## 인덱스 생성\\ndef create_index(index, mapping=None):\\n    if not es.indices.exists(index=index):\\n        return es.indices.create(index=index ,body=mapping)\\n    \\n## 인덱스 자체 삭제\\ndef delete_index(index):\\n    if es.indices.exists(index=index):\\n        return es.indices.delete(index=index)\\n    \\n###########################################################\\n# 인데스에 데이터 추가 \\n###########################################################\\ndef insert(index, doc_type, body):\\n    return es.index(index=index, doc_type=doc_type, body=body)\\n\\n# index 생성\\nINDEX_NAME = 'my_index_128_5-1'\\nres=create_index(index=INDEX_NAME, mapping=mapping)\\nprint(res)\\nprint('\\n')\\n\\nresult=es.indices.get_settings(index=INDEX_NAME)\\nprint(result)\\nprint('\\n')\\n\\n\\n# document 추가 \\nDOC_TYPE = '_doc'\\n\\n# define the dense vector\\ndense_vector1 = np.random.rand(128).tolist()\\ndense_vector2 = np.random.rand(128).tolist()\\ndense_vector3 = np.random.rand(128).tolist()\\ndense_vector4 = np.random.rand(128).tolist()\\ndense_vector5 = np.random.rand(128).tolist()\\n\\ndoc = {\\n    'vector1': dense_vector1,\\n    'vector2': dense_vector2,\\n    'vector3': dense_vector3,\\n    'vector4': dense_vector4,\\n    'vector5': dense_vector5\\n}\\n\\nres = insert(index=INDEX_NAME, doc_type=DOC_TYPE, body=doc) \\nprint(res)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 엘라스틱 서치에 인덱스 생성 하고 dense 벡터 128차원 5개 추가하는 예제임\n",
    "'''\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "\n",
    "# define the Elasticsearch client\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "\n",
    "mapping='''\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 2,\n",
    "    \"number_of_replicas\": 1\n",
    "  },\n",
    "   \"mappings\": {\n",
    "    \"dynamic\": \"true\",\n",
    "    \"_source\": {\n",
    "      \"enabled\": \"true\"\n",
    "    },\n",
    "    \"properties\": {\n",
    "      \"vector1\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "\t  \"vector2\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "      \"vector3\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "      \"vector4\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "      \"vector5\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "###########################################################\n",
    "# 인덱스 생성/삭제\n",
    "###########################################################\n",
    "## 인덱스 생성\n",
    "def create_index(index, mapping=None):\n",
    "    if not es.indices.exists(index=index):\n",
    "        return es.indices.create(index=index ,body=mapping)\n",
    "    \n",
    "## 인덱스 자체 삭제\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "    \n",
    "###########################################################\n",
    "# 인데스에 데이터 추가 \n",
    "###########################################################\n",
    "def insert(index, doc_type, body):\n",
    "    return es.index(index=index, doc_type=doc_type, body=body)\n",
    "\n",
    "# index 생성\n",
    "INDEX_NAME = 'my_index_128_5-1'\n",
    "res=create_index(index=INDEX_NAME, mapping=mapping)\n",
    "print(res)\n",
    "print('\\n')\n",
    "\n",
    "result=es.indices.get_settings(index=INDEX_NAME)\n",
    "print(result)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# document 추가 \n",
    "DOC_TYPE = '_doc'\n",
    "\n",
    "# define the dense vector\n",
    "dense_vector1 = np.random.rand(128).tolist()\n",
    "dense_vector2 = np.random.rand(128).tolist()\n",
    "dense_vector3 = np.random.rand(128).tolist()\n",
    "dense_vector4 = np.random.rand(128).tolist()\n",
    "dense_vector5 = np.random.rand(128).tolist()\n",
    "\n",
    "doc = {\n",
    "    'vector1': dense_vector1,\n",
    "    'vector2': dense_vector2,\n",
    "    'vector3': dense_vector3,\n",
    "    'vector4': dense_vector4,\n",
    "    'vector5': dense_vector5\n",
    "}\n",
    "\n",
    "res = insert(index=INDEX_NAME, doc_type=DOC_TYPE, body=doc) \n",
    "print(res)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---bi_encoder---------------------------\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 128, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n",
      "Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "---dense param---------------------------\n",
      "*dense_weight:torch.Size([128, 768])\n",
      "*dense_bias:torch.Size([128])\n",
      "---onnx_model---------------------------\n",
      "<optimum.onnxruntime.modeling_ort.ORTModelForFeatureExtraction object at 0x000001CF5534E2B0>\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 1. 검색모델 로딩\n",
    "# => bi_encoder 모델 로딩, polling_mode 설정\n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import bi_encoder, dense_model, onnx_model\n",
    "\n",
    "# bi_encoder 모델 로딩\n",
    "bi_encoder_path = \"bongsoo/klue-sbert-v1\"\n",
    "pooling_mode = 'mean' # bert면=mean, albert면 = cls\n",
    " # 출력 임베딩 크기 지정 : 0=기본 모델 임베딩크기(768), 예:128=128 츨력임베딩 크기 \n",
    "out_dimension = 128 if DIM_RESIZE_METHOD == 2 else 0 if DIM_RESIZE_METHOD != 2 else None\n",
    "    \n",
    "word_embedding_model1, bi_encoder1 = bi_encoder(model_path=bi_encoder_path, max_seq_len=512, do_lower_case=True, \n",
    "                                              pooling_mode=pooling_mode, out_dimension=out_dimension, device=device)\n",
    "print(f'---bi_encoder---------------------------')   \n",
    "print(bi_encoder1)\n",
    "print(word_embedding_model1)\n",
    "\n",
    "print(f'---dense param---------------------------')   \n",
    "# 출력 값 차원 축소 지정인 경우, token_embeddings 일때는 sentencebert 라이브러리를 이용하여 dense_model 모델 추가할수 없으므로,\n",
    "# 사용자정의 dense_model을 정의해서, 가중치와 bias를 bi_encoder모델에 것을 얻어와서 적용해서 차원 죽소함.\n",
    "# => resize 방식 보다 성능 떨어지지만, 128일때는 더 성능이 좋음\n",
    "if DIM_RESIZE_METHOD == 2:\n",
    "    #-------------------------------------------------------------------------\n",
    "    # 처음에는 아래 코드를 활용하여 해당 모델의 128 weight와 bias를 저장해 두어야 함.\n",
    "    #state_dict = bi_encoder1.state_dict()  # bi_encoder모델이 state_dict 얻어옴\n",
    "    #print(state_dict.keys())\n",
    "    #dense_weight = state_dict['2.linear.weight'] # denser 모델에 bi_encoder모델이 linear.weight 적용\n",
    "    #dense_bias = state_dict['2.linear.bias']     # denser 모델에 bi_encoder모델이 linear.bias 적용\n",
    "    \n",
    "    # 처음에  weigth, bias 파일을 저장해 둠.\n",
    "    #torch.save(dense_weight, 'klue-sbert-v1-weigth.pt')\n",
    "    #torch.save(dense_bias, 'klue-sbert-v1-bias.pt')\n",
    "    #-------------------------------------------------------------------------\n",
    "    # weigth, bias 저장해둔 파일 로딩\n",
    "    dense_weight = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-weight-128.pt')\n",
    "    dense_bias = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-bias-128.pt')\n",
    "\n",
    "    print('*dense_weight:{}'.format(dense_weight.size()))\n",
    "    print(f'*dense_bias:{dense_bias.size()}')\n",
    " \n",
    "# onnx 모델 로딩\n",
    "if IS_ONNX_MODEL == True:\n",
    "    onnx_model_path = \"bongsoo/klue-sbert-v1-onnx\"#\"bongsoo/klue-sbert-v1-onnx\"\n",
    "    onnx_tokenizer, onnx_model = onnx_model(onnx_model_path)\n",
    "    print(f'---onnx_model---------------------------')\n",
    "    print(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b67318-d790-422c-a469-874a1b5b7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 안덱싱 및 검색 조건에 맞게 임베딩 처리 하는 함수 \n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import embed_text, onnx_embed_text\n",
    "\n",
    "def embedding(paragrphs:list):\n",
    "    if IS_ONNX_MODEL == True:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs) \n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs, token_embeddings=False)\n",
    "    else:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, token_embeddings=True, return_tensor=False)\n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, return_tensor=False)  \n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb0068d-6e57-430a-8142-19a05fa9e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 안덱싱 처리\n",
    "#-------------------------------------------------------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "import kss\n",
    "from myutils import embed_text, divide_arr_avg_exten, clean_text\n",
    "\n",
    "# 인덱싱 함수 \n",
    "def index_data(createindex:bool = True):\n",
    "    \n",
    "    if createindex == True:\n",
    "        es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "        count = 0\n",
    "        # 인덱스 생성\n",
    "        with open(INDEX_FILE) as index_file:\n",
    "            source = index_file.read().strip()\n",
    "            count += 1\n",
    "            #print(f'{count}:{source}') # 인덱스 구조 출력\n",
    "            es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    # json 파일들이 있는 폴더에 .json 파일 이름들을 얻기\n",
    "    # =>DATA_FOLDER: .JSON파일들이 있는 폴더\n",
    "    files = getListOfFiles(DATA_FOLDER)\n",
    "    assert len(files) > 0 # files가 0이면 assert 발생\n",
    "    print('*file_count: {}, file_list:{}'.format(len(files), files[0:5]))\n",
    " \n",
    "    for idx, file in enumerate(tqdm(files)):\n",
    "        if \".json\" not in file:  #.json 파일이 아니면 합치지 않음\n",
    "            continue\n",
    "            \n",
    "        count = 0\n",
    "        docs = []\n",
    "    \n",
    "        # json 파일 로딩 => [SJML][text] entry만 불러옴\n",
    "        json_data = json.load(open(file, \"r\", encoding=\"utf-8\"))['SJML']['text']\n",
    "        for data in json_data:\n",
    "        #for data in json_data:\n",
    "            count += 1\n",
    "            doc = {} #dict 선언\n",
    "            \n",
    "            doc['title'] = data['title']            # 제목 설정\n",
    "            doc['paragraph'] = data['content']      # 문장 담음.\n",
    "                \n",
    "            docs.append(doc)\n",
    "            #print(f'count:{count}')\n",
    "            #print(doc['title'])\n",
    "            \n",
    "            if count % BATCH_SIZE == 0:\n",
    "                index_batch(docs)\n",
    "                docs = []\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "            # ** 10 개만 보냄\n",
    "            #if count >= 10:\n",
    "            #   break\n",
    "            \n",
    "        if docs:\n",
    "            index_batch(docs)\n",
    "            print(\"Indexed {} documents.\".format(count))   \n",
    "            \n",
    "        es.indices.refresh(index=INDEX_NAME)\n",
    "            \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    #print(\"=== End Done indexing===\")\n",
    "                   \n",
    "\n",
    "def index_batch(docs):\n",
    "        \n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        title = doc['title']\n",
    "        paragraph = doc['paragraph']\n",
    "\n",
    "        sub_contexts = []\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        paragraph = clean_text(paragraph)  # 전처리 : (한글, 숫자, 영문, (), {}, [], %, ,,.,\",')  등을 제외한 특수문자 제거\n",
    "        \n",
    "        # 입력 문단 길이가 999개 크면, 속도가 느려지므로 최대 999개 까지만 문자 입력 받음.\n",
    "        if len(paragraph) > 999:\n",
    "            #print(f'paragraph_len:{len(paragraph)}')\n",
    "            paragraph = paragraph[0:999]\n",
    "            \n",
    "        # 입력 문단을 여러 문장들로 나눔.\n",
    "        #sentences = [sentence for sentence in paragraph.split('.') if sentence != '' and len(sentence) > 10]  # '.'(마침표) 로 구분해서 sub 문장을 만듬.\n",
    "        #sentences = [sentence for sentence in kss.split_sentences(paragraph) if sentence != '' and len(sentence) > 10] # kss 이용해서 sub 문장을 만듬\n",
    "        \n",
    "        # 최대 10개 문장만 추출함.\n",
    "        sentences = []\n",
    "        count = 0\n",
    "        for sentence in kss.split_sentences(paragraph):\n",
    "            if sentence != '' and len(sentence) > 10:\n",
    "                sentences.append(sentence)\n",
    "                if count > 10:\n",
    "                    break\n",
    "                    \n",
    "        # 만약 sentences(sub 문장) 가 하나도 없으면 원본문장을 담고, 10이상이면  10개만 담음.\n",
    "        sub_contexts.append([paragraph] if len(sentences) < 1 else sentences[0:10] if len(sentences) > 10 else sentences)\n",
    "       \n",
    "        if i < 1:\n",
    "            print(sub_contexts[0])\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # 토큰 분할 임베딩 처리\n",
    "        # => sub_contexts은  1차원 리스트 임 (예:['오늘 비가 온다','오늘 눈이 온다','날씨가 좋다'])\n",
    "        token_embeds = embedding(sub_contexts[0])\n",
    "        #------------------------------------------------------------------------------------------------------------------------ \n",
    "        # 토큰 분할인 경우 처리 start=>           \n",
    "        token_embed_arr_list = []\n",
    "        tcount = 0\n",
    "        # tensor(250,768) 한문장 토큰 임베딩 얻어와서, 각 ?개 토큰씩 평균을 구함.\n",
    "        for token_embed in token_embeds:\n",
    "            token_embed = token_embed[1:-1] # 맨앞에 [CLS]와 맨뒤에 [SEP] 토큰은 뺌\n",
    "            if tcount >= MAX_TOKEN_LEN: \n",
    "                break\n",
    "             \n",
    "            #print(f'token_embed.shape:{token_embed.shape}')\n",
    "            \n",
    "            token_embed_arrs = token_embed.cpu().numpy().astype('float32')\n",
    "            #print(f'token_embed_arrs:{token_embed_arrs.shape}')\n",
    "            # 5,7,9 씩 자르면서 문장 토큰 평균을 구함\n",
    "            token_embed_divide_arrs = divide_arr_avg_exten(embed_arr=token_embed_arrs, divide_arrs=EMBED_DIVIDE_LEN) \n",
    "\n",
    "             # Dense 방식으로 차원 축소 => 평균 구한후 차원 축소하는 방식이 0.6% 정도 성능 좋음\n",
    "            if DIM_RESIZE_METHOD == 2:\n",
    "                tmp1 = torch.Tensor(token_embed_divide_arrs)\n",
    "                #tmp1 = torch.from_numpy(token_embed_divide_arrs)\n",
    "                debug1 = False\n",
    "                tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias, debug=debug1)\n",
    "                arrs = tmp2.detach().numpy().astype('float32')\n",
    "            else:  \n",
    "                arrs = token_embed_divide_arrs\n",
    "                \n",
    "            # 평균 구한 토큰들을 token_embed_arr_list 리스트에 담아둠.(50보다 크면 50개만 담음)           \n",
    "            for idx, arr in enumerate(arrs):\n",
    "                \n",
    "                 # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "                if DIM_RESIZE_METHOD == 1:\n",
    "                    darr = np.resize(arr, (DIM_RESIZE_LEN,))\n",
    "                else:\n",
    "                    darr = arr\n",
    "                    \n",
    "                token_embed_arr_list.append(darr)\n",
    "                tcount +=1\n",
    "                if tcount >=MAX_TOKEN_LEN:\n",
    "                    break\n",
    "\n",
    "        #embeddings = np.array(token_embed_arr_list)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # ES에 문단 인덱싱 처리\n",
    "        request = {}  #dict 정의\n",
    "        request[\"rfile_name\"] = title       # 제목               \n",
    "        request[\"rfile_text\"] = paragraph   # 문장\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"        \n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        # for문 돌면서 벡터 처리\n",
    "        #print(type(token_embed_arr_list))\n",
    "        #print(len(token_embed_arr_list))\n",
    "        \n",
    "        # vector 1~40 까지 값을 0으로 초기화 해줌.\n",
    "        for i in range(MAX_TOKEN_LEN):\n",
    "            if DIM_RESIZE_METHOD > 0:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((DIM_RESIZE_LEN))\n",
    "            else:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((768))\n",
    "            \n",
    "        # vector 값들을 담음.\n",
    "        for i, token_embed_arr in enumerate(token_embed_arr_list):\n",
    "            request[\"vector\"+str(i+1)] = token_embed_arr\n",
    "            \n",
    "        requests.append(request)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'node-1', 'cluster_name': 'mpower', 'cluster_uuid': '2TN-XaLkReC_BmRoReVUfw', 'version': {'number': '7.17.3', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': '5ad023604c8d7416c9eb6c0eadb62b14e766caff', 'build_date': '2022-04-19T08:11:19.070913226Z', 'build_snapshot': False, 'lucene_version': '8.11.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n",
      "*file_count: 10, file_list:['../../../data11/ai_hub/ts1/sample2/BWSC217000049039.json', '../../../data11/ai_hub/ts1/sample2/BWSC217000049056.json', '../../../data11/ai_hub/ts1/sample2/BWSC217000049067.json', '../../../data11/ai_hub/ts1/sample2/BWSC217000049075.json', '../../../data11/ai_hub/ts1/sample2/BWSC217000049087.json']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac15620291f489aa78eeb39eb4ff743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12561b187f5d4f24a5516804c56bb489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"(이름) 장군의 명량대첩을 소재로 한 대작 영화 '명량'이 '아바타'를 제치고 한국 역대 영화 흥행 1위를 기록하면서 환하게 웃는 통신사가 있다.\", '아닌 업계 유일하게 영화 제작에 투자한 sk브로드밴드다. . .', \"영화 '명량'이 개봉 15일 만에 1200만명을 돌파하고 급기야 '아바타'가 세운 역대 흥행기록(1330만)마저 갈아치우면서 덩달아 투자사들도 '초대형 대박'을 앞둔 셈이다.. .\", \"'명량' 총 제작비 189억원 가운데 sk브로드밴드가 콘텐츠펀드를 통해 투자한 금액은 수십억원에 달한다.\", '영화 유통이 종료돼봐야 수 있지만, 투자 수익금은 원금의 최소 두배 이상을 가뿐히 넘길 것으로 점쳐지고 있다. . .', 'sk브로드밴드의 지난 2분기 당기 순이익이 8억원인 점을 감안한다면, 적잖은 영업외 수익을 얻게 되는 셈이다.', '다만 콘텐츠펀드가 다른 영화작품에도 투자되고 있어, 연말 결산 펀드투자 수익금은 차이가 있을 수 있다.. .', \"'명량' 흥행돌풍에 따른 홍보효과도 만만치 않다.\", '영화가 상영될 때마다 오프닝과 엔딩 크레딧에 회사로고와 (이름) 사장 이름이 올라간다.', '메인포스터 하단에도 회사 로고가 들어가 있다. . .']\n",
      "Indexed 95 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641996818fb345e9bc4918bec04fd2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sk텔레콤은 가족결합 이용자에게 제공하는 't가족 포인트'를 오는 16일부터 종료한다고 13일 밝혔다.. .\", 't가족포인트는 가족들이 이용 요금에 따라 새 휴대폰 구매 시 사용할 수 있는 포인트를 제공하는 것으로, 유사보조금에 해당된다는 지적이 있어왔다.. .', 'skt는 이같은 법률적 문제를 감안해 포인트 제도를 종료하기로 했으며, 기존 이용 고객에게는 5월17일까지 포인트를 적립받을 수 있게 할 예정이다.', '적립된 포인트는 앞으로 3년간 사용할 수 있다.. .', 't가족 포인트를 제외한 무한멤버십, 가족간 데이터 생성 및 공유, 고객별 가입된 유무선 결합상품 혜택 등 가족고객이 받는 혜택은 그대로 유지된다.. .', \"skt 관계자는 '단말 구입 시 활용하는 포인트는 유사 지원금에 해당될 수 있어 부득이하게 종료하게 되었다'며 '고객의 실질적인 혜택을 강화한 새로운 상품과 서비스를 선보일 계획'이라고 말했다..\"]\n",
      "Indexed 99 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d7d5d9481542d4aaf4a76b45d4973b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['삼성전자가 차기 스마트폰 갤럭시s6에 사활을 걸고 있는 가운데 이에 대한 광고와 마케팅을 전담하는 제일기획이 수혜를 입을 것이란 기대감이 커지고 있다.', '제일기획 주가도 올 들어 18% 급등하면서 이에 반응하는 모습이다. . .', \"22일 전자금융투자업계에 따르면 삼성전자는 다음달 1일 '2015 mwc(모바일월드콩그레스)' 개막 직전에 '갤럭시s6'를 공개하고 상반기 이에 대한 마케팅에 총역량을 집중할 예정이다. . .\", '제일기획은 현재 삼성전자를 포함한 삼성그룹 전 계열사 물량의 80%를 맡고 있다.', '특히 전체 매출 중 약 70%는 삼성전자와 관련된 마케팅 활동에서 발생하기 때문에 삼성전자 신제품이나 실적에 직접적 영향을 받을 수밖에 없다. . .', '실제 삼성전자가 국내와 유럽시장에서 부진했던 지난해 3분기 제일기획은 충격파를 벗어나지 못했다.', '당시 제일기획 매출은 전년 동기대비 6.9% 줄었고 영업이익과 순이익도 각각 31.2%, 27.1% 감소했다.', '특히 제일기획 유럽시장 영업총이익은 254억원으로 전년 대비 15%가량 줄었다. . .', \"업계에서는 삼성전자가 '갤럭시s6'를 발판으로 점유율을 회복하기 위해 전작 대비 마케팅을 대폭 강화할 것으로 내다보고 있다. 지난해 삼성전자의 세계 스마트폰 시장 점유율이 3년만에 가장 낮은 수준으로 떨어지면서 차기작 성공에 대한 절박감이 그 어느 때보다 크기 때문이다.. . 특히 애플은 '아이폰6' 효과로 4분기 사상 최대 판매량과 실적을 갈아치웠고, 레노버는 작년 스마트폰 출하량 1억대로 3위 자리를 확고히 하며 삼성을 맹추격하는 상황.\", '삼성전자가 올해 갤럭시s6 성공에 어느 때보다 전력할 수밖에 없다.. .']\n",
      "Indexed 100 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82646088ca4042948bccdf374b45bf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'저성장 늪에 빠진 한국 경제에 4차 산업혁명이 기회다.'\", 'kt 회장이 4차 산업혁명의 가능성을 강조하며 대기업 및 중소기업 간의 협력을 국내 최고경영자들에게 제안했다.', '4차 산업혁명은 제조부터 금융, 에너지, 헬스케어 등 전 산업 분야가 ict(정보통신기술)와 융합하면서 새로운 산업과 가치를 만들게 되는 것을 말한다.. . (이름)', \"회장은 18일 서울 중구 (이름)텔에서 한국경영자총협회(경총)가 주최한 '전국 최고경영자 연찬회'에서 기(이름)에 나섰다.\", '39회를 맞은 이번 행사는 최고경영자들이 모여 급변하는 국내외 경영환경을 예측하고 지속가능경영의 실천적 방향을 모색하는 취지로 열렸다.. .', \"황 회장은 '안개 속 한국경제, 등대를 찾아라'는 이번 행사의 주제에 맞게 4차 산업혁명을 화두로 강연을 이어갔다.\", \"독일의 '인더스트리4.0', 중국의 '중국제조 2025', 일본의 '일본재흥전략' 등 4차 산업혁명의 주도권을 잡기 위한 세계 각국의 노력을 소개하며 국제 경제의 트렌드를 전했다.. .\", \"황 회장은 '지난달 열린 다보스포럼에서도 4차 산업혁명을 주제로 내걸고 사물인터넷(iot), 인공지능, 나노기술, 3d 프린터, 빅데이터 등 기술 진보가 산업 전반에 가져올 변화를 조감했다'고 전했다.\", \"이어서, '4차 산업혁명을 통한 성장의 기회를 잡기 위해 대기업들은 자신이 속한 산업의 벽을 뛰어넘는 도전적 협력으로 융합사업을 성공시켜야 한다'고 강조했다. . . 창의성을 가진 중소기업의 해외 진출을 지원하는 협력 방안도 제안했다. 전국 창(이름)혁신센터의 인프라를 활용해 한국형 히든 챔피언 'kchamp(케이챔프)'를 만들어야 한다는 것.. . 황 회장은 '4차 산업혁명이라는 거대한 변화를 잘 활용하면 현재의 위기를 기회로 바꾸고 글로벌 1위로 도약할 수 있다'며 'kt는 세계 최고의 네트워크 인프라와 서비스를 제공해 국내 기업들이 4차 산업혁명의 글로벌 리더가 될 수 있도록 지원하겠다'고 밝혔다..\"]\n",
      "Indexed 97 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeda45478e2946bc9614a4d90f891147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk텔레콤이 지난 2014년 인수했던 보안기업 nsok 지분을 또다른 자회사인 sk텔링크로 넘긴다. . .', 'sk텔레콤과 sk텔링크는 지난 22일 각각 이사회를 열고, sk텔레콤이 보유한 자회사 nsok의 지분(83.93%)을 sk텔링크로 현물 출자하고 sk텔링크의 보통주 21만9967주(2.4%)를 취득하기로 결의했다고 23일 밝혔다.. .', 'sk텔레콤은 보유한 nsok 잔여지분(16.07%)에 대한 콜옵션을 sk텔링크로 이관하고, sk텔링크는 이를 행사해 nsok를 100% 자회사화할 예정이다.. .', 'sk텔링크는 보유한 인프라와 경영시스템을 nsok에 접목해 성장 추세에 있는 물리보안 사업을 차세대 성장동력으로 육성할 계획이다.', '또 상품과 유통망 결합, 알뜰폰(mvno)를 활용한 안심폰 서비스 도입 등 양사간 시너지 창출에 적극 나설 예정이다.. .', \"sk텔링크는 '알뜰폰 사업처럼 가입자 기반 사업을 단기간 내 성공적으로 안착시킨 경험과 노하우를 보유한 만큼 물리보안 사업에서도 의미있는 성과가 기대된다'고 설명했다.. .\", '한편, sk텔레콤은 지난 2014년 4월 nsok 인수 이후 연간 매출상승률이 47%에 달하는 등 물리보안 사업을 적극 추진해왔다.', '이번 현물출자 이후에도 sk텔링크와 nsok와 적극적인 협력을 통해 비디오 클라우드와 스마트 홈 등 ict(정보통신기술)와 결합한 물리보안 서비스를 고객들에게 제공할 예정이다.', '이번 현물출자는 법원인가를 거쳐 10월 중 완료될 예정이다.']\n",
      "Indexed 98 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b367f4b0817464bbd3f5137bbc5eb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자유한국당 의원의 친형이 자택에서 흉기에 찔려 숨진 채 발견돼 누리꾼들에게 충격을 주고 있다.. .', '27일 경찰에 따르면 이날 오전 9시30분쯤 경기도 구리시 수택동의 한 아파트에서 주모씨(62)가 숨져 있는 것을 동생이 발견해 경찰에 신고했다.. .', '주씨의 뒤통수에는 구타 흔적이, 등에는 흉기에 찔린 흔적이 발견됐다.', '숨진 주씨는 (이름) 의원 3형제의 맏형이다.. .', '현재 경찰은 타살 가능성이 크다고 보고 주변 폐쇄회로(cc)tv를 분석해 용의자를 찾고 있다. . .', '사건이 보도된 후 주 의원에게 이목이 집중되고 있다. . .', '경기도 양주군에서 태어난 주 의원은 고려대학교 법학과를 졸업, 제32회 사법시험에 합격했다.', '사법연수원 23기인 주 의원은 1994년부터 서울동부지방검찰청 검사로 활동하다 1999년부터 경기도 남양주시 고문 변호사로 활동했다.. .', '이후 2008년 (이름)당(현 자유한국당)에서 공천을 받고 국회에 입성한 주 의원은 20132014년 (이름) 전 대통령을 보좌했다. . .', \"또 2018 제7회 국회를 빛낸 '바른정치언어상'과 '으뜸언어상'을 수상한 바 있다.\"]\n",
      "Indexed 86 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fdb480298b4dd38ea43592b2a36cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘(6일)은 전국이 대체로 맑겠으나 해기차에 의해 새벽부터 오전 사이, 전남과 제주도에는 눈이 오다가 밤에 대부분 그치겠다. . .', '낮 동안 소강상태를 보이는 곳도 있겠으며 특히 전남(이름)은 오늘 오후까지, 제주도산지와 울릉도는 모레까지 눈이 이어지면서 많은 눈이 쌓이는 곳이 있겠다.', '내린 눈이 얼어 도로가 미끄러운 곳이 있겠으니 시설물 관리와 교통안전에 유의하기 바란다고 기상청은 밝혔다. . .', '기상청은 또 이번 추위가 당분간 이어지면서 중부지방과 일부 남부내륙에는 아침 기온이 영하 15도 이하로, 그밖의 지역은 영하 10도 이하로 떨어지는 곳이 있겠고 낮 기온도 영하권에 머물러 매우 춥겠으니 가축의 동사, 비닐하우스 작물의 동해, 수도관 동파 등 피해가 없도록 각별히 유의하라고 밝혔다. . .', '6일 아침 기온은 서울경기 14도 경남 9도 경북 10도 전남 9도 전북 11도 충남 12도 충북 12도 (이름)서 18도 (이름)동 11도 제주 1도 등으로 예상된다.. .', '낮 기온은 서울경기 6도 경남 0도 경북 2도 전남 3도 전북 3도 충남 4도 충북 5 (이름)서 5도 (이름)동 3도 제주 1도 등으로 전망된다.']\n",
      "Indexed 99 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee54b2180d149a3a059856917854846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"613 전국동시 지방선거에 '국민투표로또'가 돌아왔다.\", '국민투표로또는 (이름) 작가가 jtbc 썰전에서 투표율 증진의 방법으로 제안했던 것을 스타트업 개발자 및 디자이너가 실제 서비스로 만든 것. . .', \"지난 4일 새롭게 문을 연 '국민투표로또' 홈페이지에는 참여방법이 자세히 나온다.\", '국민투표로또는 후원금을 바탕으로 운영되며 후원금 가운데 운영비를 제외한 나머지 금액 모두 당첨금으로 사용된다. . .', '1, 2, 3등은 각각 후원금의 50%(최대 500만원), 20%(최대 200만원), 10%(최대 100만원)를 받는다.', '남은 돈은 5만원씩 다수의 4등 당첨자들에게 돌아간다.. .', '국민투표로또는 인증사진뿐만 아니라 투표소를 배경으로 한 사진, 투표 참여를 (이름)는 사진 등 선거를 즐기는 나만의 사진으로 응모 가능하다.. .', '응모방법은 사전투표 기간(6월 89일)과 투표일(6월13일)에 국민투표로또 홈페이지에 접속해 카카오톡 로그인 후 사진과 연락처를 입력하면 된다. . .', '투표 인증사진은 작년 1월 개정된 선거법으로 인해 투표 당일 특정 후보자 벽보 앞에서 지지나 반대 표시를 한 인증사진도 가능하다.', '손가락으로 엄지를 치켜세우거나 브이(v)도 할 수 있다.']\n",
      "Indexed 95 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72d81251acc409482935cb4a804ff2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['밤사이 수도권에 많은 비가 내리면서 서울 시내 주요 도로 일부가 통제됐다.', '특히 동부간선도로는 전 구간이 통제돼 출근길 시민들의 불편이 예상된다. . .', '30일 행정안전부에 따르면 오전 8시 현재 서울 용산구 한강 잠수교, 노원구 동부간선도로 등 70개소의 도로가 통제되고 있다.', '북한산과 설악산, 오대산 등 국립공원 3곳 121개 탐방로도 통제가 이뤄지고 있다..', '동부간선도로는 집중호우로 인해 전 구간을 교통 통제 중이다.', '강변북로에서 동부간선도로로 진입하는 지점부터 수락 지하차도까지 양방향이 전면 통제되고 있다.', '경찰은 동부간선도로 통제 해제시간을 이날 낮 12시로 공지했다. .', '전날 밤 9시부터 통제됐던 잠수교는 이날 오전 5시30분부터 차량 통행이 재개됐으나 여전히 보행자 통행은 불가능하다. .', '서울외곽도로 노고산2터널 퇴계원방향 부근에는 토사가 유출돼 복구 중이다.', '편도 4차로 중 3개 차로는 이날 오전 5시45분 복구가 끝나 일부 통행이 재개됐다.. .']\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'aihub-ts1-acsampe-klue-sbert-v1-mpower10u-128d-onnx-1'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "INDEX_FILE = './data/mpower10u_128d.json'                 # 인덱스 구조 파일\n",
    "DATA_FOLDER = '../../../data11/ai_hub/ts1/sample2/'     # 인덱스할 파일들이 있는 폴더경로 \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "## 입력 ##\n",
    "# GET korquad/_count\n",
    "\n",
    "## 출력 ###\n",
    "'''\n",
    "{\n",
    "  \"count\" : 1420,\n",
    "  \"_shards\" : {\n",
    "    \"total\" : 2,\n",
    "    \"successful\" : 2,\n",
    "    \"skipped\" : 0,\n",
    "    \"failed\" : 0\n",
    "}\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 검색 처리\n",
    "#-------------------------------------------------------------------------------------\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from myutils import embed_text, divide_arr_avg_exten\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    \n",
    "    # 문장으로 비교할때=>쿼리 문장에 대한 벡터 생성해서 비교\n",
    "    #query_vector = embed_text(model=bi_encoder,paragraphs=[query])[0]\n",
    "    #------------------------------------------------------------------\n",
    "    # 토큰 평균으로 비교할때=> 쿼리 문장에 대한 모든 토큰 벡터를 생성해서 비교\n",
    "    # 토큰 분할 임베딩 처리\n",
    "    token_query_embeds = embedding([query])\n",
    "    \n",
    "    token_query_embed_arr_list = []\n",
    "    # 쿼리 문장들의 토큰들의 평균을 구함.\n",
    "    for token_query_embed in token_query_embeds:\n",
    "        \n",
    "        token_query_embed = token_query_embed[1:-1] # 맨앞에 [CLS]와 맨두에 [SEP] 토큰은 뺌\n",
    "        print(f'*token_query_embed.shape:{token_query_embed.shape}')\n",
    "        \n",
    "        tmp = token_query_embed.cpu().numpy().astype('float32')\n",
    "        tmp=tmp.mean(axis=0) #평균 구함\n",
    "        \n",
    "         # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "        if DIM_RESIZE_METHOD == 1:\n",
    "            tmp = np.resize(tmp, (DIM_RESIZE_LEN,))\n",
    "         # Dense 방식으로 차원 축소=> 평균 구하기 전에 차원 축소하는것이 성능 +0.6더 좋음\n",
    "        elif DIM_RESIZE_METHOD == 2:\n",
    "            tmp1 = torch.Tensor([tmp]) # 1차원 배열을 -> 2차둰 텐서(1,768)로 변환\n",
    "            tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias)\n",
    "            tmp = tmp2.detach().numpy().astype('float32').ravel(order='C') # 1차원 배열로 변경(128,)        \n",
    "        \n",
    "        token_query_embed_arr_list.append(tmp)\n",
    "        \n",
    "    query_vector = np.array(token_query_embed_arr_list)[0]  # 리스트를 배열로 변환  \n",
    "    \n",
    "    # print(query_vector)\n",
    "    print(f'*query_vector.shape:{query_vector.shape}\\n')\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    '''\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector2']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴(doc['summarize_vector'])\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # 문단별 40개의 벡터와 쿼리벡터를 서로 비교하여 최대값 갖는 문단들중 가장 유사한  문단 출력\n",
    "    # => script \"\"\" 안에 코드는 java 임.\n",
    "    # => \"queryVectorMag\": 0.1905 일때 100% 일치하는 값은 9.98임(즉 10점 만점임)\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "                \"script\":{\n",
    "                    \"source\": \"\"\"\n",
    "                      float max_score = 0;\n",
    "                      for(int i = 1; i <= params.VectorNum; i++) \n",
    "                      {\n",
    "                          float[] v = doc['vector'+i].vectorValue; \n",
    "                          float vm = doc['vector'+i].magnitude;  \n",
    "                          \n",
    "                          if (v[0] != 0)\n",
    "                          {\n",
    "                              float dotProduct = 0;\n",
    "\n",
    "                              for(int j = 0; j < v.length; j++) \n",
    "                              {\n",
    "                                  dotProduct += v[j] * params.queryVector[j];\n",
    "                              }\n",
    "\n",
    "                              float score = dotProduct / (vm * (float) params.queryVectorMag);\n",
    "\n",
    "                              if(score > max_score) \n",
    "                              {\n",
    "                                  max_score = score;\n",
    "                              }\n",
    "                            }\n",
    "                      }\n",
    "                      return max_score\n",
    "                    \"\"\",\n",
    "                \"params\": \n",
    "                {\n",
    "                  \"queryVector\": query_vector,\n",
    "                  \"queryVectorMag\": 0.1905,\n",
    "                  \"VectorNum\": 40\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\",\"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "\n",
    "        \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    texts = []\n",
    "    titles = [] \n",
    "    bi_scores = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        '''\n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[요약문]')\n",
    "        print(hit[\"_source\"][\"summarize\"]) \n",
    "        print()\n",
    "                \n",
    "        '''\n",
    "        #print(hit)\n",
    "        \n",
    "        # 리스트에 저장해둠\n",
    "        titles.append(hit[\"_source\"][\"rfile_name\"])\n",
    "        texts.append(hit[\"_source\"][\"rfile_text\"])\n",
    "        bi_scores.append(hit[\"_score\"])\n",
    "        \n",
    "     # 내림 차순으로 정렬\n",
    "    dec_bi_scores = reversed(np.argsort(bi_scores))\n",
    "    print(dec_bi_scores)\n",
    "    \n",
    "    # 내림차순으로 출력\n",
    "    for idx in dec_bi_scores:\n",
    "        print(\"{:.2f}\\t[제목]:{}\\n{}\\n\".format(float(bi_scores[idx]), titles[idx], texts[idx]))\n",
    "    \n",
    "    # 처리 시간들 출력\n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "    #========================================================================================================    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "INDEX_NAME = 'aihub-ts1-acsampe-klue-sbert-v1-mpower10u-128d-onnx-1'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "#INDEX_NAME = 'korquad-klue-sbert-v1.0-noavg' # 요약문 평균값 처리 안한경우\n",
    "\n",
    "SEARCH_SIZE = 5\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 추가하가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquad'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "title = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시'\n",
    "        ]\n",
    "\n",
    "paragraph = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "# 2. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(title, paragraph):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8054-dfc6-4289-8309-8f0710b16771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES 데이터 조회하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "###########################################################\n",
    "# 인덱스내 데이터 조회 => query 이용\n",
    "###########################################################\n",
    "def search(index, data=None):\n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    res = es.search(index=index, body=body)\n",
    "    return res\n",
    "###########################################################\n",
    "\n",
    "# 모든 데이터 조회\n",
    "#sr = search(index=INDEX_NAME)\n",
    "#pprint.pprint(sr)\n",
    "\n",
    "# 단일 필드 조회\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7fc52-530f-46f4-ae83-2ccca3fd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 삭제하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 삭제 => query 이용\n",
    "############################################################\n",
    "def delete(index, data):\n",
    "    if data is None:  # data가 없으면 모두 삭제\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    return es.delete_by_query(index, body=body)\n",
    "\n",
    "############################################################\n",
    "## 2: 인덱스 내의 데이터 삭제 => id 이용\n",
    "############################################################\n",
    "def delete_by_id(index, id):\n",
    "    return es.delete(index, id=id)\n",
    "\n",
    "############################################################\n",
    "## 3: 인덱스 자체 삭제\n",
    "############################################################\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# 1: query 이용 데이터 삭제\n",
    "delete(index=INDEX_NAME, data={'title':'한라산'})\n",
    "\n",
    "# 3: 인덱스 자체 삭제\n",
    "#delete_index(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659658-75cf-4339-a88a-db338bddfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 업데이트하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 업데이트=>_id 에 데이터 업데이트\n",
    "############################################################\n",
    "def update(index, id, doc, doc_type):\n",
    "    \n",
    "    body = {\n",
    "        'doc': doc\n",
    "    }\n",
    "    \n",
    "    res=es.update(index=index, id=id, body=body, doc_type=doc_type)\n",
    "    return res\n",
    "############################################################\n",
    "\n",
    "#=====================================================================\n",
    "# 검색해서, _id, _type을 구함\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print('\\n')\n",
    "print(\"===[검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    id = hits[\"_id\"]      # id\n",
    "    type = hits[\"_type\"]  # type\n",
    "    \n",
    "    print(f'id: {id}')\n",
    "    print(f'type: {type}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # update 시킴\n",
    "    print(\"===[업데이트]===\")\n",
    "    doc = {'paragraph': '제주도는 대한민국에 가장 남쪽에 있는 섬으로, 인구는 약 71만명이며, 화산섬으로 관광자원이 많은 천혜의 관광지 이다.'}\n",
    "    print(doc)\n",
    "    print('\\n')\n",
    "    \n",
    "    ur=update(index=INDEX_NAME, id=id, doc=doc, doc_type=type)\n",
    "    print(\"===[업데이트 결과]===\")\n",
    "    print(ur)\n",
    "    print('\\n')\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "# 인덱스 refresh 함\n",
    "# elasticsearch의 자동 새로고침의 시간은 1초 정도 소요\n",
    "# 따라서 코드에 아래 명령어를 입력하지 않았을 경우 검색을 하지 못할 가능성도 존재\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# 제주도로 검색해서 한번더 확인\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print(\"===[재검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    \n",
    "    print(f'id:{hits[\"_id\"]}')\n",
    "    print(f'type: {hits[\"_type\"]}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
