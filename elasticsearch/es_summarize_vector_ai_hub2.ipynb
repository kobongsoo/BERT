{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# - 문장들을  추출 요약해서 요약문장을 만들고, 요약 문장의 평균을 구하여 문장 embedding을 생성하여 ES에 인덱스에 vector 추가하고, 검색하는 예제임\n",
    "# - 말뭉치는 ai_hub에 원천말뭉치인 ts1 말뭉치에 tilte, content를 추출하여, content 요약문과 title에 대해 각각 vector를 만들어서 ES 인덱스로 추가하는 예시임.\n",
    "#\n",
    "# => 대규모 웹데이터 기반 한국어 말뭉치 데이터 \n",
    "# 말뭉치 출처: https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=624\n",
    "\n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "# - elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# - 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss\n",
    "\n",
    "# 추출 요약 설치\n",
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "import kss\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "# FutureWarning 제거\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, getListOfFiles\n",
    "device = GPU_info()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# 0. param 설정\n",
    "#------------------------------------------------------------------------------------\n",
    "seed = 111\n",
    "query_num = 500             # 쿼리 최대 갯수: KorQuAD_v1.0_dev.json 최대값은 5533개임, 0이면 모든 5533개 쿼리함.\n",
    "search_k = 5                # FAISS 검색시, 검색 계수(5=쿼리와 가장 근접한 5개 결과값을 반환함)\n",
    "avg_num = 1                 # 쿼리에 대해 sub 문장들중 최대 scorce를 갖는 문장을 몇개 찾고 평균낼지.(3=쿼리에 가장 유사한 sub문장 3개를 찾고 평균을 냄)\n",
    "faiss_index_method = 0      # 0= Cosine Similarity 적용(IndexFlatIP 사용), 1= Euclidean Distance 적용(IndexFlatL2 사용)\n",
    "\n",
    "# 토큰 임베딩 관련 param\n",
    "IS_EMBED_DIVIDE = True      #여기서는 토큰 임베딩은 True고정, True=문단의 여러 문장을, 토큰 단위로 분리후 벡터 구해서 인덱스 만듬/False=문단의 여러문장을 하나의 벡터를 구해서 인덱스 만듬.\n",
    "EMBED_DIVIDE_LEN = [5,7,9]  #5 # 문장을 몇개(토큰)으로 분리할지 (7,8,10) 일때 성능 좋음=>50.8%, (5,7,9) 일때 차원축소 128=>41.80%(81.8%) 성능 좋음\n",
    "MAX_TOKEN_LEN = 40          # 최대 몇개 token까지만 임베딩 할지\n",
    "\n",
    "# 차원 축소 관련 param\n",
    "# 차원 축소 할지, 768 값을 앞에 384 만큼만 배열 resize 시킴.  \n",
    "# - 384로 줄일대 -2% 성능 저하 발생(512: -1.2%, 256: -6%)\n",
    "DIM_RESIZE_METHOD = 2  # 0=차원축소 안함/1=resize 로 차원축소/2=Dense 모델 이용 차원축소\n",
    "DIM_RESIZE_LEN = 128\n",
    "\n",
    "# ONNX 모델 사용\n",
    "IS_ONNX_MODEL = True        # True=onnx 모델 사용\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 1. 검색모델 로딩\n",
    "# => bi_encoder 모델 로딩, polling_mode 설정\n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import bi_encoder, dense_model, onnx_model\n",
    "\n",
    "# bi_encoder 모델 로딩\n",
    "bi_encoder_path = \"bongsoo/klue-sbert-v1\"\n",
    "pooling_mode = 'mean' # bert면=mean, albert면 = cls\n",
    " # 출력 임베딩 크기 지정 : 0=기본 모델 임베딩크기(768), 예:128=128 츨력임베딩 크기 \n",
    "out_dimension = 128 if DIM_RESIZE_METHOD == 2 else 0 if DIM_RESIZE_METHOD != 2 else None\n",
    "    \n",
    "word_embedding_model1, bi_encoder1 = bi_encoder(model_path=bi_encoder_path, max_seq_len=512, do_lower_case=True, \n",
    "                                              pooling_mode=pooling_mode, out_dimension=out_dimension, device=device)\n",
    "print(f'---bi_encoder---------------------------')   \n",
    "print(bi_encoder1)\n",
    "print(word_embedding_model1)\n",
    "\n",
    "print(f'---dense param---------------------------')   \n",
    "# 출력 값 차원 축소 지정인 경우, token_embeddings 일때는 sentencebert 라이브러리를 이용하여 dense_model 모델 추가할수 없으므로,\n",
    "# 사용자정의 dense_model을 정의해서, 가중치와 bias를 bi_encoder모델에 것을 얻어와서 적용해서 차원 죽소함.\n",
    "# => resize 방식 보다 성능 떨어지지만, 128일때는 더 성능이 좋음\n",
    "if DIM_RESIZE_METHOD == 2:\n",
    "    #-------------------------------------------------------------------------\n",
    "    # 처음에는 아래 코드를 활용하여 해당 모델의 128 weight와 bias를 저장해 두어야 함.\n",
    "    #state_dict = bi_encoder1.state_dict()  # bi_encoder모델이 state_dict 얻어옴\n",
    "    #print(state_dict.keys())\n",
    "    #dense_weight = state_dict['2.linear.weight'] # denser 모델에 bi_encoder모델이 linear.weight 적용\n",
    "    #dense_bias = state_dict['2.linear.bias']     # denser 모델에 bi_encoder모델이 linear.bias 적용\n",
    "    \n",
    "    # 처음에  weigth, bias 파일을 저장해 둠.\n",
    "    #torch.save(dense_weight, 'klue-sbert-v1-weigth.pt')\n",
    "    #torch.save(dense_bias, 'klue-sbert-v1-bias.pt')\n",
    "    #-------------------------------------------------------------------------\n",
    "    # weigth, bias 저장해둔 파일 로딩\n",
    "    dense_weight = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-weight-128.pt')\n",
    "    dense_bias = torch.load('../embedding_sample/faiss/data/dense_weight/klue-sbert-v1-bias-128.pt')\n",
    "\n",
    "    print('*dense_weight:{}'.format(dense_weight.size()))\n",
    "    print(f'*dense_bias:{dense_bias.size()}')\n",
    " \n",
    "# onnx 모델 로딩\n",
    "if IS_ONNX_MODEL == True:\n",
    "    onnx_model_path = \"bongsoo/klue-sbert-v1-onnx\"#\"bongsoo/klue-sbert-v1-onnx\"\n",
    "    onnx_tokenizer, onnx_model = onnx_model(onnx_model_path)\n",
    "    print(f'---onnx_model---------------------------')\n",
    "    print(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b67318-d790-422c-a469-874a1b5b7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 안덱싱 및 검색 조건에 맞게 임베딩 처리 하는 함수 \n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import embed_text, onnx_embed_text\n",
    "\n",
    "def embedding(paragrphs:list):\n",
    "    if IS_ONNX_MODEL == True:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs) \n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs, token_embeddings=False)\n",
    "    else:\n",
    "        if IS_EMBED_DIVIDE == True: # 한 문단에 대한 40개 문장들을 토큰단위로 쪼개서 임베딩 처리함  \n",
    "            #----------------------------------------------------\n",
    "            # 한 문단에 대한 문장들의 토큰을 ?개씩 나누고 비교.\n",
    "            # - 한 문단에 대한 문장들에 대해 [tensor(250,768), tensor(243,768), tensor(111,768),..] tensor 리스트 타입으로 벡터 생성됨.\n",
    "            #----------------------------------------------------\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, token_embeddings=True, return_tensor=False)\n",
    "        else: # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "            embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, return_tensor=False)  \n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0068d-6e57-430a-8142-19a05fa9e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 안덱싱 처리\n",
    "#-------------------------------------------------------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "import kss\n",
    "from myutils import embed_text, divide_arr_avg_exten, clean_text\n",
    "\n",
    "# 인덱싱 함수 \n",
    "def index_data():\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    count = 0\n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "        count += 1\n",
    "        #print(f'{count}:{source}') # 인덱스 구조 출력\n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    # json 파일들이 있는 폴더에 .json 파일 이름들을 얻기\n",
    "    # =>DATA_FOLDER: .JSON파일들이 있는 폴더\n",
    "    files = getListOfFiles(DATA_FOLDER)\n",
    "    assert len(files) > 0 # files가 0이면 assert 발생\n",
    "    print('*file_count: {}, file_list:{}'.format(len(files), files[0:5]))\n",
    " \n",
    "    for idx, file in enumerate(tqdm(files)):\n",
    "        if \".json\" not in file:  #.json 파일이 아니면 합치지 않음\n",
    "            continue\n",
    "            \n",
    "        count = 0\n",
    "        docs = []\n",
    "    \n",
    "        # json 파일 로딩 => [SJML][text] entry만 불러옴\n",
    "        json_data = json.load(open(file, \"r\", encoding=\"utf-8\"))['SJML']['text']\n",
    "        for data in json_data:\n",
    "        #for data in json_data:\n",
    "            count += 1\n",
    "            doc = {} #dict 선언\n",
    "            \n",
    "            doc['title'] = data['title']            # 제목 설정\n",
    "            doc['paragraph'] = data['content']      # 문장 담음.\n",
    "                \n",
    "            docs.append(doc)\n",
    "            #print(f'count:{count}')\n",
    "            #print(doc['title'])\n",
    "            \n",
    "            if count % BATCH_SIZE == 0:\n",
    "                index_batch(docs)\n",
    "                docs = []\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "            # ** 10 개만 보냄\n",
    "            #if count >= 10:\n",
    "            #   break\n",
    "            \n",
    "        if docs:\n",
    "            index_batch(docs)\n",
    "            print(\"Indexed {} documents.\".format(count))   \n",
    "            \n",
    "        es.indices.refresh(index=INDEX_NAME)\n",
    "            \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    #print(\"=== End Done indexing===\")\n",
    "                   \n",
    "\n",
    "def index_batch(docs):\n",
    "        \n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        title = doc['title']\n",
    "        paragraph = doc['paragraph']\n",
    "\n",
    "        sub_contexts = []\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        paragraph = clean_text(paragraph)  # 전처리 : (한글, 숫자, 영문, (), {}, [], %, ,,.,\",')  등을 제외한 특수문자 제거\n",
    "        # 입력 문단을 여러 문장들로 나눔.\n",
    "        #sentences = [sentence for sentence in paragraph.split('.') if sentence != '' and len(sentence) > 10]  # '.'(마침표) 로 구분해서 sub 문장을 만듬.\n",
    "        sentences = [sentence for sentence in kss.split_sentences(paragraph) if sentence != '' and len(sentence) > 10] # kss 이용해서 sub 문장을 만듬\n",
    "        \n",
    "        # 만약 sentences(sub 문장) 가 하나도 없으면 원본문장을 담고, 10이상이면  10개만 담음.\n",
    "        sub_contexts.append([paragraph] if len(sentences) < 1 else sentences[0:10] if len(sentences) > 10 else sentences)\n",
    "       \n",
    "        if i < 1:\n",
    "            print(sub_contexts[0])\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # 토큰 분할 임베딩 처리\n",
    "        # => sub_contexts은  1차원 리스트 임 (예:['오늘 비가 온다','오늘 눈이 온다','날씨가 좋다'])\n",
    "        token_embeds = embedding(sub_contexts[0])\n",
    "        #------------------------------------------------------------------------------------------------------------------------ \n",
    "        # 토큰 분할인 경우 처리 start=>           \n",
    "        token_embed_arr_list = []\n",
    "        tcount = 0\n",
    "        # tensor(250,768) 한문장 토큰 임베딩 얻어와서, 각 ?개 토큰씩 평균을 구함.\n",
    "        for token_embed in token_embeds:\n",
    "            token_embed = token_embed[1:-1] # 맨앞에 [CLS]와 맨뒤에 [SEP] 토큰은 뺌\n",
    "            if tcount >= MAX_TOKEN_LEN: \n",
    "                break\n",
    "             \n",
    "            #print(f'token_embed.shape:{token_embed.shape}')\n",
    "            \n",
    "            token_embed_arrs = token_embed.cpu().numpy().astype('float32')\n",
    "            #print(f'token_embed_arrs:{token_embed_arrs.shape}')\n",
    "            # 5,7,10 씩 자르면서 문장 토큰 평균을 구함\n",
    "            token_embed_divide_arrs = divide_arr_avg_exten(embed_arr=token_embed_arrs, divide_arrs=EMBED_DIVIDE_LEN) \n",
    "\n",
    "             # Dense 방식으로 차원 축소 => 평균 구한후 차원 축소하는 방식이 0.6% 정도 성능 좋음\n",
    "            if DIM_RESIZE_METHOD == 2:\n",
    "                tmp1 = torch.Tensor(token_embed_divide_arrs)\n",
    "                #tmp1 = torch.from_numpy(token_embed_divide_arrs)\n",
    "                debug1 = False\n",
    "                tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias, debug=debug1)\n",
    "                arrs = tmp2.detach().numpy().astype('float32')\n",
    "            else:  \n",
    "                arrs = token_embed_divide_arrs\n",
    "                \n",
    "            # 평균 구한 토큰들을 token_embed_arr_list 리스트에 담아둠.(50보다 크면 50개만 담음)           \n",
    "            for idx, arr in enumerate(arrs):\n",
    "                \n",
    "                 # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "                if DIM_RESIZE_METHOD == 1:\n",
    "                    darr = np.resize(arr, (DIM_RESIZE_LEN,))\n",
    "                else:\n",
    "                    darr = arr\n",
    "                    \n",
    "                token_embed_arr_list.append(darr)\n",
    "                tcount +=1\n",
    "                if tcount >=MAX_TOKEN_LEN:\n",
    "                    break\n",
    "\n",
    "        #embeddings = np.array(token_embed_arr_list)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "        # ES에 문단 인덱싱 처리\n",
    "        request = {}  #dict 정의\n",
    "        request[\"rfile_name\"] = title       # 제목               \n",
    "        request[\"rfile_text\"] = paragraph   # 문장\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"        \n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        # for문 돌면서 벡터 처리\n",
    "        #print(type(token_embed_arr_list))\n",
    "        #print(len(token_embed_arr_list))\n",
    "        \n",
    "        # vector 1~40 까지 값을 0으로 초기화 해줌.\n",
    "        for i in range(MAX_TOKEN_LEN):\n",
    "            if DIM_RESIZE_METHOD > 0:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((DIM_RESIZE_LEN))\n",
    "            else:\n",
    "                request[\"vector\"+str(i+1)] = np.zeros((768))\n",
    "            \n",
    "        # vector 값들을 담음.\n",
    "        for i, token_embed_arr in enumerate(token_embed_arr_list):\n",
    "            request[\"vector\"+str(i+1)] = token_embed_arr\n",
    "            \n",
    "        requests.append(request)\n",
    "        #------------------------------------------------------------------------------------------------------------------------\n",
    "                \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'aihub-ts1-acsampe-klue-sbert-v1-mpower10u-128d-onnx-1'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "INDEX_FILE = './data/mpower10u_128d.json'                 # 인덱스 구조 파일\n",
    "DATA_FOLDER = '../../../data11/ai_hub/ts1/acsample/'     # 인덱스할 파일들이 있는 폴더경로 \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "## 입력 ##\n",
    "# GET korquad/_count\n",
    "\n",
    "## 출력 ###\n",
    "'''\n",
    "{\n",
    "  \"count\" : 1420,\n",
    "  \"_shards\" : {\n",
    "    \"total\" : 2,\n",
    "    \"successful\" : 2,\n",
    "    \"skipped\" : 0,\n",
    "    \"failed\" : 0\n",
    "}\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 검색 처리\n",
    "#-------------------------------------------------------------------------------------\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from myutils import embed_text, divide_arr_avg_exten\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    \n",
    "    # 문장으로 비교할때=>쿼리 문장에 대한 벡터 생성해서 비교\n",
    "    #query_vector = embed_text(model=bi_encoder,paragraphs=[query])[0]\n",
    "    #------------------------------------------------------------------\n",
    "    # 토큰 평균으로 비교할때=> 쿼리 문장에 대한 모든 토큰 벡터를 생성해서 비교\n",
    "    # 토큰 분할 임베딩 처리\n",
    "    token_query_embeds = embedding([query])\n",
    "    \n",
    "    token_query_embed_arr_list = []\n",
    "    # 쿼리 문장들의 토큰들의 평균을 구함.\n",
    "    for token_query_embed in token_query_embeds:\n",
    "        \n",
    "        token_query_embed = token_query_embed[1:-1] # 맨앞에 [CLS]와 맨두에 [SEP] 토큰은 뺌\n",
    "        print(f'*token_query_embed.shape:{token_query_embed.shape}')\n",
    "        \n",
    "        tmp = token_query_embed.cpu().numpy().astype('float32')\n",
    "        tmp=tmp.mean(axis=0) #평균 구함\n",
    "        \n",
    "         # Resize 방식으로 차원 축소(384로 줄일대 -2% 성능 저하 발생)\n",
    "        if DIM_RESIZE_METHOD == 1:\n",
    "            tmp = np.resize(tmp, (DIM_RESIZE_LEN,))\n",
    "         # Dense 방식으로 차원 축소=> 평균 구하기 전에 차원 축소하는것이 성능 +0.6더 좋음\n",
    "        elif DIM_RESIZE_METHOD == 2:\n",
    "            tmp1 = torch.Tensor([tmp]) # 1차원 배열을 -> 2차둰 텐서(1,768)로 변환\n",
    "            tmp2 = dense_model(embed_tensor=tmp1, out_f=DIM_RESIZE_LEN, weight=dense_weight, bias=dense_bias)\n",
    "            tmp = tmp2.detach().numpy().astype('float32').ravel(order='C') # 1차원 배열로 변경(128,)        \n",
    "        \n",
    "        token_query_embed_arr_list.append(tmp)\n",
    "        \n",
    "    query_vector = np.array(token_query_embed_arr_list)[0]  # 리스트를 배열로 변환  \n",
    "    \n",
    "    # print(query_vector)\n",
    "    print(f'*query_vector.shape:{query_vector.shape}\\n')\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    '''\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector2']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴(doc['summarize_vector'])\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    # 문단별 40개의 벡터와 쿼리벡터를 서로 비교하여 최대값 갖는 문단들중 가장 유사한  문단 출력\n",
    "    # => script \"\"\" 안에 코드는 java 임.\n",
    "    # => \"queryVectorMag\": 0.1905 일때 100% 일치하는 값은 9.98임(즉 10점 만점임)\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}\n",
    "            },\n",
    "                \"script\":{\n",
    "                    \"source\": \"\"\"\n",
    "                      float max_score = 0;\n",
    "                      for(int i = 1; i <= params.VectorNum; i++) \n",
    "                      {\n",
    "                          float[] v = doc['vector'+i].vectorValue; \n",
    "                          float vm = doc['vector'+i].magnitude;  \n",
    "                          \n",
    "                          if (v[0] != 0)\n",
    "                          {\n",
    "                              float dotProduct = 0;\n",
    "\n",
    "                              for(int j = 0; j < v.length; j++) \n",
    "                              {\n",
    "                                  dotProduct += v[j] * params.queryVector[j];\n",
    "                              }\n",
    "\n",
    "                              float score = dotProduct / (vm * (float) params.queryVectorMag);\n",
    "\n",
    "                              if(score > max_score) \n",
    "                              {\n",
    "                                  max_score = score;\n",
    "                              }\n",
    "                            }\n",
    "                      }\n",
    "                      return max_score\n",
    "                    \"\"\",\n",
    "                \"params\": \n",
    "                {\n",
    "                  \"queryVector\": query_vector,\n",
    "                  \"queryVectorMag\": 0.1905,\n",
    "                  \"VectorNum\": 40\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\",\"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "\n",
    "        \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    texts = []\n",
    "    titles = [] \n",
    "    bi_scores = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        '''\n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[요약문]')\n",
    "        print(hit[\"_source\"][\"summarize\"]) \n",
    "        print()\n",
    "                \n",
    "        '''\n",
    "        #print(hit)\n",
    "        \n",
    "        # 리스트에 저장해둠\n",
    "        titles.append(hit[\"_source\"][\"rfile_name\"])\n",
    "        texts.append(hit[\"_source\"][\"rfile_text\"])\n",
    "        bi_scores.append(hit[\"_score\"])\n",
    "        \n",
    "     # 내림 차순으로 정렬\n",
    "    dec_bi_scores = reversed(np.argsort(bi_scores))\n",
    "    print(dec_bi_scores)\n",
    "    \n",
    "    # 내림차순으로 출력\n",
    "    for idx in dec_bi_scores:\n",
    "        print(\"{:.2f}\\t[제목]:{}\\n{}\\n\".format(float(bi_scores[idx]), titles[idx], texts[idx]))\n",
    "    \n",
    "    # 처리 시간들 출력\n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "    #========================================================================================================    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "INDEX_NAME = 'aihub-ts1-acsampe-klue-sbert-v1-mpower10u-128d-onnx-1'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "#INDEX_NAME = 'korquad-klue-sbert-v1.0-noavg' # 요약문 평균값 처리 안한경우\n",
    "\n",
    "SEARCH_SIZE = 5\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 추가하가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquad'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "title = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시'\n",
    "        ]\n",
    "\n",
    "paragraph = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "# 2. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(title, paragraph):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8054-dfc6-4289-8309-8f0710b16771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES 데이터 조회하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "###########################################################\n",
    "# 인덱스내 데이터 조회 => query 이용\n",
    "###########################################################\n",
    "def search(index, data=None):\n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    res = es.search(index=index, body=body)\n",
    "    return res\n",
    "###########################################################\n",
    "\n",
    "# 모든 데이터 조회\n",
    "#sr = search(index=INDEX_NAME)\n",
    "#pprint.pprint(sr)\n",
    "\n",
    "# 단일 필드 조회\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7fc52-530f-46f4-ae83-2ccca3fd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 삭제하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 삭제 => query 이용\n",
    "############################################################\n",
    "def delete(index, data):\n",
    "    if data is None:  # data가 없으면 모두 삭제\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    return es.delete_by_query(index, body=body)\n",
    "\n",
    "############################################################\n",
    "## 2: 인덱스 내의 데이터 삭제 => id 이용\n",
    "############################################################\n",
    "def delete_by_id(index, id):\n",
    "    return es.delete(index, id=id)\n",
    "\n",
    "############################################################\n",
    "## 3: 인덱스 자체 삭제\n",
    "############################################################\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# 1: query 이용 데이터 삭제\n",
    "delete(index=INDEX_NAME, data={'title':'한라산'})\n",
    "\n",
    "# 3: 인덱스 자체 삭제\n",
    "#delete_index(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659658-75cf-4339-a88a-db338bddfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 업데이트하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 업데이트=>_id 에 데이터 업데이트\n",
    "############################################################\n",
    "def update(index, id, doc, doc_type):\n",
    "    \n",
    "    body = {\n",
    "        'doc': doc\n",
    "    }\n",
    "    \n",
    "    res=es.update(index=index, id=id, body=body, doc_type=doc_type)\n",
    "    return res\n",
    "############################################################\n",
    "\n",
    "#=====================================================================\n",
    "# 검색해서, _id, _type을 구함\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print('\\n')\n",
    "print(\"===[검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    id = hits[\"_id\"]      # id\n",
    "    type = hits[\"_type\"]  # type\n",
    "    \n",
    "    print(f'id: {id}')\n",
    "    print(f'type: {type}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # update 시킴\n",
    "    print(\"===[업데이트]===\")\n",
    "    doc = {'paragraph': '제주도는 대한민국에 가장 남쪽에 있는 섬으로, 인구는 약 71만명이며, 화산섬으로 관광자원이 많은 천혜의 관광지 이다.'}\n",
    "    print(doc)\n",
    "    print('\\n')\n",
    "    \n",
    "    ur=update(index=INDEX_NAME, id=id, doc=doc, doc_type=type)\n",
    "    print(\"===[업데이트 결과]===\")\n",
    "    print(ur)\n",
    "    print('\\n')\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "# 인덱스 refresh 함\n",
    "# elasticsearch의 자동 새로고침의 시간은 1초 정도 소요\n",
    "# 따라서 코드에 아래 명령어를 입력하지 않았을 경우 검색을 하지 못할 가능성도 존재\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# 제주도로 검색해서 한번더 확인\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print(\"===[재검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    \n",
    "    print(f'id:{hits[\"_id\"]}')\n",
    "    print(f'type: {hits[\"_type\"]}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
