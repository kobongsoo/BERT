{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# \n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "#\n",
    "# elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 \n",
    "# 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "# => 참고로 여기서는 title_vector 만 구함, paragrapha_vector는 cpu에서는 엄청 오래 걸려서 주석처리하였음\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import kss, numpy\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s-bert 모델 테스트\n",
    "sbert_model_path = '../../../model/sbert/klue-nli-mean-64-sts-128'\n",
    "\n",
    "# cpu 모델로 실행할때는 device=cpu, 기본은 GPU임\n",
    "embedder = SentenceTransformer(sbert_model_path, device=device)\n",
    "\n",
    "text = '나는 오늘 밥을 먹는다.'\n",
    "vectors = embedder.encode(text, convert_to_tensor=True)\n",
    "vector_list = [vector.numpy().tolist() for vector in vectors]\n",
    "\n",
    "print(f'vector_len:{len(vector_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱 함수 \n",
    "def index_data():\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    \n",
    "    count = 0\n",
    "        \n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "      \n",
    "        count += 1\n",
    "        print(f'{count}:{source}')\n",
    "      \n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    # DATA 추기\n",
    "    with open(DATA_FILE) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            json_data = json.loads(line)\n",
    "            docs = []\n",
    "            \n",
    "            for j in tqdm(json_data):\n",
    "                count += 1\n",
    "                \n",
    "                docs.append(j)\n",
    "                if count % BATCH_SIZE == 0:\n",
    "                    index_batch(docs)\n",
    "                    docs = []\n",
    "                    print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "                # ** 500 개만 보냄\n",
    "                #if count >= 500:\n",
    "                #    break\n",
    "                    \n",
    "            if docs:\n",
    "                index_batch(docs)\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                    \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"=== End Done indexing===\")\n",
    "    \n",
    "    \n",
    "# 문단(paragraph)들 분리\n",
    "# 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "# =>굳이 elasticsearch에 문단 벡터는 추가하지 않고, title 벡터만 이용해도 되므로 주석처리함\n",
    "'''\n",
    "def paragraph_index(paragraph):\n",
    "    avg_paragraph_vec = numpy.zeros((1,768))\n",
    "    sent_count = 0\n",
    "    \n",
    "    #print(paragraph)\n",
    "    # kss로 분할할때 줄바꿈 있으면, 파싱하는데 에러남.따라서 \"\\n\"는 제거함\n",
    "    paragraph = paragraph.replace(\"\\n\",\"\")\n",
    "    \n",
    "    #print(\"==Start paragraph_index==\")\n",
    "    for sent in kss.split_sentences(paragraph):\n",
    "        # 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "        avg_paragraph_vec += embed_text([sent])\n",
    "        sent_count += 1\n",
    "            \n",
    "    avg_paragraph_vec /= sent_count\n",
    "    return avg_paragraph_vec.ravel(order='C')\n",
    "'''\n",
    "\n",
    "def index_batch(docs):\n",
    "   \n",
    "    titles = [doc[\"title\"] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "    \n",
    "    # 문장이 길면 분할해서 embedding을 구해야 하는데, 여기서는 분할하지 않고 embedding을 구함\n",
    "    paragraphs = [doc[\"paragraph\"] for doc in docs]\n",
    "    paragraph_vectors = embed_text(paragraphs)\n",
    "    \n",
    "    # * cpu로 문단은 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "    #paragraph_vectors = [paragraph_index(doc[\"paragraph\"]) for doc in tqdm(docs)]\n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        \n",
    "        #request[\"title\"] = doc[\"title\"]           # 이렇게 등록은 안됨\n",
    "        #request[\"paragraph\"] = doc[\"paragraph\"]\n",
    "        \n",
    "        request = doc\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        request[\"title_vector\"] = title_vectors[i]\n",
    "        request[\"paragraph_vector\"] = paragraph_vectors[i]\n",
    "        #request[\"paragraph_vector\"] = paragraph_vectors[i]  # * cpu로 문단은 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "        \n",
    "        requests.append(request)\n",
    "        \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "    \n",
    "# embedding 모델에서 vector를 구함    \n",
    "def embed_text(input):\n",
    "    vectors =  embedder.encode(input, convert_to_tensor=True)\n",
    "    return [vector.numpy().tolist() for vector in vectors]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "INDEX_FILE = './data/index.json'\n",
    "DATA_FILE = './data/KorQuAD_v1.0_train_convert.json'\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "## 입력 ##\n",
    "# GET korquad/_count\n",
    "\n",
    "## 출력 ###\n",
    "'''\n",
    "{\n",
    "  \"count\" : 1420,\n",
    "  \"_shards\" : {\n",
    "    \"total\" : 2,\n",
    "    \"successful\" : 2,\n",
    "    \"skipped\" : 0,\n",
    "    \"failed\" : 0\n",
    "}\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 하기\n",
    "\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['paragraph_vector']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"title\", \"paragraph\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "    \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    \n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        \n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[내용]')\n",
    "        print(hit[\"_source\"][\"paragraph\"]) \n",
    "        \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "\n",
    "INDEX_NAME = 'korquad'\n",
    "SEARCH_SIZE = 3\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 추가하가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquad'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "title = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시'\n",
    "        ]\n",
    "\n",
    "paragraph = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "# 2. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(title, paragraph):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8054-dfc6-4289-8309-8f0710b16771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES 데이터 조회하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "###########################################################\n",
    "# 인덱스내 데이터 조회 => query 이용\n",
    "###########################################################\n",
    "def search(index, data=None):\n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    res = es.search(index=index, body=body)\n",
    "    return res\n",
    "###########################################################\n",
    "\n",
    "# 모든 데이터 조회\n",
    "#sr = search(index=INDEX_NAME)\n",
    "#pprint.pprint(sr)\n",
    "\n",
    "# 단일 필드 조회\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7fc52-530f-46f4-ae83-2ccca3fd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 삭제하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 삭제 => query 이용\n",
    "############################################################\n",
    "def delete(index, data):\n",
    "    if data is None:  # data가 없으면 모두 삭제\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    return es.delete_by_query(index, body=body)\n",
    "\n",
    "############################################################\n",
    "## 2: 인덱스 내의 데이터 삭제 => id 이용\n",
    "############################################################\n",
    "def delete_by_id(index, id):\n",
    "    return es.delete(index, id=id)\n",
    "\n",
    "############################################################\n",
    "## 3: 인덱스 자체 삭제\n",
    "############################################################\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# 1: query 이용 데이터 삭제\n",
    "delete(index=INDEX_NAME, data={'title':'한라산'})\n",
    "\n",
    "# 3: 인덱스 자체 삭제\n",
    "#delete_index(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659658-75cf-4339-a88a-db338bddfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 업데이트하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 업데이트=>_id 에 데이터 업데이트\n",
    "############################################################\n",
    "def update(index, id, doc, doc_type):\n",
    "    \n",
    "    body = {\n",
    "        'doc': doc\n",
    "    }\n",
    "    \n",
    "    res=es.update(index=index, id=id, body=body, doc_type=doc_type)\n",
    "    return res\n",
    "############################################################\n",
    "\n",
    "#=====================================================================\n",
    "# 검색해서, _id, _type을 구함\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print('\\n')\n",
    "print(\"===[검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    id = hits[\"_id\"]      # id\n",
    "    type = hits[\"_type\"]  # type\n",
    "    \n",
    "    print(f'id: {id}')\n",
    "    print(f'type: {type}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # update 시킴\n",
    "    print(\"===[업데이트]===\")\n",
    "    doc = {'paragraph': '제주도는 대한민국에 가장 남쪽에 있는 섬으로, 인구는 약 71만명이며, 화산섬으로 관광자원이 많은 천혜의 관광지 이다.'}\n",
    "    print(doc)\n",
    "    print('\\n')\n",
    "    \n",
    "    ur=update(index=INDEX_NAME, id=id, doc=doc, doc_type=type)\n",
    "    print(\"===[업데이트 결과]===\")\n",
    "    print(ur)\n",
    "    print('\\n')\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "# 인덱스 refresh 함\n",
    "# elasticsearch의 자동 새로고침의 시간은 1초 정도 소요\n",
    "# 따라서 코드에 아래 명령어를 입력하지 않았을 경우 검색을 하지 못할 가능성도 존재\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# 제주도로 검색해서 한번더 확인\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print(\"===[재검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    \n",
    "    print(f'id:{hits[\"_id\"]}')\n",
    "    print(f'type: {hits[\"_type\"]}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bbcc8-22dc-45e6-adbe-3eb43098bdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
