{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# \n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 \n",
    "# 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "# => 참고로 여기서는 title_vector 만 구함, paragrapha_vector는 cpu에서는 엄청 오래 걸려서 주석처리하였음\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import kss, numpy\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_len:768\n"
     ]
    }
   ],
   "source": [
    "# s-bert 모델 테스트\n",
    "sbert_model_path = 'F:\\\\AI\\\\model\\\\sbert-ts2022-04-01-distiluse-7'\n",
    "\n",
    "# cpu 모델로 실행할때는 device=cpu, 기본은 GPU임\n",
    "embedder = SentenceTransformer(sbert_model_path, device=device)\n",
    "\n",
    "text = '나는 오늘 밥을 먹는다.'\n",
    "vectors = embedder.encode(text, convert_to_tensor=True)\n",
    "vector_list = [vector.numpy().tolist() for vector in vectors]\n",
    "\n",
    "print(f'vector_len:{len(vector_list)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0b1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱 함수 \n",
    "def index_data():\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    \n",
    "    count = 0\n",
    "        \n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "      \n",
    "        count += 1\n",
    "        print(f'{count}:{source}')\n",
    "      \n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    # DATA 추기\n",
    "    with open(DATA_FILE) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            json_data = json.loads(line)\n",
    "            docs = []\n",
    "            \n",
    "            for j in tqdm(json_data):\n",
    "                count += 1\n",
    "                \n",
    "                docs.append(j)\n",
    "                if count % BATCH_SIZE == 0:\n",
    "                    index_batch(docs)\n",
    "                    docs = []\n",
    "                    print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "                # ** 500 개만 보냄\n",
    "                #if count >= 500:\n",
    "                #    break\n",
    "                    \n",
    "            if docs:\n",
    "                index_batch(docs)\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                    \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"=== End Done indexing===\")\n",
    "    \n",
    "    \n",
    "# 문단(paragraph)들 분리\n",
    "# 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "# =>굳이 elasticsearch에 문단 벡터는 추가하지 않고, title 벡터만 이용해도 되므로 주석처리함\n",
    "'''\n",
    "def paragraph_index(paragraph):\n",
    "    avg_paragraph_vec = numpy.zeros((1,768))\n",
    "    sent_count = 0\n",
    "    \n",
    "    #print(paragraph)\n",
    "    # kss로 분할할때 줄바꿈 있으면, 파싱하는데 에러남.따라서 \"\\n\"는 제거함\n",
    "    paragraph = paragraph.replace(\"\\n\",\"\")\n",
    "    \n",
    "    #print(\"==Start paragraph_index==\")\n",
    "    for sent in kss.split_sentences(paragraph):\n",
    "        # 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "        avg_paragraph_vec += embed_text([sent])\n",
    "        sent_count += 1\n",
    "            \n",
    "    avg_paragraph_vec /= sent_count\n",
    "    return avg_paragraph_vec.ravel(order='C')\n",
    "'''\n",
    "\n",
    "def index_batch(docs):\n",
    "   \n",
    "    titles = [doc[\"title\"] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "    \n",
    "    # 문장이 길면 분할해서 embedding을 구해야 하는데, 여기서는 분할하지 않고 embedding을 구함\n",
    "    paragraphs = [doc[\"paragraph\"] for doc in docs]\n",
    "    paragraph_vectors = embed_text(paragraphs)\n",
    "    \n",
    "    # * cpu로 문단은 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "    #paragraph_vectors = [paragraph_index(doc[\"paragraph\"]) for doc in tqdm(docs)]\n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request[\"title_vector\"] = title_vectors[i]\n",
    "        request[\"paragraph_vector\"] = paragraph_vectors[i]\n",
    "        #request[\"paragraph_vector\"] = paragraph_vectors[i]  # * cpu로 문단은 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "        requests.append(request)\n",
    "        \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "    \n",
    "# embedding 모델에서 vector를 구함    \n",
    "def embed_text(input):\n",
    "    vectors =  embedder.encode(input, convert_to_tensor=True)\n",
    "    return [vector.numpy().tolist() for vector in vectors]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'lenovo-x240', 'cluster_name': 'mpower', 'cluster_uuid': 'JJ1h3dNTRvSLJFW3gj5MCw', 'version': {'number': '7.17.3', 'build_flavor': 'default', 'build_type': 'zip', 'build_hash': '5ad023604c8d7416c9eb6c0eadb62b14e766caff', 'build_date': '2022-04-19T08:11:19.070913226Z', 'build_snapshot': False, 'lucene_version': '8.11.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n",
      "1:{\n",
      "  \"settings\": {\n",
      "    \"number_of_shards\": 2,\n",
      "    \"number_of_replicas\": 1\n",
      "  },\n",
      "   \"mappings\": {\n",
      "    \"dynamic\": \"true\",\n",
      "    \"_source\": {\n",
      "      \"enabled\": \"true\"\n",
      "    },\n",
      "    \"properties\": {\n",
      "      \"title\": {\n",
      "        \"type\": \"text\"\n",
      "      },\n",
      "\t  \"paragraph\": {\n",
      "        \"type\": \"text\"\n",
      "      },\n",
      "      \"title_vector\": {\n",
      "        \"type\": \"dense_vector\",\n",
      "        \"dims\": 768\n",
      "      },\n",
      "\t  \"paragraph_vector\": {\n",
      "        \"type\": \"dense_vector\",\n",
      "        \"dims\": 768\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965371cd4f1b477cb93c12153e12a5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274dda2fb8b049cb8c583a9e65ad6b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 100 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c258948a8b524b479c6a79b2f0e81607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 200 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c2e53c26f4ebfa87429e7067aaf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 300 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7747a6f014844f57a42aabedbca37497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 400 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1b5f3e9ebe466f84498feddd1bd74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 500 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57e24478a1d49f8be1ec4ba9cfb57fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 600 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4548db31a48b4d4db39ddc4553c08950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 700 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee691103290b43aa9fad5ac871e55643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 800 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d305b37dee4b1eb44f2755727fd3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 900 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d233e60e2a344f2baa28773acf225f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1000 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f372fb5bf57e40b4a8c9ea83b122645d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1100 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55297415d65f4f628d4f58893b0fe7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1200 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7599786cd54ee7a92aaa154d2f6c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1300 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e297a5d5b43c4eaa80a6063eb8f8008a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1400 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0b691040c74437aec3e6a8f91139af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1420 documents.\n",
      "=== End Done indexing===\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "INDEX_FILE = './data/index.json'\n",
    "DATA_FILE = './data/KorQuAD_v1.0_train_convert.json'\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "# GET korquad/_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 하기\n",
    "\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['paragraph_vector']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"title\", \"paragraph\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "    \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    #print(response)\n",
    "    \n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        \n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[내용]')\n",
    "        print(hit[\"_source\"][\"paragraph\"]) \n",
    "        \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Elasticsearch.info of <Elasticsearch([{'host': '192.168.0.130', 'port': 9200}])>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  한국 영화 추천\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420 total hits.\n",
      "embedding time: 21.99 ms\n",
      "search time: 31.99 ms\n",
      "\n",
      "\n",
      "index:korquad, type:_doc\n",
      "id: 3_Fmz4AB7rHLuQj1t6ui, score: 1.3586812\n",
      "[제목] 한류_(문화)\n",
      "[내용]\n",
      "한류 (韓流, 영어: Korean Wave, 스페인어: ola coreana, 말레이어: Gelombang Korea, 터키어: Kore Dalgası, 러시아어: Корейская волна, 독일어: Koreanische Welle, 태국어: กระแสเกาหลี, 타갈로그어: Along Koreano, 베트남어: Làn sóng Hàn Quốc, 우크라이나어: Корейська хвиля)는 대한민국의 대중문화를 포함한 한국과 관련된 것들이 대한민국 이외의 나라에서 인기를 얻는 현상을 뜻한다. ‘한류’라는 단어는 1990년대에 대한민국 문화의 영향력이 타국에서 급성장함에 따라 등장한 신조어이다. 초기 한류는 아시아 지역에서 주로 드라마를 통해 발현되었으며 이후 K-POP으로 분야가 확장되었다. 2010년대에 들어서는 동아시아를 넘어 중동 (북아프리카 포함), 라틴 아메리카 (중남미), 동유럽, 러시아, 중앙아시아 지역으로 넓어졌으며, 최근에는 북아메리카 (북미)와 서유럽 그리고 오세아니아 지역으로 급속히 확산되고 있다.\n",
      "1987년 6·29 선언으로 인해 대한민국에 민주주의가 들어서기 시작하면서 대중들은 대중문화에 대해 더 많은 관심을 가지기 시작했다. 이와 함께 SBS를 포함한 여러 케이블 TV, 위성방송 채널 등이 개국하면서 본격적인 상업 방송이 활성화되었고, 이는 곧 방송 환경의 발달로 이어졌다. 방송 환경의 발달은 대기업 자본의 문화 산업 참여를 유도했고, 그 결과 체계적인 생산 과정의 도입 등 문화 콘텐츠의 질을 높이는 여러 변화가 일어났다. 이 과정에서 대한민국의 문화 산업에는 ‘기획’이라는 개념이 본격적으로 자리 잡기 시작했는데, 그 예로 1990년대에 설립된 기획 영화 제작사 ‘신씨네’, 기획 아이돌 소속사 ‘SM엔터테인먼트’ 등이 있다.\n",
      "2000년대 즈음 대중문화가 발전하면서 대한민국 시장만으로는 대중문화 산업을 감당할 수 없게 되었고, 이에 기업들은 대한민국에서 벗어나 더 넓은 시장으로의 진출을 모색하기 시작했다. 그리고 얼마 안 있어 2000년 2월에 대한민국의 댄스 그룹 H.O.T.가 베이징에서 공연을 하면서 본격적인 중국에서의 한류를 일으켰다. 이후 2003년 베이비복스의 곡인 〈I’m Still Loving You〉는 발매 후 중국에서 차트 1위, 대만에서 4위, 홍콩에서 2위, 태국에서 1위를 기록하며 성공을 거두었다. 2005년 베이징에서 열린 정지훈의 공연 “Rainy day-Beijing”은 관객 4만 명을 동원하며 큰 성공을 이루었다. 한편, 2001년 보아가 일본에서 출시한 정규 음반 《Listen to My Heart》는 대한민국 노래 최초로 오리콘 앨범차트 1위에 오르는 성과를 내었다.\n",
      "2000년대 초 동북아시아와 동남아시아를 중심으로 한류가 퍼지기 전부터, 1990년대 말부터 중국에서는 한류의 씨앗이 자라고 있었다. 1992년 한·중 수교가 성사된 후 처음으로 1993년에 중국중앙방송국(中國中央電視台, CCTV)에서 드라마 〈질투〉(嫉妒)가 방영되었다. 그러나 중국수용자의 관심을 끌지는 못했다. 그 후 1997년에 CCTV에서 〈사랑이 뭐길래〉(愛情是什麼)가 방영되면서 큰 인기를 얻었다. 이후에는 〈별은 내 가슴에〉나 〈의가형제〉 같은 드라마가 중국에 수출되기도 하였다. 또 1998년에는 H.O.T의 음반이 중국에서 발매되었다. 이후 2000년 2월 베이징에서 열린 H.O.T.의 공연은 본격적으로 한류가 중국에 퍼지는 데에 큰 영향을 주었다.\n",
      "2003년 일본에 수출된 드라마 〈겨울연가〉는 NHK에서 방영된 이후 일본 중장년 여성을 중심으로 인기를 얻었다. 이러한 인기로 인해 일본에서는 〈겨울연가〉의 주인공이었던 배용준을 욘사마(ヨン様)라고 부르며 숭배하는 ‘욘사마 붐’이 일어났고, 이는 곧 드라마 촬영지를 ‘성지순례’하는 관광 붐으로 이어졌다. 이에 2004년 일본에서 대한민국을 방문한 관광객은 전년 대비 5.5% 증가했다. 2005년 〈겨울연가〉의 여주인공 역을 맡았던 최지우가 도쿄 관저에 방문했을 때 당시 일본의 총리였던 고이즈미 준이치로는 최지우와 함께 〈겨울연가〉에 대한 얘기를 나누었다. 또 한 심포지엄에서는 “자신보다 배용준이 더 인기가 많다”고 농담을 하기도 했다. 이러한 공로로 배용준은 2008년 대한민국에서 문화훈장을 수상했다.\n",
      "\n",
      "\n",
      "index:korquad, type:_doc\n",
      "id: f_Fqz4AB7rHLuQj1FrNj, score: 1.3563464\n",
      "[제목] 천우희\n",
      "[내용]\n",
      "천우희(1987년 4월 20일 ~ )는 대한민국의 배우이다. 2004년 영화 《신부수업》에 단역으로 출연하며 연기 활동을 시작하였고 2009년에는 영화 《마더》에 조연으로 출연하였다. 2011년 영화 《써니》로 주목을 받으며 그해 대종상과 청룡영화상 여우조연상 후보에 올랐다. 이후 2014년 개봉한 집단 성폭행 피해 학생의 이야기를 그린 영화 《한공주》에 주연으로 출연하여 호평을 받았다. 이 작품으로 같은 해 디렉터스 컷 어워즈 여자 신인연기자상과 한국영화평론가협회상 여우주연상, 그리고 청룡영화상 여우주연상을 수상했고, 이듬해 올해의 영화상 여우주연상, 맥스무비 최고의 영화상 최고의 여자배우상, 들꽃영화상 여우주연상, 백상예술대상 영화부문 여자 신인연기상을 수상했다.\n",
      "천우희는 1987년 4월 20일 경기도 이천시에서 1남 1녀 중 둘째로 태어났다. 오빠와는 세살 터울이었다. 아버지는 이천에서 도예를 하는 사람이었다. 천우희는 학창 시절 친구들과 두루 잘 어울리는 쾌활한 성격으로 초등학교 시절 반장을 도맡아 하였고 전교회장도 하였으며 중학교 때는 전교부회장을 하였다. 이천양정여자고등학교에 진학해서는 연극반 활동을 하였는데, 그녀가 처음으로 무대에 오른 작품은 위안부 할머니의 이야기를 다룬 《반쪽 날개로 날아온 새》였다. 천우희는 고교 시절 청소년 연기대회에서 연기상을 받기도 하였다. 천우희는 연극반 활동을 하며 연기에 흥미를 느껴 2006년 경기대학교 다중매체학부에 연기전공으로 진학하였고 이후 2011년 8월 대학을 졸업하였다.\n",
      "천우희는 2009년 개봉한 봉준호 감독의 영화 《마더》에 오디션을 통해 진태(진구 분)의 재수생 여자친구 미나 역(조연)으로 캐스팅되어 출연하며 본격적인 연기 활동을 시작했다. 300만 관객을 동원한 이 작품에서 그녀는 진태와의 베드신으로 관객들의 눈길을 끌었다. 천우희의 출연작으로 2010년 개봉한 독립영화 《이파네마 소년》과 2012년 개봉한 옴니버스 영화 《사이에서》도 2009년 촬영이 이루어졌다. 그녀는 《이파네마 소년》에서 소녀(김민지 분)의 친구 역을 맡았고, 《사이에서》 중 〈생수〉 편에서는 다방 여종업원 전나리 역을 맡았다. 이후 천우희는 2011년 개봉해 736만 명의 관객을 동원하며 흥행한 강형철 감독의 영화 《써니》에 ‘본드녀’ 상미 역으로 출연하였다. 그녀는 《써니》 오디션시 ‘욕쟁이’ 진희 역과 상미 역 오디션을 동시에 봤는데, 어느 역할을 더 하고 싶냐는 감독의 물음에 더 끌리는 캐릭터인 상미 역을 선택하였다. 천우희는 짧지만 강렬한 상미 역으로 좋은 평가를 받으며 그 해 대종상과 청룡영화상 여우조연상 후보에 올랐다. 천우희는 《써니》에 출연하기 전까진 소속사 없이 활동하였으나, 《써니》 개봉 이후 《마더》로 친분이 있던 배우 원빈의 조언을 받아 2011년 6월 소속사(나무엑터스)와도 전속계약을 맺게 되었다.\n",
      "이후 천우희는 2014년 3월 개봉한 영화 《우아한 거짓말》에서 조연인 권미란을 연기한데 이어, 4월 개봉한 집단 성폭행 피해 학생의 이야기를 그린 독립 영화 《한공주》에서 주연인 한공주를 연기했다. 3억원도 안되는 저예산으로 제작된 《한공주》는 마라케시 국제 영화제, 로테르담 국제 영화제 등의 국제 영화제에서 잇따라 최고상을 수상하며 개봉 전부터 화제가 되었다. 마라케시 국제 영화제의 심사위원 중 한 명인 프랑스의 배우 마리옹 코티야르는 “놀랍도록 섬세한 연출력이 돋보이는 영화다. 감동적인 작품이고 여주인공의 연기도 정말 놀랍고 훌륭하다”고 영화와 천우희의 연기를 칭찬하였다. 천우희도 “이 영화가 내 대표작으로 불리면 좋겠다”고 말하였다. 《써니》에 출연한 후 한때 “1년에 다섯 사람도 안 만”날 정도로 슬럼프에 빠져 있던 천우희는 《한공주》를 통해 슬럼프에서 벗어날 수 있었다. 그녀는 이 작품으로 같은 해 디렉터스 컷 어워즈 여자 신인연기자상과 한국영화평론가협회상 여우주연상, 그리고 청룡영화상 여우주연상을 수상하였고, 이듬해 올해의 영화상 여우주연상, 맥스무비 최고의 영화상 최고의 여자배우상, 들꽃영화상 여우주연상, 백상예술대상 영화부문 여자 신인연기상을 수상했다. 2016년에는 영화 《해어화》와 《곡성》에 출연했다. 《곡성》에서 존재감이 돋보인 연기로 좋은 평을 받았고 곡성을 통해 칸 영화제 비경쟁 부문에 초청되었다.\n",
      "\n",
      "\n",
      "index:korquad, type:_doc\n",
      "id: hPFlz4AB7rHLuQj18Kns, score: 1.354295\n",
      "[제목] 구혜선\n",
      "[내용]\n",
      "구혜선(具惠善, 1984년 11월 9일 ~ )은 대한민국의 배우이자 영화감독이다. 구혜선은 학창 시절 인터넷 얼짱 출신이며, 2002년에 CF로 연예계에 데뷔하였다. 2006년 드라마 《열아홉 순정》으로 얼굴을 알렸다. 2009년 무렵부터 책을 내고 그림 전시회를 열고 영화를 제작하면서 영화감독, 영화 제작사, 작가 등 여러직업을 통해 활동영역을 넓혀가고 있다. 소설 〈탱고〉는 발매 일주일 만에 삼만 부가 팔렸고, 영화감독 데뷔작인 《유쾌한 도우미》는 부산 아시아 단편 영화제 관객상을 수상했다. 첫 장편영화 《요술》은 YG 엔터테인먼트가 제작을 맡아 2010년 6월 24일에 개봉하였다. 2016년 5월 21일 안재현과 결혼을 공식 발표하였다. 또한 SNS를 통해 안재현의 공식 프로포즈 현장이 공개되 많은 팬들의 관심과 기대를 받고 있다. 결혼비용과 예식비용을 전부 소아과의 환아들을 위해 기부해 큰 화제를 낳았다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "\n",
    "INDEX_NAME = 'korquad'\n",
    "SEARCH_SIZE = 3\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
