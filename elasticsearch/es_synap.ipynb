{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# - synap 문서추출 필터를 이용해 추출한 문서들을 문장클러스터링 임베딩 10개를 생성하여 ES에 추가하는 예제\n",
    "# - synap 문서 추출해서 임베딩 벡터 생성하는 예시는 https://github.com/kobongsoo/BERT/blob/master/synap/mpower-sbert-Faiss-MRR-sentence-clustering.ipynb 참조\n",
    "#\n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "# - elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# - 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss\n",
    "\n",
    "# 추출 요약 설치\n",
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d4af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "import platform\n",
    "\n",
    "# os가 윈도우면 from eunjeon import Mecab \n",
    "if platform.system() == 'Windows':\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = '1' # 윈도우 환경에서는 쓰레드 1개로 지정함\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "import kss\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "\n",
    "# FutureWarning 제거\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging, getListOfFiles\n",
    "\n",
    "logger = mlogging(loggername=\"synap\", logfilename=\"../../log/synap\")\n",
    "device = GPU_info()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# 0. param 설정\n",
    "#------------------------------------------------------------------------------------\n",
    "seed = 111\n",
    "query_num = 0               # 쿼리 최대 갯수: KorQuAD_v1.0_dev.json 최대값은 5533개임, 0이면 모든 5533개 쿼리함.\n",
    "search_k = 5                # FAISS 검색시, 검색 계수(5=쿼리와 가장 근접한 5개 결과값을 반환함)\n",
    "avg_num = 1                 # 쿼리에 대해 sub 문장들중 최대 scorce를 갖는 문장을 몇개 찾고 평균낼지.(3=쿼리에 가장 유사한 sub문장 3개를 찾고 평균을 냄)\n",
    "faiss_index_method = 1      # 0= Cosine Similarity 적용(IndexFlatIP 사용), 1= Euclidean Distance 적용(IndexFlatL2 사용)\n",
    "\n",
    "# 임베딩 방식 (0=문장클러스터링, 1=문장평균임베딩, 2=문장임베딩)\n",
    "EMBEDDING_METHOD = 0    \n",
    "FLOAT_TYPE = 'float16'     # 'float32' 혹은 'float16' => 모델임베딩후 출력되는 벡터 float 타입을 지정.=>단 FAISS에 인덱스할때는 'float32'만 지원하므로, .astype('float32')로 형변환 해줘야 함.\n",
    "\n",
    "# 청크 분할 혹은 슬라이딩 윈도우param\n",
    "IS_SLIDING_WINDOW = False\n",
    "WINDOW_SIZE=256             # 문단을 몇 token으로 나눌지          (128,0)=>78.40%, (256, 64)=>75.20%\n",
    "SLIDING_SIZE=0              # 중첩되는 token \n",
    "\n",
    "# 클러스트링 param\n",
    "CLUSTRING_MODE = \"kmeans\"  # \"kmeans\" = k-평균 군집 분석, kmedoids =  k-대표값 군집 분석\n",
    "num_clusters = 10           # 클러스터링 계수 \n",
    "outmode = \"mean\"           # 클러스터링후 출력벡터 정의(kmeans 일때 => mean=평균벡터 출력, max=최대값벡터출력 / kmedoids 일때=>mean=평균벡터, medoid=대표값벡터)\n",
    "\n",
    "# ONNX 모델 사용 유.무\n",
    "IS_ONNX_MODEL = False\n",
    "\n",
    "# 차원 축소\n",
    "out_dimension = 128  # 768은 0으로 입력, 128=128 입력\n",
    "if out_dimension == 0:\n",
    "    dimension = 768\n",
    "\n",
    "# 문장 전처리\n",
    "remove_sentence_len = 8    # 문장 길이가 10이하면 제거 \n",
    "remove_duplication = False  # 중복된 문장 제거(*중복된 문장 제거 안할때 1%정도 정확도 좋음)\n",
    "\n",
    "use_bm25 = False           # BM25 출력 할지=True. 안할지=False\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "assert FLOAT_TYPE == 'float16' or FLOAT_TYPE == 'float32', f'FLOAT_TYPE은 float16, float32 만 입력가능합니다. 현재 입력 FLOAT_TYPE={FLOAT_TYPE}'\n",
    "assert CLUSTRING_MODE == 'kmeans' or CLUSTRING_MODE == 'kmedoids', f'CLUSTRING_MODE는 kmeans, kmedoids 만 입력가능합니다. 현재 입력 CLUSTRING_MODE={CLUSTRING_MODE}'\n",
    "assert EMBEDDING_METHOD == 0 or EMBEDDING_METHOD == 1 or EMBEDDING_METHOD == 2, f'EMBEDDING_METHOD는  0,1,2 만 입력가능합니다. 현재 입력 EMBEDDING_METHOD={EMBEDDING_METHOD}'\n",
    "assert out_dimension == 0 or out_dimension == 128, f'out_dimension는  0,128 만 입력가능합니다. 현재 입력 out_dimension={out_dimension}'\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.27:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "#es.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418e593-087a-4db8-b754-6c9f8deb2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분리 테스트\n",
    "import kss\n",
    "import time\n",
    "from myutils import split_sentences1,split_sentences\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "contexts = [\n",
    "'''\n",
    "클라우드서비스 구축 신청서\n",
    "신청인\n",
    "업체명 : \n",
    "사업자등록번호 : \n",
    "주소 : \n",
    "대표자\n",
    "전화번호 : \n",
    "서비스\n",
    "총괄 실무자\n",
    "성명 : \n",
    "전화번호 : \n",
    "이메일 : \n",
    "클라우드서비스 구축 기간\n",
    "※ 최대 6개월 이내\n",
    "클라우드서비스의 구분\n",
    "[ ] SaaS (Software as a Service) ( □ 간편등급 □ 표준등급 )\n",
    "※ 인증받은 IaaS 서비스명 : \n",
    "※ SaaS 보안인증 신청 시 중요자료를 다루는 전자결재, 회계관리, 인적자원관리, 보안서비스, PaaS 5개 분야를 제외하고는 모든 서비스 간편등급 신청이 가능\n",
    "클라우드서비스 명칭\n",
    "※ 구축하고자 하는 SaaS 서비스명\n",
    "클라우드서비스에 대한\n",
    "간략한 설명\n",
    "※ 구축될 클라우드서비스의 활용 분야(통신, 금융 등) 및 주요 기능 (이메일, Web Security)등 설명\n",
    "동의사항\n",
    "1. 본 시스템(서비스) 구축은 KISA의 클라우드 보안인증 수검을 위한 것으로, 다른 용도로 사용하지 않을 것을 서약하며, 본 용도 외 목적으로 사용중 발생하는 각종 문제에 대한 책임은 본 신청사에 있다. \n",
    "2. 구축 및 테스트를 위한 임시 사용 기간이 최대 6개월임을 인지하였으며, 연장시, 구축을 재신청하여야 함. 기간이 경과하였음에도 별도 재신청을 하지 않을 경우, IaaS 사업자는 자원을 회수하거나 접근 계정 삭제 등의 조치를 취할 수 있다. \n",
    "3. 신청사는 적절한 보안수준의 유지를 위하여 노력하여야 하며 각종 장애로 인하여 정상적인 서비스가 어려운 경우에 이를 신속하게 수리 및 복구하기 위해 성실히 협조한다. \n",
    "위와 같이 클라우드 보안인증 취득 준비를 위한 클라우드서비스 구축을 진행하고자 합니다.\n",
    "년 월 일\n",
    "신청인(대표자) \n",
    "(서명 또는 인)\n",
    "'''\n",
    "]  \n",
    "\n",
    "doc_sentences = split_sentences1(paragraphs=contexts, \n",
    "                                    remove_line=False, \n",
    "                                    remove_sentence_len=8, \n",
    "                                    remove_duplication=False, \n",
    "                                    check_en_ko=False, # 한국어 혹은 영어문장이외 제거하면, 즉 true 지정하면 1% 성능 저하됨\n",
    "                                    sentences_split_num=10000, paragraphs_num=10000000, showprogressbar=True, debug=False)\n",
    "\n",
    "print(len(doc_sentences[0]))\n",
    "\n",
    "print()\n",
    "print(f'*문장처리=>len:{len(doc_sentences[0])}, time:{time.time()-start:.4f}')\n",
    "\n",
    "for doc_sentence in doc_sentences[0]:\n",
    "    print(doc_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabad516-e286-46a4-8643-09058c504919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘라스틱 서치에 인덱스 생성 하고 dense 벡터 128차원 5개 추가하는 예제임\n",
    "'''\n",
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "\n",
    "# define the Elasticsearch client\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "\n",
    "mapping='''\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 2,\n",
    "    \"number_of_replicas\": 1\n",
    "  },\n",
    "   \"mappings\": {\n",
    "    \"dynamic\": \"true\",\n",
    "    \"_source\": {\n",
    "      \"enabled\": \"true\"\n",
    "    },\n",
    "    \"properties\": {\n",
    "      \"vector1\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "\t  \"vector2\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "      \"vector3\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "      \"vector4\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      },\n",
    "      \"vector5\": {\n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 128\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "###########################################################\n",
    "# 인덱스 생성/삭제\n",
    "###########################################################\n",
    "## 인덱스 생성\n",
    "def create_index(index, mapping=None):\n",
    "    if not es.indices.exists(index=index):\n",
    "        return es.indices.create(index=index ,body=mapping)\n",
    "    \n",
    "## 인덱스 자체 삭제\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "    \n",
    "###########################################################\n",
    "# 인데스에 데이터 추가 \n",
    "###########################################################\n",
    "def insert(index, doc_type, body):\n",
    "    return es.index(index=index, doc_type=doc_type, body=body)\n",
    "\n",
    "# index 생성\n",
    "INDEX_NAME = 'my_index_128_5-1'\n",
    "res=create_index(index=INDEX_NAME, mapping=mapping)\n",
    "print(res)\n",
    "print('\\n')\n",
    "\n",
    "result=es.indices.get_settings(index=INDEX_NAME)\n",
    "print(result)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# document 추가 \n",
    "DOC_TYPE = '_doc'\n",
    "\n",
    "# define the dense vector\n",
    "dense_vector1 = np.random.rand(128).tolist()\n",
    "dense_vector2 = np.random.rand(128).tolist()\n",
    "dense_vector3 = np.random.rand(128).tolist()\n",
    "dense_vector4 = np.random.rand(128).tolist()\n",
    "dense_vector5 = np.random.rand(128).tolist()\n",
    "\n",
    "doc = {\n",
    "    'vector1': dense_vector1,\n",
    "    'vector2': dense_vector2,\n",
    "    'vector3': dense_vector3,\n",
    "    'vector4': dense_vector4,\n",
    "    'vector5': dense_vector5\n",
    "}\n",
    "\n",
    "res = insert(index=INDEX_NAME, doc_type=DOC_TYPE, body=doc) \n",
    "print(res)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd5551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 1. 검색모델 로딩\n",
    "# => bi_encoder 모델 로딩, polling_mode 설정\n",
    "# => bi_encoder1 = SentenceTransformer(bi_encoder_path) # 오히려 성능 떨어짐. 이유는 do_lower_case나, max_seq_len등 세부 설정이 안되므로.\n",
    "#-------------------------------------------------------------------------------------\n",
    "import torch\n",
    "from myutils import bi_encoder, dense_model, onnx_model, onnx_embed_text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#bi_encoder_path = \"../../data11/model/bert/moco-sentencebertV2.0-nli_128d-sts\" \n",
    "bi_encoder_path = \"../../data11/model/kpf-sbert-128d-v1\" #\"bongsoo/kpf-sbert-v1.1\" # kpf-sbert-v1.1 # klue-sbert-v1 # albert-small-kor-sbert-v1.1\n",
    "pooling_mode = 'mean' # bert면=mean, albert면 = cls\n",
    "\n",
    "\n",
    "word_embedding_model1, bi_encoder1 = bi_encoder(model_path=bi_encoder_path, max_seq_len=512, do_lower_case=True, \n",
    "                                                pooling_mode=pooling_mode, out_dimension=out_dimension, device=device)\n",
    "  \n",
    "print(f'\\n---bi_encoder---------------------------')\n",
    "print(bi_encoder1)\n",
    "print(word_embedding_model1)\n",
    "#------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cd269-ea47-4432-823c-95e7204e6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import embed_text, onnx_embed_text\n",
    "\n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "def embedding(paragrphs:list):\n",
    "    if IS_ONNX_MODEL == True:\n",
    "        embeddings = onnx_embed_text(model=onnx_model, tokenizer=onnx_tokenizer, paragraphs=paragrphs, token_embeddings=False).astype(FLOAT_TYPE)  \n",
    "    else:\n",
    "        # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "        embeddings = embed_text(model=bi_encoder1, paragraphs=paragrphs, return_tensor=False).astype(FLOAT_TYPE)  \n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06716d8-2316-4d03-88f9-456062f7729e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b0b98-ff6d-421a-b979-401de24dd855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# 여기서 부터는 문서 인덱싱 처리 임.\n",
    "#========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bcfaf-3121-4fe6-81d8-6c542c84bc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from myutils import remove_reverse, getListOfFiles, clean_text\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 문서 추출된 TEXT들을 dataframe 형태로 만듬.\n",
    "OUT_FOLDER = '../../data11/mpower_doc/out/' # 추출된 TEXT 파일들이 있는 루트폴더\n",
    "\n",
    "# OUT_FOLDER에 모든 파일 경로를 얻어옴.\n",
    "file_paths = getListOfFiles(OUT_FOLDER)\n",
    "assert len(file_paths) > 0 # files가 0이면 assert 발생\n",
    "    \n",
    "print('*file_count: {}, file_list:{}'.format(len(file_paths), file_paths[0:5]))\n",
    "\n",
    "contexts = []\n",
    "titles = []\n",
    "contextids = []\n",
    "\n",
    "# TEXT 추출된 파일들을 읽어오면서 제목(title), 내용(contexts) 등을 저장해 둠.\n",
    "contextid = 1000\n",
    "for idx, file_path in enumerate(tqdm(file_paths)):\n",
    "    if '.ipynb_checkpoints' not in file_path:\n",
    "        sentences = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "            \n",
    "            #.PAGE:1 패턴을 가지는 문장은 제거함.\n",
    "            pattern = r\"\\.\\.PAGE:\\d+\\s?\"\n",
    "            data = clean_text(text=data, pattern=pattern)\n",
    "            \n",
    "            file_name = os.path.basename(file_path)  # 파일명만 뽑아냄\n",
    "            \n",
    "            #  filename = 5.보안사업부 사업계획.hwp.txt 이면 뒤에 hwp.txt는 제거하고 '5.보안사업부 사업계획' 문자열만 title로 저장함.\n",
    "            file_name = remove_reverse(file_name, '.')# 5.보안사업부 사업계획.hwp 출력됨\n",
    "            file_name = remove_reverse(file_name, '.')# 5.보안사업부 사업계획 출력됨\n",
    "            \n",
    "            contextid += 1\n",
    "            contexts.append(data)     # 파일 내용 저장 \n",
    "            titles.append(file_name)  # 파일명을 제목으로 저장(추후 쿼리할 문장이 됨)\n",
    "            contextids.append(contextid) # contextid 저장 \n",
    " \n",
    "# 데이터 프레임으로 만듬.\n",
    "df_contexts = pd.DataFrame((zip(contexts, contextids)), columns = ['context','contextid'])\n",
    "df_questions = pd.DataFrame((zip(titles, contextids)), columns = ['question','contextid'])\n",
    "\n",
    "print(f'*len(contexts): {len(contexts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281318d-dab9-4e17-b673-ec85fcc46979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contexts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bca9d-68e6-46cb-9702-22cb9f994352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d59d4e-614a-400b-b02a-d1b0304e65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------\n",
    "# 3. 슬라이딩 윈도우 혹은 문장 분리  \n",
    "# 1) 슬라이딩 윈도우 : 문장(문서)들을 chunk(청크: 큰 덩어리)로 분리, 다시 분리된 chunks를 kss로 문장 분리해서 sentences 만듬, 이후 chunks와 sentences를 doc_sentences에 담음\n",
    "#    최대 512 단위로 서로 겹치게 청크 단위로 분리함.\n",
    "# 2) 문장 분리 : kss와 \\n(줄바꿈)으로 문장을 분리함.\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "import time\n",
    "from myutils import sliding_window_tokenizer, split_sentences, split_sentences1, get_text_chunks\n",
    "\n",
    "#text = [\"해외에서 데이터 무제한?? 뿐 아니라 음성과 문자 요금을 할인 받을 수 있는 ++요금제가 나왔다. 날씨는 좋다. lg유플러스는 중국, 일본, 홍콩, 싱가포르, 필리핀 등 아시아 8국을 대상으로~~ 무제한 데이터와 음성, 문자를 할인해주는 '스마트 로밍 요금제' 2종을 오는 28일부터 판매한다고 27일 밝혔다. . . 우선 '스마트 로밍음성'은 하루 기본료가 2000원으로 음성발신은 1분당 1000원이며, 문자메시지(sms)와 멀티미디어 문자메시지(mms)는 1건당 150원이다. 종전 500원에서 350원 인하했다. . . '스마트 로밍패키지'는 여기에 데이터 무제한 로밍서비스가 더해져 하루 기본료가 1만1000원이다. 기본료는 로밍 기간에 상관없이 사용한 당일에만 청구된다. . . 스마트 로밍 요금제는 일단 오는 7월 말까지 프로모션 형태로 제공하고 이후 정식 요금제로 추진한다는 계획이다. . . 아울러 해외에서 무제한 데이터 로밍 서비스와 데이터로밍 차단을 신청·해지할 수 있는 로밍 전용 모바일 홈페이지((이메일))도 운영한다. . . (이름) lg유플러스 글로벌로밍팀장은 '아시아 출(이름) 여행을 계획하고 있는 고객들이 저렴한 비용으로 안심하고 로밍 서비스를 사용할 수 있도록 이번 프로모션 요금제를 출시했다'며 '지속적으로 해외 로밍 이용 고객들의 편의성 증대를 위한상품을 준비할 예정'이라고 말했다. . . . . . . . # # #. . ■ 사진설명. . lg유플러스(부회장 (이름) / (이메일) )가 4월 28일부터 중국, 일본 등 아시아 8개국*을 대상으로 데이터와 음성 문자까지 할인해서 제공하는 스마트 로밍요금제(스마트 로밍음성/스마트 로밍패키지) 2종을 출시했다.\"]\n",
    "#text =[\"오늘은 날씨가 ^^~~~~좋다. 내일은 비가 오고 춥겠다고 한다^^. 걱정이다. 오늘만큼만 매일 날씨가 좋으면 좋겠다.~~~~\"]\n",
    "contexts = df_contexts['context'].values.tolist()\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "doc_sentences = []\n",
    "tokenizer = word_embedding_model1.tokenizer\n",
    "    \n",
    "# 슬라이딩 윈도우 처리 후 chunks 리스트 만들고, 다시 chunks를 kss로 문장 분리해서, 최종 chunks와 sentences를 doc_sentences에 담음\n",
    "if IS_SLIDING_WINDOW == True:\n",
    "    for idx, context in enumerate(tqdm(contexts)):\n",
    "        \n",
    "        clean_context = clean_text(context)  # 전처리 : (한글, 숫자, 영문, (), {}, [], %, ,,.,\",')  등을 제외한 특수문자 제거\n",
    "        \n",
    "        pattern = r'^\\d+(\\.\\d+)*'  # 문장 맨앞에 '4.1.2.3' 패턴 제거,  r'^\\d+(\\.\\d+)*\\s+' # 문장 맨앞에 '4.1.2.3띄어쓰기' 있는 패턴 제거\n",
    "        clean_context = clean_text(text=clean_context, pattern=pattern)\n",
    "                        \n",
    "        pattern = r'^\\d+\\.'  # 문장 맨 앞에 '4.' 패턴 제거\n",
    "        clean_context = clean_text(text=clean_context, pattern=pattern)\n",
    "                        \n",
    "        pattern = r'^\\d+\\)'  # 문장 맨 앞에 '4)' 패턴 제거\n",
    "        clean_context = clean_text(text=clean_context, pattern=pattern)\n",
    "         \n",
    "              \n",
    "        #chunks = sliding_window_tokenizer(tokenizer = tokenizer, paragraph=clean_context, window_size=WINDOW_SIZE, sliding_size=SLIDING_SIZE)\n",
    "        \n",
    "        # 청크 클러스터링만 수행함\n",
    "        chunks = get_text_chunks(tokenizer = tokenizer, paragraph=clean_context, chunk_token_size=WINDOW_SIZE)\n",
    "        doc_sentences.append(chunks)\n",
    "        \n",
    "        #sentences = split_sentences1(paragraphs=chunks, remove_line=False, remove_sentence_len=remove_sentence_len, sentences_split_num=10000, paragraphs_num=10000000, debug=False)\n",
    "\n",
    "        # chunks 1차원 리스트[A,B,C]와 sentences 2차원 리스트[[a,b],[b,c],[d,e]] 를 합쳐서, doc_sentencs[A,a,b,B,b,c,C,d,e] 에 담음.\n",
    "        #arr = []\n",
    "        #[arr.extend([chunks[i], *sentences[i]]) for i in range(len(chunks))]\n",
    "\n",
    "        # doc_sentences 리스트에 추가 \n",
    "        #doc_sentences.append(arr)\n",
    "        \n",
    "# 문장 분리해서 doc_sentences에 담음.\n",
    "else:\n",
    "    doc_sentences = split_sentences1(paragraphs=contexts, \n",
    "                                    remove_line=False, \n",
    "                                    remove_sentence_len=remove_sentence_len, \n",
    "                                    remove_duplication=remove_duplication, \n",
    "                                    check_en_ko=False, # 한국어 혹은 영어문장이외 제거하면, 즉 true 지정하면 1% 성능 저하됨\n",
    "                                    sentences_split_num=10000, paragraphs_num=10000000, showprogressbar=True, debug=False)\n",
    "\n",
    "logger.info(f'*문장처리=>len:{len(doc_sentences[0])}, time:{time.time()-start:.4f}')\n",
    "\n",
    "len_list = []\n",
    "for i, doc_sentence in enumerate(doc_sentences):\n",
    "    doc_sentence_len = len(doc_sentence)\n",
    "    if i < 301:\n",
    "        print(f'[{i}] {doc_sentence_len}/{df_questions[\"question\"][i]}')\n",
    "    len_list.append(doc_sentence_len)\n",
    "    \n",
    "logger.info(f'*문장 길이=>평균:{sum(len_list) / len(len_list)} / MAX: {max(len_list)} / MIN: {min(len_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b38aa-1653-4e25-8aac-a4bffbbfea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# ES 인덱스 생성\n",
    "# -in : es : ElasticSearch 객체.\n",
    "# -in : create : 기존에 동일한 인덱스는 삭제하고 다시 생성.\n",
    "#---------------------------------------------------------------------------\n",
    "def create_index(es, create:bool = True):\n",
    "    \n",
    "    if create == True:\n",
    "        es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "        count = 0\n",
    "        \n",
    "        # 인덱스 생성\n",
    "        with open(INDEX_FILE) as index_file:\n",
    "            source = index_file.read().strip()\n",
    "            count += 1\n",
    "            print(f'{count}:{source}') # 인덱스 구조 출력\n",
    "            es.indices.create(index=INDEX_NAME, body=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34732814-3661-4382-8cd8-8f8b0316ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# 인덱스 batch 처리\n",
    "# - in: ES 객체\n",
    "# - in: docs=인덱스 처리할 data\n",
    "# - in: vector_len=한문서에 인덱싱할 벡터수=클러스터링수와 동일(기본=10개)\n",
    "# - in: dim_size=벡터 차원(기본=128)\n",
    "#---------------------------------------------------------------------------\n",
    "def index_batch(es, docs, vector_len:int=10, dim_size:int=128):\n",
    "        \n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        rfile_name = doc['rfile_name']\n",
    "        rfile_text = doc['rfile_text']\n",
    "        dense_vectors = doc['dense_vectors']\n",
    "        \n",
    "        #--------------------------------------------------------------------\n",
    "        # ES에 문단 인덱싱 처리\n",
    "        request = {}  #dict 정의\n",
    "        request[\"rfile_name\"] = rfile_name       # 제목               \n",
    "        request[\"rfile_text\"] = rfile_text   # 문장\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"        \n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        # vector 1~40 까지 값을 0으로 초기화 해줌.\n",
    "        for i in range(vector_len):\n",
    "            request[\"vector\"+str(i+1)] = np.zeros((dim_size))\n",
    "            \n",
    "        # vector 값들을 담음.\n",
    "        for i, dense_vector in enumerate(dense_vectors):\n",
    "            request[\"vector\"+str(i+1)] = dense_vector\n",
    "            \n",
    "        requests.append(request)\n",
    "        #--------------------------------------------------------------------\n",
    "                \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80aebdc-4ea8-4384-bf25-de323e627e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분리된 문장들에 대해 클러스터링 실행\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "#문단에 문장들의 임베딩을 구하여 각각 클러스터링 처리함.\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "# Then, we perform k-means clustering using sklearn:\n",
    "from sklearn.cluster import KMeans\n",
    "from myutils import embed_text, fassi_index, clustering_embedding, kmedoids_clustering_embedding\n",
    "\n",
    "def index_data():\n",
    "    #클러스터링 계수는 문단의 계수보다는 커야 함. \n",
    "    #assert num_clusters <= len(doc_sentences), f\"num_clusters:{num_clusters} > len(doc_sentences):{len(doc_sentences)}\"\n",
    "    #-------------------------------------------------------------\n",
    "    # 각 문단의 문장들에 벡터를 구하고 리스트에 저장해 둠.\n",
    "    start = time.time()\n",
    "    cluster_list = []\n",
    "\n",
    "    rfile_names = df_questions['contextid'].values.tolist()\n",
    "    rfile_texts = df_questions['question'].values.tolist()\n",
    "\n",
    "    docs = []\n",
    "    count = 0\n",
    "    for i, sentences in enumerate(tqdm(doc_sentences)):\n",
    "        embeddings = embedding(sentences)\n",
    "        if i < 3:\n",
    "            print(f'[{i}] sentences---------------------------EMBEDDING_METHOD={EMBEDDING_METHOD}')\n",
    "            print(sentences)\n",
    "            print(f'embeddings.shape: {embeddings.shape}')\n",
    "\n",
    "        # 0=문장클러스터링 임베딩\n",
    "        if EMBEDDING_METHOD == 0:\n",
    "            if CLUSTRING_MODE == \"kmeans\":\n",
    "                # 각 문단에 분할한 문장들의 임베딩 값을 입력해서 클러스터링 하고 평균값을 구함.\n",
    "                #emb1 = clustering_embedding(embeddings = embeddings, outmode=outmode, num_clusters= 50, seed=seed)\n",
    "                emb = clustering_embedding(embeddings = embeddings, outmode=outmode, num_clusters= num_clusters, seed=seed).astype(FLOAT_TYPE) \n",
    "            else:\n",
    "                emb = kmedoids_clustering_embedding(embeddings = embeddings, outmode=outmode, num_clusters= num_clusters, seed=seed).astype(FLOAT_TYPE) \n",
    "        # 1= 문장평균임베딩\n",
    "        elif EMBEDDING_METHOD == 1:\n",
    "            # 문장들에 대해 임베딩 값을 구하고 평균 구함.\n",
    "            arr = np.array(embeddings).astype(FLOAT_TYPE)\n",
    "            emb = arr.mean(axis=0).reshape(1,-1) #(128,) 배열을 (1,128) 형태로 만들기 위해 reshape 해줌\n",
    "        # 2=문장임베딩\n",
    "        else:\n",
    "            emb = embeddings\n",
    "\n",
    "        #emb.astype('float16')\n",
    "        if i < 2:\n",
    "            print(f'emb.shape: {emb.shape}')\n",
    "            print(f'emb:{emb[0]}')\n",
    "\n",
    "        #--------------------------------------------------- \n",
    "        count += 1\n",
    "        doc = {} #dict 선언\n",
    "\n",
    "        \n",
    "        doc['rfile_name'] = rfile_names[i]      # contextid 담음\n",
    "        doc['rfile_text'] = rfile_texts[i]      # text 담음.\n",
    "        doc['dense_vectors'] = emb\n",
    "\n",
    "        docs.append(doc)\n",
    "        #---------------------------------------------------    \n",
    "\n",
    "        # Faiss index 생성하고 추가 \n",
    "        #index = fassi_index(embeddings=emb, method=faiss_index_method)\n",
    "        #faissindexlist.append(index)\n",
    "\n",
    "        if count % BATCH_SIZE == 0:\n",
    "            index_batch(es, docs, vector_len=num_clusters, dim_size=dimension)\n",
    "            docs = []\n",
    "            print(\"Indexed {} documents.\".format(count))\n",
    "\n",
    "    if docs:\n",
    "        index_batch(es, docs, vector_len=num_clusters, dim_size=dimension)\n",
    "        print(\"Indexed {} documents.\".format(count))   \n",
    "\n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "    print(f'*인덱싱 시간 : {time.time()-start:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'mpower_128d_10_float16'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "INDEX_FILE = './data/mpower10u_128d_10.json'                 # 인덱스 구조 파일\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "create_index(es, True)\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9b23c-e3af-4768-835c-1530e6f44a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f2ded-7967-424e-b1c0-8621e104d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# 여기서 부터는 문서 검색 처리임.\n",
    "#========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d112d7-bb70-46ee-b4a3-8c624371ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# 검색\n",
    "#-----------------------------------------------------------\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "#-------------------------\n",
    "# param\n",
    "#-------------------------\n",
    "\n",
    "query_num = 0               # 쿼리 최대 갯수: KorQuAD_v1.0_dev.json 최대값은 5533개임, 0이면 모든 5533개 쿼리함.\n",
    "INDEX_NAME = 'mpower-kpf-128d-f16-variable'  # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "SEARCH_SIZE = 5             # 검색 계수\n",
    "\n",
    "# ES 벡터 크기 값(임의이 값지정) =>벡터의 크기는 각 구성 요소의 제곱 합의 제곱근으로 정의된다.. \n",
    "# 예를 들어, 벡터 [1, 2, 3]의 크기는 sqrt(1^2 + 2^2 + 3^2) 즉, 3.7416이 된다.\n",
    "# 클수록 -> 스코어는 작아짐, 작을수록 -> 스코어 커짐.\n",
    "VECTOR_MAG = 0.8   \n",
    "SEARCH_RESULT_OUT_TXT = 'sresult.txt'  # 검색결과 쿼리와 스코어를 저장하는 파일명\n",
    "\n",
    "#-------------------------\n",
    "\n",
    "# elastic 서버 접속 \n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "es = Elasticsearch(\"http://10.10.4.10:9200/\")\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7fcd2-875c-4a9b-9f7c-2c805b49ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# 쿼리 df 만듬.\n",
    "# => 인덱스내 데이터 조회 => query 이용해서 데이터 조회 후 쿼리 df 만듬\n",
    "# \n",
    "# GET /index명/_search\n",
    "#{\n",
    "#  \"_source\": [\"rfile_name\",\"rfile_text\"], \n",
    "#  \"query\": {\n",
    "#    \"match_all\": {}\n",
    "#  }\n",
    "# }\t\n",
    "#-----------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "def search(index_name, data=None, source:list=None):\n",
    "    \n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "    \n",
    "    if source is None:\n",
    "        body = {\"query\": data}\n",
    "    else:\n",
    "        body = {\"_source\":source, \"query\": data}\n",
    "    \n",
    "    #print(body)\n",
    "    \n",
    "    res = es.search(index=index_name, body=body)\n",
    "    return res\n",
    "\n",
    "# 쿼리로 rfile_name 1001 부터 1252까지 쿼리하면서 rfile_name과 rfile_text 불러옴.\n",
    "rfile_list = []\n",
    "\n",
    "for i in range(253):\n",
    "    contextid = 1000+i\n",
    "    data = {'rfile_name': contextid}\n",
    "    res=search(index_name=INDEX_NAME, data=data, source=[\"rfile_name\",\"rfile_text\"])\n",
    "\n",
    "    for hits in res['hits']['hits']:\n",
    "        rfile_name = hits['_source']['rfile_name']\n",
    "        rfile_text = hits['_source']['rfile_text']\n",
    "        \n",
    "        if rfile_name and rfile_text:\n",
    "            docs = {}\n",
    "            docs['rfile_name'] = rfile_name\n",
    "            docs['rfile_text'] = rfile_text\n",
    "            \n",
    "        rfile_list.append(docs)\n",
    "        break\n",
    "\n",
    "# 리스트를 불러와서 질의 dataframe 만듬\n",
    "contextids = []\n",
    "questions = []\n",
    "\n",
    "for i, rfile in enumerate(rfile_list):\n",
    "    rfile_name = rfile['rfile_name']\n",
    "    rfile_text = rfile['rfile_text']\n",
    "    \n",
    "    contextids.append(rfile_name)\n",
    "    questions.append(rfile_text)\n",
    "    \n",
    "    if i < 10:\n",
    "        print(f'{rfile_name} : {rfile_text}')\n",
    " \n",
    "# dataframe으로 만듬\n",
    "df_questions = pd.DataFrame((zip(questions, contextids)), columns = ['question','contextid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fdb14-f484-4f99-bedf-9bc787b771f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49700d-944b-48cf-a0a0-2a6ec3b6137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#----------------------------------------------------------\n",
    "# 질의 데이터 불러옴.\n",
    "#----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "filepath = '../../data11/mpower_doc/mquestion.json'\n",
    "\n",
    "# json파일 불러오기\n",
    "with open(filepath, 'r', encoding='UTF-8') as f:\n",
    "    json_data = json.load(f)\n",
    "df_questions = pd.DataFrame(json_data)\n",
    "\n",
    "print(df_questions.tail())\n",
    "print()\n",
    "print(df_questions['contextid'][0:5])\n",
    "print(df_questions['question'][0:5])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a879b-434f-4d87-beb1-d8db6cf41978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 쿼리문장 샘플링 후 임베딩 처리함\n",
    "#user_querys = [\"독도에서 사고가 나서 실종자가 발생했다.\", \"오늘 날씨가 흐리고 비가 오겠다.\"]\n",
    "#-------------------------------------------------------------------------------------\n",
    "import time\n",
    "from myutils import df_sampling, make_query_script\n",
    "\n",
    "#--------------------------------------------------\n",
    "# 쿼리 샘플링함.\n",
    "if query_num == 0:   # query_num = 0 이면 모든 쿼리\n",
    "    user_querys = df_questions['question'].values.tolist()\n",
    "else:   # query_num > 0이면 해당 계수만큼 랜덤하게 샘플링하여 쿼리 목록을 만듬.\n",
    "    df_questions = df_sampling(df=df_questions, num=query_num, seed=seed)\n",
    "    user_querys = df_questions['question'].values.tolist()\n",
    "  \n",
    "print(f'Query-----------------------------------------------------')\n",
    "print(user_querys[0:4])\n",
    "print()\n",
    "\n",
    "#--------------------------------------------------\n",
    "# 쿼리들에 대해 임베딩 값 구함\n",
    "start_embedding_time = time.time()\n",
    "\n",
    "embed_querys = embedding(user_querys)\n",
    "\n",
    "end_embedding_time = time.time() - start_embedding_time\n",
    "print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "print(f'*embed_querys.shape:{embed_querys.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d3d3d-9056-4a69-8ce7-57c3a22f4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_querys[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e1790-8a87-402c-9e83-4f19605ea4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_querys[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81bbbe-795e-44ae-a3ca-458a21c62a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# 쿼리 스크립트 구성후, ES로 쿼리 날림.\n",
    "#-------------------------------------------------------------------------------------\n",
    "from myutils import make_query_script\n",
    "\n",
    "bi_predictions_list = []\n",
    "start_search_time = time.time()\n",
    "    \n",
    " # 검색 결과를 파일로 저장\n",
    "with open(SEARCH_RESULT_OUT_TXT, 'w', encoding='utf-8') as f:\n",
    "        \n",
    "    for i, embed_query in enumerate(embed_querys):\n",
    "        script_query = make_query_script(query_vector=embed_query, vectormag=VECTOR_MAG, vectornum=10) # 쿼리를 만듬.\n",
    "\n",
    "        # 실제 ES로 검색 쿼리 날림\n",
    "        response = es.search(\n",
    "            index=INDEX_NAME,\n",
    "            body={\n",
    "                \"size\": SEARCH_SIZE * 3,  # 일단 3배 쿼리해둠.\n",
    "                \"query\": script_query,\n",
    "                \"_source\":{\"includes\": [\"rfile_name\",\"rfile_text\"]}\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #if i < 5:\n",
    "        #    print(\"{} total hits.\\n\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "\n",
    "        # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "        #print(response)\n",
    "        rfilename = []\n",
    "        rfiletext = [] \n",
    "        bi_scores = []\n",
    "        for hit in response[\"hits\"][\"hits\"]: \n",
    "            tmp = hit[\"_source\"][\"rfile_name\"]\n",
    "            # 중복 제거\n",
    "            if tmp and tmp not in rfilename:\n",
    "                rfilename.append(hit[\"_source\"][\"rfile_name\"])\n",
    "                rfiletext.append(hit[\"_source\"][\"rfile_text\"])\n",
    "                bi_scores.append(hit[\"_score\"])\n",
    "\n",
    "        if i < 3:\n",
    "            print(f'bi_scores: {bi_scores}')\n",
    "            print(f'frilename: {rfilename}')\n",
    "        \n",
    "         # 내림 차순으로 정렬\n",
    "        dec_bi_scores = reversed(np.argsort(bi_scores))\n",
    "        #if i < 3:\n",
    "        #    print(dec_bi_scores)\n",
    "        #    print()\n",
    "\n",
    "        # 내림차순으로 출력\n",
    "        #tmp_bi_predictions_list = []\n",
    "        #tmp_bi_predictions_list.append(rfilename)\n",
    "\n",
    "        # 파일로 쿼리와 결과 값 저장\n",
    "        writequery = \"[{}] {}\".format(i, user_querys[i])\n",
    "        if i < 5:\n",
    "            print(writequery)\n",
    "            \n",
    "        f.write('\\n'+writequery+'\\n')\n",
    "\n",
    "        for idx in dec_bi_scores:\n",
    "            writetext = \"{:.2f}\\t[contextid:{}] {}\".format(float(bi_scores[idx]), rfilename[idx], rfiletext[idx])\n",
    "            f.write(writetext+'\\n')\n",
    "            \n",
    "            if i < 5:\n",
    "                print(writetext)\n",
    "\n",
    "\n",
    "        # 2D 예측검색결과 리스트에 추가 \n",
    "        bi_predictions_list.append(rfilename[0:SEARCH_SIZE])\n",
    "\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    print(\"*검색시간: {:.2f} ms\".format(end_search_time * 1000)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527a0a1-21cb-450e-8004-4527aab060e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------\n",
    "# 6. MRR 계산\n",
    "# => 정답 리스트[2,3,1,4] 과 예측검색리스트[[1,2,5,1],[3,4,2,1],[6,5,4,1], [2,3,4,1]]를 입력하여 MRR 스코어 구함\n",
    "##--------------------------------------------------------------------------------------------------\n",
    "from myutils import mean_reciprocal_rank\n",
    "\n",
    "# 정답, 여기서는 contextid를 리스트로 만듬.\n",
    "ground_truths_list = df_questions['contextid'].values.tolist()\n",
    "#print(f'gtlen:{len(ground_truths_list)}')\n",
    "#print(ground_truths_list[0:9])\n",
    "\n",
    "logger.info(f'---Parameter------------------------------------------------------------')\n",
    "logger.info(f'쿼리/인덱싱 : EMBEDDING_METHOD={EMBEDDING_METHOD}(0=문장클러스터링, 1=문장평균임베딩, 2=문장임베딩), FLOAT_TYPE={FLOAT_TYPE}, seed={seed}, query_num={query_num}, search_k={search_k}, avg_num={avg_num}, faiss_index_method={faiss_index_method}')\n",
    "logger.info(f'슬라이딩 윈도우 : IS_SLIDING_WINDOW={IS_SLIDING_WINDOW}, WINDOW_SIZE={WINDOW_SIZE}, SLIDING_SIZE={SLIDING_SIZE}')\n",
    "logger.info(f'클러스터링 : CLUSTRING_MODE={CLUSTRING_MODE}, num_clusters={num_clusters}, outmode={outmode}')\n",
    "logger.info(f'ONNX: IS_ONNX_MODEL={IS_ONNX_MODEL}')\n",
    "logger.info(f'차원 축소: out_dimension={out_dimension}')\n",
    "logger.info(f'전처리 : remove_sentence_len={remove_sentence_len}, remove_duplication={remove_duplication}')\n",
    "\n",
    "# MRR 계산\n",
    "bi_ranks, bi_score = mean_reciprocal_rank(ground_truths_list, bi_predictions_list)\n",
    "\n",
    "# BI-MRR 출력\n",
    "logger.info(f'----------------------------------------------------------------------------')\n",
    "logger.info('*BI-ENCODER:{}'.format(bi_encoder_path))\n",
    "logger.info('*BI-MRR:{:.4f}'.format(bi_score))\n",
    "logger.info(f'*Ranks({len(bi_ranks)}):{bi_ranks[0:10]}')\n",
    "\n",
    "# 10개씩 출력해봄.\n",
    "if len(bi_ranks) > 10:\n",
    "    print()\n",
    "    print(f'BI_RANKS 10개씩 출력')\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    subarrays = [bi_ranks[i:i+10] for i in range(0, len(bi_ranks), 10)]\n",
    "    # Print the resulting subarrays\n",
    "    for i, subarray in enumerate(subarrays):\n",
    "        print(f\"{i}: {subarray}\")\n",
    "    \n",
    "# 검색 한 계슈\n",
    "#logger.info(f'---------------------------------------------------------------------------')\n",
    "search_count = 0\n",
    "nosearch_count = 0\n",
    "nosearch_list = []\n",
    "for i,item in enumerate(bi_ranks):\n",
    "    if item != 0:\n",
    "        search_count += 1\n",
    "    else:\n",
    "        nosearch_count += 1\n",
    "        nosearch_list.append(i)\n",
    "    \n",
    "logger.info('*검색률: {}/{}({:.2f}%)'.format(search_count, len(bi_ranks), (search_count/len(bi_ranks))*100))\n",
    "logger.info(f'---------------------------------------------------------------------------')\n",
    "\n",
    "print()\n",
    "print('*검색실패 : {}'.format(nosearch_count))\n",
    "for i, nosearch in enumerate(nosearch_list):\n",
    "    print(f'[{nosearch}] : {df_questions[\"question\"][nosearch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903662ba-c4ef-4739-8f76-7380b52de9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
