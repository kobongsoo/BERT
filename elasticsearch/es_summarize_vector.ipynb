{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# - 문장들을  추출 요약해서 요약문장을 만들고, 요약 문장의 평균을 구하여 문장 embedding을 생성하여 ES에 인덱스에 vector 추가하고, 검색하는 예제임\n",
    "# \n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "# - elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# - 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss\n",
    "\n",
    "# 추출 요약 설치\n",
    "#!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "import kss\n",
    "import numpy as np\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "# 추출 요약\n",
    "from summarizer.sbert import SBertSummarizer\n",
    "\n",
    "# FutureWarning 제거\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info\n",
    "device = GPU_info()\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "seed_everything(111)\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s-bert 모델 테스트\n",
    "#sbert_model_path = '../../../model/sbert/klue-sbert-v1.0'\n",
    "sbert_model_path = 'bongsoo/albert-small-kor-sbert-v1'\n",
    "\n",
    "#=====================================================================\n",
    "# 임베딩 모델 설정\n",
    "# - cpu 모델로 실행할때는 device=cpu, 기본은 GPU임\n",
    "embedder = SentenceTransformer(sbert_model_path, device=device)\n",
    "\n",
    "text = '나는 오늘 밥을 먹는다.'\n",
    "vectors = embedder.encode(text, convert_to_tensor=True)\n",
    "print(f'vector_len:{len(vectors)}')\n",
    "#=====================================================================\n",
    "# 추출요약 모델 설정\n",
    "summarizer_sbert_model_path = '../../../model/sbert/sbert-albert-small-nli-cls-64-sts-128ss'\n",
    "summarizer_model = SBertSummarizer(sbert_model_path)\n",
    "print(summarizer_model)\n",
    "#=====================================================================\n",
    "# crossencoder 모델 설정\n",
    "crossencoder_model_path = 'bongsoo/albert-small-kor-sbert-v1'\n",
    "crossencoder = CrossEncoder(crossencoder_model_path, device=device)\n",
    "print(crossencoder)\n",
    "#====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8240dd-9107-44ec-842e-eeb8a65f7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 테스트 \n",
    "\n",
    "paragraph = '''\n",
    "1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. \n",
    "이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. \n",
    "여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. \n",
    "그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. \n",
    "또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. \n",
    "결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. \n",
    "그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다. \n",
    "한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. \n",
    "이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. \n",
    "1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. \n",
    "이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. \n",
    "또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. \n",
    "이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다. \n",
    "이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. \n",
    "첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. \n",
    "또한 그것을 이어받는 동기도 중요한 역할을 한다. \n",
    "여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. \n",
    "주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. \n",
    "여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. \n",
    "마지막에는 주요 주제를 회상하면서 조용히 마친다.\n",
    "'''\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "paragraph_summarize = summarizer_model(paragraph, \n",
    "                                        min_length=10, \n",
    "                                        num_sentences=3)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print(f'time:{end_time}')\n",
    "print(f'summarize:{paragraph_summarize}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱 함수 \n",
    "def index_data():\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    \n",
    "    count = 0\n",
    "       \n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "      \n",
    "        count += 1\n",
    "        print(f'{count}:{source}')\n",
    "      \n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    # DATA 추기\n",
    "    with open(DATA_FILE) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            json_data = json.loads(line)\n",
    "            docs = []\n",
    "            \n",
    "            for data in tqdm(json_data):\n",
    "                count += 1\n",
    "                doc = {} # dict로 선언\n",
    "                \n",
    "                paragraph = data['paragraph']\n",
    "                \n",
    "                #================================================================\n",
    "                # 추출요약 적용된 경우에는 입력문장에 대한 요약문장 추출함.\n",
    "                if SUMMARIZER == True:\n",
    "                    paragraph_summarize = summarizer_model(paragraph, \n",
    "                                                           min_length=MIN_LENGTH, \n",
    "                                                           num_sentences=SUMMARIZER_NUM_SENTENCE)\n",
    "                    # 요약문이 있으면 제목. + 요약문  담고, 없으면 제목. + 원본문장 담음.\n",
    "                    if paragraph_summarize:\n",
    "                        doc['summarize'] = paragraph_summarize\n",
    "                    else:\n",
    "                        doc['summarize'] = paragraph\n",
    "                #================================================================\n",
    "                \n",
    "                doc['title'] = data['title']            # 제목 설정\n",
    "                doc['paragraph'] = data['paragraph']    # 문장 담음.\n",
    "                \n",
    "                docs.append(doc)\n",
    "                \n",
    "                if count % BATCH_SIZE == 0:\n",
    "                    index_batch(docs)\n",
    "                    docs = []\n",
    "                    print(\"Indexed {} documents.\".format(count))\n",
    "                  \n",
    "                # ** 10 개만 보냄\n",
    "                #if count >= 10:\n",
    "                #    break\n",
    "                    \n",
    "            if docs:\n",
    "                index_batch(docs)\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "                    \n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"=== End Done indexing===\")\n",
    "    \n",
    "    \n",
    "# 문단(paragraph)들 분리\n",
    "# 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "# =>굳이 elasticsearch에 문단 벡터는 추가하지 않고, title 벡터만 이용해도 되므로 주석처리함\n",
    "\n",
    "def paragraph_index(paragraph):\n",
    "    avg_paragraph_vec = np.zeros((1,768))\n",
    "    sent_count = 0\n",
    "    \n",
    "    # ** kss로 분할할때 히브리어: מר, 기타 이상한 특수문자 있으면 에러남. \n",
    "    # 따라서 여기서는 그냥 . 기준으로 문장을 나누고 평균을 구함\n",
    "    # 하나의 문장을 읽어와서 .기준으로 나눈다.\n",
    "    sentences = [sentence for sentence in paragraph.split('. ') if sentence != '']\n",
    "    for sent in sentences:\n",
    "        # 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "        avg_paragraph_vec += embed_text([sent])\n",
    "        sent_count += 1\n",
    "        \n",
    "    '''\n",
    "    # kss로 분할할때 줄바꿈 있으면, 파싱하는데 에러남.따라서 \"\\n\"는 제거함\n",
    "    paragraph = paragraph.replace(\"\\n\",\"\")\n",
    "    \n",
    "    print(\"==Start paragraph_index==\")\n",
    "    print(paragraph)\n",
    "    for sent in kss.split_sentences(paragraph):\n",
    "        # 문장으로 나누고, 해당 vector들의 평균을 구함.\n",
    "        avg_paragraph_vec += embed_text([sent])\n",
    "        sent_count += 1\n",
    "    '''\n",
    "    \n",
    "    avg_paragraph_vec /= sent_count\n",
    "    return avg_paragraph_vec.ravel(order='C') # 1차원 배열로 변경\n",
    "\n",
    "\n",
    "def index_batch(docs):\n",
    "   \n",
    "    # 제목 벡터를 구함\n",
    "    titles = [doc[\"title\"] for doc in docs]  \n",
    "    title_vectors = embed_text(titles)      \n",
    "    \n",
    "    # 원본문장 벡터를 구함 => 전체벡터를 구함\n",
    "    paragraphs = [doc[\"paragraph\"] for doc in docs]   \n",
    "     # * cpu로 문장별 평균을 구하는 경우에는 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "    #paragraph_vectors = [paragraph_index(doc[\"paragraph\"]) for doc in tqdm(docs)]\n",
    "    paragraph_vectors = embed_text(paragraphs)\n",
    "      \n",
    "    #=========================================================================\n",
    "    # 요약문 각 문장별 벡터를 구하고 평균을 냄\n",
    "    if SUMMARIZE_AVG_INDEX == True:\n",
    "        summarize_vectors = [paragraph_index(doc[\"summarize\"]) for doc in docs]\n",
    "    # 요약문 전체 벡터를 구함    \n",
    "    else:\n",
    "        paragraph_summarizes = [doc[\"summarize\"] for doc in docs]\n",
    "        summarize_vectors = embed_text(paragraph_summarizes)\n",
    "     #=========================================================================    \n",
    "   \n",
    "    requests = []\n",
    "    \n",
    "    # ES request 할 리스트 정의\n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        request = {}  #dict 정의\n",
    "        \n",
    "        # 요약벡터 = 타이틀벡터 + 요약벡터 의 평균으로 함\n",
    "        summarize_vector = (title_vectors[i] + summarize_vectors[i]) / 2\n",
    "        \n",
    "        request[\"title\"] = doc[\"title\"]            # 제목               \n",
    "        request[\"paragraph\"] = doc[\"paragraph\"]    # 문장\n",
    "        request[\"summarize\"] = doc[\"summarize\"]    # 요약문\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"        \n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        request[\"title_vector\"] = title_vectors[i]          # 제목 벡터\n",
    "        request[\"paragraph_vector\"] = paragraph_vectors[i]  # 문장 벡터\n",
    "        request[\"summarize_vector\"] = summarize_vector #summarize_vectors[i]  # 요약문 벡터\n",
    "        \n",
    "        requests.append(request)\n",
    "        \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "    \n",
    "# embedding 모델에서 vector를 구함    \n",
    "def embed_text(input):\n",
    "    vectors =  embedder.encode(input, convert_to_tensor=True)\n",
    "    return [vector.numpy().tolist() for vector in vectors]\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# ElasticSearch(이하:ES) 데이터 인텍싱\n",
    "# - ElasticSearch(이하:ES)에 KorQuAD_v1.0_train_convert.json 파일에 vector값을 구하여 index 함\n",
    "#\n",
    "# => index 명 : korquad\n",
    "# => index 구조 : index_1.json 파일 참조\n",
    "# => BATCH_SIZE : 100 => 100개의 vector값을 구하여, 한꺼번에 ES에 인텍스 데이터를 추가함.\n",
    "#======================================================================================\n",
    "INDEX_NAME = 'korquad-albert-small-kor-sbert-v1'      # ES 인덱스 명 (*소문자로만 지정해야 함)\n",
    "INDEX_FILE = './data/index_summarize.json'            # 인덱스 구조 파일\n",
    "DATA_FILE = './data/KorQuAD_v1.0_train_convert.json'  # 인덱싱할 파일경로\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "SUMMARIZER = True            # TRUE면 추출요약을 한다.\n",
    "SUMMARIZER_NUM_SENTENCE = 2  # 추출요약할때 몇문장으로 요약할지\n",
    "SUMMARIZE_AVG_INDEX = True   # True면 요약문장들에 대해 각 문장의 embedding을 구하고, 평균을 낸다.\n",
    "MIN_LENGTH = 20              # 추출요약할때 해당 길이보다 작은 문장은 제외함.\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info())\n",
    "\n",
    "# 2. index 처리\n",
    "index_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca387ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kibana 콘솔창에 접속해서 계수 확인\n",
    "# http://192.168.0.130:5601/app/dev_tools 에 접속해서 해야함\n",
    "\n",
    "## 입력 ##\n",
    "# GET korquad/_count\n",
    "\n",
    "## 출력 ###\n",
    "'''\n",
    "{\n",
    "  \"count\" : 1420,\n",
    "  \"_shards\" : {\n",
    "    \"total\" : 2,\n",
    "    \"successful\" : 2,\n",
    "    \"skipped\" : 0,\n",
    "    \"failed\" : 0\n",
    "}\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 하기\n",
    "\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['summarize_vector']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴(doc['summarize_vector'])\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"title\", \"summarize\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "\n",
    "        \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    summarizes = []\n",
    "    titles = [] \n",
    "    bi_scores = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        '''\n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[요약문]')\n",
    "        print(hit[\"_source\"][\"summarize\"]) \n",
    "        print()\n",
    "                \n",
    "        '''\n",
    "        # 리스트에 저장해둠\n",
    "        titles.append(hit[\"_source\"][\"title\"])\n",
    "        summarizes.append(hit[\"_source\"][\"summarize\"])\n",
    "        bi_scores.append(hit[\"_score\"])\n",
    "        \n",
    "    #========================================================================================================\n",
    "    # corssencoder 처리 \n",
    "    # => 위에서 응답받은 문장들을 다시 한번 [query, title] 문장쌍으로 만들어서 스코어를 구한다.\n",
    "    #========================================================================================================\n",
    "        \n",
    "    # crossencoder로 스코어 구하기 위해 [query, title] 쌍으로 문장을 만든다.\n",
    "    \n",
    "    start_cross_time = time.time()\n",
    "    \n",
    "    sentence_combinations = [[query, hit[\"_source\"][\"title\"]] for hit in response[\"hits\"][\"hits\"] if hit != '']\n",
    "    #print(sentence_combinations)\n",
    "    #print('\\n')\n",
    "        \n",
    "    cross_scores = crossencoder.predict(sentence_combinations)+1 \n",
    "    \n",
    "    end_cross_time = time.time() - start_cross_time\n",
    "        \n",
    "    # 내림 차순으로 정렬\n",
    "    dec_cross_scores = reversed(np.argsort(cross_scores))\n",
    "    \n",
    "    # 내림차순으로 출력\n",
    "    for idx in dec_cross_scores:\n",
    "        print(\"cross:{:.2f}\\bi:{:.2f}\\t[제목]:{}\\n{}\\n\".format(cross_scores[idx], bi_scores[idx], titles[idx], summarizes[idx]))\n",
    "    \n",
    "    # 처리 시간들 출력\n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print(\"cross time: {:.2f} ms\".format(end_cross_time * 1000))\n",
    "    print('\\n')\n",
    "     #========================================================================================================    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "# ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#====================================================================\n",
    "\n",
    "INDEX_NAME = 'korquad-albert-small-kor-sbert-v1' # 요약문 평균값 처리\n",
    "#INDEX_NAME = 'korquad-klue-sbert-v1.0-noavg' # 요약문 평균값 처리 안한경우\n",
    "\n",
    "SEARCH_SIZE = 20\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d090f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 추가하가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquad'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "title = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시'\n",
    "        ]\n",
    "\n",
    "paragraph = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "# 2. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(title, paragraph):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8054-dfc6-4289-8309-8f0710b16771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES 데이터 조회하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "###########################################################\n",
    "# 인덱스내 데이터 조회 => query 이용\n",
    "###########################################################\n",
    "def search(index, data=None):\n",
    "    if data is None: #모든 데이터 조회\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    res = es.search(index=index, body=body)\n",
    "    return res\n",
    "###########################################################\n",
    "\n",
    "# 모든 데이터 조회\n",
    "#sr = search(index=INDEX_NAME)\n",
    "#pprint.pprint(sr)\n",
    "\n",
    "# 단일 필드 조회\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "print(sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7fc52-530f-46f4-ae83-2ccca3fd5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 삭제하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 삭제 => query 이용\n",
    "############################################################\n",
    "def delete(index, data):\n",
    "    if data is None:  # data가 없으면 모두 삭제\n",
    "        data = {\"match_all\":{}}\n",
    "    else:\n",
    "        data = {\"match\": data}\n",
    "        \n",
    "    body = {\"query\": data}\n",
    "    return es.delete_by_query(index, body=body)\n",
    "\n",
    "############################################################\n",
    "## 2: 인덱스 내의 데이터 삭제 => id 이용\n",
    "############################################################\n",
    "def delete_by_id(index, id):\n",
    "    return es.delete(index, id=id)\n",
    "\n",
    "############################################################\n",
    "## 3: 인덱스 자체 삭제\n",
    "############################################################\n",
    "def delete_index(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        return es.indices.delete(index=index)\n",
    "\n",
    "\n",
    "# 1: query 이용 데이터 삭제\n",
    "delete(index=INDEX_NAME, data={'title':'한라산'})\n",
    "\n",
    "# 3: 인덱스 자체 삭제\n",
    "#delete_index(index=INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659658-75cf-4339-a88a-db338bddfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# ES index에 데이터 업데이트하기\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquad'\n",
    "\n",
    "# 1. elasticsearch 접속\n",
    "es = Elasticsearch(\"http://192.168.0.27:9200/\")\n",
    "print(es.info)\n",
    "\n",
    "############################################################\n",
    "## 1: 인덱스 내의 데이터 업데이트=>_id 에 데이터 업데이트\n",
    "############################################################\n",
    "def update(index, id, doc, doc_type):\n",
    "    \n",
    "    body = {\n",
    "        'doc': doc\n",
    "    }\n",
    "    \n",
    "    res=es.update(index=index, id=id, body=body, doc_type=doc_type)\n",
    "    return res\n",
    "############################################################\n",
    "\n",
    "#=====================================================================\n",
    "# 검색해서, _id, _type을 구함\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print('\\n')\n",
    "print(\"===[검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    id = hits[\"_id\"]      # id\n",
    "    type = hits[\"_type\"]  # type\n",
    "    \n",
    "    print(f'id: {id}')\n",
    "    print(f'type: {type}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # update 시킴\n",
    "    print(\"===[업데이트]===\")\n",
    "    doc = {'paragraph': '제주도는 대한민국에 가장 남쪽에 있는 섬으로, 인구는 약 71만명이며, 화산섬으로 관광자원이 많은 천혜의 관광지 이다.'}\n",
    "    print(doc)\n",
    "    print('\\n')\n",
    "    \n",
    "    ur=update(index=INDEX_NAME, id=id, doc=doc, doc_type=type)\n",
    "    print(\"===[업데이트 결과]===\")\n",
    "    print(ur)\n",
    "    print('\\n')\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "# 인덱스 refresh 함\n",
    "# elasticsearch의 자동 새로고침의 시간은 1초 정도 소요\n",
    "# 따라서 코드에 아래 명령어를 입력하지 않았을 경우 검색을 하지 못할 가능성도 존재\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "\n",
    "# 제주도로 검색해서 한번더 확인\n",
    "sr = search(index=INDEX_NAME, data = {'title': '제주도'})\n",
    "\n",
    "print(\"===[재검색 결과]===\")\n",
    "for hits in sr[\"hits\"][\"hits\"]:\n",
    "    \n",
    "    print(f'id:{hits[\"_id\"]}')\n",
    "    print(f'type: {hits[\"_type\"]}')\n",
    "    print(f'title:{hits[\"_source\"][\"title\"]}')\n",
    "    print(f'paragraph:{hits[\"_source\"][\"paragraph\"]}')\n",
    "    \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
