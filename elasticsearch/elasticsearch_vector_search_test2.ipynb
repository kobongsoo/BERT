{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cd1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ElasticSearch 텍스트 임베딩 테스트 예제\n",
    "# -여기서는 elasticsearch 7.17.3 때를 기준으로 설명함.\n",
    "# -** 따라서 elasticsearch python 모듈도 7.17.3 을 설치해야 함\n",
    "#\n",
    "# elasticsearch 모듈 8.x 부터는 구문의 많이 변경되었음.\n",
    "# 예 : index 생성:  body로 모든 변수들를 지정하는 데시, 명시적으로 모든 변수들을 최상으로 지정해 줘야함.\n",
    "# => 참고: https://towardsdatascience.com/important-syntax-updates-of-elasticsearch-8-in-python-4423c5938b17   \n",
    "#\n",
    "# =>ElasticSearch 7.3.0 버전부터는 cosine similarity 검색을 지원한다.\n",
    "# => 데이터로 고차원벡터를 집어넣고, 벡터형식의 데이터를 쿼리(검색어)로 하여 \n",
    "# 코사인 유사도를 측정하여 가장 유사한 데이터를 찾는다.\n",
    "# => 여기서는 ElasticSearch와 S-BERT를 이용함\n",
    "# => ElasticSearch에 index 파일은 index_1.json /데이터 파일은 KorQuAD_v1.0_train_convert.json 참조\n",
    "#\n",
    "# => 참고자료 : https://skagh.tistory.com/32\n",
    "#\n",
    "# => 참고로 여기서는 title_vector 만 구함, paragrapha_vector는 cpu에서는 엄청 오래 걸려서 주석처리하였음\n",
    "\n",
    "#===========================================================================================\n",
    "\n",
    "# sentenceTransformers 라이브러리 설치\n",
    "#!pip install -U sentence-transformers\n",
    "\n",
    "# elasticsearch 서버 접속 모듈 설치\n",
    "# !pip install elasticsearch==7.17\n",
    "\n",
    "# 한국어 문장 분리기(kss) 설치\n",
    "#!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9d4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kss, numpy\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "# elastic 서버 접속 테스트\n",
    "#es = Elasticsearch(\"https://192.168.0.91:9200/\", verify_certs=False)\n",
    "#es = Elasticsearch(\"http://192.168.0.130:9200/\")\n",
    "#es.info()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# onnxmodel 일때는 True로 해줌.\n",
    "onnxmodel=True\n",
    "\n",
    "# es 인덱스 관련 변수들 정의\n",
    "INDEX_NAME = 'korquadonnx'       #인덱스 명\n",
    "INDEX_FILE = './data/index.json' #인덱스 구성 파일\n",
    "\n",
    "# es 접속 서버 \n",
    "ES_SERVER_IP_PORT= \"http://192.168.0.27:9200/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#===============================================================================================\n",
    "# 1. sbert 모델 로딩\n",
    "#===============================================================================================\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# s-bert 모델 \n",
    "sbert_model_path = '../../sentencebert_v1.0'\n",
    "\n",
    "# cpu 모델로 실행할때는 device=cpu, 기본은 GPU임\n",
    "embedder = SentenceTransformer(sbert_model_path, device=device)\n",
    "\n",
    "text = '나는 오늘 밥을 먹는다.'\n",
    "vectors = embedder.encode(text, convert_to_tensor=True)\n",
    "vector_list = [vector.numpy().tolist() for vector in vectors]\n",
    "\n",
    "print(f'vector_len:{len(vector_list)}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1baebba8-b9fa-4782-8c3a-1507449c3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<optimum.onnxruntime.modeling_ort.ORTModelForFeatureExtraction object at 0x000001E7B21B44F0>\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "#===============================================================================================\n",
    "# 1. ONNX 모델 로딩\n",
    "#===============================================================================================\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "model_path='../../sbertonnx'\n",
    "vocab_path='../../sbertonnx'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(vocab_path)\n",
    "model=ORTModelForFeatureExtraction.from_pretrained(model_path)\n",
    "print(model)\n",
    "\n",
    "# 테스트 \n",
    "text = '나는 오늘 밥을 먹는다.'\n",
    "input_token=tokenizer(text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "#print(input_token)\n",
    "\n",
    "outputs = model(**input_token)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "vectors = []\n",
    "for hidden in last_hidden_state:\n",
    "    # 평균값 구함\n",
    "    mean_hidden = torch.mean(hidden, dim=0)\n",
    "    vectors.append(mean_hidden)\n",
    "    \n",
    "print(len(vectors[0]))\n",
    "#print(vectors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca8aefa-139e-48ee-aeb0-45aafa0aeac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Elasticsearch.info of <Elasticsearch([{'host': '192.168.0.130', 'port': 9200}])>>\n"
     ]
    }
   ],
   "source": [
    "#===============================================================================================\n",
    "# 2. elasticsearch 접속\n",
    "#===============================================================================================\n",
    "es = Elasticsearch(ES_SERVER_IP_PORT)\n",
    "print(es.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03f63aa-552b-4062-b9f0-88693321d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bong9\\anaconda3\\envs\\pytorch\\lib\\site-packages\\elasticsearch\\connection\\base.py:208: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:{\n",
      "  \"settings\": {\n",
      "    \"number_of_shards\": 2,\n",
      "    \"number_of_replicas\": 1\n",
      "  },\n",
      "   \"mappings\": {\n",
      "    \"dynamic\": \"true\",\n",
      "    \"_source\": {\n",
      "      \"enabled\": \"true\"\n",
      "    },\n",
      "    \"properties\": {\n",
      "      \"title\": {\n",
      "        \"type\": \"text\"\n",
      "      },\n",
      "\t  \"paragraph\": {\n",
      "        \"type\": \"text\"\n",
      "      },\n",
      "      \"title_vector\": {\n",
      "        \"type\": \"dense_vector\",\n",
      "        \"dims\": 768\n",
      "      },\n",
      "\t  \"paragraph_vector\": {\n",
      "        \"type\": \"dense_vector\",\n",
      "        \"dims\": 768\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#===============================================================================================\n",
    "# 3. Indices(index) 생성\n",
    "#===============================================================================================\n",
    "\n",
    "# index 삭제 후 재생성\n",
    "def index_create(INDEX_NAME, INDEX_FILE):\n",
    "    # 인덱스 삭제\n",
    "    es.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "    \n",
    "    count = 0\n",
    "    # 인덱스 생성\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "      \n",
    "        count += 1\n",
    "        print(f'{count}:{source}')\n",
    "      \n",
    "        es.indices.create(index=INDEX_NAME, body=source)\n",
    "       \n",
    "# korquadonnx 인덱스 생성    \n",
    "index_create(INDEX_NAME, INDEX_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62aaaa4c-3d3b-4922-9bde-05959666ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_batch(docs):\n",
    "   \n",
    "    titles = [doc[\"title\"] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "    \n",
    "    # 문장이 길면 분할해서 embedding을 구해야 하는데, 여기서는 분할하지 않고 embedding을 구함\n",
    "    paragraphs = [doc[\"paragraph\"] for doc in docs]\n",
    "    paragraph_vectors = embed_text(paragraphs)\n",
    "    \n",
    "    # * cpu로 문단은 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "    #paragraph_vectors = [paragraph_index(doc[\"paragraph\"]) for doc in tqdm(docs)]\n",
    "    requests = []\n",
    "    \n",
    "    for i, doc in enumerate(tqdm(docs)):\n",
    "        \n",
    "        #request[\"title\"] = doc[\"title\"]           # 이렇게 등록은 안됨\n",
    "        #request[\"paragraph\"] = doc[\"paragraph\"]\n",
    "        \n",
    "        request = doc\n",
    "        \n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        \n",
    "        request[\"title_vector\"] = title_vectors[i]\n",
    "        request[\"paragraph_vector\"] = paragraph_vectors[i]\n",
    "        #request[\"paragraph_vector\"] = paragraph_vectors[i]  # * cpu로 문단은 임베딩하는데 너무 오래 걸리므로 주석처리함\n",
    "        \n",
    "        requests.append(request)\n",
    "        \n",
    "    # batch 단위로 한꺼번에 es에 데이터 insert 시킴     \n",
    "    bulk(es, requests)\n",
    "    \n",
    "# embedding 모델에서 vector를 구함    \n",
    "def embed_text(input):\n",
    "    #===================================================================\n",
    "    # onnx 모델일때 => last_hidden_state 의 평균값으로 임베딩값구함 \n",
    "    #===================================================================\n",
    "    if onnxmodel: \n",
    "        # onnx 모델일때\n",
    "        input_token=tokenizer(input, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "        #print(input_token)\n",
    "        outputs = model(**input_token)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        #print(last_hidden_state.shape)\n",
    "\n",
    "        vectors = []\n",
    "        for hidden in last_hidden_state:\n",
    "            # 평균값 구함\n",
    "            mean_hidden = torch.mean(hidden, dim=0)\n",
    "            vectors.append(mean_hidden)\n",
    "        #return vectors\n",
    "    #===================================================================\n",
    "    # sentenctbert 모델일때\n",
    "    #===================================================================\n",
    "    else:\n",
    "        vectors =  embedder.encode(input, convert_to_tensor=True)\n",
    "    \n",
    "    return [vector.numpy().tolist() for vector in vectors] \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437324f3-e124-4526-bcac-0027f5b287cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06d61b99a1a42038a6b35011a5d8764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 15 documents(end).\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================================\n",
    "# 4. ES indices(index) 데이터 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "#===============================================================================================\n",
    "\n",
    "# ES에 이미 생성된 index\n",
    "INDEX_NAME = 'korquadonnx'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "# 1.추가할 데이터 준비\n",
    "titles = [\n",
    "    '제주도', \n",
    "    '한라산',\n",
    "    '서울특별시',\n",
    "    '파리',\n",
    "    '로마',\n",
    "    '프랑스',\n",
    "    '마이크로소프트',\n",
    "    '캥거루',\n",
    "    '호랑이',\n",
    "    '애플',\n",
    "    '뉴턴',\n",
    "    '레오나르도 다빈치',\n",
    "    '기린',\n",
    "    '해바라기',\n",
    "    '튤립',\n",
    "        ]\n",
    "\n",
    "paragraphs = [\n",
    "    '대한민국의 남서쪽에 있는 섬. 행정구역상 광역자치단체인 제주특별자치도의 관할. 한국의 섬 중에서 가장 크고 인구가 많은 섬으로 면적은 1833.2㎢이다. 제주도 다음 2번째 큰 섬인 거제도의 5배 정도 된다. 인구는 약 68만 명.',\n",
    "    '대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.',\n",
    "    '대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다',\n",
    "    '파리는 프랑스의 수도이자 최대도시이다. 파리는 세계적으로도 문화, 정치, 외교, 경제의 큰 영향력을 끼치는 대도시이며, 특히 파리는 금융업이 발달해 있다',\n",
    "    '로마는 이탈리아의 수도이자 최대 도시로, 라치오주의 주도이며, 테베레 강 연안에 있다.',\n",
    "    '프랑스 공화국 약칭 프랑스는 서유럽의 본토와 남아메리카의 프랑스령 기아나를 비롯 대륙에 걸쳐 있는 해외 레지옹과 해외 영토 국가로서 EU 소속 국가 중 영토가 가장 넓다. 수도는 파리이다.',\n",
    "    '컴퓨팅 파워를 지원해주는 클라우드 컴퓨팅 사업을 중심으로, 기업들을 지원하는 파워포인트, 워드와 엑셀, 원노트, 아웃룩, 팀즈 등의 오피스 365, Xbox 게임, 컴퓨터 운영체제 소프트웨어인 윈도우 등의 사업을 하는 미국의 기업이다.',\n",
    "    '유대류 캥거루과에 속하는 초식동물의 총칭. 호주 대륙 본토 및 호주 북쪽의 뉴기니 섬에서 서식한다.',\n",
    "    '호랑이 또는 범, 칡범, 갈범은 식육목 고양이과에 속하는 맹수다. 어린 개체는 개호주라 부른다. 고양이과 동물 중 그 크기가 가장 크다',\n",
    "    '애플은 미국 캘리포니아의 아이폰, 아이패드, 애플 워치, 에어팟, 아이맥, 맥북, 맥 스튜디오와 맥 프로, 홈팟 등의 하드웨어와 iOS, iPadOS, macOS 등의 소프트웨어를 설계, 디자인하는 기업이다.',\n",
    "    '아이작 뉴턴 경은 잉글랜드의 수학자, 물리학자, 천문학자이다. 1687년 발간된 자연철학의 수학적 원리는 고전역학과 만유인력의 기본 바탕을 제시하며, 과학사에서 영향력 있는 저서 중의 하나로 꼽힌다',\n",
    "    '레오나르도 디 세르 피에로 다 빈치는 이탈리아 르네상스를 대표하는 석학이다. 화가이자 조각가, 발명가, 건축가, 해부학자, 지리학자, 음악가였다.',\n",
    "    '기린은 기린과의 포유동물로서 기린속에 속하는 동물의 총칭이다. 목이 긴 육상 동물이며 또한 가장 큰 반추 동물이기도 하다.',\n",
    "    '해바라기는 중앙아메리카가 원산지인 한해살이풀로, 해를 닮은 노란 꽃이 상당히 인상적인 식물이다.',\n",
    "    '튤립은 백합과의 여러해살이풀로 산자고속 식물의 총칭이다. 울금향이라고도 한다. 남동 유럽과 중앙아시아가 원산지이다.',\n",
    "            ]\n",
    "\n",
    "# {'paragraph': \"\", 'title': \"\"}\n",
    "\n",
    "\n",
    "doc = {}\n",
    "docs = []\n",
    "count = 0\n",
    "\n",
    "# 3. batch 사이즈 만큼식 ES에 추가\n",
    "# => 추가할 데이터는 {'paragraph': 내용, 'title': 제목} 기존 입려된 방식대로(사전) 입력 되어야 함\n",
    "for title, paragraph in zip(titles, paragraphs):\n",
    "    doc = {}\n",
    "    doc['paragraph'] = paragraph\n",
    "    doc['title'] = title\n",
    "    docs.append(doc)\n",
    "    count += 1\n",
    "    if count % BATCH_SIZE == 0:\n",
    "        index_batch(docs)\n",
    "        docs = []\n",
    "        print(\"Indexed {} documents.\".format(count))\n",
    "   \n",
    "# docs 이 있으면 전송\n",
    "if docs:\n",
    "    index_batch(docs)\n",
    "    print(\"Indexed {} documents(end).\".format(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d3e7a7-846c-4250-96b6-eb3e88cfb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query():\n",
    "    \n",
    "    query = input(\"검색어 입력: \")\n",
    "    \n",
    "    start_embedding_time = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    end_embedding_time = time.time() - start_embedding_time\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['paragraph_vector']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print('query\\n')\n",
    "    #print(script_query)\n",
    "    \n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"title\", \"paragraph\"]}\n",
    "        }\n",
    "    )\n",
    "    end_search_time = time.time() - start_search_time\n",
    "    \n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"])) \n",
    "    print(\"embedding time: {:.2f} ms\".format(end_embedding_time * 1000)) \n",
    "    print(\"search time: {:.2f} ms\".format(end_search_time * 1000)) \n",
    "    print('\\n')\n",
    "    \n",
    "    # 쿼리 응답 결과값에서 _id, _score, _source 등을 뽑아냄\n",
    "    # print(response)\n",
    "    \n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        \n",
    "        print(\"index:{}, type:{}\".format(hit[\"_index\"], hit[\"_type\"]))\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"])) \n",
    "        \n",
    "        print(f'[제목] {hit[\"_source\"][\"title\"]}')\n",
    "        \n",
    "        print('[내용]')\n",
    "        print(hit[\"_source\"][\"paragraph\"]) \n",
    "        \n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ffc733-f3cf-4679-8492-7a5d35c5bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  가장 가보고 싶은 곳은?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 total hits.\n",
      "embedding time: 8.00 ms\n",
      "search time: 29.99 ms\n",
      "\n",
      "\n",
      "index:korquadonnx, type:_doc\n",
      "id: P15yA4EBWwNQ7PQ9mihl, score: 1.2043557\n",
      "[제목] 한라산\n",
      "[내용]\n",
      "대한민국에서 가장 큰 섬인 제주도에 있으며 대한민국의 실효지배 영토 내의 최고봉이자 가장 높은 산(해발 1,947m). 대한민국의 국립공원 중 하나이다. 국립공원 전역이 유네스코 세계유산으로 지정되었다.\n",
      "\n",
      "index:korquadonnx, type:_doc\n",
      "id: Ql5yA4EBWwNQ7PQ9mihl, score: 1.1954284\n",
      "[제목] 로마\n",
      "[내용]\n",
      "로마는 이탈리아의 수도이자 최대 도시로, 라치오주의 주도이며, 테베레 강 연안에 있다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  너무나 예쁜 것들\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 total hits.\n",
      "embedding time: 6.00 ms\n",
      "search time: 26.99 ms\n",
      "\n",
      "\n",
      "index:korquadonnx, type:_doc\n",
      "id: S15yA4EBWwNQ7PQ9mihl, score: 1.2000068\n",
      "[제목] 해바라기\n",
      "[내용]\n",
      "해바라기는 중앙아메리카가 원산지인 한해살이풀로, 해를 닮은 노란 꽃이 상당히 인상적인 식물이다.\n",
      "\n",
      "index:korquadonnx, type:_doc\n",
      "id: QF5yA4EBWwNQ7PQ9mihl, score: 1.1165975\n",
      "[제목] 서울특별시\n",
      "[내용]\n",
      "대한민국의 수도인 서울은 현대적인 고층 빌딩, 첨단 기술의 지하철, 대중문화와 예것이 공존하는 대도시. 주목할 만한 명소로는 초현대적 디자인의 컨벤션 홀인 동대문디자인플라자, 한때 7,000여 칸의 방이 자리하던 경복궁, 조계사가 있다\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어 입력:  무서운 것들\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 total hits.\n",
      "embedding time: 5.98 ms\n",
      "search time: 29.99 ms\n",
      "\n",
      "\n",
      "index:korquadonnx, type:_doc\n",
      "id: Rl5yA4EBWwNQ7PQ9mihl, score: 1.1253055\n",
      "[제목] 호랑이\n",
      "[내용]\n",
      "호랑이 또는 범, 칡범, 갈범은 식육목 고양이과에 속하는 맹수다. 어린 개체는 개호주라 부른다. 고양이과 동물 중 그 크기가 가장 크다\n",
      "\n",
      "index:korquadonnx, type:_doc\n",
      "id: RV5yA4EBWwNQ7PQ9mihl, score: 1.0958793\n",
      "[제목] 캥거루\n",
      "[내용]\n",
      "유대류 캥거루과에 속하는 초식동물의 총칭. 호주 대륙 본토 및 호주 북쪽의 뉴기니 섬에서 서식한다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================================\n",
    "# 5. ES 인덱싱된 내용 검색 \n",
    "# => cosineSimilarity 스크립트를 이용하여 ES로 query 함(*이때 SEARCH_SIZE를 몇개로 할지 지정할수 있음)\n",
    "# => 쿼리 응답 결과 값에서 _id, _score, _source 등을 뽑아냄\n",
    "#==============================================================================================\n",
    "INDEX_NAME = 'korquadonnx'\n",
    "SEARCH_SIZE = 2\n",
    "\n",
    "# 2. query 처리\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c76dd-7a48-4d1f-8c90-82cbb59bc367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0a7e3-daab-4ac4-9f66-d76aef0396a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2816dd-19c7-4753-8c17-bfa0df37c1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19509709-2646-44b5-af00-6cc73a551582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
