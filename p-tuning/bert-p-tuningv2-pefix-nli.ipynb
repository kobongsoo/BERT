{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6758d6d2-adb0-48a6-b62c-523410ea2168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bert-p-tuing_2022-07-12.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================================\n",
    "# P-TUNING NLI(Natural Language Interference:자연어 추론) 훈련 예제\n",
    "#\n",
    "# => input_ids : [CLS]senetence1(전제)[SEP]sentence2(가설)\n",
    "# => attention_mask : 1111111111(전체,가설)0000000(그외)\n",
    "# => token_type_ids : 0000000(전제)1111111(가설)00000000(그외)\n",
    "# => laels : 참(수반:entailment), 거짓(모순:contradiction), 모름(중립:neutral)\n",
    "#\n",
    "#\n",
    "# prefix-tuning => GPT-2, T5등 LM에서 접두사 prompt를 추가하여 훈련시키는 방식\n",
    "#\n",
    "# p-tuning => P-tuning은 prefix-tuning보다 유연 합니다. \n",
    "# 시작할 때뿐만 아니라 프롬프트 중간에 학습 가능한 토큰을 삽입하기 때문입니다. \n",
    "# https://github.com/THUDM/P-tuning\n",
    "#\n",
    "# p-tuing v2 => 새로운 방식이 아니라 NLU 향상을 위해, MLM 모델에 prefix-tuning을 적용한 방식\n",
    "# https://github.com/THUDM/P-tuning-v2\n",
    "#=======================================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, set_seed\n",
    "import os\n",
    "from os import sys\n",
    "from transformers import BertTokenizer, HfArgumentParser, TrainingArguments\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "logger = mlogging(loggername=\"bert-p-tuing\", logfilename=\"bert-p-tuing\")\n",
    "device = GPU_info()\n",
    "\n",
    "model_path = '../../data11/model/bert/bert-multilingual-cased'\n",
    "\n",
    "#tokenize 설정\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path, TOKENIZERS_PARALLELISM=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8631b209-d129-4a7a-bf7c-f4c64c79c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "20\n",
      "True\n",
      "steps\n",
      "3e-05\n"
     ]
    }
   ],
   "source": [
    "# 인자들 설정\n",
    "#======================================================\n",
    "# data args 인자 설정 \n",
    "class data_args:\n",
    "    dataset_name='rte'\n",
    "    pad_to_max_length = True\n",
    "    max_seq_length = 128\n",
    "    overwrite_cache = True\n",
    "    max_train_samples = None\n",
    "    max_eval_samples = None\n",
    "    max_predict_samples = None\n",
    "    \n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# 허깅페이스 TrainingArguments 설정\n",
    "training_args = TrainingArguments(\"p-tuning-bert-test\")\n",
    "\n",
    "# run_rte_bert.sh 에 사용된 인자만 새롭게 정의함.\n",
    "training_args.do_train=True\n",
    "training_args.do_eval=True\n",
    "training_args.seed = 111\n",
    "training_args.per_device_train_batch_size=32  #batch_size\n",
    "training_args.learning_rate =3e-5        #lr\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"steps\"  #eval 언제마다 할지 => no, steps, epoch\n",
    "training_args.num_train_epochs=50 # epochs\n",
    "training_args.output_dir = '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-20'\n",
    "\n",
    "#training_args.report_to = \"none\"  # 기본은 all로 , all이면 wandb에도 기록된다.\n",
    "#======================================================\n",
    "   \n",
    "#======================================================\n",
    "# p-tuningv2 prefixt 튜닝일때 설정값 지정 \n",
    "pre_seq_len = 20             # prefix 계수\n",
    "prefix_projection = True     # True = two-layer MLP 사용함(Multi-layer perceptron(다중퍼셉트론))\n",
    "prefix_hidden_size = 512     # prefix hidden size\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "#seed 설정\n",
    "#set_seed(training_args.seed)\n",
    "seed_everything(training_args.seed)\n",
    "#======================================================\n",
    "\n",
    "print(training_args.fp16)\n",
    "print(training_args.get_process_log_level())\n",
    "print(training_args.do_train)\n",
    "print(training_args.evaluation_strategy)\n",
    "print(training_args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ff649f-7706-49b3-a6cf-cf7cc02d4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at ../../data11/korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "loading data... LOOKING AT ../../data11/korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "tokenize sentences, it could take a lot of time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize sentences [took %.3f s] 7.264301300048828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ba3932351246dd99859df02a59fdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 힛걸 진심 최고다 그 어떤 히어로보다 멋지다 + 힛걸 진심 최고로 멋지다.\n",
      "tokens: [CLS] [UNK] 진 ##심 최고 ##다 그 어떤 히 ##어로 ##보다 멋 ##지 ##다 [SEP] [UNK] 진 ##심 최고 ##로 멋 ##지 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: entailment\n",
      "features: ClassificationFeatures(input_ids=[101, 100, 9708, 71013, 83491, 11903, 8924, 55910, 10025, 81483, 80001, 9270, 12508, 11903, 102, 100, 9708, 71013, 83491, 11261, 9270, 12508, 11903, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n",
      "tokens: [CLS] 100 ##분 ##간 [UNK] 그 ##래 ##도 [UNK] 2 ##점 ##준 ##다 [SEP] 100 ##분 ##간 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10407, 37712, 18784, 100, 8924, 37388, 12092, 100, 123, 34907, 54867, 11903, 102, 10407, 37712, 18784, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../data11/korpora/klue-nli/cached_BertTokenizer_128_klue-nli-v1.1_train.json 1.776066541671753\n",
      "Creating features from dataset file at ../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "loading data... LOOKING AT ../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 0.8249270915985107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab8559755914f6bbf0b6a45f68713fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다. + 어떤 방에서도 흡연은 금지됩니다.\n",
      "tokens: [CLS] 흡 ##연 ##자 ##분 ##들은 발 ##코 ##니 ##가 있는 방 ##이 ##면 발 ##코 ##니 ##에서 흡 ##연 ##이 가 ##능 ##합 ##니다 . [SEP] 어떤 방 ##에서 ##도 흡 ##연 ##은 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10020, 25486, 13764, 37712, 22879, 9323, 25517, 25503, 11287, 13767, 9328, 10739, 14867, 9323, 25517, 25503, 11489, 10020, 25486, 10739, 8843, 74986, 33188, 48345, 119, 102, 55910, 9328, 11489, 12092, 10020, 25486, 10892, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기 불편함이 많았다.\n",
      "tokens: [CLS] 10 ##명이 함께 사 ##용 ##하기 불 ##편 ##함 ##없 ##이 만 ##족 ##했다 . [SEP] 10 ##명이 함께 사 ##용 ##하기 불 ##편 ##함 ##이 많 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10150, 66923, 19653, 9405, 24974, 22440, 9368, 50450, 48533, 119136, 10739, 9248, 52560, 12490, 119, 102, 10150, 66923, 19653, 9405, 24974, 22440, 9368, 50450, 48533, 10739, 9249, 27303, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../data11/korpora/klue-nli/cached_BertTokenizer_128_klue-nli-v1.1_dev.json 0.2087082862854004\n"
     ]
    }
   ],
   "source": [
    "#nli 데이터 셋 로딩\n",
    "# 학습 data loader 생성\n",
    "#sys.path.append('..')\n",
    "from myutils import ClassificationDataset, KlueNLICorpus, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = data_args.max_seq_length   # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "batch_size = training_args.per_device_train_batch_size        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "\n",
    "# 훈련할 csv 파일\n",
    "file_fpath = '../../data11/korpora/klue-nli/klue-nli-v1.1_train.json'\n",
    "#file_fpath = 'Korpora/nsmc/ratings_train.txt'\n",
    "cache = True   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 설정\n",
    "corpus = KlueNLICorpus()\n",
    "\n",
    "# 학습 dataset 생성\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath,max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "# 평가 dataset 생성\n",
    "file_fpath = '../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json'\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "eval_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ac7667-27c3-46a6-8b73-51007e5887d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# config 설정 \n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=3,  #nli이므로 3으로 함\n",
    "    #label2id=dataset.label2id,\n",
    "    #id2label=dataset.id2label,\n",
    "    finetuning_task=data_args.dataset_name,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "#========================================================================\n",
    "# 훈련 모델에 따라 아래값들을 바꿔줘야 함.\n",
    "#========================================================================\n",
    "#get_model 에서 --prefix인경우 config 인자 설정해 주고 있음.\n",
    "\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.pre_seq_len = pre_seq_len             # prefix 계수\n",
    "config.prefix_projection = prefix_projection    # True = two-layer MLP 사용함(Multi-layer perceptron(다중퍼셉트론))\n",
    "config.prefix_hidden_size = prefix_hidden_size     # prefix hidden size\n",
    "#========================================================================\n",
    "\n",
    "print(config.num_hidden_layers)\n",
    "print(config.num_attention_heads)\n",
    "print(config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96dfcf9-f7be-479b-b92c-38f493739967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# p-tuing v2 => 새로운 방식이 아니라 NLU 향상을 위해, MLM 모델에 prefix-tuning을 적용한 방식\n",
    "# 참고 소스 : https://github.com/THUDM/P-tuning-v2\n",
    "#==============================================================================\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from torch._C import NoopLogger\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutput, Seq2SeqLMOutput\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r'''\n",
    "    The torch.nn model to encode the prefix\n",
    "    Input shape: (batch-size, prefix-length)\n",
    "    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        if self.prefix_projection:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n",
    "            self.trans = torch.nn.Sequential(\n",
    "                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.trans(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values\n",
    "    \n",
    "class BertPrefixForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # ***bert 모델은 grad 업데이트 안함(역전파 끔)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers  # 12\n",
    "        self.n_head = config.num_attention_heads # 12\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads  # 64 = 768 % 12\n",
    "\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "\n",
    "        bert_param = 0\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            bert_param += param.numel()\n",
    "        all_param = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "        total_param = all_param - bert_param\n",
    "        print('total param is {}'.format(total_param)) # 9860105\n",
    "    \n",
    "    def get_prompt(self, batch_size):\n",
    "        # [32,20] 만듬 (32:batch_size, 20 은 self.prefix_tokens arange로 생성한 값)\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device)\n",
    "        #print(f'prefix_tokens=>{prefix_tokens.shape}')\n",
    "        \n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        #print(f'past_key_values1=>{past_key_values.shape}')\n",
    "            \n",
    "        # bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            batch_size,\n",
    "            self.pre_seq_len,\n",
    "            self.n_layer * 2, \n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )\n",
    "        # batch_size:32, pre_seq_len:20, nlayer:12, nhead:12, n_emb:64\n",
    "        #print(f'*batch_size:{batch_size}, pre_seq_len:{self.pre_seq_len}, nlayer:{self.n_layer}, nhead:{self.n_head}, n_emb:{self.n_embd}') \n",
    "        \n",
    "        #print(f'past_key_values2=>{past_key_values.shape}')  # torch.Size([32, 20, 24, 12, 64])\n",
    "            \n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        #print(f'past_key_values3=>{past_key_values.shape}')  # torch.Size([32, 20, 24, 12, 64])\n",
    "            \n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        # torch.Size([2, 32, 12, 20, 64]), len\"12\n",
    "        #print(f'past_key_values4=>{past_key_values[0].shape}, len\"{len(past_key_values)}')\n",
    "        \n",
    "        return past_key_values\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        #print(f'return_dict=>{return_dict}') # True\n",
    "            \n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # past_key_values 는 디코딩할때 빠르게 찾기 위한 값인데..여기서는 왜 필요한지 모르겠음.\n",
    "        past_key_values = self.get_prompt(batch_size=batch_size)\n",
    "        \n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)\n",
    "        #print(f'prefix_attention_mask=>{prefix_attention_mask.shape}') # torch.Size([32, 20])\n",
    "        \n",
    "        #print(f'attention_mask1=>{attention_mask.shape}') # torch.Size([32, 128])\n",
    "        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1) # 128(기존 attention_mask) + 20(prefix_attention_mask) = 148\n",
    "        #print(f'*attention_mask2=>{attention_mask.shape}') # torch.Size([32, 148])\n",
    "\n",
    "        #print(f'*input_ids=>{input_ids.shape}')\n",
    "        #print(f'*token_type_ids=>{token_type_ids.shape}')\n",
    "        #print(f'*past_key_values=>{past_key_values[0].shape}')\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbbd6d0-ced7-4815-a70c-f7d7a52f7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param is 9867011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../data11/model/bert/bert-multilingual-cased were not used when initializing BertPrefixForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertPrefixForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertPrefixForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertPrefixForSequenceClassification were not initialized from the model checkpoint at ../../data11/model/bert/bert-multilingual-cased and are newly initialized: ['classifier.bias', 'prefix_encoder.trans.0.weight', 'prefix_encoder.trans.2.weight', 'prefix_encoder.trans.2.bias', 'prefix_encoder.embedding.weight', 'prefix_encoder.trans.0.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertPrefixForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (prefix_encoder): PrefixEncoder(\n",
       "    (embedding): Embedding(20, 768)\n",
       "    (trans): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# p-tuning 모델\n",
    "model = BertPrefixForSequenceClassification.from_pretrained(model_path, config=config, revision=\"main\")\n",
    "\n",
    "# NLI 모델에서 레벨은 3개지(참,거짓,모름) 이므로, num_labels=3을 입력함\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e12fe9-eece-44a8-81a1-f7636d72fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:29:55,852 - bert-p-tuing - INFO - [Epoch 40/50] Iteration 31200 -> Train Loss: 0.5740, Train Accuracy: 0.764\n",
      "2022-07-12 17:30:02,078 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f2c5f1d3f7415ab1d0b951767add29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:30:06,093 - bert-p-tuing - INFO - [Epoch 40/50] Validatation Accuracy:0.6853333333333333\n",
      "2022-07-12 17:30:06,095 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:30:06,095 - bert-p-tuing - INFO - === 처리시간: 4.018 초 ===\n",
      "2022-07-12 17:30:06,096 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349c1345f59a403fadb9dad24927dd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:30:30,931 - bert-p-tuing - INFO - [Epoch 41/50] Iteration 31600 -> Train Loss: 0.5768, Train Accuracy: 0.766\n",
      "2022-07-12 17:31:02,054 - bert-p-tuing - INFO - [Epoch 41/50] Iteration 32000 -> Train Loss: 0.5788, Train Accuracy: 0.765\n",
      "2022-07-12 17:31:07,033 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64296f013ff14060913259b052dd1630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:31:11,081 - bert-p-tuing - INFO - [Epoch 41/50] Validatation Accuracy:0.687\n",
      "2022-07-12 17:31:11,083 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:31:11,084 - bert-p-tuing - INFO - === 처리시간: 4.051 초 ===\n",
      "2022-07-12 17:31:11,084 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9be37153424a4aac3f37305ca17fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:31:37,503 - bert-p-tuing - INFO - [Epoch 42/50] Iteration 32400 -> Train Loss: 0.5736, Train Accuracy: 0.769\n",
      "2022-07-12 17:32:08,380 - bert-p-tuing - INFO - [Epoch 42/50] Iteration 32800 -> Train Loss: 0.5810, Train Accuracy: 0.761\n",
      "2022-07-12 17:32:11,984 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2a46a139964124a041a93880819ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:32:15,989 - bert-p-tuing - INFO - [Epoch 42/50] Validatation Accuracy:0.688\n",
      "2022-07-12 17:32:15,990 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:32:15,991 - bert-p-tuing - INFO - === 처리시간: 4.007 초 ===\n",
      "2022-07-12 17:32:15,991 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2944f077284523992cc4ad56470efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:32:43,472 - bert-p-tuing - INFO - [Epoch 43/50] Iteration 33200 -> Train Loss: 0.5703, Train Accuracy: 0.765\n",
      "2022-07-12 17:33:14,296 - bert-p-tuing - INFO - [Epoch 43/50] Iteration 33600 -> Train Loss: 0.5767, Train Accuracy: 0.766\n",
      "2022-07-12 17:33:16,466 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960b657d892b4200954160b692583a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:33:20,505 - bert-p-tuing - INFO - [Epoch 43/50] Validatation Accuracy:0.689\n",
      "2022-07-12 17:33:20,506 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:33:20,507 - bert-p-tuing - INFO - === 처리시간: 4.041 초 ===\n",
      "2022-07-12 17:33:20,508 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dfc264f2b14cfb8cfc6045ebc4688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:33:49,402 - bert-p-tuing - INFO - [Epoch 44/50] Iteration 34000 -> Train Loss: 0.5708, Train Accuracy: 0.767\n",
      "2022-07-12 17:34:20,152 - bert-p-tuing - INFO - [Epoch 44/50] Iteration 34400 -> Train Loss: 0.5747, Train Accuracy: 0.767\n",
      "2022-07-12 17:34:20,917 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36110626a77a4c23a363320c1f86dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:34:24,949 - bert-p-tuing - INFO - [Epoch 44/50] Validatation Accuracy:0.6906666666666667\n",
      "2022-07-12 17:34:24,951 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:34:24,951 - bert-p-tuing - INFO - === 처리시간: 4.035 초 ===\n",
      "2022-07-12 17:34:24,952 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1973bb115743bc9c8bd1a582d00b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:34:55,360 - bert-p-tuing - INFO - [Epoch 45/50] Iteration 34800 -> Train Loss: 0.5721, Train Accuracy: 0.767\n",
      "2022-07-12 17:35:25,415 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28e7b9f743c4647b71000f3622f13cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:35:29,404 - bert-p-tuing - INFO - [Epoch 45/50] Validatation Accuracy:0.6903333333333334\n",
      "2022-07-12 17:35:29,405 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:35:29,406 - bert-p-tuing - INFO - === 처리시간: 3.991 초 ===\n",
      "2022-07-12 17:35:29,406 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7974e532a0c4034ae22a0265f74748f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:35:30,539 - bert-p-tuing - INFO - [Epoch 46/50] Iteration 35200 -> Train Loss: 0.5724, Train Accuracy: 0.765\n",
      "2022-07-12 17:36:01,305 - bert-p-tuing - INFO - [Epoch 46/50] Iteration 35600 -> Train Loss: 0.5799, Train Accuracy: 0.764\n",
      "2022-07-12 17:36:30,008 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3098d41d9b947aa93088030653701d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:36:34,041 - bert-p-tuing - INFO - [Epoch 46/50] Validatation Accuracy:0.6913333333333334\n",
      "2022-07-12 17:36:34,043 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:36:34,043 - bert-p-tuing - INFO - === 처리시간: 4.035 초 ===\n",
      "2022-07-12 17:36:34,044 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7b0009bc354d2cb9dd9b4f3aec35ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:36:36,543 - bert-p-tuing - INFO - [Epoch 47/50] Iteration 36000 -> Train Loss: 0.5648, Train Accuracy: 0.768\n",
      "2022-07-12 17:37:07,100 - bert-p-tuing - INFO - [Epoch 47/50] Iteration 36400 -> Train Loss: 0.5745, Train Accuracy: 0.764\n",
      "2022-07-12 17:37:34,454 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9da4905cee49a39c118cc5a9167497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:37:38,505 - bert-p-tuing - INFO - [Epoch 47/50] Validatation Accuracy:0.6886666666666666\n",
      "2022-07-12 17:37:38,507 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:37:38,507 - bert-p-tuing - INFO - === 처리시간: 4.053 초 ===\n",
      "2022-07-12 17:37:38,508 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5111fbb63a3242638331e65ffcaeb5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:37:42,454 - bert-p-tuing - INFO - [Epoch 48/50] Iteration 36800 -> Train Loss: 0.5697, Train Accuracy: 0.771\n",
      "2022-07-12 17:38:13,088 - bert-p-tuing - INFO - [Epoch 48/50] Iteration 37200 -> Train Loss: 0.5690, Train Accuracy: 0.772\n",
      "2022-07-12 17:38:39,157 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ccdbdf4d5f4d7ca85f021d4ec874a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:38:43,201 - bert-p-tuing - INFO - [Epoch 48/50] Validatation Accuracy:0.691\n",
      "2022-07-12 17:38:43,202 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:38:43,203 - bert-p-tuing - INFO - === 처리시간: 4.046 초 ===\n",
      "2022-07-12 17:38:43,204 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7ac95457d84b8e820f0861d0ce4f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:38:48,613 - bert-p-tuing - INFO - [Epoch 49/50] Iteration 37600 -> Train Loss: 0.5726, Train Accuracy: 0.765\n",
      "2022-07-12 17:39:19,327 - bert-p-tuing - INFO - [Epoch 49/50] Iteration 38000 -> Train Loss: 0.5765, Train Accuracy: 0.768\n",
      "2022-07-12 17:39:43,781 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cd738b56254f41bcef3b85d959ab5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:39:47,783 - bert-p-tuing - INFO - [Epoch 49/50] Validatation Accuracy:0.6933333333333334\n",
      "2022-07-12 17:39:47,784 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:39:47,785 - bert-p-tuing - INFO - === 처리시간: 4.004 초 ===\n",
      "2022-07-12 17:39:47,786 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c59616c48a40229b0ffd7befa61009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:39:54,394 - bert-p-tuing - INFO - [Epoch 50/50] Iteration 38400 -> Train Loss: 0.5571, Train Accuracy: 0.773\n",
      "2022-07-12 17:40:25,525 - bert-p-tuing - INFO - [Epoch 50/50] Iteration 38800 -> Train Loss: 0.5679, Train Accuracy: 0.762\n",
      "2022-07-12 17:40:48,802 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5e0a5ff5c84016831a3da49144f947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 17:40:53,021 - bert-p-tuing - INFO - [Epoch 50/50] Validatation Accuracy:0.6926666666666667\n",
      "2022-07-12 17:40:53,023 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-12 17:40:53,024 - bert-p-tuing - INFO - === 처리시간: 4.222 초 ===\n",
      "2022-07-12 17:40:53,025 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "import time\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "logger.info(f\"=== model: {model_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")\n",
    "\n",
    "##################################################\n",
    "# 변수 설정\n",
    "##################################################\n",
    "epochs = training_args.num_train_epochs            # epochs\n",
    "learning_rate = training_args.learning_rate  # 학습률\n",
    "p_itr = 400           # 손실률 보여줄 step 수\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "num_warmup_steps = total_steps * 0.1\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도와 총 손실률 계산\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_len += len(labels)    \n",
    "            total_loss += loss.item()\n",
    "            #print('pred:{}, correct:{}'.format(pred, correct))\n",
    "\n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "                list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "        #if itr > 5:\n",
    "        #    break\n",
    "   \n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "    # 평가 시작\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "    \n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    " \n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=labels)\n",
    "    \n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            #loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            # 총 손실류 구함\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_test_correct += correct.sum().item()\n",
    "            total_test_len += len(labels)\n",
    "    \n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(f'-END-\\n')\n",
    "    ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc71684-c3aa-494e-b193-49eb7305cfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f49d7d5-7983-4488-abbc-99f7d1b012f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b71fed-9d62-46f7-ae76-4805e6b9a9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-20/tokenizer_config.json',\n",
       " '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-20/special_tokens_map.json',\n",
       " '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-20/vocab.txt',\n",
       " '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-20/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체모델 저장\n",
    "OUTPATH = training_args.output_dir\n",
    "\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH\n",
    "os.makedirs(VOCAB_PATH,exist_ok=True)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4dc7b-668b-44a4-a786-7adb30b9230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, OUTPATH + 'pytorch_model_torch.bin') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588369b2-ae3e-49e1-83da-20982ced20f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
