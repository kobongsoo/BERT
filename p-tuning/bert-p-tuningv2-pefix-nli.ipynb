{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6758d6d2-adb0-48a6-b62c-523410ea2168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bert-p-tuing_2022-07-13.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================================\n",
    "# P-TUNING NLI(Natural Language Interference:자연어 추론) 훈련 예제\n",
    "#\n",
    "# => input_ids : [CLS]senetence1(전제)[SEP]sentence2(가설)\n",
    "# => attention_mask : 1111111111(전체,가설)0000000(그외)\n",
    "# => token_type_ids : 0000000(전제)1111111(가설)00000000(그외)\n",
    "# => laels : 참(수반:entailment), 거짓(모순:contradiction), 모름(중립:neutral)\n",
    "#\n",
    "#\n",
    "# prefix-tuning => GPT-2, T5등 LM에서 접두사 prompt를 추가하여 훈련시키는 방식\n",
    "#\n",
    "# p-tuning => P-tuning은 prefix-tuning보다 유연 합니다. \n",
    "# 시작할 때뿐만 아니라 프롬프트 중간에 학습 가능한 토큰을 삽입하기 때문입니다. \n",
    "# https://github.com/THUDM/P-tuning\n",
    "#\n",
    "# p-tuing v2 => 새로운 방식이 아니라 NLU 향상을 위해, MLM 모델에 prefix-tuning을 적용한 방식\n",
    "# https://github.com/THUDM/P-tuning-v2\n",
    "#=======================================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, set_seed\n",
    "import os\n",
    "from os import sys\n",
    "from transformers import BertTokenizer, HfArgumentParser, TrainingArguments\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "logger = mlogging(loggername=\"bert-p-tuing\", logfilename=\"bert-p-tuing\")\n",
    "device = GPU_info()\n",
    "\n",
    "model_path = '../../data11/model/bert/bert-multilingual-cased'\n",
    "\n",
    "#tokenize 설정\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path, TOKENIZERS_PARALLELISM=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, strip_accents=False, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8631b209-d129-4a7a-bf7c-f4c64c79c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "20\n",
      "True\n",
      "steps\n",
      "3e-05\n"
     ]
    }
   ],
   "source": [
    "# 인자들 설정\n",
    "#======================================================\n",
    "# data args 인자 설정 \n",
    "class data_args:\n",
    "    dataset_name='rte'\n",
    "    pad_to_max_length = True\n",
    "    max_seq_length = 128\n",
    "    overwrite_cache = True\n",
    "    max_train_samples = None\n",
    "    max_eval_samples = None\n",
    "    max_predict_samples = None\n",
    "    \n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# 허깅페이스 TrainingArguments 설정\n",
    "training_args = TrainingArguments(\"p-tuning-bert-test\")\n",
    "\n",
    "# run_rte_bert.sh 에 사용된 인자만 새롭게 정의함.\n",
    "training_args.do_train=True\n",
    "training_args.do_eval=True\n",
    "training_args.seed = 111\n",
    "training_args.per_device_train_batch_size=32  #batch_size\n",
    "training_args.learning_rate =3e-5        #lr\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"steps\"  #eval 언제마다 할지 => no, steps, epoch\n",
    "training_args.num_train_epochs=30 # epochs\n",
    "training_args.output_dir = '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-50'\n",
    "\n",
    "#training_args.report_to = \"none\"  # 기본은 all로 , all이면 wandb에도 기록된다.\n",
    "#======================================================\n",
    "   \n",
    "#======================================================\n",
    "# p-tuningv2 prefixt 튜닝일때 설정값 지정 \n",
    "pre_seq_len = 50             # prefix 계수\n",
    "prefix_projection = True     # True = two-layer MLP 사용함(Multi-layer perceptron(다중퍼셉트론))\n",
    "prefix_hidden_size = 512     # prefix hidden size\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "#seed 설정\n",
    "#set_seed(training_args.seed)\n",
    "seed_everything(training_args.seed)\n",
    "#======================================================\n",
    "\n",
    "print(training_args.fp16)\n",
    "print(training_args.get_process_log_level())\n",
    "print(training_args.do_train)\n",
    "print(training_args.evaluation_strategy)\n",
    "print(training_args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ff649f-7706-49b3-a6cf-cf7cc02d4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at ../../data11/korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "loading data... LOOKING AT ../../data11/korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "tokenize sentences, it could take a lot of time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize sentences [took %.3f s] 7.6594157218933105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b82e82c3e164d91a52aa52df4dfab28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 힛걸 진심 최고다 그 어떤 히어로보다 멋지다 + 힛걸 진심 최고로 멋지다.\n",
      "tokens: [CLS] [UNK] 진 ##심 최고 ##다 그 어떤 히 ##어로 ##보다 멋 ##지 ##다 [SEP] [UNK] 진 ##심 최고 ##로 멋 ##지 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: entailment\n",
      "features: ClassificationFeatures(input_ids=[101, 100, 9708, 71013, 83491, 11903, 8924, 55910, 10025, 81483, 80001, 9270, 12508, 11903, 102, 100, 9708, 71013, 83491, 11261, 9270, 12508, 11903, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "*** Example ***\n",
      "sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n",
      "tokens: [CLS] 100 ##분 ##간 [UNK] 그 ##래 ##도 [UNK] 2 ##점 ##준 ##다 [SEP] 100 ##분 ##간 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10407, 37712, 18784, 100, 8924, 37388, 12092, 100, 123, 34907, 54867, 11903, 102, 10407, 37712, 18784, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../data11/korpora/klue-nli/cached_BertTokenizer_128_klue-nli-v1.1_train.json 1.8428149223327637\n",
      "Creating features from dataset file at ../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "loading data... LOOKING AT ../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 0.856285572052002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e633144ef5492aa8558b20868d0a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다. + 어떤 방에서도 흡연은 금지됩니다.\n",
      "tokens: [CLS] 흡 ##연 ##자 ##분 ##들은 발 ##코 ##니 ##가 있는 방 ##이 ##면 발 ##코 ##니 ##에서 흡 ##연 ##이 가 ##능 ##합 ##니다 . [SEP] 어떤 방 ##에서 ##도 흡 ##연 ##은 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10020, 25486, 13764, 37712, 22879, 9323, 25517, 25503, 11287, 13767, 9328, 10739, 14867, 9323, 25517, 25503, 11489, 10020, 25486, 10739, 8843, 74986, 33188, 48345, 119, 102, 55910, 9328, 11489, 12092, 10020, 25486, 10892, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "*** Example ***\n",
      "sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기 불편함이 많았다.\n",
      "tokens: [CLS] 10 ##명이 함께 사 ##용 ##하기 불 ##편 ##함 ##없 ##이 만 ##족 ##했다 . [SEP] 10 ##명이 함께 사 ##용 ##하기 불 ##편 ##함 ##이 많 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10150, 66923, 19653, 9405, 24974, 22440, 9368, 50450, 48533, 119136, 10739, 9248, 52560, 12490, 119, 102, 10150, 66923, 19653, 9405, 24974, 22440, 9368, 50450, 48533, 10739, 9249, 27303, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../data11/korpora/klue-nli/cached_BertTokenizer_128_klue-nli-v1.1_dev.json 0.21090316772460938\n"
     ]
    }
   ],
   "source": [
    "#nli 데이터 셋 로딩\n",
    "# 학습 data loader 생성\n",
    "#sys.path.append('..')\n",
    "from myutils import ClassificationDataset, KlueNLICorpus, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = data_args.max_seq_length   # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "batch_size = training_args.per_device_train_batch_size        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "\n",
    "# 훈련할 csv 파일\n",
    "file_fpath = '../../data11/korpora/klue-nli/klue-nli-v1.1_train.json'\n",
    "#file_fpath = 'Korpora/nsmc/ratings_train.txt'\n",
    "cache = True   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 설정\n",
    "corpus = KlueNLICorpus()\n",
    "\n",
    "# 학습 dataset 생성\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath,max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "# 평가 dataset 생성\n",
    "file_fpath = '../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json'\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "eval_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ac7667-27c3-46a6-8b73-51007e5887d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# config 설정 \n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=3,  #nli이므로 3으로 함\n",
    "    #label2id=dataset.label2id,\n",
    "    #id2label=dataset.id2label,\n",
    "    finetuning_task=data_args.dataset_name,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "#========================================================================\n",
    "# 훈련 모델에 따라 아래값들을 바꿔줘야 함.\n",
    "#========================================================================\n",
    "#get_model 에서 --prefix인경우 config 인자 설정해 주고 있음.\n",
    "\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.pre_seq_len = pre_seq_len             # prefix 계수\n",
    "config.prefix_projection = prefix_projection    # True = two-layer MLP 사용함(Multi-layer perceptron(다중퍼셉트론))\n",
    "config.prefix_hidden_size = prefix_hidden_size     # prefix hidden size\n",
    "#========================================================================\n",
    "\n",
    "print(config.num_hidden_layers)\n",
    "print(config.num_attention_heads)\n",
    "print(config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96dfcf9-f7be-479b-b92c-38f493739967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# p-tuing v2 => 새로운 방식이 아니라 NLU 향상을 위해, MLM 모델에 prefix-tuning을 적용한 방식\n",
    "# 참고 소스 : https://github.com/THUDM/P-tuning-v2\n",
    "# \n",
    "# p-tuning-v2의 주요 기여는 원래 입력 전에 사용자 정의 길이의 레이어 프롬프트를 추가하고 \n",
    "# 다운스트림 작업에 대한 후속 교육에서 BERT 모델의 모든 매개변수를 고정하고 이러한 프롬프트만 교육하는 것임.\n",
    "# 설명 : https://zhuanlan.zhihu.com/p/459305102\n",
    "#\n",
    "# => P-tuning-v2의 구현 방식은 prefix N 시퀀스를 생성한 다음, 원래 bert 모델과 연결한다. 이때 bert의 past_key_values(*여기서는 decoding 속도 개선 목적이 아님)를 이용함\n",
    "# => bert의 past_key_values로 prefix에대한 key 와 value 를 넘겨줘서, 기존 입력 key, value와 연결시키도록 함.\n",
    "# => get_prompt() 함수 : prefix를 past_key_value 형식(batch_size, num_heads, sequence_length - 1, embed_size_per_head)으로 조정(만듬)\n",
    "# => attention_mask : 기존 attention_mask +  prefix_attention_mask \n",
    "#==============================================================================\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from torch._C import NoopLogger\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutput, Seq2SeqLMOutput\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r'''\n",
    "    The torch.nn model to encode the prefix\n",
    "    Input shape: (batch-size, prefix-length)\n",
    "    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        if self.prefix_projection:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n",
    "            self.trans = torch.nn.Sequential(\n",
    "                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                \n",
    "                # num_hidden_layers(12)*2*dim_embedding 인데, \n",
    "                # 여기서 *2는 key와 value를 기존 입력 key와 value로 연결시키는 구조이므로, *2를 해준것임.\n",
    "                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.trans(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values\n",
    "    \n",
    "class BertPrefixForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        # ***bert 모델은 grad 업데이트 안함(역전파 끔)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers  # 12\n",
    "        self.n_head = config.num_attention_heads # 12\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads  # 64 = 768 % 12\n",
    "\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "\n",
    "        bert_param = 0\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            bert_param += param.numel()\n",
    "        all_param = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "        total_param = all_param - bert_param\n",
    "        print('total param is {}'.format(total_param)) # 9860105\n",
    "    \n",
    "    def get_prompt(self, batch_size):\n",
    "        # [32,20] 만듬 (32:batch_size, 20 은 self.prefix_tokens arange로 생성한 값)\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device)\n",
    "        #print(f'prefix_tokens=>{prefix_tokens.shape}')\n",
    "        \n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        #print(f'past_key_values1=>{past_key_values.shape}')\n",
    "            \n",
    "        # bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            batch_size,\n",
    "            self.pre_seq_len,\n",
    "            self.n_layer * 2, \n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )\n",
    "        # batch_size:32, pre_seq_len:20, nlayer:12, nhead:12, n_emb:64\n",
    "        #print(f'*batch_size:{batch_size}, pre_seq_len:{self.pre_seq_len}, nlayer:{self.n_layer}, nhead:{self.n_head}, n_emb:{self.n_embd}') \n",
    "        \n",
    "        #print(f'past_key_values2=>{past_key_values.shape}')  # torch.Size([32, 20, 24, 12, 64])\n",
    "            \n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        #print(f'past_key_values3=>{past_key_values.shape}')  # torch.Size([32, 20, 24, 12, 64])\n",
    "            \n",
    "        # (batch_size, num_heads, sequence_length - 1, embed_size_per_head) 식으로 만들어줌.    \n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        # torch.Size([2, 32, 12, 20, 64]), len\"12\n",
    "        #print(f'past_key_values4=>{past_key_values[0].shape}, len\"{len(past_key_values)}')\n",
    "        \n",
    "        return past_key_values\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        #print(f'return_dict=>{return_dict}') # True\n",
    "            \n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # past_key_values를 지정하면, 어텐션 블록에서, 미리 계산된 키 및 값 숨겨진 상태를 포함한다.\n",
    "        # 즉 p-tuning-v2는 추가된 prefix value 와 key를 기존 입력 value와 key를 서로 연결시키는 구조이다.\n",
    "        # 따라서 prefix의 past_key_values를 구하는데 (batch_size, num_heads, sequence_length - 1, embed_size_per_head) 형태로 구하면 된다.\n",
    "        # 참고 : https://zhuanlan.zhihu.com/p/459305102\n",
    "        # => past_key_values 를 지정하지 않으면 아래와 같은 에러가 발생함.\n",
    "        # RuntimeError: The size of tensor a (128) must match the size of tensor b (148) at non-singleton dimension 3\n",
    "        past_key_values = self.get_prompt(batch_size=batch_size)\n",
    "        \n",
    "        # attention_mask : 기존 attention_mask +  prefix_attention_mask 더함.\n",
    "        # prefix_attention_mask 를 구하고, \n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)\n",
    "        #print(f'prefix_attention_mask=>{prefix_attention_mask.shape}') # torch.Size([32, 20])\n",
    "        \n",
    "        # 기존 attention_mask +  prefix_attention_mask 합침.\n",
    "        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1) # 128(기존 attention_mask) + 20(prefix_attention_mask) = 148\n",
    "        \n",
    "        #print(f'*attention_mask2=>{attention_mask.shape}') # torch.Size([32, 148])\n",
    "        #print(f'*input_ids=>{input_ids.shape}')\n",
    "        #print(f'*token_type_ids=>{token_type_ids.shape}')\n",
    "        #print(f'*past_key_values=>{past_key_values[0].shape}')\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbbd6d0-ced7-4815-a70c-f7d7a52f7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param is 9890051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../data11/model/bert/bert-multilingual-cased were not used when initializing BertPrefixForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertPrefixForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertPrefixForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertPrefixForSequenceClassification were not initialized from the model checkpoint at ../../data11/model/bert/bert-multilingual-cased and are newly initialized: ['prefix_encoder.trans.0.bias', 'classifier.weight', 'prefix_encoder.trans.0.weight', 'prefix_encoder.trans.2.weight', 'prefix_encoder.trans.2.bias', 'classifier.bias', 'prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertPrefixForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (prefix_encoder): PrefixEncoder(\n",
       "    (embedding): Embedding(50, 768)\n",
       "    (trans): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=512, out_features=18432, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# p-tuning 모델\n",
    "model = BertPrefixForSequenceClassification.from_pretrained(model_path, config=config, revision=\"main\")\n",
    "\n",
    "# NLI 모델에서 레벨은 3개지(참,거짓,모름) 이므로, num_labels=3을 입력함\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e12fe9-eece-44a8-81a1-f7636d72fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:50:15,326 - bert-p-tuing - INFO - === model: ../../data11/model/bert/bert-multilingual-cased ===\n",
      "2022-07-13 11:50:15,329 - bert-p-tuing - INFO - num_parameters: 187743491\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19c1e5776d14055bda7f77397d532b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b54f9ea5fe4618bdfeed1772d7feaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122958/1942215713.py:77: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-07-13 11:50:50,619 - bert-p-tuing - INFO - [Epoch 1/30] Iteration 400 -> Train Loss: 1.0953, Train Accuracy: 0.362\n",
      "2022-07-13 11:51:23,124 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182d4f726d9b455a95cbed1c3bca8e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122958/1942215713.py:134: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-07-13 11:51:27,458 - bert-p-tuing - INFO - [Epoch 1/30] Validatation Accuracy:0.4603333333333333\n",
      "2022-07-13 11:51:27,459 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:51:27,461 - bert-p-tuing - INFO - === 처리시간: 4.337 초 ===\n",
      "2022-07-13 11:51:27,462 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0147e04e6bbe43e89d7c0527f1bc1433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:51:29,438 - bert-p-tuing - INFO - [Epoch 2/30] Iteration 800 -> Train Loss: 1.0743, Train Accuracy: 0.401\n",
      "2022-07-13 11:52:03,441 - bert-p-tuing - INFO - [Epoch 2/30] Iteration 1200 -> Train Loss: 1.0013, Train Accuracy: 0.487\n",
      "2022-07-13 11:52:34,656 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896e21bdc9194769b872669d0e8ff95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:52:39,009 - bert-p-tuing - INFO - [Epoch 2/30] Validatation Accuracy:0.5423333333333333\n",
      "2022-07-13 11:52:39,011 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:52:39,012 - bert-p-tuing - INFO - === 처리시간: 4.356 초 ===\n",
      "2022-07-13 11:52:39,013 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382c09879b1c4beb925a655e99dcd6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:52:42,539 - bert-p-tuing - INFO - [Epoch 3/30] Iteration 1600 -> Train Loss: 0.9489, Train Accuracy: 0.523\n",
      "2022-07-13 11:53:16,740 - bert-p-tuing - INFO - [Epoch 3/30] Iteration 2000 -> Train Loss: 0.9208, Train Accuracy: 0.548\n",
      "2022-07-13 11:53:46,593 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a04427dfca44e3b8228b88f06d59b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:53:50,962 - bert-p-tuing - INFO - [Epoch 3/30] Validatation Accuracy:0.575\n",
      "2022-07-13 11:53:50,964 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:53:50,965 - bert-p-tuing - INFO - === 처리시간: 4.373 초 ===\n",
      "2022-07-13 11:53:50,966 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bda1ba316fa466ab278e749e10f3adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:53:56,088 - bert-p-tuing - INFO - [Epoch 4/30] Iteration 2400 -> Train Loss: 0.8844, Train Accuracy: 0.573\n",
      "2022-07-13 11:54:30,382 - bert-p-tuing - INFO - [Epoch 4/30] Iteration 2800 -> Train Loss: 0.8497, Train Accuracy: 0.611\n",
      "2022-07-13 11:54:58,706 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d6d0d39e2e4bb9b82c0758c7943d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:55:03,112 - bert-p-tuing - INFO - [Epoch 4/30] Validatation Accuracy:0.5976666666666667\n",
      "2022-07-13 11:55:03,114 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:55:03,115 - bert-p-tuing - INFO - === 처리시간: 4.410 초 ===\n",
      "2022-07-13 11:55:03,116 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32321a7710e4b4aa2d106f251f064bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:55:09,820 - bert-p-tuing - INFO - [Epoch 5/30] Iteration 3200 -> Train Loss: 0.8224, Train Accuracy: 0.643\n",
      "2022-07-13 11:55:44,260 - bert-p-tuing - INFO - [Epoch 5/30] Iteration 3600 -> Train Loss: 0.7853, Train Accuracy: 0.666\n",
      "2022-07-13 11:56:11,104 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbcf657e64243ba8c2f147f801b2b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:56:15,442 - bert-p-tuing - INFO - [Epoch 5/30] Validatation Accuracy:0.6176666666666667\n",
      "2022-07-13 11:56:15,444 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:56:15,444 - bert-p-tuing - INFO - === 처리시간: 4.341 초 ===\n",
      "2022-07-13 11:56:15,444 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d777a312934a83b1eaad3f2af01df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:56:23,715 - bert-p-tuing - INFO - [Epoch 6/30] Iteration 4000 -> Train Loss: 0.7694, Train Accuracy: 0.674\n",
      "2022-07-13 11:56:58,044 - bert-p-tuing - INFO - [Epoch 6/30] Iteration 4400 -> Train Loss: 0.7461, Train Accuracy: 0.684\n",
      "2022-07-13 11:57:23,234 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a1582144dc459aa8cf5ca70adaa2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:57:27,612 - bert-p-tuing - INFO - [Epoch 6/30] Validatation Accuracy:0.643\n",
      "2022-07-13 11:57:27,613 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:57:27,614 - bert-p-tuing - INFO - === 처리시간: 4.381 초 ===\n",
      "2022-07-13 11:57:27,615 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b7b577373b44818cd9a378104ad69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:57:37,325 - bert-p-tuing - INFO - [Epoch 7/30] Iteration 4800 -> Train Loss: 0.7487, Train Accuracy: 0.687\n",
      "2022-07-13 11:58:11,664 - bert-p-tuing - INFO - [Epoch 7/30] Iteration 5200 -> Train Loss: 0.7330, Train Accuracy: 0.693\n",
      "2022-07-13 11:58:35,299 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b7b828ce4e47c8bd383135fd00e786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:58:39,635 - bert-p-tuing - INFO - [Epoch 7/30] Validatation Accuracy:0.648\n",
      "2022-07-13 11:58:39,636 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:58:39,638 - bert-p-tuing - INFO - === 처리시간: 4.338 초 ===\n",
      "2022-07-13 11:58:39,638 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d780467f2a694f24a37eb384687ffcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:58:50,926 - bert-p-tuing - INFO - [Epoch 8/30] Iteration 5600 -> Train Loss: 0.7225, Train Accuracy: 0.698\n",
      "2022-07-13 11:59:25,445 - bert-p-tuing - INFO - [Epoch 8/30] Iteration 6000 -> Train Loss: 0.7104, Train Accuracy: 0.703\n",
      "2022-07-13 11:59:47,525 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5612d811458f4ee18eca3eba85b4c1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:59:51,888 - bert-p-tuing - INFO - [Epoch 8/30] Validatation Accuracy:0.661\n",
      "2022-07-13 11:59:51,890 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 11:59:51,890 - bert-p-tuing - INFO - === 처리시간: 4.365 초 ===\n",
      "2022-07-13 11:59:51,891 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677987aedb32425b9a9e0ce507c5346a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:00:04,700 - bert-p-tuing - INFO - [Epoch 9/30] Iteration 6400 -> Train Loss: 0.7211, Train Accuracy: 0.697\n",
      "2022-07-13 12:00:39,039 - bert-p-tuing - INFO - [Epoch 9/30] Iteration 6800 -> Train Loss: 0.6956, Train Accuracy: 0.715\n",
      "2022-07-13 12:00:59,601 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06cad0413ee40daaff5a591057efe9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:01:03,971 - bert-p-tuing - INFO - [Epoch 9/30] Validatation Accuracy:0.6606666666666666\n",
      "2022-07-13 12:01:03,972 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:01:03,973 - bert-p-tuing - INFO - === 처리시간: 4.371 초 ===\n",
      "2022-07-13 12:01:03,973 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dde42563ee48389db85b347fb0f978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:01:18,343 - bert-p-tuing - INFO - [Epoch 10/30] Iteration 7200 -> Train Loss: 0.7014, Train Accuracy: 0.708\n",
      "2022-07-13 12:01:52,490 - bert-p-tuing - INFO - [Epoch 10/30] Iteration 7600 -> Train Loss: 0.6889, Train Accuracy: 0.714\n",
      "2022-07-13 12:02:11,511 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f4256e095843d9ba5ae0f37887caca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:02:15,894 - bert-p-tuing - INFO - [Epoch 10/30] Validatation Accuracy:0.6606666666666666\n",
      "2022-07-13 12:02:15,895 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:02:15,896 - bert-p-tuing - INFO - === 처리시간: 4.384 초 ===\n",
      "2022-07-13 12:02:15,896 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00200d0de2bc40179e12e08e5b45c884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:02:31,821 - bert-p-tuing - INFO - [Epoch 11/30] Iteration 8000 -> Train Loss: 0.7021, Train Accuracy: 0.708\n",
      "2022-07-13 12:03:06,242 - bert-p-tuing - INFO - [Epoch 11/30] Iteration 8400 -> Train Loss: 0.6840, Train Accuracy: 0.719\n",
      "2022-07-13 12:03:23,787 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c95ec1b703e45eda15638ccdf67bef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:03:28,111 - bert-p-tuing - INFO - [Epoch 11/30] Validatation Accuracy:0.665\n",
      "2022-07-13 12:03:28,112 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:03:28,112 - bert-p-tuing - INFO - === 처리시간: 4.325 초 ===\n",
      "2022-07-13 12:03:28,113 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4081e464f95d4ecd8fda3043142493c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:03:45,625 - bert-p-tuing - INFO - [Epoch 12/30] Iteration 8800 -> Train Loss: 0.6795, Train Accuracy: 0.721\n",
      "2022-07-13 12:04:20,098 - bert-p-tuing - INFO - [Epoch 12/30] Iteration 9200 -> Train Loss: 0.6690, Train Accuracy: 0.722\n",
      "2022-07-13 12:04:36,109 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170101c362fe4981aec1a598223d1e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:04:40,497 - bert-p-tuing - INFO - [Epoch 12/30] Validatation Accuracy:0.6653333333333333\n",
      "2022-07-13 12:04:40,500 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:04:40,501 - bert-p-tuing - INFO - === 처리시간: 4.392 초 ===\n",
      "2022-07-13 12:04:40,502 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c71ce9f2c2c4f3ab9b875adaa73b09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:04:59,704 - bert-p-tuing - INFO - [Epoch 13/30] Iteration 9600 -> Train Loss: 0.6734, Train Accuracy: 0.719\n",
      "2022-07-13 12:05:34,066 - bert-p-tuing - INFO - [Epoch 13/30] Iteration 10000 -> Train Loss: 0.6785, Train Accuracy: 0.720\n",
      "2022-07-13 12:05:48,493 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8e9d880d6b40d799f05c9e1a2d9f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:05:52,880 - bert-p-tuing - INFO - [Epoch 13/30] Validatation Accuracy:0.672\n",
      "2022-07-13 12:05:52,882 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:05:52,883 - bert-p-tuing - INFO - === 처리시간: 4.390 초 ===\n",
      "2022-07-13 12:05:52,884 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7dae0f8816453a8d988d3a0a3019d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:06:13,406 - bert-p-tuing - INFO - [Epoch 14/30] Iteration 10400 -> Train Loss: 0.6640, Train Accuracy: 0.723\n",
      "2022-07-13 12:06:47,690 - bert-p-tuing - INFO - [Epoch 14/30] Iteration 10800 -> Train Loss: 0.6662, Train Accuracy: 0.722\n",
      "2022-07-13 12:07:00,436 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dd618dd7cf46ada07277782db1aa26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:07:04,792 - bert-p-tuing - INFO - [Epoch 14/30] Validatation Accuracy:0.6743333333333333\n",
      "2022-07-13 12:07:04,793 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:07:04,793 - bert-p-tuing - INFO - === 처리시간: 4.358 초 ===\n",
      "2022-07-13 12:07:04,794 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42229441154743258fcc9fd3a157e7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:07:26,916 - bert-p-tuing - INFO - [Epoch 15/30] Iteration 11200 -> Train Loss: 0.6545, Train Accuracy: 0.731\n",
      "2022-07-13 12:08:01,408 - bert-p-tuing - INFO - [Epoch 15/30] Iteration 11600 -> Train Loss: 0.6509, Train Accuracy: 0.729\n",
      "2022-07-13 12:08:12,708 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfb2804f42b4d99bf491cdb82fb8303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:08:17,104 - bert-p-tuing - INFO - [Epoch 15/30] Validatation Accuracy:0.6753333333333333\n",
      "2022-07-13 12:08:17,105 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:08:17,106 - bert-p-tuing - INFO - === 처리시간: 4.399 초 ===\n",
      "2022-07-13 12:08:17,107 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc57b843b2cf4f17af8243423987992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:08:40,812 - bert-p-tuing - INFO - [Epoch 16/30] Iteration 12000 -> Train Loss: 0.6589, Train Accuracy: 0.728\n",
      "2022-07-13 12:09:15,180 - bert-p-tuing - INFO - [Epoch 16/30] Iteration 12400 -> Train Loss: 0.6551, Train Accuracy: 0.725\n",
      "2022-07-13 12:09:24,845 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f57d480c0f4eb79c6bf69cb962947a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:09:29,226 - bert-p-tuing - INFO - [Epoch 16/30] Validatation Accuracy:0.6746666666666666\n",
      "2022-07-13 12:09:29,228 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:09:29,229 - bert-p-tuing - INFO - === 처리시간: 4.385 초 ===\n",
      "2022-07-13 12:09:29,230 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e90f5d834f4a2284f63cd1811354db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:09:54,391 - bert-p-tuing - INFO - [Epoch 17/30] Iteration 12800 -> Train Loss: 0.6449, Train Accuracy: 0.736\n",
      "2022-07-13 12:10:28,822 - bert-p-tuing - INFO - [Epoch 17/30] Iteration 13200 -> Train Loss: 0.6499, Train Accuracy: 0.734\n",
      "2022-07-13 12:10:36,998 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca8dcdbb96c4568bba8f1e0f3f0d90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:10:41,342 - bert-p-tuing - INFO - [Epoch 17/30] Validatation Accuracy:0.6766666666666666\n",
      "2022-07-13 12:10:41,344 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:10:41,346 - bert-p-tuing - INFO - === 처리시간: 4.348 초 ===\n",
      "2022-07-13 12:10:41,347 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983aa6322dd04facb9dc8ee17eb5759c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:11:08,138 - bert-p-tuing - INFO - [Epoch 18/30] Iteration 13600 -> Train Loss: 0.6430, Train Accuracy: 0.736\n",
      "2022-07-13 12:11:42,478 - bert-p-tuing - INFO - [Epoch 18/30] Iteration 14000 -> Train Loss: 0.6514, Train Accuracy: 0.734\n",
      "2022-07-13 12:11:49,044 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3f5c91c4a14a36b73c5aa8d0cd45ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:11:53,378 - bert-p-tuing - INFO - [Epoch 18/30] Validatation Accuracy:0.6813333333333333\n",
      "2022-07-13 12:11:53,379 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:11:53,380 - bert-p-tuing - INFO - === 처리시간: 4.336 초 ===\n",
      "2022-07-13 12:11:53,380 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46467e9820c4e3bb6f19853c4f1e27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:12:21,678 - bert-p-tuing - INFO - [Epoch 19/30] Iteration 14400 -> Train Loss: 0.6393, Train Accuracy: 0.739\n",
      "2022-07-13 12:12:55,964 - bert-p-tuing - INFO - [Epoch 19/30] Iteration 14800 -> Train Loss: 0.6426, Train Accuracy: 0.739\n",
      "2022-07-13 12:13:01,132 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a14620d9344f1f96de878748f8f538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:13:05,487 - bert-p-tuing - INFO - [Epoch 19/30] Validatation Accuracy:0.6816666666666666\n",
      "2022-07-13 12:13:05,488 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:13:05,489 - bert-p-tuing - INFO - === 처리시간: 4.358 초 ===\n",
      "2022-07-13 12:13:05,490 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e01f33f17ef4df19d276c873a8359b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:13:35,377 - bert-p-tuing - INFO - [Epoch 20/30] Iteration 15200 -> Train Loss: 0.6408, Train Accuracy: 0.732\n",
      "2022-07-13 12:14:09,683 - bert-p-tuing - INFO - [Epoch 20/30] Iteration 15600 -> Train Loss: 0.6285, Train Accuracy: 0.742\n",
      "2022-07-13 12:14:13,183 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124e98d7dcfb42f9a2246c1ecb2add74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:14:17,539 - bert-p-tuing - INFO - [Epoch 20/30] Validatation Accuracy:0.68\n",
      "2022-07-13 12:14:17,541 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:14:17,542 - bert-p-tuing - INFO - === 처리시간: 4.358 초 ===\n",
      "2022-07-13 12:14:17,543 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c54b802dc148af9aa7918f8f2d96b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:14:48,812 - bert-p-tuing - INFO - [Epoch 21/30] Iteration 16000 -> Train Loss: 0.6323, Train Accuracy: 0.742\n",
      "2022-07-13 12:15:22,950 - bert-p-tuing - INFO - [Epoch 21/30] Iteration 16400 -> Train Loss: 0.6330, Train Accuracy: 0.739\n",
      "2022-07-13 12:15:24,951 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68e9c72603445678d5042636a610bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:15:29,256 - bert-p-tuing - INFO - [Epoch 21/30] Validatation Accuracy:0.682\n",
      "2022-07-13 12:15:29,257 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:15:29,258 - bert-p-tuing - INFO - === 처리시간: 4.308 초 ===\n",
      "2022-07-13 12:15:29,259 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9187648d241d460697de45ee56c4b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:16:02,054 - bert-p-tuing - INFO - [Epoch 22/30] Iteration 16800 -> Train Loss: 0.6212, Train Accuracy: 0.747\n",
      "2022-07-13 12:16:36,279 - bert-p-tuing - INFO - [Epoch 22/30] Iteration 17200 -> Train Loss: 0.6363, Train Accuracy: 0.738\n",
      "2022-07-13 12:16:36,738 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aee63fe7e1841529fcafbda1423ddf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:16:41,131 - bert-p-tuing - INFO - [Epoch 22/30] Validatation Accuracy:0.68\n",
      "2022-07-13 12:16:41,133 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:16:41,133 - bert-p-tuing - INFO - === 처리시간: 4.395 초 ===\n",
      "2022-07-13 12:16:41,134 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d12518cca604410b9d244f3b3a807bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:17:15,608 - bert-p-tuing - INFO - [Epoch 23/30] Iteration 17600 -> Train Loss: 0.6282, Train Accuracy: 0.739\n",
      "2022-07-13 12:17:48,511 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b1c73ede1247e7832a77cfec883f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:17:52,913 - bert-p-tuing - INFO - [Epoch 23/30] Validatation Accuracy:0.6853333333333333\n",
      "2022-07-13 12:17:52,915 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:17:52,916 - bert-p-tuing - INFO - === 처리시간: 4.405 초 ===\n",
      "2022-07-13 12:17:52,917 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc569c8b2f048f5b1edd11e77b943be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:17:54,637 - bert-p-tuing - INFO - [Epoch 24/30] Iteration 18000 -> Train Loss: 0.6326, Train Accuracy: 0.742\n",
      "2022-07-13 12:18:29,092 - bert-p-tuing - INFO - [Epoch 24/30] Iteration 18400 -> Train Loss: 0.6283, Train Accuracy: 0.741\n",
      "2022-07-13 12:19:00,955 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df50b7705b84104babaaecae091d9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:19:05,362 - bert-p-tuing - INFO - [Epoch 24/30] Validatation Accuracy:0.683\n",
      "2022-07-13 12:19:05,364 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:19:05,365 - bert-p-tuing - INFO - === 처리시간: 4.410 초 ===\n",
      "2022-07-13 12:19:05,366 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750727d9550f4262ac9da2d31fd9df6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:19:08,565 - bert-p-tuing - INFO - [Epoch 25/30] Iteration 18800 -> Train Loss: 0.6198, Train Accuracy: 0.747\n",
      "2022-07-13 12:19:42,998 - bert-p-tuing - INFO - [Epoch 25/30] Iteration 19200 -> Train Loss: 0.6231, Train Accuracy: 0.745\n",
      "2022-07-13 12:20:13,345 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1871d1315047f2bc0eb86d586f0fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:20:17,749 - bert-p-tuing - INFO - [Epoch 25/30] Validatation Accuracy:0.686\n",
      "2022-07-13 12:20:17,751 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:20:17,751 - bert-p-tuing - INFO - === 처리시간: 4.407 초 ===\n",
      "2022-07-13 12:20:17,752 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa285a236564206b33e08069002d203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:20:22,532 - bert-p-tuing - INFO - [Epoch 26/30] Iteration 19600 -> Train Loss: 0.6254, Train Accuracy: 0.749\n",
      "2022-07-13 12:20:57,157 - bert-p-tuing - INFO - [Epoch 26/30] Iteration 20000 -> Train Loss: 0.6190, Train Accuracy: 0.743\n",
      "2022-07-13 12:21:25,880 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734c7cf2e37b449da7c7679859a79f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:21:30,234 - bert-p-tuing - INFO - [Epoch 26/30] Validatation Accuracy:0.6856666666666666\n",
      "2022-07-13 12:21:30,235 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:21:30,236 - bert-p-tuing - INFO - === 처리시간: 4.357 초 ===\n",
      "2022-07-13 12:21:30,238 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5e80ef45ef4fe383c9219ae21f311b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:21:36,567 - bert-p-tuing - INFO - [Epoch 27/30] Iteration 20400 -> Train Loss: 0.6154, Train Accuracy: 0.747\n",
      "2022-07-13 12:22:10,672 - bert-p-tuing - INFO - [Epoch 27/30] Iteration 20800 -> Train Loss: 0.6285, Train Accuracy: 0.743\n",
      "2022-07-13 12:22:37,805 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5f509df7494dc58217388111445043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:22:42,219 - bert-p-tuing - INFO - [Epoch 27/30] Validatation Accuracy:0.6873333333333334\n",
      "2022-07-13 12:22:42,221 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:22:42,221 - bert-p-tuing - INFO - === 처리시간: 4.416 초 ===\n",
      "2022-07-13 12:22:42,221 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ece16f512de4e4a95bd9c97d6981421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:22:50,111 - bert-p-tuing - INFO - [Epoch 28/30] Iteration 21200 -> Train Loss: 0.6094, Train Accuracy: 0.751\n",
      "2022-07-13 12:23:24,406 - bert-p-tuing - INFO - [Epoch 28/30] Iteration 21600 -> Train Loss: 0.6212, Train Accuracy: 0.747\n",
      "2022-07-13 12:23:49,974 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c782d5a9d2146febd8cd987903c376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:23:54,363 - bert-p-tuing - INFO - [Epoch 28/30] Validatation Accuracy:0.6846666666666666\n",
      "2022-07-13 12:23:54,365 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:23:54,366 - bert-p-tuing - INFO - === 처리시간: 4.392 초 ===\n",
      "2022-07-13 12:23:54,367 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2735771dfc472698c4696824a05817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:24:03,805 - bert-p-tuing - INFO - [Epoch 29/30] Iteration 22000 -> Train Loss: 0.6177, Train Accuracy: 0.751\n",
      "2022-07-13 12:24:38,288 - bert-p-tuing - INFO - [Epoch 29/30] Iteration 22400 -> Train Loss: 0.6118, Train Accuracy: 0.749\n",
      "2022-07-13 12:25:02,269 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f33de9c3c345d98e7182726fd50fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:25:06,635 - bert-p-tuing - INFO - [Epoch 29/30] Validatation Accuracy:0.6863333333333334\n",
      "2022-07-13 12:25:06,637 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:25:06,638 - bert-p-tuing - INFO - === 처리시간: 4.369 초 ===\n",
      "2022-07-13 12:25:06,638 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcb646137034396bd4b5094d47cc89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:25:17,576 - bert-p-tuing - INFO - [Epoch 30/30] Iteration 22800 -> Train Loss: 0.6133, Train Accuracy: 0.749\n",
      "2022-07-13 12:25:51,995 - bert-p-tuing - INFO - [Epoch 30/30] Iteration 23200 -> Train Loss: 0.6101, Train Accuracy: 0.748\n",
      "2022-07-13 12:26:14,514 - bert-p-tuing - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2152a7c756d46d88849e2f1695abaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:26:18,879 - bert-p-tuing - INFO - [Epoch 30/30] Validatation Accuracy:0.6893333333333334\n",
      "2022-07-13 12:26:18,881 - bert-p-tuing - INFO - ---------------------------------------------------------\n",
      "2022-07-13 12:26:18,882 - bert-p-tuing - INFO - === 처리시간: 4.368 초 ===\n",
      "2022-07-13 12:26:18,883 - bert-p-tuing - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "import time\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "logger.info(f\"=== model: {model_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")\n",
    "\n",
    "##################################################\n",
    "# 변수 설정\n",
    "##################################################\n",
    "epochs = training_args.num_train_epochs            # epochs\n",
    "learning_rate = training_args.learning_rate  # 학습률\n",
    "p_itr = 400           # 손실률 보여줄 step 수\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "num_warmup_steps = total_steps * 0.1\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도와 총 손실률 계산\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_len += len(labels)    \n",
    "            total_loss += loss.item()\n",
    "            #print('pred:{}, correct:{}'.format(pred, correct))\n",
    "\n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "                list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "        #if itr > 5:\n",
    "        #    break\n",
    "   \n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "    # 평가 시작\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "    \n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    " \n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=labels)\n",
    "    \n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            #loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            # 총 손실류 구함\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_test_correct += correct.sum().item()\n",
    "            total_test_len += len(labels)\n",
    "    \n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(f'-END-\\n')\n",
    "    ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc71684-c3aa-494e-b193-49eb7305cfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f49d7d5-7983-4488-abbc-99f7d1b012f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b71fed-9d62-46f7-ae76-4805e6b9a9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-50/tokenizer_config.json',\n",
       " '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-50/special_tokens_map.json',\n",
       " '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-50/vocab.txt',\n",
       " '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-nli-50/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체모델 저장\n",
    "OUTPATH = training_args.output_dir\n",
    "\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH\n",
    "os.makedirs(VOCAB_PATH,exist_ok=True)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87b4dc7b-668b-44a4-a786-7adb30b9230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, OUTPATH + 'pytorch_model_torch.bin') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588369b2-ae3e-49e1-83da-20982ced20f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
