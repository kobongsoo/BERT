{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758d6d2-adb0-48a6-b62c-523410ea2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================\n",
    "# P-TUNING Q&A=기계독해 훈련 예제\n",
    "#\n",
    "# => input_ids : [CLS]질문[SEP]지문[SEP]\n",
    "# => attention_mask : 1111111111(질문, 지문 모두 1)\n",
    "# => token_type_ids : 0000000(질문)1111111(지문)\n",
    "# => start_positions : 45 (질문에 대한 지문에서의 답변 시작 위치)\n",
    "# => end_positions : 60 (질문에 대한 지문에서의 답변 끝 위치)\n",
    "#\n",
    "# prefix-tuning => GPT-2, T5등 LM에서 접두사 prompt를 추가하여 훈련시키는 방식\n",
    "#\n",
    "# p-tuning => P-tuning은 prefix-tuning보다 유연 합니다. \n",
    "# 시작할 때뿐만 아니라 프롬프트 중간에 학습 가능한 토큰을 삽입하기 때문입니다. \n",
    "# https://github.com/THUDM/P-tuning\n",
    "#\n",
    "# p-tuing v2 => 새로운 방식이 아니라 NLU 향상을 위해, MLM 모델에 prefix-tuning을 적용한 방식\n",
    "# https://github.com/THUDM/P-tuning-v2\n",
    "#=======================================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoTokenizer, set_seed\n",
    "import os\n",
    "from os import sys\n",
    "from transformers import BertTokenizer, HfArgumentParser, TrainingArguments\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "logger = mlogging(loggername=\"bert-p-tuing\", logfilename=\"bert-p-tuing\")\n",
    "device = GPU_info()\n",
    "\n",
    "model_path = '../../data11/model/bert/bert-multilingual-cased'\n",
    "\n",
    "#tokenize 설정\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_path, TOKENIZERS_PARALLELISM=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path, strip_accents=False, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631b209-d129-4a7a-bf7c-f4c64c79c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인자들 설정\n",
    "#======================================================\n",
    "# data args 인자 설정 \n",
    "class data_args:\n",
    "    dataset_name='QA'\n",
    "    #pad_to_max_length = True\n",
    "    max_seq_length = 128\n",
    "    #overwrite_cache = True\n",
    "    #max_train_samples = None\n",
    "    #max_eval_samples = None\n",
    "    #max_predict_samples = None\n",
    "    \n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "# 허깅페이스 TrainingArguments 설정\n",
    "training_args = TrainingArguments(\"p-tuning-bert-test\")\n",
    "\n",
    "# run_rte_bert.sh 에 사용된 인자만 새롭게 정의함.\n",
    "training_args.do_train=True\n",
    "training_args.do_eval=True\n",
    "training_args.seed = 111\n",
    "training_args.per_device_train_batch_size=32  #batch_size\n",
    "training_args.learning_rate =3e-5        #lr\n",
    "training_args.save_strategy = \"no\"\n",
    "training_args.evaluation_strategy = \"steps\"  #eval 언제마다 할지 => no, steps, epoch\n",
    "training_args.num_train_epochs = 10 # epochs\n",
    "training_args.output_dir = '../../data11/model/bert/bert-multilingual-cased-p-tuing-pp-qa-20'\n",
    "\n",
    "#training_args.report_to = \"none\"  # 기본은 all로 , all이면 wandb에도 기록된다.\n",
    "#======================================================\n",
    "   \n",
    "#======================================================\n",
    "# p-tuningv2 prefixt 튜닝일때 설정값 지정 \n",
    "pre_seq_len = 20             # prefix 계수\n",
    "prefix_projection = True     # True = two-layer MLP 사용함(Multi-layer perceptron(다중퍼셉트론))\n",
    "prefix_hidden_size = 512     # prefix hidden size\n",
    "#======================================================\n",
    "\n",
    "#======================================================\n",
    "#seed 설정\n",
    "#set_seed(training_args.seed)\n",
    "seed_everything(training_args.seed)\n",
    "#======================================================\n",
    "\n",
    "print(training_args.fp16)\n",
    "print(training_args.get_process_log_level())\n",
    "print(training_args.do_train)\n",
    "print(training_args.evaluation_strategy)\n",
    "print(training_args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff649f-7706-49b3-a6cf-cf7cc02d4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 data loader 생성\n",
    "sys.path.append('..')\n",
    "from myutils import KorQuADCorpus, QADataset, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = data_args.max_seq_length   # 질문 + 지문 최대 크기\n",
    "doc_stride = 64     # 지문이 128을 넘을 경우, 얼만큼씩 다음 지문으로 대체할지\n",
    "max_query_length = 32  # 질문 최대 크기\n",
    "batch_size = training_args.per_device_train_batch_size        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "cache = False   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 설정\n",
    "corpus = KorQuADCorpus()\n",
    "\n",
    "\n",
    "# 학습 dataset 생성\n",
    "print('create train_loader===========================================================')\n",
    "train_file_fpath = '../../data11/korpora/korQuAD/KorQuAD_v1.0_train.json'\n",
    "train_dataset = QADataset(file_fpath=train_file_fpath, tokenizer=tokenizer, corpus=corpus, max_seq_length=max_seq_len, max_query_length = max_query_length, doc_stride= doc_stride, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "print('end train_loader===========================================================')\n",
    "\n",
    "print('create eval_loader===========================================================')\n",
    "eval_file_fpath = '../../data11/korpora/korQuAD/KorQuAD_v1.0_dev.json'\n",
    "eval_dataset = QADataset(file_fpath=eval_file_fpath, tokenizer=tokenizer, corpus=corpus, max_seq_length=max_seq_len, max_query_length = max_query_length, doc_stride= doc_stride, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "print('end eval_loader===========================================================')\n",
    "\n",
    "print('train_loader_len: {}, eval_loader_len: {}'.format(len(train_loader), len(eval_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac7667-27c3-46a6-8b73-51007e5887d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정 \n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,  # q&a 는 2\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "#========================================================================\n",
    "# 훈련 모델에 따라 아래값들을 바꿔줘야 함.\n",
    "#========================================================================\n",
    "#get_model 에서 --prefix인경우 config 인자 설정해 주고 있음.\n",
    "\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.pre_seq_len = pre_seq_len             # prefix 계수\n",
    "config.prefix_projection = prefix_projection    # True = two-layer MLP 사용함(Multi-layer perceptron(다중퍼셉트론))\n",
    "config.prefix_hidden_size = prefix_hidden_size     # prefix hidden size\n",
    "#========================================================================\n",
    "\n",
    "print(config.num_hidden_layers)\n",
    "print(config.num_attention_heads)\n",
    "print(config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dfcf9-f7be-479b-b92c-38f493739967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# p-tuing v2 => 새로운 방식이 아니라 NLU 향상을 위해, MLM 모델에 prefix-tuning을 적용한 방식\n",
    "# 참고 소스 : https://github.com/THUDM/P-tuning-v2\n",
    "# \n",
    "# p-tuning-v2의 주요 기여는 원래 입력 전에 사용자 정의 길이의 레이어 프롬프트를 추가하고 \n",
    "# 다운스트림 작업에 대한 후속 교육에서 BERT 모델의 모든 매개변수를 고정하고 이러한 프롬프트만 교육하는 것임.\n",
    "# 설명 : https://zhuanlan.zhihu.com/p/459305102\n",
    "#\n",
    "# => P-tuning-v2의 구현 방식은 prefix N 시퀀스를 생성한 다음, 원래 bert 모델과 연결한다. 이때 bert의 past_key_values(*여기서는 decoding 속도 개선 목적이 아님)를 이용함\n",
    "# => bert의 past_key_values로 prefix에대한 key 와 value 를 넘겨줘서, 기존 입력 key, value와 연결시키도록 함.\n",
    "# => get_prompt() 함수 : prefix를 past_key_value 형식(batch_size, num_heads, sequence_length - 1, embed_size_per_head)으로 조정(만듬)\n",
    "# => attention_mask : 기존 attention_mask +  prefix_attention_mask \n",
    "#==============================================================================\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    r'''\n",
    "    The torch.nn model to encode the prefix\n",
    "    Input shape: (batch-size, prefix-length)\n",
    "    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.prefix_projection = config.prefix_projection\n",
    "        if self.prefix_projection:\n",
    "            # Use a two-layer MLP to encode the prefix\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n",
    "            self.trans = torch.nn.Sequential(\n",
    "                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                \n",
    "                # num_hidden_layers(12)*2*dim_embedding 인데, \n",
    "                # 여기서 *2는 key와 value를 기존 입력 key와 value로 연결시키는 구조이므로, *2를 해준것임.\n",
    "                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "            )\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size)\n",
    "\n",
    "    def forward(self, prefix: torch.Tensor):\n",
    "        if self.prefix_projection:\n",
    "            prefix_tokens = self.embedding(prefix)\n",
    "            past_key_values = self.trans(prefix_tokens)\n",
    "        else:\n",
    "            past_key_values = self.embedding(prefix)\n",
    "        return past_key_values\n",
    " \n",
    "# BertPrefixForQuestionAnswering 클래스\n",
    "class BertPrefixForQuestionAnswering(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz,\n",
    "            seqlen,\n",
    "            self.n_layer * 2, \n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )\n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        return past_key_values\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        past_key_values = self.get_prompt(batch_size=batch_size)\n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)\n",
    "        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbd6d0-ced7-4815-a70c-f7d7a52f7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# p-tuning 모델\n",
    "model = BertPrefixForQuestionAnswering.from_pretrained(model_path, config=config, revision=\"main\")\n",
    "\n",
    "# NLI 모델에서 레벨은 3개지(참,거짓,모름) 이므로, num_labels=3을 입력함\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e12fe9-eece-44a8-81a1-f7636d72fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작\n",
    "import time\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "logger.info(f\"=== model: {model_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")\n",
    "\n",
    "##################################################\n",
    "# 변수 설정\n",
    "##################################################\n",
    "epochs = training_args.num_train_epochs            # epochs\n",
    "learning_rate = training_args.learning_rate  # 학습률\n",
    "p_itr = 1000           # 손실률 보여줄 step 수\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "num_warmup_steps = total_steps * 0.1\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        start_positions = data['start_positions'].to(device)\n",
    "        end_positions = data['end_positions'].to(device)\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도와 총 손실률 계산\n",
    "            # start 포지션 정확도 구함\n",
    "            \n",
    "            # argmax = 최대 인덱스값 리턴함\n",
    "            start_pred = torch.argmax(F.softmax(start_scores), dim=1)\n",
    "            start_correct = start_pred.eq(start_positions)\n",
    "\n",
    "            # end 포지션 정확도 구함\n",
    "            end_pred = torch.argmax(F.softmax(end_scores), dim=1)\n",
    "            end_correct = start_pred.eq(end_positions)\n",
    "            \n",
    "            # start 포지션과 end 포지션 정확도를 더하고 2로 나줌\n",
    "            total_correct += (start_correct.sum().item() + end_correct.sum().item()) / 2\n",
    "                \n",
    "            total_len += len(start_positions)    \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "                list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "        #if itr > 5:\n",
    "        #    break\n",
    "   \n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "    # 평가 시작\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "    \n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        start_positions = data['start_positions'].to(device)\n",
    "        end_positions = data['end_positions'].to(device)\n",
    " \n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "    \n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            loss = outputs.loss\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "            \n",
    "            # 총 손실류 구함\n",
    "            # start 포지션 정확도 구함\n",
    "            start_pred = torch.argmax(F.softmax(start_scores), dim=1)\n",
    "            start_correct = start_pred.eq(start_positions)\n",
    "            \n",
    "            # end 포지션 정확도 구함\n",
    "            end_pred = torch.argmax(F.softmax(end_scores), dim=1)\n",
    "            end_correct = start_pred.eq(end_positions)\n",
    "            \n",
    "            # start 포지션과 end 포지션 정확도를 더하고 2로 나줌\n",
    "            total_test_correct += (start_correct.sum().item() + end_correct.sum().item()) / 2\n",
    "            total_test_len += len(start_positions)\n",
    "    \n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(f'-END-\\n')\n",
    "    ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc71684-c3aa-494e-b193-49eb7305cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49d7d5-7983-4488-abbc-99f7d1b012f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b71fed-9d62-46f7-ae76-4805e6b9a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전체모델 저장\n",
    "OUTPATH = training_args.output_dir\n",
    "\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH\n",
    "os.makedirs(VOCAB_PATH,exist_ok=True)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4dc7b-668b-44a4-a786-7adb30b9230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, OUTPATH + 'pytorch_model_torch.bin') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
