{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b592bf7-2bd2-4afa-b7dc-bf52447e3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어쌍으로 된 말뭉치 다운로드\n",
    "# => bert 모델을 사용하여 영어에서 다른언어 모델로 학습을 이전시키는 작업에 사용되는 corpus\n",
    "#https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/multilingual/get_parallel_data_wikimatrix.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8ff0d2-4f3a-4c54-86c7-d92add0777d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script downloads the WikiMatrix corpus (https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix)\n",
    " and create parallel sentences tsv files that can be used to extend existent sentence embedding models to new languages.\n",
    "The WikiMatrix mined parallel sentences from Wikipedia in various languages.\n",
    "Further information can be found in our paper:\n",
    "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
    "https://arxiv.org/abs/2004.09813\n",
    "\"\"\"\n",
    "import os\n",
    "import sentence_transformers.util\n",
    "import gzip\n",
    "import csv\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# 소스 와 타겟 언어 정의\n",
    "source_languages = set(['en'])                            #Languages our (monolingual) teacher model understands\n",
    "target_languages = set(['de', 'fr', 'ko'])    #New languages we want to extend to\n",
    "\n",
    "# \n",
    "num_dev_sentences = 1000         #Number of sentences we want to use for development\n",
    "threshold = 1.075                #Only use sentences with a LASER similarity score above the threshold\n",
    "\n",
    "# 전체 소스와 타겟으로 번역된 corpus를 download_folder 에 받고 나서, \n",
    "# 위 1.075 스코어 이상인 것만 parallel_sentences_folder 폴더에 훈련데이터와 평가 데이터가 생성됨\n",
    "download_url = \"https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/\"\n",
    "download_folder = \"../korpora/WikiMatrix/\"\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(download_folder), exist_ok=True)\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd159eb-087c-4d3b-9d7a-1888ea90d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-fr.tsv.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ef42b48f9b4a20ad6d0ad2b93e35c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/755M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 1000 dev sentences parallel-sentences/WikiMatrix-en-fr-dev.tsv.gz\n",
      "Write 1024061 train sentences parallel-sentences/WikiMatrix-en-fr-train.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.en-ko.tsv.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b5d779685e45a99d21ae233d96a908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/154M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 1000 dev sentences parallel-sentences/WikiMatrix-en-ko-dev.tsv.gz\n",
      "Write 34559 train sentences parallel-sentences/WikiMatrix-en-ko-train.tsv.gz\n",
      "Download https://dl.fbaipublicfiles.com/laser/WikiMatrix/v1/WikiMatrix.de-en.tsv.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96d9e745d2f436986a72040f05771db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/650M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 1000 dev sentences parallel-sentences/WikiMatrix-en-de-dev.tsv.gz\n",
      "Write 344476 train sentences parallel-sentences/WikiMatrix-en-de-train.tsv.gz\n",
      "---DONE---\n"
     ]
    }
   ],
   "source": [
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        filename_train = os.path.join(parallel_sentences_folder, \"WikiMatrix-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        filename_dev = os.path.join(parallel_sentences_folder, \"WikiMatrix-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "\n",
    "        if not os.path.exists(filename_train) and not os.path.exists(filename_dev):\n",
    "            langs_ordered = sorted([source_lang, target_lang])\n",
    "            wikimatrix_filename = \"WikiMatrix.{}-{}.tsv.gz\".format(*langs_ordered)\n",
    "            wikimatrix_filepath = os.path.join(download_folder, wikimatrix_filename)\n",
    "\n",
    "            if not os.path.exists(wikimatrix_filepath):\n",
    "                print(\"Download\", download_url+wikimatrix_filename)\n",
    "                try:\n",
    "                    sentence_transformers.util.http_get(download_url+wikimatrix_filename, wikimatrix_filepath)\n",
    "                except:\n",
    "                    print(\"Was not able to download\", download_url+wikimatrix_filename)\n",
    "                    continue\n",
    "\n",
    "            if not os.path.exists(wikimatrix_filepath):\n",
    "                continue\n",
    "\n",
    "            train_sentences = []\n",
    "            dev_sentences = []\n",
    "            dev_sentences_set = set()\n",
    "            extract_dev_sentences = True\n",
    "\n",
    "            with gzip.open(wikimatrix_filepath, 'rt', encoding='utf8') as fIn:\n",
    "                for line in fIn:\n",
    "                    score, sent1, sent2 = line.strip().split('\\t')\n",
    "                    sent1 = sent1.strip()\n",
    "                    sent2 = sent2.strip()\n",
    "                    score = float(score)\n",
    "\n",
    "                    if score < threshold:\n",
    "                        break\n",
    "\n",
    "                    if sent1 == sent2:\n",
    "                        continue\n",
    "\n",
    "                    if langs_ordered.index(source_lang) == 1: #Swap, so that src lang is sent1\n",
    "                        sent1, sent2 = sent2, sent1\n",
    "\n",
    "                    # Avoid duplicates in development set\n",
    "                    if sent1 in dev_sentences_set or sent2 in dev_sentences_set:\n",
    "                        continue\n",
    "\n",
    "                    if extract_dev_sentences:\n",
    "                        dev_sentences.append([sent1, sent2])\n",
    "                        dev_sentences_set.add(sent1)\n",
    "                        dev_sentences_set.add(sent2)\n",
    "\n",
    "                        if len(dev_sentences) >= num_dev_sentences:\n",
    "                            extract_dev_sentences = False\n",
    "                    else:\n",
    "                        train_sentences.append([sent1, sent2])\n",
    "\n",
    "            print(\"Write\", len(dev_sentences), \"dev sentences\", filename_dev)\n",
    "            with gzip.open(filename_dev, 'wt', encoding='utf8') as fOut:\n",
    "                for sents in dev_sentences:\n",
    "                    fOut.write(\"\\t\".join(sents))\n",
    "                    fOut.write(\"\\n\")\n",
    "\n",
    "            print(\"Write\", len(train_sentences), \"train sentences\", filename_train)\n",
    "            with gzip.open(filename_train, 'wt', encoding='utf8') as fOut:\n",
    "                for sents in train_sentences:\n",
    "                    fOut.write(\"\\t\".join(sents))\n",
    "                    fOut.write(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"---DONE---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6302f9b-79b2-44c8-8162-be2d256f837c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
