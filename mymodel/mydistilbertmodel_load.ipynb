{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64b8458-c9f6-42f9-b194-186c898055bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-04-13.log\n",
      "logfilepath:qnadataset_2022-04-13.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "logfilepath:../log/test_2022-04-13.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# by kobongsoo . 2022-04-13\n",
    "# 기존 distilbert 모델에 출력 dimension=768을 128 로 줄인 커스터마이징된 모델을 불러와서 cosin 유사도 측정하는 예제임\n",
    "#\n",
    "# => 커스터마이징된 모델은 반드시 동일한 MYDistilBertModel 클래스가 구현되어 있어야 함.\n",
    "# => cosin 유사도 측정시, 768이나 128일때 별반 차이 없은 것으로 여겨짐. \n",
    "#====================================================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from myutils import GPU_info, seed_everything, mlogging, pytorch_cos_sim\n",
    "\n",
    "device = GPU_info()\n",
    "seed_everything(111)\n",
    "logger=mlogging(loggername=\"test\", logfilename=\"../log/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27424df-00e5-4f19-a34c-a3723005287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기 위해서는 반드시 기존 MyDistilBertModel class 와 동일한 class가 똑같이 구현되어 있어야 함\n",
    "class MyDistilBertModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                model_path: str,\n",
    "                in_dim: int = 768,\n",
    "                out_dim: int = 768,\n",
    "                nlabel: int = 2,\n",
    "                drop_rate: int = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "        self.activefunc = nn.Tanh()\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "       \n",
    "        #self.classifier = nn.Linear(out_dim, nlabel)\n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "               input_ids,\n",
    "               #token_type_ids = None,  #distilbert에는 token_type_ids가 없다.\n",
    "               attention_mask \n",
    "               ):\n",
    "        output = self.model(input_ids, attention_mask)\n",
    "        h = output.last_hidden_state  #distilber는 맨마지막 hidden state만 리턴한다.\n",
    "        out1 = self.drop(h)\n",
    "        out2 = self.activefunc(self.fc(out1))\n",
    "        \n",
    "        #logits = self.classifier(self.drop(pooled_h))\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032f6c2a-f522-40c3-a30c-2994fadb55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDistilBertModel(\n",
      "  (model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(167550, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (activefunc): Tanh()\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 불러올때는 torch.load로 불러옴\n",
    "model_path = '../model/distilbert/mydistilbert/'\n",
    "mymodel = torch.load(model_path+'pytorch_model.bin')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772f78ea-741d-4426-bec6-2aab2e8137ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "odict_keys(['model.embeddings.word_embeddings.weight', 'model.embeddings.position_embeddings.weight', 'model.embeddings.LayerNorm.weight', 'model.embeddings.LayerNorm.bias', 'model.transformer.layer.0.attention.q_lin.weight', 'model.transformer.layer.0.attention.q_lin.bias', 'model.transformer.layer.0.attention.k_lin.weight', 'model.transformer.layer.0.attention.k_lin.bias', 'model.transformer.layer.0.attention.v_lin.weight', 'model.transformer.layer.0.attention.v_lin.bias', 'model.transformer.layer.0.attention.out_lin.weight', 'model.transformer.layer.0.attention.out_lin.bias', 'model.transformer.layer.0.sa_layer_norm.weight', 'model.transformer.layer.0.sa_layer_norm.bias', 'model.transformer.layer.0.ffn.lin1.weight', 'model.transformer.layer.0.ffn.lin1.bias', 'model.transformer.layer.0.ffn.lin2.weight', 'model.transformer.layer.0.ffn.lin2.bias', 'model.transformer.layer.0.output_layer_norm.weight', 'model.transformer.layer.0.output_layer_norm.bias', 'model.transformer.layer.1.attention.q_lin.weight', 'model.transformer.layer.1.attention.q_lin.bias', 'model.transformer.layer.1.attention.k_lin.weight', 'model.transformer.layer.1.attention.k_lin.bias', 'model.transformer.layer.1.attention.v_lin.weight', 'model.transformer.layer.1.attention.v_lin.bias', 'model.transformer.layer.1.attention.out_lin.weight', 'model.transformer.layer.1.attention.out_lin.bias', 'model.transformer.layer.1.sa_layer_norm.weight', 'model.transformer.layer.1.sa_layer_norm.bias', 'model.transformer.layer.1.ffn.lin1.weight', 'model.transformer.layer.1.ffn.lin1.bias', 'model.transformer.layer.1.ffn.lin2.weight', 'model.transformer.layer.1.ffn.lin2.bias', 'model.transformer.layer.1.output_layer_norm.weight', 'model.transformer.layer.1.output_layer_norm.bias', 'model.transformer.layer.2.attention.q_lin.weight', 'model.transformer.layer.2.attention.q_lin.bias', 'model.transformer.layer.2.attention.k_lin.weight', 'model.transformer.layer.2.attention.k_lin.bias', 'model.transformer.layer.2.attention.v_lin.weight', 'model.transformer.layer.2.attention.v_lin.bias', 'model.transformer.layer.2.attention.out_lin.weight', 'model.transformer.layer.2.attention.out_lin.bias', 'model.transformer.layer.2.sa_layer_norm.weight', 'model.transformer.layer.2.sa_layer_norm.bias', 'model.transformer.layer.2.ffn.lin1.weight', 'model.transformer.layer.2.ffn.lin1.bias', 'model.transformer.layer.2.ffn.lin2.weight', 'model.transformer.layer.2.ffn.lin2.bias', 'model.transformer.layer.2.output_layer_norm.weight', 'model.transformer.layer.2.output_layer_norm.bias', 'model.transformer.layer.3.attention.q_lin.weight', 'model.transformer.layer.3.attention.q_lin.bias', 'model.transformer.layer.3.attention.k_lin.weight', 'model.transformer.layer.3.attention.k_lin.bias', 'model.transformer.layer.3.attention.v_lin.weight', 'model.transformer.layer.3.attention.v_lin.bias', 'model.transformer.layer.3.attention.out_lin.weight', 'model.transformer.layer.3.attention.out_lin.bias', 'model.transformer.layer.3.sa_layer_norm.weight', 'model.transformer.layer.3.sa_layer_norm.bias', 'model.transformer.layer.3.ffn.lin1.weight', 'model.transformer.layer.3.ffn.lin1.bias', 'model.transformer.layer.3.ffn.lin2.weight', 'model.transformer.layer.3.ffn.lin2.bias', 'model.transformer.layer.3.output_layer_norm.weight', 'model.transformer.layer.3.output_layer_norm.bias', 'model.transformer.layer.4.attention.q_lin.weight', 'model.transformer.layer.4.attention.q_lin.bias', 'model.transformer.layer.4.attention.k_lin.weight', 'model.transformer.layer.4.attention.k_lin.bias', 'model.transformer.layer.4.attention.v_lin.weight', 'model.transformer.layer.4.attention.v_lin.bias', 'model.transformer.layer.4.attention.out_lin.weight', 'model.transformer.layer.4.attention.out_lin.bias', 'model.transformer.layer.4.sa_layer_norm.weight', 'model.transformer.layer.4.sa_layer_norm.bias', 'model.transformer.layer.4.ffn.lin1.weight', 'model.transformer.layer.4.ffn.lin1.bias', 'model.transformer.layer.4.ffn.lin2.weight', 'model.transformer.layer.4.ffn.lin2.bias', 'model.transformer.layer.4.output_layer_norm.weight', 'model.transformer.layer.4.output_layer_norm.bias', 'model.transformer.layer.5.attention.q_lin.weight', 'model.transformer.layer.5.attention.q_lin.bias', 'model.transformer.layer.5.attention.k_lin.weight', 'model.transformer.layer.5.attention.k_lin.bias', 'model.transformer.layer.5.attention.v_lin.weight', 'model.transformer.layer.5.attention.v_lin.bias', 'model.transformer.layer.5.attention.out_lin.weight', 'model.transformer.layer.5.attention.out_lin.bias', 'model.transformer.layer.5.sa_layer_norm.weight', 'model.transformer.layer.5.sa_layer_norm.bias', 'model.transformer.layer.5.ffn.lin1.weight', 'model.transformer.layer.5.ffn.lin1.bias', 'model.transformer.layer.5.ffn.lin2.weight', 'model.transformer.layer.5.ffn.lin2.bias', 'model.transformer.layer.5.output_layer_norm.weight', 'model.transformer.layer.5.output_layer_norm.bias', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "# state_dict를 출력해봄\n",
    "state_dict = mymodel.state_dict()\n",
    "print(type(state_dict))\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f49d69d-202c-45e7-bb65-7bd9286ecdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101, 122278,  10892,   9379,  11287,   9580,  11664, 123665,  11287,\n",
      "          10015,  12692, 118632,  11903,    102,      0,      0,      0,      0],\n",
      "        [   101, 122278,  10892,   9034,  10739,   9580,  11664, 123665,  11287,\n",
      "          10015,  12692, 118632,  11903,    102,      0,      0,      0,      0],\n",
      "        [   101, 122278,  10892,   8843, 118707,  10015,  62211,    117,   9034,\n",
      "          10739,   9583,  15891,  11506,    102,      0,      0,      0,      0],\n",
      "        [   101, 119676, 123323,  10892, 136093,  11287,   9254,  76820,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 122278, 140417,  11018, 131258,  11467, 121653,  28750,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 129345,  10892, 120578,  15303,  10015, 154112,  19105,    117,\n",
      "         120233,  15303, 123665,  11287,   9685, 118632,  11903,    102,      0],\n",
      "        [   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119581,\n",
      "         119603, 120640,  11925,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 136591, 119603, 119803,  10892,    125,    110,   9069, 119803,\n",
      "          10622,   9638, 118891,  41521,  17342, 120364,  22096,    102,      0],\n",
      "        [   101,  47364, 120035,  11018, 126336,  21611, 122626,  20173, 120558,\n",
      "           9737,  11018, 121362, 120466,  11925,    102,      0,      0,      0],\n",
      "        [   101, 128051,  15303,  70672, 119674,  12638, 105383, 121057,  11287,\n",
      "          58248,  66421,  11903,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 122278, 123665,  11018,   9379,  11287, 123658,  11664,  42608,\n",
      "            100,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9450, 119444, 159780,  50266, 121456,  37905,  81785,  10193,\n",
      "          48506,   8892,  10622,   9010,  17706,    102,      0,      0,      0],\n",
      "        [   101, 120824,  11102, 123665,  10530, 134288,  10622, 126551, 108436,\n",
      "          16139,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 120569,  37115,  18398,  10530, 121752, 119643,  10622, 119850,\n",
      "          28750,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 138367,  10892, 128051, 126550, 133359,  11513, 132086,  12490,\n",
      "            102,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 123665,  11287,   9685,  11903,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9521,   8924,  37388,  12092,  54780,  28911,   9303,    119,\n",
      "           8904, 119250,  16985,    119,    102,      0,      0,      0,      0],\n",
      "        [   101, 129345, 121492,  18382, 120426, 120053,  11287,   8977,  41919,\n",
      "          23925, 120364,  33188,  48345,    119,    102,      0,      0,      0],\n",
      "        [   101, 121467,  33188,  48345,    119, 127364,  40364, 119583,  10739,\n",
      "           9100, 136592,    119,    102,      0,      0,      0,      0,      0],\n",
      "        [   101,   8924,  30873,  14867, 120578,  10150, 131192, 119994,  31398,\n",
      "           9952, 119217,    119, 129345,    100,    119,    102,      0,      0],\n",
      "        [   101, 139896, 134170,  11287,   9056,   9141,  12965, 119210, 119081,\n",
      "          48345,    119,   9074, 121962,  14843, 118671,  48549,    136,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [\n",
    "    '오늘은 비가 오고 날씨가 흐리겠다',\n",
    "    '오늘은 눈이 오고 날씨가 흐리겠다',\n",
    "    '오늘은 가끔 흐리고, 눈이 올수 있다',\n",
    "    '여기 식당은 파스타가 맛있다',\n",
    "    '오늘 증시는 내림으로 마감 하였다',\n",
    "    '내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다',\n",
    "    '서울은 대한민국에 수도이며, 정치 경제 중심지이다',\n",
    "    '내년 경제 성장은 4%대 성장을 이룰거라 예상된다',\n",
    "    '프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다',\n",
    "    '올해에는 대통령 선거와 지방선거가 동시에 열린다',\n",
    "    '오늘 날씨는 비가 내리고 매우 춥다',\n",
    "    '손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다',\n",
    "    '건조한 날씨에 산불을 조심해야 한다',\n",
    "    '윈도우11 OS에 검색 기능을 강화 하였다',\n",
    "    '한국은행은 올해 하반기 금리를 동결했다',\n",
    "    '날씨가 좋다',\n",
    "    '안 그래도 되는데 뭐. 괜찮아.',\n",
    "    '내일 저녁까지 보수 공사가 끝날 것으로 예상합니다.',\n",
    "    '감사합니다. 후회 없는 결정이 될 겁니다.',\n",
    "    '그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다.',\n",
    "    '프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?'\n",
    "    ]\n",
    "\n",
    "# ** 멀티로 한번에 tokenizer 할때는 반드시 padding=True 해야 함.(그래야 최대 길이 token에 맞춰서 padding 됨)\n",
    "test = tokenizer(test_sentence, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=256, \n",
    "                 return_tensors=\"pt\")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df847b9a-54a4-4e8c-a773-f941820584a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([21, 18, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = mymodel(**test)\n",
    "print(type(last_hidden_state)), print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c351cb1-96c5-4e86-82d3-40edf77608ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 10:59:47,770 - test - INFO - ---------------------------------------------------------\n",
      "2022-04-13 10:59:47,772 - test - INFO - 오늘은 비가 오고 날씨가 흐리겠다, 유사도:0.9999997019767761\n",
      "2022-04-13 10:59:47,773 - test - INFO - 오늘은 눈이 오고 날씨가 흐리겠다, 유사도:0.9361810684204102\n",
      "2022-04-13 10:59:47,775 - test - INFO - 오늘 날씨는 비가 내리고 매우 춥다, 유사도:0.8694930672645569\n",
      "2022-04-13 10:59:47,776 - test - INFO - 안 그래도 되는데 뭐. 괜찮아., 유사도:0.8220617771148682\n",
      "2022-04-13 10:59:47,777 - test - INFO - 감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.7808585166931152\n",
      "2022-04-13 10:59:47,778 - test - INFO - 그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.7807663679122925\n",
      "2022-04-13 10:59:47,779 - test - INFO - 내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:0.6715965270996094\n",
      "2022-04-13 10:59:47,780 - test - INFO - 내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다, 유사도:0.6471870541572571\n",
      "2022-04-13 10:59:47,781 - test - INFO - 건조한 날씨에 산불을 조심해야 한다, 유사도:0.6443738341331482\n",
      "2022-04-13 10:59:47,782 - test - INFO - 한국은행은 올해 하반기 금리를 동결했다, 유사도:0.5938227772712708\n",
      "2022-04-13 10:59:47,783 - test - INFO - 오늘 증시는 내림으로 마감 하였다, 유사도:0.5848532319068909\n",
      "2022-04-13 10:59:47,784 - test - INFO - 프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:0.5822303295135498\n",
      "2022-04-13 10:59:47,784 - test - INFO - 손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다, 유사도:0.5100446939468384\n",
      "2022-04-13 10:59:47,785 - test - INFO - 여기 식당은 파스타가 맛있다, 유사도:0.4588366150856018\n",
      "2022-04-13 10:59:47,786 - test - INFO - 오늘은 가끔 흐리고, 눈이 올수 있다, 유사도:0.4220626652240753\n",
      "2022-04-13 10:59:47,787 - test - INFO - 올해에는 대통령 선거와 지방선거가 동시에 열린다, 유사도:0.39745602011680603\n",
      "2022-04-13 10:59:47,788 - test - INFO - 프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다, 유사도:0.3880769610404968\n",
      "2022-04-13 10:59:47,788 - test - INFO - 서울은 대한민국에 수도이며, 정치 경제 중심지이다, 유사도:0.3317304849624634\n",
      "2022-04-13 10:59:47,789 - test - INFO - 내년 경제 성장은 4%대 성장을 이룰거라 예상된다, 유사도:0.3142300546169281\n",
      "2022-04-13 10:59:47,790 - test - INFO - 날씨가 좋다, 유사도:0.23553307354450226\n",
      "2022-04-13 10:59:47,791 - test - INFO - 윈도우11 OS에 검색 기능을 강화 하였다, 유사도:0.1834021806716919\n",
      "2022-04-13 10:59:47,792 - test - INFO - ---------------------------------------------------------\n",
      "2022-04-13 10:59:47,793 - test - INFO - === 유사도 처리시간: 0.030 초 ===\n",
      "2022-04-13 10:59:47,796 - test - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = torch.mean(last_hidden_state[0], dim=0)\n",
    "#print(in_mean_sequence.shape)\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(last_hidden_state):\n",
    "    out_mean_sequence = torch.mean(hidden, dim=0)\n",
    "    simul_score = pytorch_cos_sim(in_mean_sequence, out_mean_sequence)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    #print(test_sentence[index-1])\n",
    "    logger.info(f'{test_sentence[index-1]}, 유사도:{value[0][0]}')\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
