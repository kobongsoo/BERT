{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af7bc1d-12c8-48e0-8964-7301c0308705",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "# 허깅페이스 Tokenizer 예제\n",
    "# - 참고 : https://huggingface.co/docs/tokenizers/training_from_memory\n",
    "# - 참고 : https://towardsdatascience.com/training-bpe-wordpiece-and-unigram-tokenizers-from-scratch-using-hugging-face-3dd174850713\n",
    "#\n",
    "# xml-roberta-base 모델 : Unigram Tokenizer 사용함.\n",
    "# - tokenizer.json 파일을 보면 Tokenzier모델 속성을 알수 있음.\n",
    "#\n",
    "# [Tokenizer 과정]\n",
    "#  1. Tokenizer 모델, 트레이너 정의\n",
    "#  2. Normalizer, Pre-tokenizers, Decoders, Post-processors 등을 설정\n",
    "#  3. 훈련시작 및 Tokenizer 저장(*pretrained 연동해서 저장)\n",
    "#  4. (option) Tokenizer된 vocab들을 파일(.txt)로 저장\n",
    "#  5. (option) PretrainedTokenizer로 변환(*단 unigram으로 된 tokeizer여야만 됨)\n",
    "###############################################################################################\n",
    "\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.pre_tokenizers import Whitespace, WhitespaceSplit, Metaspace, Punctuation, Digits\n",
    "from tokenizers.normalizers import Lowercase, NFC, NFD, NFKC, NFKD, Nmt, Normalizer,Precompiled, Replace, Sequence, Strip, StripAccents\n",
    "\n",
    "#===============================================================================================\n",
    "# 토크너나이저 모델 종류 (BPE=BPE(GPT2), UNI=Unigram(xml-roberta-base), WL=WordLevel, WP=WordPiece(bert))\n",
    "model_type = 'WP' # BPE, UNI, WL, WP\n",
    "vocab_size = 32000  # vocab 크기 정의\n",
    "\n",
    "# 훈련 말뭉치(*리스트)\n",
    "# -> [\"./test1.txt','./test2.txt'] 리스트로 여러개 파일을 넣을수 있음\n",
    "corpus_path = ['../../../korpora/kowiki/kowiki-202206-nlp-corpus-mecab.txt']\n",
    "\n",
    "# out tokenizer 파일 출력 경로\n",
    "tokenizer_path = './kowiki-2022-nlp-corpus-mecab-WP.json'\n",
    "\n",
    "# vocab 파일 저장 경로\n",
    "vocab_out = './kowiki-2022-nlp-corpus-mecab-WP.txt'\n",
    "\n",
    "# Pretrained 토크너나이저로 변환된 파일들 생성할 폴더\n",
    "pretrained_tokenizer_folder = './kowiki-2022-nlp-corpus-mecab-WP'\n",
    "#===============================================================================================\n",
    "\n",
    "if model_type == 'WL' or model_type=='WP':\n",
    "    unk_token = \"[UNK]\" # unk 토큰 정의(예: distilbert 모델일때)\n",
    "    spl_tokens = [\"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\", \"[PAD]\"] # special 토큰 정의 (예: distilbert 모델일때)\n",
    "else:\n",
    "    unk_token = \"<unk>\"  # unk 토큰 정의(예: xml-roberta-base 모델일때) \n",
    "    spl_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<mask>\", \"<pad>\"] # special 토큰 정의 (예: xml-roberta-base 모델일때)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b1f6990-b7d5-4f15-94b0-00b5e684c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "# 1. Tokenizer 모델, 트레이너 정의\n",
    "# 2. Normalizer, Pre-tokenizers, Decoders, Post-processors 등을 설정\n",
    "# => 모델에 따라 변경해 줘야 함.(tokenizer.json 파일을 불러와서 봐야함)\n",
    "# => API 참조 : https://huggingface.co/docs/tokenizers/main/en/api/normalizers\n",
    "###############################################################################################\n",
    "\n",
    "# 모델 타입에 따라 Tokenizer 모델, 트레이너 얻는 함수\n",
    "def prepare_tokenizer_trainer(modeltype):\n",
    "    \n",
    "    # 타입 검사\n",
    "    tmpmodeltype = ['BPE', 'UNI', 'WL', 'WP'] \n",
    "    assert modeltype in tmpmodeltype, f'잘못된 모델타입 입력 입니다.=>modeltype:{modeltype}'\n",
    "    \n",
    "    if modeltype == 'BPE': # GPT-2 모델\n",
    "        \n",
    "        tokenizer = Tokenizer(models.BPE(unk_token = unk_token))\n",
    "        trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens = spl_tokens)\n",
    "        \n",
    "        # 1> normalizer : 없음\n",
    "\n",
    "        # 2> pre-tokenizer : ByteLevel - add_prefix_space:False, trim_offsets:true\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "        # 3> decoder : ByteLevel - add_prefix_space:true, trim_offsets:true\n",
    "        tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "        # 4> post_processor : ByteLevel -add_prefix_space:true, trim_offsets:false\n",
    "        tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "        \n",
    "    elif modeltype == 'UNI': # xml-roberta-base 모델\n",
    "        tokenizer = Tokenizer(models.Unigram())\n",
    "        trainer = trainers.UnigramTrainer(vocab_size=vocab_size, unk_token= unk_token, special_tokens = spl_tokens)\n",
    "        \n",
    "        # 1> normalizer : Precompiled 인데, 어떻게 사용하는지 모름.(*따라서 일단 NFKC() 로 지정함)\n",
    "        #tokenizer.normalizer = normalizers.Precompiled()\n",
    "        tokenizer.normalizer = normalizers.NFKC()\n",
    "        \n",
    "        # 2> pre-tokenizer : WhitespaceSplit, Metaspace => 여러개를 묶을때 Sequence를 사용함\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([WhitespaceSplit(), Metaspace()])\n",
    "\n",
    "        # 3> decoder : Metaspace\n",
    "        tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "        # 4> post_processor : TemplateProcessing\n",
    "        tokenizer.post_processor = processors.TemplateProcessing(single=\"<s> $A </s>\", \n",
    "                                                                 pair=\"<s> $A </s> </s> $B:1 </s>:1\", \n",
    "                                                                 special_tokens=[(\"<s>\", 0), (\"</s>\", 1)])\n",
    "\n",
    "    elif modeltype == 'WL': # distilbert 모델\n",
    "        tokenizer = Tokenizer(models.WordLevel(unk_token = unk_token))\n",
    "        trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens = spl_tokens)\n",
    "        \n",
    "        # 1> normalizer : ??      \n",
    "        tokenizer.normalizer = normalizers.Sequence([NFKC(), Lowercase()])\n",
    "\n",
    "        # 2> pre-tokenizer : BertPreTokenizer\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "        # 3> decoder : WordPiece\n",
    "        #tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "        # 4> post_processor : TemplateProcessing\n",
    "        tokenizer.post_processor = processors.BertProcessing((\"[CLS]\",0), (\"[SEP]\",1))\n",
    "        \n",
    "    elif modeltype == 'WP': # distilbert 모델\n",
    "        tokenizer = Tokenizer(models.WordPiece(unk_token = unk_token))\n",
    "        trainer = trainers.WordPieceTrainer(vocab_size=vocab_size, special_tokens = spl_tokens)\n",
    "        \n",
    "        # 1> normalizer : BertNormalizer\n",
    "        tokenizer.normalizer = normalizers.BertNormalizer(clean_text = True, \n",
    "                                                          handle_chinese_chars = True, \n",
    "                                                          strip_accents = False, \n",
    "                                                          lowercase = False)\n",
    "\n",
    "        # 2> pre-tokenizer : BertPreTokenizer\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "        # 3> decoder : WordPiece\n",
    "        tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "        # 4> post_processor : TemplateProcessing\n",
    "        tokenizer.post_processor = processors.TemplateProcessing(single=\"[CLS] $A [SEP]\", \n",
    "                                                                 pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\", \n",
    "                                                                 special_tokens=[(\"[CLS]\",0), (\"[SEP]\",1)])\n",
    "            \n",
    "    return tokenizer, trainer\n",
    "\n",
    "# Tokenizer 모델, 트레이너 선택\n",
    "tokenizer, trainer = prepare_tokenizer_trainer(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c158861-8ca7-497f-bac1-a0c992a5e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "# 3. 훈련시작 및 Tokenizer 저장\n",
    "###############################################################################################\n",
    "\n",
    "# 훈련 시작 \n",
    "tokenizer.train(files=corpus_path, trainer=trainer)\n",
    "\n",
    "# 훈련된 tokenizer 파일로 저장 \n",
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5842abd2-6131-44ca-b2f6-e2835e240915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '여기', '##는', 'BE', '##RT', '##와', '같', '##은', '인공지능', '모델', '##을', '구현', '##하', '##고', ',', 'sim', '##ulate', '하', '##는', '곳', '##입', '##니다', '.', '[SEP]']\n",
      "[0, 20090, 15334, 24589, 30167, 15919, 11760, 15234, 28665, 19848, 15316, 20465, 15375, 15333, 16, 20326, 26867, 14627, 15334, 11841, 15383, 18970, 18, 1]\n",
      "\n",
      "\n",
      "<unk>:None\n",
      "\n",
      "\n",
      "('[CLS]', 0)\n",
      "('여기', 20090)\n",
      "('##는', 15334)\n",
      "('BE', 24589)\n",
      "('##RT', 30167)\n",
      "('##와', 15919)\n",
      "('같', 11760)\n",
      "('##은', 15234)\n",
      "('인공지능', 28665)\n",
      "('모델', 19848)\n",
      "('##을', 15316)\n",
      "('구현', 20465)\n",
      "('##하', 15375)\n",
      "('##고', 15333)\n",
      "(',', 16)\n",
      "('sim', 20326)\n",
      "('##ulate', 26867)\n",
      "('하', 14627)\n",
      "('##는', 15334)\n",
      "('곳', 11841)\n",
      "('##입', 15383)\n",
      "('##니다', 18970)\n",
      "('.', 18)\n",
      "('[SEP]', 1)\n",
      "['[CLS]', '[SEP]', '[UNK]', '[MASK]', '[PAD]', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abdab751ce94693a43b45856f83a5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################################\n",
    "# 4. (option) Tokenizer된 vocab들을 파일(.txt)로 저장\n",
    "###############################################################################################\n",
    "\n",
    "# 저장된 tokenizer 파일 불러옴.\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# tokenizer 테스트 \n",
    "#text = [\"여기는 BERT와 같은 인공지능 모델을 구현하고, simulate 하는 곳입니다.\", \"두개의 Text 문장 테스트 결과 입니다.\"]\n",
    "text = \"여기는 BERT와 같은 인공지능 모델을 구현하고, simulate 하는 곳입니다.\"\n",
    "output = tokenizer.encode(text)\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print('\\n')\n",
    "print(f'<unk>:{tokenizer.token_to_id(\"<unk>\")}')\n",
    "print('\\n')      \n",
    "for token in zip(output.tokens, output.ids):\n",
    "    print(token)\n",
    "    \n",
    "# toeknizer vocab을 id 순번(id_to_token)으로 리스트에 저장함.\n",
    "vocab = []\n",
    "vocablen = tokenizer.get_vocab_size() # tokenizer 길이 얻어옴\n",
    "for i in range(vocablen):\n",
    "    vocab.append(tokenizer.id_to_token(i))\n",
    "    \n",
    "print(vocab[0:20])\n",
    "\n",
    "# txt 파일로 저장\n",
    "# -  vocab을 txt 파일로 저장함\n",
    "from tqdm.notebook import tqdm\n",
    "with open(vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(vocab):\n",
    "        f.write(word+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575366ec-1c20-4bc3-8842-1f46a09b543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 20090, 15334, 24589, 30167, 15919, 11760, 15234, 28665, 19848, 15316, 20465, 15375, 15333, 16, 20326, 26867, 14627, 15334, 11841, 15383, 18970, 18, 1]\n",
      "[CLS]:0\n",
      "여기:20090\n",
      "##는:15334\n",
      "BE:24589\n",
      "##RT:30167\n",
      "##와:15919\n",
      "같:11760\n",
      "##은:15234\n",
      "인공지능:28665\n",
      "모델:19848\n",
      "##을:15316\n",
      "구현:20465\n",
      "##하:15375\n",
      "##고:15333\n",
      ",:16\n",
      "sim:20326\n",
      "##ulate:26867\n",
      "하:14627\n",
      "##는:15334\n",
      "곳:11841\n",
      "##입:15383\n",
      "##니다:18970\n",
      ".:18\n",
      "[SEP]:1\n",
      "[CLS] 여기는 BERT와 같은 인공지능 모델을 구현하고, simulate 하는 곳입니다. [SEP]\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################\n",
    "# 5. (option) PretrainedTokenizer로 변환\n",
    "# - tokenizer.json 파일을 RobertaTokenizerFast 토크너나이저로 변환\n",
    "###############################################################################################\n",
    "from transformers import PreTrainedTokenizerFast, DistilBertTokenizerFast, RobertaTokenizerFast, GPT2TokenizerFast, AutoTokenizer\n",
    "import os\n",
    "\n",
    "def pretrain_tokenizer(modeltype, special_tokens, out_path):\n",
    "      # 타입 검사\n",
    "    tmpmodeltype = ['BPE', 'UNI', 'WL', 'WP'] \n",
    "    assert modeltype in tmpmodeltype, f'잘못된 모델타입 입력 입니다.=>modeltype:{modeltype}'\n",
    "    \n",
    "    if modeltype == 'BPE': # GPT-2 모델\n",
    "        pretrain_tokenizer = GPT2TokenizerFast(tokenizer_file = out_path, additional_special_tokens = special_tokens)\n",
    "    elif modeltype == 'UNI': # xml-roberta-base 모델\n",
    "        pretrain_tokenizer = RobertaTokenizerFast(tokenizer_file = out_path, additional_special_tokens = special_tokens)\n",
    "    elif modeltype == 'WL' or modeltype == 'WP': \n",
    "        pretrain_tokenizer = DistilBertTokenizerFast(tokenizer_file = tokenizer_path, additional_special_tokens = spl_tokens)\n",
    "        \n",
    "    return pretrain_tokenizer\n",
    "\n",
    "pre_tokenizer = pretrain_tokenizer(modeltype=model_type, special_tokens = spl_tokens, out_path=tokenizer_path)\n",
    "\n",
    "# PreTrainedTokenizerFast tokenizer 저장\n",
    "os.makedirs(pretrained_tokenizer_folder, exist_ok=True)\n",
    "pre_tokenizer.save_pretrained(pretrained_tokenizer_folder)\n",
    "\n",
    "\n",
    "# PretrainedTokenizer 테스트\n",
    "# 저장된 PretrainedTokenizer 불러옴\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_folder, do_lower_case=False)\n",
    "text = \"여기는 BERT와 같은 인공지능 모델을 구현하고, simulate 하는 곳입니다.\"\n",
    "out_encode = tokenizer.encode(text)\n",
    "print(out_encode)\n",
    "\n",
    "for idx in out_encode:\n",
    "    print(f'{tokenizer.convert_ids_to_tokens(idx)}:{idx}')\n",
    "\n",
    "print(tokenizer.decode(out_encode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744874d4-7593-4893-80de-762ee067e4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
