{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc455fee-5827-4a53-af31-552e70ba2f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "device: cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# XLM-Roberta 신규 vocab 추가하기\n",
    "# - tokenizer.add_tokens(new_vocab) 처럼 add_tokens 함수를 이용함\n",
    "#   => add_tokens 함수를 이용하면 중복제거 로직 필요 없음.\n",
    "#\n",
    "# - XLM-Roberta는 GPT-2와 같은 ByteLevelTokenizer 이용함.\n",
    "# - 신규 vocab 추가하면 added_tokens.json에 추가됨.\n",
    "# - 신규 vocab 들은 sentencepiece 형식이어야 함. (즉 word=_ 추가, subword=그대로)\n",
    "# - sentencepiece 만드는 방법은 'tokenizer_sample/make_mecab_vocab.ipynb' 소스 참조\n",
    "\n",
    "##################################################################################\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, RobertaTokenizerFast, RobertaConfig, RobertaModel, RobertaForMaskedLM\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f95018-eb29-454a-a16e-f225ccbddd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*len:250002\n",
      "[0, 212233, 1180, 13968, 201539, 205473, 697, 74168, 6, 48637, 10068, 3659, 5769, 2]\n",
      "['<s>', '인공지능', '에서', '가장', '큰', '문제점', '은', '데이터', '', '쉬', '프', '트', '이다', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# 기존 tokenizer를 불러옴\n",
    "vocab_path = '../../../model/xml-roberta-base'\n",
    "#vocab_path = 'xlm-roberta-base'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(vocab_path)\n",
    "\n",
    "print(f'*len:{len(tokenizer)}')\n",
    "\n",
    "# tokenizer 테스트 \n",
    "sentence = '인공지능에서 가장큰 문제점은 데이터 쉬프트이다'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(output)\n",
    "\n",
    "decode_list=[]\n",
    "for out in output:\n",
    "    decode_list.append(tokenizer.decode(out))\n",
    "    \n",
    "print(decode_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d98ebe7-4206-43f2-b8c2-af9db6063260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "F:\\AnacondaEnv\\daEnv\\bong\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,                \n",
    "            add_special_tokens = True,\n",
    "            max_length = 128,     \n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,  \n",
    "            return_tensors = 'pt' # return pytorch tensors\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a437d6c-a945-463b-9b40-2eca0ae47424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0, 212233,   1180,  13968, 201539, 205473,    697,  74168,      6,\n",
       "          48637,  10068,   3659,   5769,      2,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9add684-6cef-427a-934c-b22fb53a88c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52789bbf44d341698d9cc4ca08a971f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*len:32001\n",
      "['▁투입', '▁번의', '▁소개', '▁동의']\n"
     ]
    }
   ],
   "source": [
    "# 신규 vocab 파일을 불러옴\n",
    "# => 신규 vocab 파일은 sentencepiece 방식에 vocab이어야 함\n",
    "# => tokenizer_sample/make_mecab_vocab.ipynb 소스 참조\n",
    "\n",
    "new_vocab = []\n",
    "new_vocab_path = '../../../korpora/moco-corpus-kowiki2022-nouns-32000-sp.txt'\n",
    "\n",
    "with open(new_vocab_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "for vocab in tqdm(data):\n",
    "     new_vocab.append(vocab)\n",
    "\n",
    "print(f'*len:{len(new_vocab)}')\n",
    "print(new_vocab[1111:1115])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93dc2671-cb52-4a20-8155-45e4b50287ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../../model/xml-roberta-base/moco-corpus-kowiki2022\\\\tokenizer_config.json',\n",
       " '../../../model/xml-roberta-base/moco-corpus-kowiki2022\\\\special_tokens_map.json',\n",
       " '../../../model/xml-roberta-base/moco-corpus-kowiki2022\\\\unigram.json',\n",
       " '../../../model/xml-roberta-base/moco-corpus-kowiki2022\\\\added_tokens.json',\n",
       " '../../../model/xml-roberta-base/moco-corpus-kowiki2022\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신규 tokenize 추가 \n",
    "#new_vocab = ['모코엠시스', '엠파워', '보안파일서버'] # 추가할 vocab 들\n",
    "new_tokenizer = tokenizer.add_tokens(new_vocab)\n",
    "\n",
    "# 신규 추가한 tokenzier를 저장함\n",
    "# => 저장후에는 해당 폴더에 added_tokenis.json 파일 생성됨.\n",
    "new_tokenizer_path = '../../../model/xml-roberta-base/moco-corpus-kowiki2022'\n",
    "os.makedirs(new_tokenizer_path, exist_ok=True)\n",
    "tokenizer.save_pretrained(new_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d22adfc-baf9-4731-839e-82b39c70d232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*len:278325\n",
      "[0, 276048, 6, 1180, 272437, 17626, 205473, 697, 252934, 87237, 277231, 31599, 5769, 2]\n",
      "['<s>', '인공지능', '', '에서', '가장', '큰', '문제점', '은', '데이', '터', '쉬프', '트', '이다', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# 추가한 tokenier 불러와봄.\n",
    "new_tokenizer_path = '../../../model/xml-roberta-base/moco-corpus-kowiki2022'\n",
    "new_tokenizer = RobertaTokenizerFast.from_pretrained(new_tokenizer_path, do_lower_case=False)\n",
    "\n",
    "print(f'*len:{len(new_tokenizer)}')\n",
    "\n",
    "# tokenizer 테스트 \n",
    "sentence = \"인공지능에서 가장큰 문제점은 데이터 쉬프트이다\"\n",
    "output = new_tokenizer.encode(sentence)\n",
    "print(output)\n",
    "\n",
    "decode_list=[]\n",
    "for out in output:\n",
    "    decode_list.append(new_tokenizer.decode(out))\n",
    "    \n",
    "print(decode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff314c79-4c75-4a66-bf41-3acd53d06ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "encoded_dict = new_tokenizer.encode_plus(\n",
    "            sentence,                \n",
    "            add_special_tokens = True,\n",
    "            max_length = 128,     \n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,  \n",
    "            return_tensors = 'pt' # return pytorch tensors\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c52c7b4-33ba-4337-ab59-e74e5a5239a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0, 276048,      6,   1180, 272437,  17626, 205473,    697, 252934,\n",
       "          87237, 277231,  31599,   5769,      2,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc7261-2488-40d7-978b-0675979a4d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
