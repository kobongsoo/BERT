{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfff2218-1752-4a3e-aab1-597732f1b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================\n",
    "# xlm-roberta-base 모델은 tokenizer가 unigram임.\n",
    "# => 여기서는 SentencePieceBPETokenizer 를 이용하여 word 추출하는 방법에 대해 설명함\n",
    "# => ** SentencePieceBPETokenizer를 이용하여 추출된 vocab들은 \n",
    "#    'tokenizer_sample/make_mecab_vocab.ipynb' 방식 보다 명사 추출이 떨어지는것 같음\n",
    "#\n",
    "# -출처: Unigram tokenzier 사용방법: https://towardsdatascience.com/training-bpe-wordpiece-and-unigram-tokenizers-from-scratch-using-hugging-face-3dd174850713\n",
    "# -출처 : mecab 사용한 헝태소 분석 : https://keep-steady.tistory.com/37\n",
    "# -출처 : mecab 윈도우 설치 방법 : https://uwgdqo.tistory.com/363\n",
    "#================================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 윈도우일때 사용 법\n",
    "# 설치방법은 아래 참조\n",
    "# 첨조 : https://uwgdqo.tistory.com/363\n",
    "#mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\")\n",
    "#mecab.pos('아버지가방에들어가신다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e32cac-4522-4e23-97bb-8abde8e69599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'To estimate the time waited for reads incurred by rereading data blocks that had tobe written to disk because of a request from another instance, multiply the statistic(for example, the time waited for db ﬁle sequential reads) by the percentage of readI/O caused by previous cache ﬂushes as shown in this formula:', 'Where \"lock buffers for read\" is the value for lock converts from N to S derived fromV$LOCK_ACTIVITY and \"physical reads\" is from the V$SYSSTAT view.']\n"
     ]
    }
   ],
   "source": [
    "# load korean corpus for tokenizer training\n",
    "corpus_path = '../../../korpora/kowiki/moco-corpus.txt'\n",
    "#corpus_path = '../../../korpora/bong_eval.txt'\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494f9b0f-f2ae-4223-ac8e-aee06c90370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab 형태소 분석==>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c55dd47c1494807a887481baee5c192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3291463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Refer', 'to', 'the', 'V', '$', 'SYSTEM', '_', 'EVENT', 'view', 'for', 'time', 'waited', 'and', 'average', 'waits', 'for', 'thefollowing', 'actions', ':'], ['To', 'estimate', 'the', 'time', 'waited', 'for', 'reads', 'incurred', 'by', 'rereading', 'data', 'blocks', 'that', 'had', 'tobe', 'written', 'to', 'disk', 'because', 'of', 'a', 'request', 'from', 'another', 'instance', ',', 'multiply', 'the', 'statistic', '(', 'for', 'example', ',', 'the', 'time', 'waited', 'for', 'db', 'ﬁ', 'le', 'sequential', 'reads', ')', 'by', 'the', 'percentage', 'of', 'readI', '/', 'O', 'caused', 'by', 'previous', 'cache', 'ﬂ', 'ushes', 'as', 'shown', 'in', 'this', 'formula', ':'], ['Where', '\"', 'lock', 'buffers', 'for', 'read', '\"', 'is', 'the', 'value', 'for', 'lock', 'converts', 'from', 'N', 'to', 'S', 'derived', 'fromV', '$', 'LOCK', '_', 'ACTIVITY', 'and', '\"', 'physical', 'reads', '\"', 'is', 'from', 'the', 'V', '$', 'SYSSTAT', 'view', '.']]\n",
      "3291463\n",
      "mecab 분석 데이터 저장==>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25afcaa332cc4683b8fb782bc9cdffd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3291463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#===================================================================\n",
    "# Mecab 선언\n",
    "# - 출처 : mecab 사용한 헝태소 분석 : https://keep-steady.tistory.com/37\n",
    "# - 출처 : mecab 윈도우 설치 방법 : https://uwgdqo.tistory.com/363\n",
    "#\n",
    "# 리눅스 일때\n",
    "#mecab_tokenizer = Mecab()\n",
    "# 윈도우 os 일때 \n",
    "mecab_tokenizer = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\")\n",
    "#===================================================================\n",
    "\n",
    "#'어릴때' -> '어릴, 때'   for normal case\n",
    "print(f'*mecab 형태소 분석==>')\n",
    "total_morph=[]\n",
    "for sentence in tqdm(data):\n",
    "    # 문장단위 mecab 적용(morphs = 명사)\n",
    "    morph_sentence = mecab_tokenizer.morphs(sentence)\n",
    "    # 문장단위 저장\n",
    "    total_morph.append(morph_sentence)\n",
    "                        \n",
    "print(total_morph[:3])\n",
    "print(len(total_morph))\n",
    "\n",
    "# mecab 적용한 데이터 저장\n",
    "print(f'*mecab 분석 데이터 저장==>')\n",
    "#mecab_corpus_path = '../../../korpora/kowiki/mecab-kowiki-202206-nlp-corpus.txt'\n",
    "mecab_corpus_path = '../../../korpora/kowiki/moco-corpus-mecab.txt'\n",
    "\n",
    "# ex) 1 line: '어릴 때 보 고 지금 다시 봐도 재밌 어요 ㅋㅋ'\n",
    "with open(mecab_corpus_path, 'w', encoding='utf-8') as f:\n",
    "    for line in tqdm(total_morph):\n",
    "        f.write(' '.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c5b9-d837-4ffd-a8af-f898279cb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SetnecePieceBPETokenzer 정의 후 훈련 \n",
    "# mecab 적용한 데이터 불러옴\n",
    "mecab_corpus_path = '../../../korpora/kowiki-202206-nlp-corpus-mecab.txt'\n",
    "\n",
    "stokenizer = SentencePieceBPETokenizer(add_prefix_space=True)\n",
    "\n",
    "# 훈련\n",
    "stokenizer.train(\n",
    "    files = [mecab_corpus_path],\n",
    "    vocab_size = 32000,  # 최대 vocab 계수 \n",
    "    special_tokens = [\"<cls>\", \"<eos>\", \"<mask>\", \"<unk>\", \"<pad>\"],  # speical token 지정\n",
    "    min_frequency = 100,   # 빈도수 \n",
    "    show_progress = True,\n",
    "    #limit_alphabet=10000, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf33b0-170c-49b2-a69b-9f8c5b5eb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = stokenizer.get_vocab()\n",
    "print(f'vcoab 길이:{len(vocab)}')\n",
    "sort_vocab = sorted(vocab, key=lambda x: vocab[x])\n",
    "print(sort_vocab[0:100])  # sort 해서 vocab 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c70463-8c93-4293-9cb8-7aa2453b3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab을 파일로 저장함\n",
    "from tqdm.notebook import tqdm\n",
    "vocab_out = '../../../korpora/kowiki-202206-nlp-corpus-mecab-vocab-32000.txt'\n",
    "with open(vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(sort_vocab):\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c6e37-79da-4d80-892a-932b3b6c5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 2. 훈련한 SetnecePieceBPETokenzer 를 PreTrainedTokenizerFast 와 연동\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "transforer_tokenizer = PreTrainedTokenizerFast(tokenizer_object=stokenizer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf02cd1-fbfd-478b-92c0-edd3bcfd7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# PreTrainedTokenizerFast tokenizer 저장\n",
    "import os\n",
    "OUT_PATH = '../../../korpora/kowiki-202206-nlp-corpus-vocab'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "transforer_tokenizer.save_pretrained(OUT_PATH)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
