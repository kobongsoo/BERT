#----------------------------------------------------------------------
# FastAPI ì´ìš©í•œ ì„ë² ë”© ì„œë²„ ì˜ˆì œ2
# - ì„¤ì¹˜ :pip install fastapi[all]
# - python ì—…ë°ì´íŠ¸(ì˜µì…˜) : conda install -c anaconda python=3.10 (3.10ì´ìƒ í•„ìš”)
# - ì‹¤í–‰ : uvicorn model1:app --host=0.0.0.0 --port=9000 --limit-concurrency=200
# - POST í…ŒìŠ¤íŠ¸ docs : IP/docs
# - ì¶œì²˜ : https://fastapi.tiangolo.com/ko/
# - elasticsearhëŠ” 7.17 ì„¤ì¹˜í•´ì•¼ í•¨. => pip install elasticsearch==7.17
#----------------------------------------------------------------------
import torch
import argparse
import time
import os
import platform
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm

# osê°€ ìœˆë„ìš°ë©´ from eunjeon import Mecab 
if platform.system() == 'Windows':
    os.environ["OMP_NUM_THREADS"] = '1' # ìœˆë„ìš° í™˜ê²½ì—ì„œëŠ” ì“°ë ˆë“œ 1ê°œë¡œ ì§€ì •í•¨

# FastAPI ê´€ë ¨    
import uvicorn
from enum import Enum
from typing import Union, Dict, List, Optional
from typing_extensions import Annotated
from fastapi import FastAPI, Query, Cookie, Form, Request, HTTPException, BackgroundTasks
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import asyncio
import threading
import httpx
    
# ES ê´€ë ¨
from elasticsearch import Elasticsearch, helpers
from elasticsearch.helpers import bulk

# myutils ê´€ë ¨
import sys
sys.path.append('..')
from myutils import bi_encoder, dense_model, onnx_model, onnx_embed_text
from myutils import seed_everything, GPU_info, mlogging, getListOfFiles, get_options
from myutils import remove_reverse, clean_text, make_max_query_script, make_avg_query_script, make_query_script, create_index, mpower_index_batch
from myutils import embed_text, clustering_embedding, kmedoids_clustering_embedding
from myutils import split_sentences1, make_docs_df, get_sentences, es_delete, es_delete_by_id, es_update, es_search

# FutureWarning ì œê±°
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning) 

import openai
from bardapi import Bard
from bardapi import BardCookies

#---------------------------------------------------------------------------
# ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ => í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í• ë•Œ global í•´ì¤˜ì•¼ í•¨.

# FastAPI ì„œë²„ ê´€ë ¨ 
SETTINGS_FILE = './data/settings.yaml'  # ì„¤ì •íŒŒì¼ ê²½ë¡œ (yaml íŒŒì¼)
#------------------------------------
# args ì²˜ë¦¬
#------------------------------------

# ì„¤ì •ê°’ settings.yaml íŒŒì¼ ë¡œë”©
settings = get_options(file_path=SETTINGS_FILE)
assert len(settings) > 2, f'load settings error!!=>len(settigs):{len(settings)}'
    
#------------------------------------
# param
#------------------------------------
logfilepath = settings['env']['LOG_PATH']
SEED = settings['env']['SEED']
DEVICE = settings['env']['GPU']
DATA_FOLDER = settings['env']['DATA_FOLDER']
ENV_URL = settings['env']['URL']
assert DEVICE=='auto' or DEVICE=='cpu', f'DEVICE setting error!!. DEVICE is auto or cpu=>DEVICE:{DEVICE}'
    
LOGGER = mlogging(loggername="embed-server", logfilename=logfilepath) # ë¡œê·¸
seed_everything(SEED)
if DEVICE == 'auto':
    DEVICE = GPU_info() # GPU í˜¹ì€ CPU
    
LOGGER.info(f'*í™˜ê²½ Settings: LOG_PATH:{logfilepath}, SEED:{SEED}, DEVICE:{DEVICE}')
    
# ëª¨ë¸ ì •ë³´ ë¡œë”©
MODEL_PATH = settings['model']['MODEL_PATH']  
POLLING_MODE = settings['model']['POLLING_MODE']   # í´ë§ëª¨ë“œ*(mean, cls, max ì¤‘ 1ë‚˜)
OUT_DIMENSION = settings['model']['OUT_DIMENSION'] # ì„ë² ë”© ëª¨ë¸ ì°¨ì› ìˆ˜ (128, 0=768)
if OUT_DIMENSION == 768:
    OUT_DIMENSION = 0
LOGGER.info(f'*ëª¨ë¸ Settings: MODEL_PATH:{MODEL_PATH}, POLLING_MODE:{POLLING_MODE}, OUT_DIMENSION:{OUT_DIMENSION}')
    
# ì„ë² ë”© ì •ë³´ ë¡œë”©
EMBEDDING_METHOD = settings['embedding']['EMBEDDING_METHOD'] # ì„ë² ë”© ë°©ì‹ (0=ë¬¸ì¥í´ëŸ¬ìŠ¤í„°ë§, 1=ë¬¸ì¥í‰ê· ì„ë² ë”©, 2=ë¬¸ì¥ì„ë² ë”©)
FLOAT_TYPE = settings['embedding']['FLOAT_TYPE'] # ì„ë² ë”© ë²¡í„° float íƒ€ì…('float32', 'float16')
LOGGER.info(f'*ì„ë² ë”© Settings: EMBEDDING_METHOD:{EMBEDDING_METHOD}, FLOAT_TYPE:{FLOAT_TYPE}')

# ES ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
ES_URL = settings['es']['ES_URL']
ES_INDEX_FILE = settings['es']['ES_INDEX_FILE'] # ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
Q_METHOD = settings['es']['Q_METHOD']     # ê²€ìƒ‰ì‹œ ES ìŠ¤í¬ë¦½íŠ¸ ì–´ë–¤í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì§€.(0=ì„ë² ë”©ì´ ì—¬ëŸ¬ê°œì¼ë•Œ MAX(ê¸°ë³¸), 1=ì„ë² ë”©ì´ ì—¬ëŸ¬ê°œì¼ë•Œ í‰ê· , 2=ì„ë² ë”©ì´1ê°œì¼ë•Œ)
BATCH_SIZE = settings['es']['BATCH_SIZE'] # ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 20ì´ë©´ 20ê°œì”© ESì— ì¸ë±ì‹±í•¨.
MIN_SCORE = settings['es']['MIN_SCORE']   # ê²€ìƒ‰ 1.30 ìŠ¤ì½”ì–´ ì´í•˜ë©´ ì œê±°

LOGGER.info(f'*ES Settings: ES_URL:{ES_URL}, Q_METHOD:{Q_METHOD}, ES_INDEX_FILE:{ES_INDEX_FILE}, BATCH_SIZE:{BATCH_SIZE}, MIN_SCORE:{MIN_SCORE}')

# í´ëŸ¬ìŠ¤í„°ë§ ì „ì—­ ë³€ìˆ˜
CLUSTRING_MODE = settings['custring']['CLUSTRING_MODE'] # "kmeans" = k-í‰ê·  êµ°ì§‘ ë¶„ì„, kmedoids =  k-ëŒ€í‘œê°’ êµ°ì§‘ ë¶„ì„
NUM_CLUSTERS = settings['custring']['NUM_CLUSTERS'] # í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜ 
OUTMODE = settings['custring']['OUTMODE']# í´ëŸ¬ìŠ¤í„°ë§í›„ ì¶œë ¥ë²¡í„° ì •ì˜(kmeans ì¼ë•Œ => mean=í‰ê· ë²¡í„° ì¶œë ¥, max=ìµœëŒ€ê°’ë²¡í„°ì¶œë ¥ / kmedoids ì¼ë•Œ=>mean=í‰ê· ë²¡í„°, medoid=ëŒ€í‘œê°’ë²¡í„°)
NUM_CLUSTERS_VARIABLE = settings['custring']['NUM_CLUSTERS_VARIABLE']# Trueì´ë©´ ë¬¸ì¥ê¸¸ì´ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ë§ìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ í•¨, Falseì´ë©´ í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜ê°€ ê³ ì •.
LOGGER.info(f'*í´ëŸ¬ìŠ¤í„°ë§ Settings: CLUSTRING_MODE:{CLUSTRING_MODE}, NUM_CLUSTERS:{NUM_CLUSTERS}, NUM_CLUSTERS_VARIABLE:{NUM_CLUSTERS_VARIABLE}, OUTMODE:{OUTMODE}')

# ë¬¸ì¥ ì „ì²˜ë¦¬
REMOVE_SENTENCE_LEN = settings['preprocessing']['REMOVE_SENTENCE_LEN'] # ë¬¸ì¥ ê¸¸ì´ê°€ 8ì´í•˜ë©´ ì œê±° 
REMOVE_DUPLICATION = settings['preprocessing']['REMOVE_DUPLICATION']# ì¤‘ë³µëœ ë¬¸ì¥ ì œê±°(*ì¤‘ë³µëœ ë¬¸ì¥ ì œê±° ì•ˆí• ë•Œ 1%ì •ë„ ì •í™•ë„ ì¢‹ìŒ)
LOGGER.info(f'*ë¬¸ì¥ì „ì²˜ë¦¬ Settings: REMOVE_SENTENCE_LEN:{REMOVE_SENTENCE_LEN}, REMOVE_DUPLICATION:{REMOVE_DUPLICATION}')

# ê²€ìƒ‰ ê´€ë ¨
# VECTOR_MAG = ES ë²¡í„° í¬ê¸° ê°’(ì„ì˜ì´ ê°’ì§€ì •) =>ë²¡í„°ì˜ í¬ê¸°ëŠ” ê° êµ¬ì„± ìš”ì†Œì˜ ì œê³± í•©ì˜ ì œê³±ê·¼ìœ¼ë¡œ ì •ì˜ëœë‹¤.. 
# ì˜ˆë¥¼ ë“¤ì–´, ë²¡í„° [1, 2, 3]ì˜ í¬ê¸°ëŠ” sqrt(1^2 + 2^2 + 3^2) ì¦‰, 3.7416ì´ ëœë‹¤.í´ìˆ˜ë¡ -> ìŠ¤ì½”ì–´ëŠ” ì‘ì•„ì§, ì‘ì„ìˆ˜ë¡ -> ìŠ¤ì½”ì–´ ì»¤ì§.
VECTOR_MAG = settings['search']['VECTOR_MAG']
LOGGER.info(f'*ê²€ìƒ‰ Settings: VECTOR_MAG:{VECTOR_MAG}')

# ì–´ë–¤ LLM ëª¨ë¸ì„ ì‚¬ìš©í• ì§€
LLM_MODEL = settings['llm_model']['model_type']
LOGGER.info(f'*llm_model_type: {LLM_MODEL}(0=sllm, 1=gpt, 2=bard)')

# í”„ë¡¬í”„íŠ¸
PROMPT_CONTEXT = settings['llm_model']['prompt']['prompt_context']  # context ê°€ ìˆì„ë•Œ(ê²€ìƒ‰ëœ ë‚´ìš©ì´ ìˆì„ë•Œ)
PROMPT_NO_CONTEXT = settings['llm_model']['prompt']['prompt_no_context']
LOGGER.info(f'*PROMPT_CONTEXT: {PROMPT_CONTEXT}, PROMPT_NO_CONTEXT: {PROMPT_NO_CONTEXT}')

# sLLM ëª¨ë¸
lora_weights = settings['llm_model']['sllm']['lora_weights']
llm_model_path = settings['llm_model']['sllm']['llm_model_path']
uselora_weight = settings['llm_model']['sllm']['uselora_weight']
load_8bit = settings['llm_model']['sllm']['load_8bit']
LOGGER.info(f'--0.*sllm Settings: lora_weights:{lora_weights}, llm_model_path:{llm_model_path}, uselora_weight:{uselora_weight}, load_8bit:{load_8bit}, load_8bit:{load_8bit}')

# BARD ê´€ë ¨
BARD_TOKEN = settings['llm_model']['bard']['BARD_TOKEN']
BARD_1PSIDTS_TOKEN = settings['llm_model']['bard']['BARD_1PSIDTS_TOKEN']
BARD_1PSIDCC_TOKEN = settings['llm_model']['bard']['BARD_1PSIDCC_TOKEN']
LOGGER.info(f'*--1.bard Settings: BARD_TOKEN:{BARD_TOKEN}, BARD_1PSIDTS_TOKEN:{BARD_1PSIDTS_TOKEN}, BARD_1PSIDCC_TOKEN:{BARD_1PSIDCC_TOKEN}')

# GPT ê´€ë ¨
GPT_TOKEN = settings['llm_model']['gpt']['GPT_TOKEN']
GPT_MODEL = settings['llm_model']['gpt']['GPT_MODEL']
LOGGER.info(f'*--2.bard Settings: GPT_MODEL:{GPT_MODEL}')
            
# GPT ê´€ë ¨ ê°’
# **key ì§€ì •
openai.api_key = GPT_TOKEN
# ëª¨ë¸ - GPT 3.5 Turbo ì§€ì •
# => ëª¨ë¸ ëª©ë¡ì€ : https://platform.openai.com/docs/models/gpt-4 ì°¸ì¡°
gpt_model = GPT_MODEL  #"gpt-4"#"gpt-3.5-turbo" #gpt-4-0314

#---------------------------------------------------------------------------

#---------------------------------------------------------------------------
# ì„ë² ë”© BERT ëª¨ë¸ ë¡œë”©
# => bi_encoder ëª¨ë¸ ë¡œë”©, polling_mode ì„¤ì •
# => bi_encoder1 = SentenceTransformer(bi_encoder_path) # ì˜¤íˆë ¤ ì„±ëŠ¥ ë–¨ì–´ì§. ì´ìœ ëŠ” do_lower_caseë‚˜, max_seq_lenë“± ì„¸ë¶€ ì„¤ì •ì´ ì•ˆë˜ë¯€ë¡œ.
#------------------------------------    
#BI_ENCODER1 = 0          # bi_encoder ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ 
#WORD_EMBDDING_MODEL1 = 0 # bi_encoder ì›Œë“œì„ë² ë”©ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

try:
    WORD_EMBDDING_MODEL1, BI_ENCODER1 = bi_encoder(model_path=MODEL_PATH, max_seq_len=512, do_lower_case=True, 
                                                   pooling_mode=POLLING_MODE, out_dimension=OUT_DIMENSION, device=DEVICE)
except Exception as e:
    LOGGER.error(f'bi_encoder load fail({MODEL_PATH})=>{e}')
    assert False, f'bi_encoder load fail({MODEL_PATH})=>{e}'
    
LOGGER.info(f'\n---bi_encoder---------------------------')
LOGGER.info(BI_ENCODER1)
LOGGER.info(WORD_EMBDDING_MODEL1)
LOGGER.info(f'\n----------------------------------------')
  
# ëª¨ë¸ ì €ì¥
#output_path = "../../data11/model/kpf-sbert-128d-v1"
#BI_ENCODER1.save(output_path)
#---------------------------------------------------------------------------

#---------------------------------------------------------------------------
# sLLM ëª¨ë¸ ë¡œë”©
#---------------------------------------------------------------------------
if llm_model_path:
    try:
        start_time = time.time()

        # tokenizer ë¡œë”©
        sllmtokenizer = transformers.AutoTokenizer.from_pretrained(llm_model_path)

        # ì›ë³¸ ëª¨ë¸ ë¡œë”©
        sllmmodel = transformers.AutoModelForCausalLM.from_pretrained(llm_model_path, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map="auto")

        if uselora_weight:
            sllmmodel = PeftModel.from_pretrained(sllmmodel, lora_weights, torch_dtype=torch.float16) # loRA ëª¨ë¸ ë¡œë”©

        if not load_8bit:
            sllmmodel.half()

        sllmmodel.eval()

        end_time = time.time() - start_time
        print("load sllm model time: {:.2f} ms\n".format(end_time * 1000)) 
        print(sllmmodel)
    except Exception as e:
        LOGGER.error(f'sllm load fail({llm_model_path})=>{e}')
        assert False, f'sllm load fail({llm_model_path})=>{e}'
        
#---------------------------------------------------------------------------
# ì„ë² ë”© ì²˜ë¦¬ í•¨ìˆ˜ 
# -in : paragrphs ë¬¸ë‹¨ ë¦¬ìŠ¤íŠ¸
#---------------------------------------------------------------------------
# ì¡°ê±´ì— ë§ê²Œ ì„ë² ë”© ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ 
def embedding(paragraphs:list)->list:
    # í•œ ë¬¸ë‹¨ì— ëŒ€í•œ 40ê°œ ë¬¸ì¥ ë°°ì—´ë“¤ì„ í•œêº¼ë²ˆì— ì„ë² ë”© ì²˜ë¦¬í•¨
    embeddings = embed_text(model=BI_ENCODER1, paragraphs=paragraphs, return_tensor=False).astype(FLOAT_TYPE)    
    return embeddings

#---------------------------------------------------------------------------
# ë¹„ë™ê¸° ì„ë² ë”© ì²˜ë¦¬ í•¨ìˆ˜
#---------------------------------------------------------------------------
async def async_embedding(paragraphs: list) -> list:
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, embedding, paragraphs)
#---------------------------------------------------------------------------
      
#---------------------------------------------------------------------------
#ë¬¸ë‹¨ì— ë¬¸ì¥ë“¤ì˜ ì„ë² ë”©ì„ êµ¬í•˜ì—¬ ê°ê° í´ëŸ¬ìŠ¤í„°ë§ ì²˜ë¦¬í•¨.
#---------------------------------------------------------------------------
def index_data(es, df_contexts, doc_sentences:list):
    #í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜ëŠ” ë¬¸ë‹¨ì˜ ê³„ìˆ˜ë³´ë‹¤ëŠ” ì»¤ì•¼ í•¨. 
    #assert num_clusters <= len(doc_sentences), f"num_clusters:{num_clusters} > len(doc_sentences):{len(doc_sentences)}"
    #-------------------------------------------------------------
    # ê° ë¬¸ë‹¨ì˜ ë¬¸ì¥ë“¤ì— ë²¡í„°ë¥¼ êµ¬í•˜ê³  ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•´ ë‘ .
    start = time.time()
    cluster_list = []

    rfile_names = df_contexts['contextid'].values.tolist()
    rfile_texts = df_contexts['question'].values.tolist()

    if OUT_DIMENSION == 0:
        dimension = 768
    else:
        dimension = 128

    clustering_num = NUM_CLUSTERS
        
    docs = []
    count = 0
    for i, sentences in enumerate(tqdm(doc_sentences)):
        embeddings = embedding(sentences)
        if i < 3:
            print(f'[{i}] sentences-------------------')
            if len(sentences) > 5:
                print(sentences[:5])
            else:
                print(sentences)
                
        LOGGER.info(f'*[index_data] embeddings.shape: {embeddings.shape}')
        print()
        
        #----------------------------------------------------------------
        multiple = 1
        
        # [bong][2023-04-28] ì„ë² ë”© ì¶œë ¥ ê³„ìˆ˜ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜ë¥¼ ë‹¬ë¦¬í•¨.
        if NUM_CLUSTERS_VARIABLE == True:
            embeddings_len = embeddings.shape[0]
            if embeddings_len > 2000:
                multiple = 6
            elif embeddings_len > 1000:
                multiple = 5 # 5ë°°
            elif embeddings_len > 600:
                multiple = 4 # 4ë°°
            elif embeddings_len > 300:
                multiple = 3 # 3ë°°
            elif embeddings_len > 100:
                multiple = 2 # 2ë°°
        #----------------------------------------------------------------
        
        # 0=ë¬¸ì¥í´ëŸ¬ìŠ¤í„°ë§ ì„ë² ë”©
        if EMBEDDING_METHOD == 0:
            if CLUSTRING_MODE == "kmeans":
                # ê° ë¬¸ë‹¨ì— ë¶„í• í•œ ë¬¸ì¥ë“¤ì˜ ì„ë² ë”© ê°’ì„ ì…ë ¥í•´ì„œ í´ëŸ¬ìŠ¤í„°ë§ í•˜ê³  í‰ê· ê°’ì„ êµ¬í•¨.
                # [bong][2023-04-28] ë¬¸ì¥ì´ ë§ì€ ê²½ìš°ì—ëŠ” í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜ë¥¼ 2,3ë°°ìˆ˜ë¡œ í•¨
                emb = clustering_embedding(embeddings = embeddings, outmode=OUTMODE, num_clusters=(clustering_num*multiple), seed=SEED).astype(FLOAT_TYPE) 
            else:
                emb = kmedoids_clustering_embedding(embeddings = embeddings, outmode=OUTMODE, num_clusters=(clustering_num*multiple), seed=SEED).astype(FLOAT_TYPE) 
            
        # 1= ë¬¸ì¥í‰ê· ì„ë² ë”©
        elif EMBEDDING_METHOD == 1:
            # ë¬¸ì¥ë“¤ì— ëŒ€í•´ ì„ë² ë”© ê°’ì„ êµ¬í•˜ê³  í‰ê·  êµ¬í•¨.
            arr = np.array(embeddings).astype(FLOAT_TYPE)
            emb = arr.mean(axis=0).reshape(1,-1) #(128,) ë°°ì—´ì„ (1,128) í˜•íƒœë¡œ ë§Œë“¤ê¸° ìœ„í•´ reshape í•´ì¤Œ
            clustering_num = 1  # í‰ê· ê°’ì¼ë•ŒëŠ” NUM_CLUSTERS=1ë¡œ í•´ì¤Œ.
        # 2=ë¬¸ì¥ì„ë² ë”©
        else:
            emb = embeddings

        LOGGER.info(f'*[index_data] cluster emb.shape: {emb.shape}')
        print()
        
        #--------------------------------------------------- 
        # docsì— ì €ì¥ 
        #  [bong][2023-04-28] ì—¬ëŸ¬ê°œ ë²¡í„°ì¸ ê²½ìš°ì—ëŠ” ë²¡í„°ë¥¼ 10ê°œì”© ë¶„ë¦¬í•´ì„œ ì—¬ëŸ¬ê°œ docsë¥¼ ë§Œë“¬.
        for j in range(multiple):
            count += 1
            doc = {}                                #dict ì„ ì–¸
            doc['rfile_name'] = rfile_names[i]      # contextid ë‹´ìŒ
            doc['rfile_text'] = rfile_texts[i]      # text ë‹´ìŒ.
            doc['dense_vectors'] = emb[j * clustering_num : (j+1) * clustering_num] # emb ë‹´ìŒ.
            docs.append(doc)
        #---------------------------------------------------    

            if count % BATCH_SIZE == 0:
                mpower_index_batch(es, ES_INDEX_NAME, docs, vector_len=clustering_num, dim_size=dimension)
                docs = []
                LOGGER.info("[index_data](1) Indexed {} documents.".format(count))

    if docs:
        mpower_index_batch(es, ES_INDEX_NAME, docs, vector_len=clustering_num, dim_size=dimension)
        LOGGER.info("[index_data](2) Indexed {} documents.".format(count))   

    es.indices.refresh(index=ES_INDEX_NAME)

    LOGGER.info(f'*ì¸ë±ì‹± ì‹œê°„ : {time.time()-start:.4f}\n')
    print()
#---------------------------------------------------------------------------

#---------------------------------------------------------------------------
# ES ì„ë² ë”© ë²¡í„° ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
# - in : esindex=ì¸ë±ìŠ¤ëª…, query=ì¿¼ë¦¬ , search_size=ê²€ìƒ‰ì¶œë ¥ê³„ìˆ˜
# - option: qmethod=0 í˜¹ì€ 1 í˜¹ì€ 2(0=maxë²¡í„° êµ¬í•˜ê¸°, 1=í‰ê· ë²¡í„° êµ¬í•˜ê¸°, 2=ì„ë² ë”©ë²¡í„°ê°€ 1ê°œì¸ ê²½ìš° (default=0)), uid_list=ê²€ìƒ‰í•  uid ë¦¬ìŠ¤íŠ¸(*ì— íŒŒì›Œì—ì„œëŠ” ê²€ìƒ‰í•  ë¬¸ì„œidë¥¼ ì§€ì •í•´ì„œ ê²€ìƒ‰í•´ì•¼ ê²€ìƒ‰ì†ë„ê°€ ëŠë¦¬ì§€ ì•ŠìŒ)
#---------------------------------------------------------------------------
def es_embed_query(esindex:str, query:str, search_size:int, qmethod:int=0, uids:list=None):
    
    error: str = 'success'
    
    query = query.strip()
    
    #print(f'search_size: {search_size}')
    
    # 1.elasticsearch ì ‘ì†
    es = Elasticsearch(ES_URL)   
    
    if not query:
        error = 'query is empty'
    elif search_size < 1:
        error = 'search_size < 1'
    elif not es.indices.exists(esindex):
         error = 'esindex is not exist'
    elif qmethod < 0 or qmethod > 2:
        error = 'qmenthod is not variable'
    
    if error != 'success':
        LOGGER.error(f'[es_embed_query] {error}')
        return error, None
        
    #time.sleep(20)
    
    # LOGGER.info(f'es.info:{es.info()}')

    # 2. ê²€ìƒ‰ ë¬¸ì¥ embedding í›„ ë²¡í„°ê°’ 
    # ì¿¼ë¦¬ë“¤ì— ëŒ€í•´ ì„ë² ë”© ê°’ êµ¬í•¨
    start_embedding_time = time.time()
    embed_query = embedding([query])
    end_embedding_time = time.time() - start_embedding_time
    print("*embedding time: {:.2f} ms".format(end_embedding_time * 1000)) 
    print(f'*embed_querys.shape:{embed_query.shape}\n')

    # 3. ì¿¼ë¦¬ ë§Œë“¬
    # - ì¿¼ë¦¬ 1ê°œë§Œ í•˜ë¯€ë¡œ, embed_query[0]ìœ¼ë¡œ ì…ë ¥í•¨.
    if qmethod == 0:
        script_query = make_max_query_script(query_vector=embed_query[0], vectormag=VECTOR_MAG, vectornum=10, uid_list=uids) # max ì¿¼ë¦¬ë¥¼ ë§Œë“¬.
    elif qmethod == 1:
        script_query = make_avg_query_script(query_vector=embed_query[0], vectormag=VECTOR_MAG, vectornum=10, uid_list=uids) # í‰ê·  ì¿¼ë¦¬ë¥¼ ë§Œë“¬.
    else:
        script_query = make_query_script(query_vector=embed_query[0], uid_list=uids) # ì„ë² ë”© ë²¡í„°ê°€ 1ê°œì¸ê²½ìš°=>ê¸°ë³¸ ì¿¼ë¦¬ ë§Œë“¬.
        
    #print(script_query)
    #print()

    # 4. ì‹¤ì œ ESë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ë‚ ë¦¼
    response = es.search(
        index=esindex,
        body={
            #"size": search_size * 3, # 3ë°° ì •ë„ ì–»ì–´ì˜´
            "size": search_size,
            "query": script_query,
            "_source":{"includes": ["rfile_name","rfile_text"]}
        }
    )
    
    #LOGGER.info(f'[es_embed_query] response:{response}')

    # 5. ê²°ê³¼ ë¦¬í„´
    # - ì¿¼ë¦¬ ì‘ë‹µ ê²°ê³¼ê°’ì—ì„œ _id, _score, _source ë“±ì„ ë½‘ì•„ë‚´ê³  ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í›„ ê²°ê³¼ê°’ ë¦¬í„´
    #print(response)
    
    rfilename = []
    count = 0
    docs = []
    for hit in response["hits"]["hits"]: 
        tmp = hit["_source"]["rfile_name"]
        
        # ì¤‘ë³µ ì œê±°
        if tmp and tmp not in rfilename:
            rfilename.append(tmp)
            doc = {}  #dict ì„ ì–¸
            doc['rfile_name'] = hit["_source"]["rfile_name"]      # contextid ë‹´ìŒ
            doc['rfile_text'] = hit["_source"]["rfile_text"]      # text ë‹´ìŒ.
            doc['score'] = hit["_score"]
            docs.append(doc)
            
            count += 1
            if count >= search_size:
                break
                
    LOGGER.info(f'[es_embed_query] query:{query} docs:{docs}')

    return error, docs # ì¿¼ë¦¬,  rfilename, rfiletext, ìŠ¤ì½”ì–´ ë¦¬í„´ 

#---------------------------------------------------------------------------
# ë¹„ë™ê¸° ES ì„ë² ë”© ë²¡í„° ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
#---------------------------------------------------------------------------
async def async_es_embed_query(esindex:str, query:str, search_size:int, qmethod:int, uids:list=None):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, es_embed_query, esindex, query, search_size, qmethod, uids)
#---------------------------------------------------------------------------

#---------------------------------------------------------------------------
# ES ì„ë² ë”© ë„íë¨¼íŠ¸ ì‚­ì œ í•¨ìˆ˜
# -> ì‚­ì œí•  _idë¥¼ ì–»ì–´ì™€ì„œ ì‚­ì œí•¨.
# - in : esindex=ì¸ë±ìŠ¤ëª…, rfile_name=íŒŒì¼ ìœ ë‹ˆí¬í•œ ê°’(sfileid, contxtsid) 
#---------------------------------------------------------------------------
def es_embed_delete(esindex:str, rfile_name:str):
    
    error: int = 0
    
    # 1.elasticsearch ì ‘ì†
    es = Elasticsearch(ES_URL)   
        
    data = {'rfile_name': rfile_name}
    
    # 2. ì¿¼ë¦¬ ê²€ìƒ‰í›„ _id ì–»ì–´ì˜´.
    id_list = []
    res=es_search(es, index_name=esindex, data=data)
    for hits in res['hits']['hits']:
        esid=hits['_id']
        #print(f'id:{esid}')
        id_list.append(esid)
        
    # 3. ì‚­ì œí•¨.
    if len(id_list) > 0:  
        for id in id_list:
            print(f'id:{id}')
            res=es_delete_by_id(es, index_name=esindex, id=id)
            print(f'res:{res}')
            
        return 0
    else:
        return 1002
#---------------------------------------------------------------------------
#---------------------------------------------------------------------------
# ë¹„ë™ê¸° ES ì„ë² ë”© ë„íë¨¼íŠ¸ ì‚­ì œ í•¨ìˆ˜
#---------------------------------------------------------------------------
async def async_es_embed_delete(esindex:str, rfile_name:str):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, es_embed_delete, esindex, rfile_name)
#---------------------------------------------------------------------------

#------------------------------------------------------------------------
# PROMPT ìƒì„±
#------------------------------------------------------------------------
def make_prompt(docs, query):
     # prompt êµ¬ì„±
    context:str = ''

    for doc in docs:
        score = doc['score']
        if score > MIN_SCORE:
            rfile_text = doc['rfile_text']
            if rfile_text:
                context += rfile_text + '\n\n'
                
    if context:
        prompt = PROMPT_CONTEXT.format(query=query, context=context)
    else:
        prompt = PROMPT_NO_CONTEXT.format(query=query)
        '''
        if LLM_MODEL == 0:  #SLLM ëª¨ë¸ì¼ë•Œ 
            prompt = PROMPT_NO_CONTEXT.format(query=query)
        else:   # GPT í˜¹ì€ BARDì¸ ê²½ìš°, contextê°€ ì—†ìœ¼ë©´  í”„ë¡¬í”„íŠ¸ëŠ” ì¿¼ë¦¬ë§Œ ìƒì„±í•¨.
            prompt = query
        '''  
    return prompt, context
#------------------------------------------------------------------------

#---------------------------------------------------------------------------
# ë¹„ë™ê¸° PROMPT ìƒì„±
#---------------------------------------------------------------------------
async def async_make_prompt(docs, query:str):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, make_prompt, docs, query)
#---------------------------------------------------------------------------

#------------------------------------------------------------------------
# sLLM ì´ìš©í•œ text ìƒì„±
#------------------------------------------------------------------------
def generate_text_sLLM(prompt):
    
    max_new_tokens = 256
    eos_str = sllmtokenizer.decode(sllmtokenizer.eos_token_id)
    start_time = time.time()
    #print(f'eos_str:{eos_str}')
    
    #prompt = query
    #prompt = f"### ì§ˆë¬¸: {input_text}\n\n### ë§¥ë½: {context}\n\n### ë‹µë³€:" if context else f"### ì§ˆë¬¸: {input_text}\n\n### ë‹µë³€:"
    #prompt = f"### ì§ˆë¬¸ : ê°„ëµíˆ ë‹µë³€í•´ì¤˜.{query}\r\n###ë‹µë³€:"
    #prompt = f"### ì§ˆë¬¸: {query}\n\n### ë‹µë³€:"
    #print(prompt)

    # config ì„¤ì •
    generation_config = GenerationConfig(
        temperature=0.5,
        top_p=0.75,
        top_k=40,
        num_beams=1,
        bos_token_id=sllmtokenizer.bos_token_id,  # ì‹œì‘í† í° 
        eos_token_id=sllmtokenizer.eos_token_id,  # end í† í°
        pad_token_id=sllmtokenizer.pad_token_id   # padding í† í°
    )

    # í”„ë¡¬í”„íŠ¸ tokenizer 
    inputs = sllmtokenizer(prompt, return_tensors="pt").to(DEVICE)
    input_ids = inputs["input_ids"]
    #print(input_ids)
       
    # Without streaming
    # generate ì²˜ë¦¬
    with torch.no_grad():
        generation_output = sllmmodel.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=False,
            max_new_tokens=max_new_tokens,
        )

    # ì¶œë ¥
    s = generation_output.sequences[0]
    output = sllmtokenizer.decode(s)

    end_time = time.time() - start_time
    print("*Textìƒì„±ì‹œê°„: {:.2f} ms\n".format(end_time * 1000)) 
    #print(output.replace(eos_str, ''))
    #print()
    return output.replace(eos_str, '')
#------------------------------------------------------------------------

#------------------------------------------------------------------
# êµ¬ê¸€ bardë¥¼ ì´ìš©í•œ text ìƒì„±
#
# ì„¸ì…˜í‚¤ë¥¼ ì´ìš©í•˜ì—¬ êµ¬ê¸€ bard í…ŒìŠ¤íŠ¸ ì˜ˆì œ
# ì¶œì²˜ : https://github.com/dsdanielpark/Bard-API
#
# token ê°’ì–»ê¸°
# https://bard.google.com/ ë°©ë¬¸
# ì½˜ì†”ìš© F12
# ì„¸ì…˜: ì• í”Œë¦¬ì¼€ì´ì…˜ â†’ ì¿ í‚¤ â†’ ì¿ í‚¤ ê°’ì„ ë³µì‚¬í•©ë‹ˆë‹¤ __Secure-1PSID.
# -> ì°¸ê³ ë¡œ ë°˜ë“œì‹œ ë’¤ì— .ìœ¼ë¡œ ëë‚˜ê³  .í¬í•¨í•´ì„œ ê¸¸ì´ê°€ 72ìì„.
#------------------------------------------------------------------
from bardapi import BardCookies
#token = 'XQhPmzE3Wa_GqgDH1Z9YcRwZieE0STZi0ANZ557Zcm9Lio8QeIQtQvdd8evImbUrF-ZapQ.' # bard __Secure-1PSID í† í° ì…ë ¥
#token1 = 'XQhPmzE3Wa_GqgDH1Z9YcRwZieE0STZi0ANZ557Zcm9Lio8QeIQtQvdd8evImbUrF-ZapQ.' # bard __Secure-1PSIDTS í† í° ì…ë ¥
def generate_text_bard(prompt:str, token:str, token1:str=None, token2:str=None):
    #print(f'[generate_text_bard] prompt: {prompt}')
    print(f'[generate_text_bard] token: {token}, token1: {token1}, token2:{token2}')
   
    assert token, f'token is not empty'
    
    token = token.strip()
    
    # __Secure-1PSID í† í°ë§Œ ì´ìš©í•˜ëŠ” ê²½ìš° "SNlM0e value not found. Double-check __Secure-1PSID value or pass it as token='xxxxx'" ì—ëŸ¬ê°€ ìì£¼ ë°œìƒí•˜ì—¬,
    # __Secure-1PSIDTS í•¨ê»˜ ì´ìš©í•˜ëŠ” ë©€í‹°í† í° ë°©ì‹ìœ¼ë¡œ í•¨. (ì¶œì²˜ : https://github.com/dsdanielpark/Bard-API/issues/99)
    if token1 and token2:
        token1 = token1.strip()
        token2 = token2.strip()
        cookie_dict = {
            "__Secure-1PSID": token,
            "__Secure-1PSIDTS": token1,
            "__Secure-1PSIDCC": token2,
            # Any cookie values you want to pass session object.
        }
        
        bard = BardCookies(cookie_dict=cookie_dict)
    else:
        bard = Bard(token=token,timeout=30) # Set timeout in seconds
        
    output = bard.get_answer(prompt)['content']
    
    #print(f'[generate_text_bard] output: {output}')
    return output
#------------------------------------------------------------------

#-----------------------------------------
# GPTë¥¼ ì´ìš©í•œ text ìƒì„±
#-----------------------------------------
MESSAGES:list = []
def generate_text_GPT(prompt, messages):
    
    #print(f'len(messages):{len(messages)}') 
    #print()
    
    #-----------------------------------------
    # *** gptì— ë©”ì‹œì§€ëŠ” ê³„ì† ëŒ€í™” ë‚´ìš©ì´ ìœ ì§€ê°€ ë˜ë¯€ë¡œ, ë¹„ìš©ì´ ë°œìƒí•¨.
    # ë”°ë¼ì„œ ìµœê·¼ 2ê°œ ëŒ€í™”ë§Œ ìœ ì§€í•¨.
    #if len(messages) >= 2:
    #    messages = messages[len(messages)-2:]  # ìµœê·¼ 2ê°œì˜ ëŒ€í™”ë§Œ ê°€ì ¸ì˜¤ê¸°
    messages = []  # ë¬´ì¡°ê±´ ìµœê·¼ëŒ€í™” ì´ˆê¸°í™”
    #-----------------------------------------
        
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.append( {"role": "user", "content": prompt})
    print(messages)

    # ChatGPT-API í˜¸ì¶œí•˜ê¸°
    response = openai.ChatCompletion.create(
        model=gpt_model,
        messages=messages,
        max_tokens=512, # í† í° ìˆ˜ 
        temperature=1,  # temperature 0~2 ë²”ìœ„ : ì‘ì„ìˆ˜ë¡ ì •í˜•í™”ëœ ë‹µë³€, í´ìˆ˜ë¡ ìœ ì—°í•œ ë‹µë³€(2ëŠ” ì—‰ëš±í•œ ë‹µë³€ì„ í•˜ë¯€ë¡œ, 1.5ì •ë„ê°€ ì¢‹ì€ê²ƒ ê°™ìŒ=ê¸°ë³¸ê°’ì€=1)
        top_p=0.1, # ê¸°ë³¸ê°’ì€ 1 (0.1ì´ë¼ê³  í•˜ë©´ 10% í† í°ë“¤ì—ì„œ ì¶œë ¥ í† í°ë“¤ì„ ì„ íƒí•œë‹¤ëŠ” ì˜ë¯¸)
        frequency_penalty=0.5, # ì¼ë°˜ì ìœ¼ë¡œ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ë‹¨ì–´ë¥¼ ì–µì œí•˜ëŠ” ì •ë„
        presence_penalty=0.5 # ë™ì¼í•œ ë‹¨ì–´ë‚˜ êµ¬ë¬¸ì´ ë°˜ë³µë˜ëŠ” ê²ƒì„ ì–µì œí•˜ëŠ” ì •ë„
        #stop=["ë‹¤.","ë‹¤!"] # . ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
    )

    #print(response)
    #print()
    #answer = response['choices'][0]['message']['content'] + 'ë‹¤.' # ë’¤ì— 'ë‹¤' ë¶™ì—¬ì¤Œ.
    answer = response['choices'][0]['message']['content']
    return answer
#------------------------------------------------------------------

#---------------------------------------------------------------------------
# ì´ì „ ë‹µë³€/ì‘ë‹µ ë¬¸ì¥ë“¤ì¤‘ ì˜¤ë˜ëœê²ƒ ì œê±°í•˜ë©°, ë¬¸ì¥ì„ êµ¬ë¶„ì <hr> ë¡œ êµ¬ë¶„í•´ì„œ ë‹µë³€/ì‘ë‹µ ë¬¸ë‹¨ì„ ë§Œë“ ëŠ” í•¨ìˆ˜
#---------------------------------------------------------------------------
def remove_prequery(prequery:str, remove_count:int=4):
    
    if prequery:
        # prequeryëŠ” 5ê°œ ì´ìƒì´ë©´ ë¬´ì¡°ê±´ ì˜¤ë˜ëœ ë¬¸ì¥/ë‹µë³€ì€ ì œê±°í•¨
        hr_count = prequery.count("<hr>")
        #print(f'4) hr_count:{hr_count}')

        remove_count -= 1
        if hr_count > remove_count: # 4 ê°œ ì´ìƒì´ë©´ <hr>ë¡œ êµ¬ë¶„í•´ì„œ ì˜¤ë˜ëœ ë¬¸ì¥/ë‹µë³€ì€ ì œê±°í•¨.
            hr_list = prequery.split("<hr>") # <hr>ë¡œ êµ¬ë¶„
            hr_list.pop(0)                   # ì œì¼ ì˜¤ë˜ëœ <hr> êµ¬ë¶„í•´ì„œ ì²«ë²ˆì§¸ ë¬¸ì¥/ë‹µë³€ì€ ì œê±°.
            prequery = "<hr>".join(hr_list)  # ë‹¤ì‹œ hr êµ¬ë¶„ëœ ë¬¸ì¥/ë‹µë³€ ì¡°í•©.
            #print(f'5) prequery:{prequery}')
    
    return prequery
#---------------------------------------------------------------------------

#---------------------------------------------------------------------------
# context ë¬¸ìì—´ì„ ì…ë ¥ë°›ì•„ì„œ,\n\n ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ë¶„í›„, ë¬¸ë‹¨ ë§¨ ì²«ë²ˆì§¸ ë¬¸ì¥ titleë“¤ì„
# ì¡°í•©í•´ì„œ title_str ë§Œë“¤ê³  ,return í•˜ëŠ” í•¨ìˆ˜
# -> titleì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ìˆìœ¼ë©´ urlë§í¬ ìƒì„±í•¨
#---------------------------------------------------------------------------
def get_title_with_urllink(context:str):
    
    titles_str:str = ''  # titlesë¥¼ strí˜•ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì „ì†¡í•¨. 
    
    # contextì—ì„œ titleë§Œ ë½‘ì•„ëƒ„
    titles = []
    context_list = context.split("\n\n")  #\n\nìœ¼ë¡œ êµ¬ë¶„.
    for context1 in context_list:
        context1 = context1.strip()
        context2 = context1.split("\n")
        if len(context2) > 0:
            titles.append(context2[0].strip())
    
    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ìˆœì„œìœ ì§€.
    titles2 = []
   
    for idx, title in enumerate(titles):
        if title and title not in titles2:
            titles2.append(title)
            
            # ì‹¤ì œ titleì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì´ ê²½ë¡œì— ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ url ë§í¬ ìƒì„±í•¨.
            if os.path.isfile(DATA_FOLDER + title + ".txt"):
                
                # htmlë¿Œë¦´ë•Œ ì¤‘ê°„ì— ìŒë”°ì˜´í‘œê°€ ìˆìœ¼ë©´ ì—ëŸ¬ ë‚˜ë¯€ë¡œ, "(ìŒë”°ì˜´í‘œ) ëŒ€ì‹ ì— ;s&s; ë¡œ ì¹˜í™˜í•´ì„œ ì „ì†¡í•¨. 
                # => ì´í›„ chat01.htmlì—ì„œ ;s&s; ë¬¸ìì—´ì„ ë‹¤ì‹œ "(ìŒë”°ì˜´í‘œ)ë¡œ ì¹˜í™˜í•´ì¤Œ.
                # => ì°¸ê³ ë¡œ " ëŒ€ì‹ ì— í™‘ë”°ì˜´í‘œ(') í•´ë„ ë˜ëŠ”ë°, openPopup í•¨ìˆ˜ëŠ” ë°˜ë“œì‹œ "(ìŒë”°ì˜´í‘œ)ë¡œ ë¬¶ì–´ì ¸ì•¼ ë™ì‘í•˜ë¯€ë¡œ ì´ë ‡ê²Œ ì²˜ë¦¬í•¨.
                title = f"<a href='javascript:void(0);' onclick=;s&s;;openPopup('{ENV_URL}/doc?name={title}');;s&s;>{title}</a>"
                #title = f"<a href='/doc?name={title}'>{title}</a>"
                
            if idx == 0:
                titles_str = title
            else:
                titles_str += ', ' + title
                
    return titles_str
#---------------------------------------------------------------------------

#------------------------------------------------------------------
# BERTë¡œ ë¬¸ë‹¨ ê²€ìƒ‰ í›„ sLLM ë¡œ Text ìƒì„±.
#------------------------------------------------------------------
def search_docs(esindex:str, query:str, search_size:int, llm_model_type:int=0, model_key:str='', model_key1:str='', model_key2:str='', qmethod:int=0, checkdocs:bool=True):
    error:str = 'success'
    
    assert query, f'query is empty'
    assert esindex, f'esindex is empty'
    
    query = query.strip()
    esindex = esindex.strip()
    #print(f'model_key:{model_key}')
    
    if model_key:
        model_key = model_key.strip()
        #print(f'model_key:{model_key}')
    
    LOGGER.info(f'[search_docs] esindex:{esindex}, query:{query}, search_size:{search_size}, llm_model_type:{llm_model_type}, model_key:{model_key}')
    
    query_split = query.split('##')
    prefix = query_split[0]  
    docs = []
    response:str = 'ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”'
    embed_context:str = ''
    bllm_model_query = True # Trueì´ë©´ llm_model ì¿¼ë¦¬í•¨.
    
    if checkdocs == False: # íšŒì‚¬ë¬¸ì„œê²€ìƒ‰ ì²´í¬í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ëƒ¥ ì¿¼ë¦¬ ê·¸ëŒ€ë¡œ prompt ì„¤ì •í•¨.
        query1=query
        #prompt=query1
        prompt, embed_context = make_prompt(docs='', query=query1)   
    elif prefix == '@':  # ì¼ë°˜ì¿¼ë¦¬ì¼ë•ŒëŠ” @## prefix ì…ë ¥í›„ ì§ˆë¬¸ì…ë ¥í•¨. 
        query1 = query_split[1]
        #prompt=query1
        prompt, embed_context = make_prompt(docs='', query=query1)   
        LOGGER.error(f'[search_docs]: prefix: {prefix}\n')
    else:
        query1 = query
        
        # esë¡œ ì„ë² ë”© ì¿¼ë¦¬ ì‹¤í–‰
        try:
            error, docs = es_embed_query(esindex, query1, search_size, qmethod)
        except Exception as e:
            error = f'es_embed_query fail'
            msg = f'{error}=>{e}'
            LOGGER.error(f'[search_docs]: {msg}\n')
            raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
         
        # ê²€ìƒ‰ëœ ë¬¸ë‹¨ë“¤ ì¶œë ¥
        print(docs)
        print()
        
        # prompt ìƒì„±    
        prompt, embed_context = make_prompt(docs=docs, query=query1)
        if len(embed_context) < 2:
            bllm_model_query = False
            
    LOGGER.info(f'[search_docs] prompt:{prompt}, bllm_model_query:{bllm_model_query}')
  
    # llm_model_query == Trueì¼ë•Œë§Œ ì¿¼ë¦¬í•¨.
    if bllm_model_query == True:
        # sllMìœ¼ë¡œ text ìƒì„±
        try:
            if llm_model_type == 0:
                response = generate_text_sLLM(prompt=prompt)
            elif llm_model_type == 1:
                response = generate_text_GPT(prompt=prompt, messages=MESSAGES)
            elif llm_model_type == 2: # bard ì¼ë•Œ
                response = generate_text_bard(prompt=prompt, token=model_key, token1=model_key1, token2=model_key2)
        except Exception as e:
            error = f'generate_text_xxx fail=>model:{llm_model_type}'
            msg = f'{error}=>{e}'
            LOGGER.error(f'[search_docs]: {msg}\n')
            raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)

        if error != 'success':
            raise HTTPException(status_code=404, detail=error, headers={"X-Error": error},)
     
    # sllmëª¨ë¸ì¼ë•Œ
    if llm_model_type == 0:
        # ì‘ë‹µì„ íŒŒì‹±í•´ì„œ ì‘ë‹µë§Œ ë½‘ì•„ë‚´ì„œ return í•¨.
        answer:str = "ë‹µë³€ ì—†ìŒ."
        answers:list = []
        questions:list = []
        contexts:list = []
        context:str = ""

        #print(f'response')
        #print(response)

        # í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì‘ë‹µ, ì§ˆì˜ ,ë¬¸ë‹¨ìœ¼ë¡œ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜.
        if response:
            answers = response.split("###ì‘ë‹µ:")
            #print(answer[0])
            #print()
            questions = answers[0].split("###ì§ˆì˜:")
            #print(questions[0])
            #print()
            contexts = questions[0].split("###ë¬¸ì„œ:")
            #print(contexts[1])
            #print()

            if len(answers) > 1:
                answer = answers[1].strip()

            if len(contexts) > 1:
                context = contexts[1].strip()

            query = query1
            
            if context == '':
                context = '**ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íšŒì‚¬ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.**'
                
            LOGGER.info(f'[search_docs] answer:{answer}')
  
        return query, answer, context
           
    # gpt í˜¹ì€ bardì¼ë•Œ
    if llm_model_type == 1 or llm_model_type == 2:
        query = query1
        answer = response
        context:str = ''
        
        '''
        if checkdocs == True:
            if len(docs) > 0:
                for doc in docs:
                    score = doc['score']
                    if score > MIN_SCORE:
                        rfile_text = doc['rfile_text']
                        if rfile_text:
                            context += rfile_text + '\n\n'
        '''
        LOGGER.info(f'[search_docs] answer:{answer}')
        return query, answer, embed_context
    
#---------------------------------------------------------------------------

#---------------------------------------------------------------------------
# ë¹„ë™ê¸° BERTë¡œ ë¬¸ë‹¨ ê²€ìƒ‰ í›„ LLMëª¨ë¸ ë¡œ Text ìƒì„±.
#---------------------------------------------------------------------------
async def async_search_docs(esindex:str, query:str, search_size:int, llm_model_type:int=0, model_key:str='', model_key1:str='', model_key2:str='', qmethod:int=0, checkdocs:bool=True):
    loop = asyncio.get_running_loop()
    #print(f'[async_search_docs] esindex :{esindex}')
    ##print(f'[async_search_docs] query :{query}')
    #print(f'[async_search_docs] llm_model_type :{llm_model_type}')
    #print(f'[async_search_docs] model_key :{model_key}')
    
    return await loop.run_in_executor(None, search_docs, esindex, query, search_size, llm_model_type, model_key, model_key1, model_key2, qmethod, checkdocs)

    
# http://10.10.4.10:9000/docs=>swagger UI, http://10.10.4.10:9000/redoc=>ReDoc UI ê°ê° ë¹„í™œì„±í™” í•˜ë ¤ë©´
# => docs_url=None, redoc_url=None í•˜ë©´ ëœë‹¤.
#app = FastAPI(redoc_url=None) #FastAPI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±(*redoc UI ë¹„í™œì„±í™”)
app = FastAPI()
templates = Jinja2Templates(directory="templates") # html íŒŒì¼ì´ ìˆëŠ” ê²½ë¡œë¥¼ ì§€ì •.

#=========================================================
# ì¹´ì¹´ì˜¤ ì³‡ë´‡ ì—°ë™ í…ŒìŠ¤íŠ¸ 
# - ì„ë² ë”© ë¹„êµí•˜ì—¬ ê°€ì¥ ì í•©í•œ ë¬¸ì„œ ë¦¬í„´
#=========================================================
@app.post("/chatbot")
async def chabot(content: Dict):
    #user_id = content["userRequest"]["user"]["id"]  # id
    query = content["userRequest"]["utterance"]  # ì§ˆë¬¸
    content1 = content["userRequest"]
    LOGGER.info(f'/test-----\content1:{content1}')

    #text = "ë‹µë³€\n" + question 
    #LOGGER.info(f'/test-----\text:{text}')

    search_size = 2      # ê²€ìƒ‰ ê³„ìˆ˜
    esindex = "qaindex"  # qaindex
    checkdocs = True     # True = index ê²€ìƒ‰ / False = index ê²€ìƒ‰ ì•ˆí•˜ê³ , ë°”ë¡œ LLM ì‘ë‹µí•¨
    
    LOGGER.info(f'/test-----\query:{query}, search_size:{search_size}, esindex:{esindex}, checkdocs:{checkdocs}, LLM_MODEL:{LLM_MODEL}, Q_METHOD:{Q_METHOD}')
    
    # esë¡œ ì„ë² ë”© ì¿¼ë¦¬ ì‹¤í–‰
    try:
        error, docs = es_embed_query(esindex, query, search_size, Q_METHOD)
    except Exception as e:
        error = f'es_embed_query fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'[search_docs]: {msg}\n')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
        
    #LOGGER.info(f'/test-----\question:{question}, answer:{answer}')
    
    context:str = ''

    for doc in docs:
        score = doc['score']
        if score > MIN_SCORE:
            rfile_text = doc['rfile_text']
            if rfile_text:
                 context += rfile_text+'\n['+str(score)+']' + '\n\n'  # ë‚´ìš©ê³¼ socore ì¶œë ¥
                
    if len(context) < 5:
        context = 'ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'
        
    # ë‹µë³€ í…Œê¸‹íŠ¸ ì„¤ì •
    content = {
        "version": "2.0",
        "template": {
            "outputs": [
                {
                    "simpleText": {
                        "text": context
                    }
                }
            ]
        }
    }
    
    return JSONResponse(content=content)


async def call_callback(callback_url, query1):
    async with httpx.AsyncClient() as client:
        #api_response = await client.get("https://some-api.com/data")
        #api_data = api_response.json()
        
        start_time = time.time()
        callbackurl = callback_url
        query = "ë‹µë³€:" + query1
        
        LOGGER.info(f'callback-----query:{query}, callbackurl:{callbackurl}')
        
        search_size = 2      # ê²€ìƒ‰ ê³„ìˆ˜
        esindex = "qaindex"  # qaindex
        checkdocs = False     # True = index ê²€ìƒ‰ / False = index ê²€ìƒ‰ ì•ˆí•˜ê³ , ë°”ë¡œ LLM ì‘ë‹µí•¨

        LOGGER.info(f'/callback-----\query:{query}, search_size:{search_size}, esindex:{esindex}, checkdocs:{checkdocs}, LLM_MODEL:{LLM_MODEL}, Q_METHOD:{Q_METHOD}')

        if LLM_MODEL == 0:       # SLLM
            question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=0, model_key='', model_key1='', model_key2='', qmethod=Q_METHOD, checkdocs=checkdocs)
        elif LLM_MODEL == 1:     # gpt
            question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=1, model_key='', model_key1='', model_key2='', qmethod=Q_METHOD, checkdocs=checkdocs)
        elif LLM_MODEL == 2:     # GPT
            question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=2, model_key=BARD_TOKEN, model_key1=BARD_1PSIDTS_TOKEN, model_key2=BARD_1PSIDCC_TOKEN, qmethod=Q_METHOD, checkdocs=checkdocs)

        #LOGGER.info(f'/test-----\question:{question}, answer:{answer}')

        # ì†Œìš”ëœ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        end_time = time.time()
        elapsed_time = end_time - start_time

        answer += '\n(' + str(elapsed_time) + ')'   # ì‘ë‹µì‹œê°„ ì¶”ê°€
    
        callback_response = await client.post(
            callbackurl,
            json={
                "version": "2.0",
                "template": {
                    "outputs": [
                        {
                            "simpleText": {
                                "text": answer
                            }
                        }
                    ]
                }
            }
        )
        
        LOGGER.info(f"callback_response:{callback_response}")

        if callback_response.status_code == 200:
            LOGGER.info("Callback í˜¸ì¶œ ì„±ê³µ")
        else:
            LOGGER.info(f"Callback í˜¸ì¶œ ì‹¤íŒ¨: {callback_response.status_code}")
        
        
@app.post("/chatbot3")
async def chabot3(content: Dict):
    query = content["userRequest"]["utterance"]  # ì§ˆë¬¸
    callbackurl = content["userRequest"]["callbackUrl"] # callbackurl
    
    content1 = content["userRequest"]
    LOGGER.info(f'/test-----\content1:{content1}')
    
    # ë¹„ë™ê¸° ì‘ì—…ì„ ìŠ¤ì¼€ì¤„ë§
    asyncio.create_task(call_callback(callbackurl, query))
    
    LOGGER.info(f'chabot3-----query:{query}, callbackurl:{callbackurl}')
    
     # ë‹µë³€ í…Œê¸‹íŠ¸ ì„¤ì •
    content = {
        "version": "2.0",
        "useCallback": True,
        "data": {
           "text" : "ë‹µë³€ì„ ì°¾ëŠ”ì¤‘ ì´ì—ìš”ğŸ˜˜ \n15ì´ˆ ì •ë„ ê±¸ë¦´ê²ƒ ê°™ì•„ìš”. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"
        }
    }
    
    return JSONResponse(content=content)
   
#=========================================================
# ì¹´ì¹´ì˜¤ ì³‡ë´‡ ì—°ë™ í…ŒìŠ¤íŠ¸ 2. 
#=========================================================
@app.post("/chatbot2")
async def chabot2(content: Dict):
       
    start_time = time.time()
    
    query = content["userRequest"]["utterance"]  # ì§ˆë¬¸
    callback_url = content["userRequest"]["callbackUrl"] # callbackurl
    
    content1 = content["userRequest"]
    LOGGER.info(f'/test-----\content1:{content1}')

    search_size = 2      # ê²€ìƒ‰ ê³„ìˆ˜
    esindex = "qaindex"  # qaindex
    checkdocs = False     # True = index ê²€ìƒ‰ / False = index ê²€ìƒ‰ ì•ˆí•˜ê³ , ë°”ë¡œ LLM ì‘ë‹µí•¨
    
    LOGGER.info(f'/test-----\query:{query}, search_size:{search_size}, esindex:{esindex}, checkdocs:{checkdocs}, LLM_MODEL:{LLM_MODEL}, Q_METHOD:{Q_METHOD}')
    
    if LLM_MODEL == 0:       # SLLM
        question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=0, model_key='', model_key1='', model_key2='', qmethod=Q_METHOD, checkdocs=checkdocs)
    elif LLM_MODEL == 1:     # gpt
        question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=1, model_key='', model_key1='', model_key2='', qmethod=Q_METHOD, checkdocs=checkdocs)
    elif LLM_MODEL == 2:     # GPT
        question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=2, model_key=BARD_TOKEN, model_key1=BARD_1PSIDTS_TOKEN, model_key2=BARD_1PSIDCC_TOKEN, qmethod=Q_METHOD, checkdocs=checkdocs)
        
    #LOGGER.info(f'/test-----\question:{question}, answer:{answer}')
    
    # ì†Œìš”ëœ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    answer += '(' + str(elapsed_time) + ')'   # ì‘ë‹µì‹œê°„ ì¶”ê°€
    
    # ë‹µë³€ í…Œê¸‹íŠ¸ ì„¤ì •
    content = {
        "version": "2.0",
        "template": {
            "outputs": [
                {
                    "simpleText": {
                        "text": answer
                    }
                }
            ]
        }
    }
    
    return JSONResponse(content=content)
    
#=========================================================
# ë£¨íŠ¸=>ì •ë³´ ì¶œë ¥
# => http://127.0.0.1:9000/
#=========================================================
@app.get("/")
async def root():
    return {"ì„œë²„": "ë¬¸ì„œì„ë² ë”©AI API ì„œë²„", 
            "*ì„ë² ë”©ëª¨ë¸":{"ëª¨ë¸ê²½ë¡œ": MODEL_PATH, "í´ë§ë°©ì‹((mean=í‰ê· ê°’, cls=ë¬¸ì¥ëŒ€í‘œê°’, max=ìµœëŒ€ê°’)": POLLING_MODE, "ì¶œë ¥ì°¨ì›(128, 0=768)": OUT_DIMENSION,"ì„ë² ë”©ë°©ì‹(0=ë¬¸ì¥í´ëŸ¬ìŠ¤í„°ë§, 1=ë¬¸ì¥í‰ê· ì„ë² ë”©, 2=ë¬¸ì¥ì„ë² ë”©)": EMBEDDING_METHOD, "ì¶œë ¥ë²¡í„°íƒ€ì…('float32', 'float16')": FLOAT_TYPE},
            "*ESì„œë²„":{"URL":ES_URL, "ì¸ë±ìŠ¤íŒŒì¼ê²½ë¡œ": ES_INDEX_FILE, "ìµœì†ŒìŠ¤ì½”ì–´": MIN_SCORE, "ë°°ì¹˜í¬ê¸°": BATCH_SIZE, "ê²€ìƒ‰ìŠ¤í¬ë¦½íŠ¸((0=ì„ë² ë”©ì´ ì—¬ëŸ¬ê°œì¼ë•Œ MAX(ê¸°ë³¸), 1=ì„ë² ë”©ì´ ì—¬ëŸ¬ê°œì¼ë•Œ í‰ê· , 2=ì„ë² ë”©ì´1ê°œì¼ë•Œ))": Q_METHOD},
            "*í´ëŸ¬ìŠ¤í„°ë§":{"í´ëŸ¬ìŠ¤í„°ë§ ê°€ë³€(True=ë¬¸ì¥ê³„ìˆ˜ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„°ë§ê³„ìˆ˜ë¥¼ ë‹¤ë¥´ê²Œí•¨)": NUM_CLUSTERS_VARIABLE, "ë°©ì‹(kmeans=k-í‰ê·  êµ°ì§‘ ë¶„ì„, kmedoids=k-ëŒ€í‘œê°’ êµ°ì§‘ ë¶„ì„)": CLUSTRING_MODE, "ê³„ìˆ˜": NUM_CLUSTERS, "ì¶œë ¥(mean=í‰ê· ë²¡í„° ì¶œë ¥, max=ìµœëŒ€ê°’ë²¡í„°ì¶œë ¥)": OUTMODE},
            "*ë¬¸ì¥ì „ì²˜ë¦¬":{"ì œê±°ë¬¸ì¥ê¸¸ì´(ì„¤ì •ê¸¸ì´ë³´ë‹¤ ì‘ì€ ë¬¸ì¥ì€ ì œê±°ë¨)": REMOVE_SENTENCE_LEN, "ì¤‘ë³µë¬¸ì¥ì œê±°(True=ì¤‘ë³µëœë¬¸ì¥ì€ ì œê±°ë¨)": REMOVE_DUPLICATION},
            "*ê²€ìƒ‰":{"*ê²€ìƒ‰ë¹„êµë²¡í„°ê°’": VECTOR_MAG},
            "*LLM":{"ëª¨ë¸íƒ€ì…(0=SLLM, 1=GPT, 2=BARD)":LLM_MODEL },
            "*í”„ë¡¬í”„íŠ¸":{"ì»¨í…ìŠ¤íŠ¸ O": PROMPT_CONTEXT, "ì»¨í…ìŠ¤íŠ¸ X": PROMPT_NO_CONTEXT}
            }

#=========================================================
# GET : ì…ë ¥ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ê°’ ë¦¬í„´(ë¹„ë™ê¸°)
# => http://127.0.0.1:9000/vectors?sentence="ì˜¤ëŠ˜ì€ ë¹„ê°€ ì˜¨ë‹¤"&sentence="ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë‹¤"
# - in : ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤', 'ë‚´ì¼ì€ ë¹„ê°€ ì˜¨ë‹¤'] )
# - out: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°
#=========================================================
@app.get("/vectors", status_code=200)
async def get_vector(sentences: List[str] = Query(..., description="sentences", min_length=1, max_length=255, alias="sentence")):

    # embedding í•¨ìˆ˜ë¥¼ async í•¨ìˆ˜ë¡œ wrappingí•œ async_embedding í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    try:
        embeddings = await async_embedding(sentences)
    except Exception as e:
        error = f'async_embedding fail({ES_URL})'
        msg = f'{error}=>{e}'
        LOGGER.error(f'/vectors {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
        
    embeddings_str = [",".join(str(elem) for elem in sublist) for sublist in embeddings]
    return {"vectors": embeddings_str}

#=========================================================
# POST: es/{ì¸ë±ìŠ¤ëª…}/docs (ì…ë ¥ docs(ë¬¸ì„œ)ì— ëŒ€í•œ ì„ë² ë”©ê°’ êµ¬í•˜ê³  ElasticSearch(ì´í•˜:ES) ì¶”ê°€.(ë™ê¸°))
# => http://127.0.0.1:9000/es/{ì¸ë±ìŠ¤ëª…}/docs
# - in : docs: ë¬¸ì„œ (ì˜ˆ: ['ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤', 'ë‚´ì¼ì€ ë¹„ê°€ ì˜¨ë‹¤'] ), titles: ë¬¸ì„œì œëª©, uids(ë¬¸ì„œ ê³ ìœ id)
# - in : esindexname : ES ì¸ë±ìŠ¤ëª…, createindex=True(True=ë¬´ì¡°ê±´ ì¸ë±ìŠ¤ìƒì„±. ë§Œì•½ ìˆìœ¼ë©´ ì‚­ì œí›„ ìƒì„±/ Flase=ìˆìœ¼ë©´ ì¶”ê°€, ì—†ìœ¼ë©´ ìƒì„±)
# - in : infilepath : Trueì´ë©´ documnetsì— filepath ì…ë ¥ë˜ê³ , ì´ë•ŒëŠ” fileë¥¼ ë¡œë”©í•¨. Falseì´ë©´ documentsë¡œëŠ” ë¬¸ì„œë‚´ìš©ì´ ë“¤ì–´ì˜´.
# - out: ES ì„±ê³µ ì‹¤íŒ¨??
#=========================================================
class DocsEmbedIn(BaseModel):
    uids: list       # uid(ë¬¸ì„œ ê³ ìœ id)->rfilename
    titles: list     # ì œëª©->rfiletext
    documents: list  # ë¬¸ì„œë‚´ìš© í˜¹ì€ file ê²½ë¡œ (infilepath=Trueì´ë©´, filepath ì…ë ¥ë¨)

@app.post("/es/{esindex}/docs")
def embed_documents(esindex:str, Data:DocsEmbedIn, infilepath:bool=False, createindex:bool=False):
    error:str = 'success'
        
    documents = Data.documents
    uids = Data.uids
    titles = Data.titles
    
    # ì „ì—­ë³€ìˆ˜ë¡œ ES ì¸ë±ìŠ¤ëª… ì €ì¥í•´ ë‘ .
    global ES_INDEX_NAME
    ES_INDEX_NAME = esindex
    
    LOGGER.info(f'/es/{esindex}/docs start-----\nES_URL:{ES_URL}, esindex:{esindex}, createindex:{createindex}, uids:{uids}, titles:{titles}')

    # ì¸ì ê²€ì‚¬
    if len(documents) < 1:
        error = 'documents len < 1'
    elif len(uids) < 1:
        error = 'uid not found'
    elif len(titles) < 1:
        error = 'titles not found'
    elif not esindex:
        error = 'esindex not found'
     
    if error != 'success':
        LOGGER.error(f'/embed/es {error}')
        raise HTTPException(status_code=404, detail=error, headers={"X-Error": error},)
    
    # 1.elasticsearch ì ‘ì†
    try:
        es = Elasticsearch(ES_URL)
        LOGGER.info(f'/embed/es 1.Elasticsearch connect success=>{ES_URL}')
    except Exception as e:
        error = f'Elasticsearch connect fail({ES_URL})'
        msg = f'{error}=>{e}'
        LOGGER.error(f'/embed/es {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
        
    #LOGGER.info(f'es.info:{es.info()}')

    # 2. ì¶”ì¶œëœ ë¬¸ì„œë“¤ ë¶ˆëŸ¬ì™€ì„œ dfë¡œ ë§Œë“¬
    try:                                                                
        df_contexts = make_docs_df(documents, titles, uids, infilepath)
        LOGGER.info(f'/embed/es 2.load_docs success')
    except Exception as e:
        error = f'load docs fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'/embed/es {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
                                                                    
    # 3. ë¬¸ì¥ ì¶”ì¶œ
    try:
        doc_sentences = get_sentences(df=df_contexts, remove_sentnece_len=REMOVE_SENTENCE_LEN, remove_duplication=REMOVE_DUPLICATION)
        LOGGER.info(f'/embed/es 3.get_sentences success=>len(doc_sentences):{len(doc_sentences)}')
    except Exception as e:
        error = f'get_sentences fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'/embed/es {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
   
    # 4.ES ì¸ë±ìŠ¤ ìƒì„±
    try:
        create_index(es, ES_INDEX_FILE, ES_INDEX_NAME, createindex)
        LOGGER.info(f'/embed/es 4.create_index success=>index_file:{ES_INDEX_FILE}, index_name:{ES_INDEX_NAME}')
    except Exception as e:
        error = f'create_index fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'/embed/es {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)

    # 5. index ì²˜ë¦¬
    try:
        index_data(es, df_contexts, doc_sentences)
        LOGGER.info(f'/embed/es 5.index_data success\nend-----\n')
    except Exception as e:
        error = f'index_data fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'/embed/es {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
#=========================================================

#=========================================================
# GET : es/{ì¸ë±ìŠ¤ëª…}/docs ê²€ìƒ‰(ë¹„ë™ê¸°)
# => http://127.0.0.1:9000/es/{ì¸ë±ìŠ¤}/docs?query=ì¿¼ë¦¬ë¬¸ì¥&search_size=5
# - in : query=ì¿¼ë¦¬í•  ë¬¸ì¥, search_size=ê²€ìƒ‰ê³„ìˆ˜(ëª‡ê°œê¹Œì§€ ê²€ìƒ‰ ì¶œë ¥ í• ì§€)
# - out: ê²€ìƒ‰ ê²°ê³¼(ìŠ¤ì½”ì–´, rfile_name, rfile_text)
#=========================================================

@app.get("/es/{esindex}/docs")
async def search_documents(esindex:str, 
                     query: str = Query(..., min_length=1),     # ... ëŠ” í•„ìˆ˜ ì…ë ¥ ì´ê³ , min_length=1ì€ ìµœì†Œê°’ì´ 1ì„. ì‘ìœ¼ë©´ 422 Unprocessable Entity ì‘ë‹µë°˜í™˜ë¨
                     search_size: int = Query(..., gt=0),       # ... ëŠ” í•„ìˆ˜ ì…ë ¥ ì´ê³ , gt=0ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•œë‹¤. ì‘ìœ¼ë©´ 422 Unprocessable Entity ì‘ë‹µë°˜í™˜ë¨
                     qmethod: int=0,                            # option: qmethod=0 í˜¹ì€ 1(0=maxë²¡í„° êµ¬í•˜ê¸°, 1=í‰ê· ë²¡í„° êµ¬í•˜ê¸° (default=0))
                     ):                          
                    
      
    error:str = 'success'
    query = query.strip()
    LOGGER.info(f'\nget /es/{esindex}/docs start-----\nquery:{query}, search_size:{search_size}')
    
    try:
        # esë¡œ ì„ë² ë”© ì¿¼ë¦¬ ì‹¤í–‰
        error, docs = await async_es_embed_query(esindex, query, search_size, qmethod)
    except Exception as e:
        error = f'async_es_embed_query fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'get /es/{esindex}/docs {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
    
    if error != 'success':
        raise HTTPException(status_code=404, detail=error, headers={"X-Error": error},)
            
    return {"query":query, "docs": docs}
#=========================================================

#=========================================================
# POST : es/{ì¸ë±ìŠ¤ëª…}/docs/uids => uid ëª©ë¡ì— ëŒ€í•œ ê²€ìƒ‰(ë¹„ë™ê¸°)
# => http://127.0.0.1:9000/es/{ì¸ë±ìŠ¤}/docs/uid?query=ì¿¼ë¦¬ë¬¸ì¥&search_size=5
# - in : query=ì¿¼ë¦¬í•  ë¬¸ì¥, search_size=ê²€ìƒ‰ê³„ìˆ˜(ëª‡ê°œê¹Œì§€ ê²€ìƒ‰ ì¶œë ¥ í• ì§€)
# - in(data) : DocsUidsIn=ê²€ìƒ‰í•  uid ëª©ë¡
# - out: ê²€ìƒ‰ ê²°ê³¼(ìŠ¤ì½”ì–´, rfile_name, rfile_text)
#=========================================================
class DocsUidsIn(BaseModel):
    uids: list       # uid(ë¬¸ì„œ ê³ ìœ id)->rfilename
    
@app.post("/es/{esindex}/docs/uids")
async def search_documents_uid(esindex:str, 
                     Data:DocsUidsIn,
                     query: str = Query(..., min_length=1),     # ... ëŠ” í•„ìˆ˜ ì…ë ¥ ì´ê³ , min_length=1ì€ ìµœì†Œê°’ì´ 1ì„. ì‘ìœ¼ë©´ 422 Unprocessable Entity ì‘ë‹µë°˜í™˜ë¨
                     search_size: int = Query(..., gt=0),       # ... ëŠ” í•„ìˆ˜ ì…ë ¥ ì´ê³ , gt=0ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•œë‹¤. ì‘ìœ¼ë©´ 422 Unprocessable Entity ì‘ë‹µë°˜í™˜ë¨
                     qmethod: int=0,                            # option: qmethod=0 í˜¹ì€ 1(0=maxë²¡í„° êµ¬í•˜ê¸°, 1=í‰ê· ë²¡í„° êµ¬í•˜ê¸° (default=0))
                     ):    
    
    error:str = 'success'
    query = query.strip()
    uids = Data.uids 
    LOGGER.info(f'\npost /es/{esindex}/docs/uids start-----\nquery:{query}, search_size:{search_size}, len(uids):{len(uids)}')
            
    try:
        # esë¡œ ì„ë² ë”© ì¿¼ë¦¬ ì‹¤í–‰
        error, docs = await async_es_embed_query(esindex, query, search_size, qmethod, uids)
    except Exception as e:
        error = f'async_es_embed_query fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'get /es/{esindex}/docs {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
    
    if error != 'success':
        raise HTTPException(status_code=404, detail=error, headers={"X-Error": error},)
            
    return {"query":query, "docs": docs}
#=========================================================

#=========================================================
# DELETE : ES/{ì¸ë±ìŠ¤ëª…}/docs ê²€ìƒ‰(ë¹„ë™ê¸°)
# => http://127.0.0.1:9000/es/{ì¸ë±ìŠ¤}/docs?uid=rfile_name
# - in : uid=ì‚­ì œí•  ë¬¸ì„œ ìœ ë‹ˆí¬í•œ id
# - out: ??
#=========================================================
@app.delete("/es/{esindex}/docs")
async def delete_documents(esindex:str,
                           uid:str = Query(...,min_length=1)):
    error:int = 0
    uid = uid.strip()
    LOGGER.info(f'\ndelete /es/{esindex}/docs start-----\nid:{uid}')
    
    try:
        error = await async_es_embed_delete(esindex, uid)
    except Exception as e:
        error = f'async_es_embed_delete fail'
        msg = f'{error}=>{e}'
        LOGGER.error(f'delete /es/{esindex}/docs {msg}')
        raise HTTPException(status_code=404, detail=msg, headers={"X-Error": error},)
        
    if error != 0:
        raise HTTPException(status_code=404, detail=error, headers={"X-Error": error},)
  

#=========================================================================================
# ì²´íŒ… UI
# - bard ì´ìš©
#========================================================================================= 

#=========================================================
# ë©”ì¸ bart_chat.html í˜¸ì¶œí•˜ëŠ” api  
#=========================================================
@app.get("/chat")
async def form(request: Request):
    return templates.TemplateResponse("chat01.html", {"request": request})

#=========================================================
# ê²€ìƒ‰ ì²˜ë¦¬ api
#=========================================================
@app.post("/es/{esindex}/chat")
async def search_documents(esindex:str,
                     request: Request
                     ): 
    
    start_time = time.time()
    
    form = await request.form()
    search_size = 2
    
    query = form.get("query").strip()
    prequery = form.get("prequery").strip()
    checkdocsstr = form.get("checkdocs")
    #print(f'==>checkdocsstr :{checkdocsstr}')
    checkdocs = True
    if checkdocsstr == None: # ì²´í¬ë²„íŠ¼ ê°’ì€ Falseì¼ë•Œ Noneìœ¼ë¡œ ë“¤ì–´ì˜¤ê³ , Trueì´ë©´ onìœ¼ë¡œ ë“¤ì–´ì˜´. ë”°ë¼ì„œ Noneìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´ False í•´ì¤Œ.
        checkdocs=False
    
    print(f'checkdocs :{checkdocs}')
    
    #print(f'1) /es/{esindex}/docs/bard/chat')
    #print(f'2) prequery:{prequery}')
    #print(f'3) query:{query}')
    
    # ì´ì „ ë‹µë³€/ì‘ë‹µ ë¬¸ë‹¨ë“¤ ê³„ìˆ˜ê°€ 4ë¥¼ ë„˜ìœ¼ë©´, ê°€ì¥ì˜¤ë˜ëœ ë¬¸ë‹¨ì„ ì œê±°í•˜ê³ , ê° ë¬¸ë‹¨ë³„ <hr> êµ¬ë¶„ìë¥¼ ë„£ì–´ì„œ prequeryë¥¼ ë§Œë“ ë‹¤.
    prequery = remove_prequery(prequery, 4)

    # ìƒˆë¡œìš´ ëŒ€í™” ì‹œë„ì¸ ê²½ìš°, ê¸°ì¡´ preanswer ì´ˆê¸°í™” í•¨.
    if query.startswith("@##ìƒˆë¡œìš´ ëŒ€í™”"):
        prequery=""

    if LLM_MODEL == 0:       # SLLM
        question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=0, model_key='', model_key1='', model_key2='', qmethod=Q_METHOD, checkdocs=checkdocs)
    elif LLM_MODEL == 1:     # gpt
        question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=1, model_key='', model_key1='', model_key2='', qmethod=Q_METHOD, checkdocs=checkdocs)
    elif LLM_MODEL == 2:     # GPT
        question, answer, context1 = await async_search_docs(esindex, query, search_size, llm_model_type=2, model_key=BARD_TOKEN, model_key1=BARD_1PSIDTS_TOKEN, model_key2=BARD_1PSIDCC_TOKEN, qmethod=Q_METHOD, checkdocs=checkdocs)
        
     # contextì—ì„œ titleë§Œ ë½‘ì•„ë‚´ì„œ urlë§í¬ ë§Œë“¬.
    titles_str = get_title_with_urllink(context1)
    
     # ì†Œìš”ëœ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    end_time = time.time()
    elapsed_time = end_time - start_time

    # htmlë¡œ í‘œê¸°í• ë•Œ ì¤‘ê°„ì— "(ìŒë”°ì˜´í‘œ) ìˆìœ¼ë©´ ì•ˆë˜ë¯€ë¡œ , ìŒë”°ì˜´í‘œë¥¼ '(í™‘ë”°ì˜´í‘œ)ë¡œ ì¹˜í™˜
    question = question.replace('"',"'")
    answer = answer.replace('"',"'") + '\n( ì‘ë‹µì‹œê°„:' + str(elapsed_time) + ')'
    prequery = prequery.replace('"',"'")
    titles_str = titles_str.replace('"',"'")
    
    return templates.TemplateResponse("chat01.html", {"request": request, "question":question, "answer": answer, "preanswer": prequery, "titles": titles_str})

        
    
        