{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820aa697-ef0d-42e5-8f0f-04852a14e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# BERT와 sLLM 모델을 이용한 질의응답 서비스 구축 예\n",
    "# - 여기서는 문서들을 전처리하고, 임베딩 하는 과정임.\n",
    "#\n",
    "# 질의 응답 시스템 과정\n",
    "# 문서들 전처리 : \n",
    "#    단락별루 분할(\\n\\n) - 불용어 제거 -문장별루 분할.\n",
    "# 임베딩 : \n",
    "#    kpf-sbert-v1.1로  문장 평균 임베딩벡터 구함 - es에 문장별루 단락text와 평균벡터 저장.\n",
    "# 프롬프트생성 및 입력 : \n",
    "#   검색어 입력(회사:과장일때 휴가 일수는 얼마?)-bert로 임베딩 검색(*스코어가 0.6이상인 경우 체택)-sLLM에 검색된 단락 text를 문맥으로 해서 prompt 구성\n",
    "#   sLLM에 prompt 입력-응답 결과 출력\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging, getListOfFiles\n",
    "\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "DEVICE = GPU_info() # GPU 혹은 CPU\n",
    "LOGGER = mlogging(loggername=\"sllm-test\", logfilename='../../log/sll-test.txt') # 로그\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e33b1-6cbf-4fc8-ae82-50577909b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_FOLDER에 파일들을 불러와서 DF로 만듬.\n",
    "# -문서는 \\n으로 구분해야 하며, 맨앞에는 title이 와야 하고, \\n 다음 문단들이 와야함. 문단들은 \\n 구분됨.\n",
    "# -예: 회사 개요\\n{회사내용}\\n{제품구성}\n",
    "'''\n",
    "# 파일이 여러개인 경우 폴더 지정\n",
    "DATA_FOLDER = '../../data11/mpower_doc/사규개정-out-renew/'\n",
    "\n",
    "files = getListOfFiles(DATA_FOLDER)\n",
    "assert len(files) > 0 # files가 0이면 assert 발생\n",
    "print('*file_count: {}, file_list:{}'.format(len(files), files[0:5]))\n",
    "'''\n",
    "\n",
    "files = [\"../../data11/mpower_doc/사규개정-out-renew/회사 개요.txt\",]\n",
    "count = 0  # **카운터가 문서에 uid가 되므로, 유일무이한 값므로 지정할것.\n",
    "\n",
    "titles = []\n",
    "contextids = []\n",
    "contexts = []\n",
    "\n",
    "for idx, file_path in enumerate(files):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        print(data)\n",
    "        data_list = data.split('\\n\\n')  # '\\n\\n' 구분으로 다락 구분\n",
    "        count += 1\n",
    "        \n",
    "        # titles, contextids 임의로 구해서 데이터 프레임 만듬.\n",
    "        for i in range(len(data_list)):\n",
    "            if i > 0:\n",
    "                titles.append(data_list[0])  # 문서 제목은 맨처음 문장이고 \\n\\n로 구분되어야 함.\n",
    "                contextids.append(count)\n",
    "                contexts.append(data_list[0] + '\\n' + data_list[i]) # 두번째는 문서 제목+문서내용 합처서 문장만듬.\n",
    "\n",
    "# 데이터 프레임으로 만듬.\n",
    "df_contexts = pd.DataFrame((zip(contexts, titles, contextids)), columns = ['context','question', 'contextid'])     \n",
    "\n",
    "print(f'len:{len(df_contexts)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c584631-299d-4cee-9873-a4fa0ad367fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장들로 분리\n",
    "from myutils import get_sentences\n",
    "doc_sentences = get_sentences(df=df_contexts, remove_sentnece_len=1, remove_duplication=False)\n",
    "\n",
    "print(f'len:{len(doc_sentences)}, 1: {doc_sentences[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48638f8-0fbf-4c38-92c0-46adf2a51916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 추가\n",
    "from tqdm.notebook import tqdm\n",
    "from myutils import embed_text, bi_encoder, mpower_index_batch\n",
    "from myutils import create_index\n",
    "\n",
    "# ES 관련\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "    \n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "def embedding(paragraphs:list)->list:\n",
    "    # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "    embeddings = embed_text(model=BI_ENCODER1, paragraphs=paragraphs, return_tensor=False).astype(FLOAT_TYPE)    \n",
    "    return embeddings\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#문단에 문장들의 임베딩을 구하여 각각 클러스터링 처리함.\n",
    "#---------------------------------------------------------------------------\n",
    "def index_data(es, df_contexts, doc_sentences:list):\n",
    "    #클러스터링 계수는 문단의 계수보다는 커야 함. \n",
    "    #assert num_clusters <= len(doc_sentences), f\"num_clusters:{num_clusters} > len(doc_sentences):{len(doc_sentences)}\"\n",
    "    #-------------------------------------------------------------\n",
    "    # 각 문단의 문장들에 벡터를 구하고 리스트에 저장해 둠.\n",
    "    start = time.time()\n",
    "    cluster_list = []\n",
    "\n",
    "    rfile_names = df_contexts['contextid'].values.tolist()\n",
    "    rfile_texts = df_contexts['context'].values.tolist()\n",
    "\n",
    "    if OUT_DIMENSION == 0:\n",
    "        dimension = 768\n",
    "    else:\n",
    "        dimension = 128\n",
    "\n",
    "    clustering_num = NUM_CLUSTERS\n",
    "        \n",
    "    docs = []\n",
    "    count = 0\n",
    "    for i, sentences in enumerate(tqdm(doc_sentences)):\n",
    "        embeddings = embedding(sentences)\n",
    "        if i < 3:\n",
    "            print(f'[{i}] sentences-------------------')\n",
    "            if len(sentences) > 5:\n",
    "                print(sentences[:5])\n",
    "            else:\n",
    "                print(sentences)\n",
    "                \n",
    "            LOGGER.info(f'*[index_data] embeddings.shape: {embeddings.shape}')\n",
    "            print()\n",
    "        \n",
    "        #----------------------------------------------------------------\n",
    "        multiple = 1\n",
    "        \n",
    "        # [bong][2023-04-28] 임베딩 출력 계수에 따라 클러스터링 계수를 달리함.\n",
    "        if NUM_CLUSTERS_VARIABLE == True:\n",
    "            embeddings_len = embeddings.shape[0]\n",
    "            if embeddings_len > 2000:\n",
    "                multiple = 6\n",
    "            elif embeddings_len > 1000:\n",
    "                multiple = 5 # 5배\n",
    "            elif embeddings_len > 600:\n",
    "                multiple = 4 # 4배\n",
    "            elif embeddings_len > 300:\n",
    "                multiple = 3 # 3배\n",
    "            elif embeddings_len > 100:\n",
    "                multiple = 2 # 2배\n",
    "        #----------------------------------------------------------------\n",
    "        \n",
    "        # 0=문장클러스터링 임베딩\n",
    "        if EMBEDDING_METHOD == 0:\n",
    "            if CLUSTRING_MODE == \"kmeans\":\n",
    "                # 각 문단에 분할한 문장들의 임베딩 값을 입력해서 클러스터링 하고 평균값을 구함.\n",
    "                # [bong][2023-04-28] 문장이 많은 경우에는 클러스터링 계수를 2,3배수로 함\n",
    "                emb = clustering_embedding(embeddings = embeddings, outmode=OUTMODE, num_clusters=(clustering_num*multiple), seed=SEED).astype(FLOAT_TYPE) \n",
    "            else:\n",
    "                emb = kmedoids_clustering_embedding(embeddings = embeddings, outmode=OUTMODE, num_clusters=(clustering_num*multiple), seed=SEED).astype(FLOAT_TYPE) \n",
    "            \n",
    "        # 1= 문장평균임베딩\n",
    "        elif EMBEDDING_METHOD == 1:\n",
    "            # 문장들에 대해 임베딩 값을 구하고 평균 구함.\n",
    "            arr = np.array(embeddings).astype(FLOAT_TYPE)\n",
    "            emb = arr.mean(axis=0).reshape(1,-1) #(128,) 배열을 (1,128) 형태로 만들기 위해 reshape 해줌\n",
    "            clustering_num = 1  # 평균값일때는 NUM_CLUSTERS=1로 해줌.\n",
    "        # 2=문장임베딩\n",
    "        else:\n",
    "            emb = embeddings\n",
    "\n",
    "        if i < 3:\n",
    "            LOGGER.info(f'*[index_data] cluster emb.shape: {emb.shape}')\n",
    "            print()\n",
    "        \n",
    "        #--------------------------------------------------- \n",
    "        # docs에 저장 \n",
    "        #  [bong][2023-04-28] 여러개 벡터인 경우에는 벡터를 10개씩 분리해서 여러개 docs를 만듬.\n",
    "        for j in range(multiple):\n",
    "            count += 1\n",
    "            doc = {}                                #dict 선언\n",
    "            doc['rfile_name'] = rfile_names[i]      # contextid 담음\n",
    "            doc['rfile_text'] = rfile_texts[i]      # text 담음.\n",
    "            doc['dense_vectors'] = emb[j * clustering_num : (j+1) * clustering_num] # emb 담음.\n",
    "            docs.append(doc)\n",
    "        #---------------------------------------------------    \n",
    "\n",
    "            if count % BATCH_SIZE == 0:\n",
    "                mpower_index_batch(es, ES_INDEX_NAME, docs, vector_len=clustering_num, dim_size=dimension)\n",
    "                docs = []\n",
    "                LOGGER.info(\"[index_data](1) Indexed {} documents.\".format(count))\n",
    "\n",
    "    if docs:\n",
    "        mpower_index_batch(es, ES_INDEX_NAME, docs, vector_len=clustering_num, dim_size=dimension)\n",
    "        LOGGER.info(\"[index_data](2) Indexed {} documents.\".format(count))   \n",
    "\n",
    "    es.indices.refresh(index=ES_INDEX_NAME)\n",
    "\n",
    "    LOGGER.info(f'*인덱싱 시간 : {time.time()-start:.4f}\\n')\n",
    "    print()\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# param--------------------------------------------------------------------\n",
    "OUT_DIMENSION = 128   # 128 혹은 768이면 0입력\n",
    "EMBEDDING_METHOD=1  # 0=클러스터링 임베딩, 1=평균임베딩\n",
    "NUM_CLUSTERS=10     # 클러스터링 임베딩일때 클러스터링 수 \n",
    "NUM_CLUSTERS_VARIABLE=False # 클러스터링 임베딩일때 클러스터링을 문장계수마다 다르계할지.\n",
    "\n",
    "\n",
    "MODEL_PATH = '../../data11/model/kpf-sbert-128d-v1'\n",
    "POLLING_MODE = 'mean' # 폴링모드 \n",
    "FLOAT_TYPE = 'float16' # float32 혹은 float16\n",
    "\n",
    "# ES 접속\n",
    "ES_URL = 'http://10.10.4.10:9200/'\n",
    "ES_INDEX_NAME = 'mpower_doc_128d_1'\n",
    "ES_INDEX_FILE = './data/mpower10u_128d_1.json'\n",
    "BATCH_SIZE=20       # ES 배치 사이즈\n",
    "CREATE_INDEX = False # True이면 기존에 인덱스가 있다면 제거하고 다시 생성.\n",
    "# param--------------------------------------------------------------------\n",
    "\n",
    "es = Elasticsearch(ES_URL)\n",
    "create_index(es, ES_INDEX_FILE, ES_INDEX_NAME, create=CREATE_INDEX)\n",
    "\n",
    "# 임베딩 모델 로딩\n",
    "WORD_EMBDDING_MODEL1, BI_ENCODER1 = bi_encoder(model_path=MODEL_PATH, max_seq_len=512, do_lower_case=True, \n",
    "                                               pooling_mode=POLLING_MODE, out_dimension=OUT_DIMENSION, device=DEVICE)\n",
    "\n",
    "print(BI_ENCODER1)\n",
    "print()\n",
    "try:\n",
    "    index_data(es, df_contexts, doc_sentences)\n",
    "except Exception as e:\n",
    "    error = f'index_data fail'\n",
    "    msg = f'{error}=>{e}'\n",
    "    LOGGER.error(f'/embed/es {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990059ea-bdc3-4328-9e0d-c2653cee6b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
