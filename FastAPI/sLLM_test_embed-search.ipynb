{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820aa697-ef0d-42e5-8f0f-04852a14e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/sll-test.txt_2023-06-23.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "\n",
      "\n",
      "prompt 테스트----------------------\n",
      "\n",
      "다음 내용을 파악하여 질문에 대해 간략한 답변을 작성하세요.\n",
      "### 내용: 제주도는 최남단에 있는 섬으로, 길이는 40km가 되는 대한민국에서 가장큰 섬이다.\n",
      "### 질문: 제주도의 크기는 얼마?\n",
      "### 답변:\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# BERT와 sLLM 모델을 이용한 질의응답 서비스 구축 예\n",
    "# - sLLM_test_embed.ipynb에서 임베딩된 문서들을 이용하여 프롬프트 생성 및 입력하는 예시임.\n",
    "#\n",
    "# 질의 응답 시스템 과정\n",
    "# 문서들 전처리 : \n",
    "#    단락별루 분할(\\n\\n) - 불용어 제거 -문장별루 분할.\n",
    "# 임베딩 : \n",
    "#    kpf-sbert-v1.1로  문장 평균 임베딩벡터 구함 - es에 문장별루 단락text와 평균벡터 저장.\n",
    "# 프롬프트생성 및 입력 : \n",
    "#   검색어 입력(회사:과장일때 휴가 일수는 얼마?)-bert로 임베딩 검색(*스코어가 0.6이상인 경우 체택)-sLLM에 검색된 단락 text를 문맥으로 해서 prompt 구성\n",
    "#   sLLM에 prompt 입력-응답 결과 출력\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "LOGGER = mlogging(loggername=\"sllm-test\", logfilename='../../log/sll-test.txt') # 로그\n",
    "\n",
    "# param-----------------------------------\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "DEVICE = GPU_info() # GPU 혹은 CPU\n",
    "OUT_DIMENSION = 128   # 출력 dimension 128 혹은 0(768)\n",
    "EMBEDDING_METHOD=1 # 1=평균\n",
    "NUM_CLUSTERS=10\n",
    "NUM_CLUSTERS_VARIABLE=False\n",
    "   \n",
    "BATCH_SIZE=20\n",
    "FLOAT_TYPE = 'float16'\n",
    "\n",
    "# ES 접속\n",
    "ES_URL = 'http://10.10.4.10:9200/'             # elasticsearch 접속 url\n",
    "ES_INDEX_NAME = 'mpower_doc_128d_1'            # elasticsearch 인덱스명\n",
    "ES_INDEX_FILE = './data/mpower10u_128d_1.json' # 인덱스 생성 파일\n",
    "SEARCH_SIZE = 3                                # 검색 계수\n",
    "MIN_SCORE = 1.3                                # 검색 1.4 스코어 이하면 제거\n",
    "\n",
    "# 임베딩 모델 param\n",
    "MODEL_PATH = '../../data11/model/kpf-sbert-128d-v1'\n",
    "POLLING_MODE = 'mean'\n",
    "\n",
    "# sLLM 모델 param\n",
    "LLM_MODEL_TYPE = 2      # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델.\n",
    "\n",
    "lora_weights:str = '../../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'      # lora weight 경로\n",
    "llm_model_path:str ='../../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'     # llm 모델경로(KoAlpaca-Polyglot-5.8B 모델이 가장 속도가 빠르고, 잘 응답하는것 같음.)\n",
    "uselora_weight = False # Lora 사용하는 경우 True\n",
    "load_8bit = True       # 8bit 로딩 \n",
    "\n",
    "# prompt_template(모델에 따라 변경)\n",
    "PROMPT_DICT = {\n",
    "    #\"prompt_context\":(\"아래 내용을 가지고 질문에 대해 간략히 답변해 주세요\\n\\n### 내용: {context}\\n\\n### 질문: {query}\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"질문에 대해 간략히 답변해 주세요\\n\\n### 질문: {query}\\n\\n### 답변:\")\n",
    "     \n",
    "    #\"prompt_context\":(\"### 질문: {query}\\n\\n아래 내용을 가지고 질문에 대해 간략히 답변해 주세요\\n\\n### 내용: {context}\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"### 질문: {query}\\n\\n질문에 대해 간략히 답변해 주세요\\n\\n### 답변:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"### 질문: {context}\\n\\n위 내용을 간략히 요약해 주세요\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"### 질문: {context}\\n\\n위 내용을 간략히 요약해 주세요\\n\\n### 답변:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"{context}\\n위 내용을 간략히 요약해 주세요\\n\\n\"),\n",
    "    #\"prompt_no_context\":(\"{context}\\n위 내용을 간략히 요약해 주세요\\n\\n\"),\n",
    "    \n",
    "    \"prompt_context\":(\"다음 내용을 파악하여 질문에 대해 간략한 답변을 작성하세요.\\n### 내용: {context}\\n### 질문: {query}\\n### 답변:\"),\n",
    "    \"prompt_no_context\":(\"질문에 대해 간략한 답변을 작성하세요.\\n### 질문: {query} ### 답변:\")\n",
    "}\n",
    "#-------------------------------------------\n",
    "\n",
    "# prompt 테스트 \n",
    "print(f'\\n\\nprompt 테스트----------------------\\n')\n",
    "query='제주도의 크기는 얼마?'\n",
    "#context = ''\n",
    "context = '제주도는 최남단에 있는 섬으로, 길이는 40km가 되는 대한민국에서 가장큰 섬이다.'\n",
    "if context:\n",
    "    prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "else:\n",
    "    prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "    \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b593a7b-4d34-4c08-b625-d5a7cf45d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# 임베딩 모델 로딩\n",
    "#----------------------------------------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "from myutils import embed_text, bi_encoder, mpower_index_batch\n",
    "\n",
    "TOKENIZER1, BI_ENCODER1 = bi_encoder(model_path=MODEL_PATH, max_seq_len=512, do_lower_case=True, pooling_mode=POLLING_MODE, out_dimension=OUT_DIMENSION, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990059ea-bdc3-4328-9e0d-c2653cee6b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so.11.0'), PosixPath('/MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# sLLM 모델 로딩\n",
    "#----------------------------------------------------------------------\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "if LLM_MODEL_TYPE == 0:     # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델. == True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # tokenizer 로딩\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model_path)\n",
    "\n",
    "    # 원본 모델 로딩\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(llm_model_path, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "    if uselora_weight:\n",
    "        model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16) # loRA 모델 로딩\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "    print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3ed00d-724c-49c0-8504-a1cd1a5834ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES 관련\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "#--------------------------------------------\n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "#--------------------------------------------\n",
    "def embedding(paragraphs:list)->list:\n",
    "    # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "    embeddings = embed_text(model=BI_ENCODER1, paragraphs=paragraphs, return_tensor=False).astype(FLOAT_TYPE)    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# ES임베딩 쿼리 벡터\n",
    "# -쿼리 임베딩 벡터를 구하고, es에 접속해서 rfile_text 뽑아냄\n",
    "#------------------------------------------------------------------------\n",
    "def es_embed_query(query:str):\n",
    "    assert query, f'query is empty'\n",
    "    \n",
    "    # 1.elasticsearch 접속\n",
    "    es = Elasticsearch(ES_URL)  \n",
    "\n",
    "    # 임베딩 구함.\n",
    "    start_time = time.time()\n",
    "    embed_query = embedding([query])[0]\n",
    "    \n",
    "    #print(len(embed_query))\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector1']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴\n",
    "                \"params\": {\"query_vector\": embed_query}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print(script_query)\n",
    "\n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=ES_INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\", \"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"*ES 검색시간: {:.2f} ms\".format(end_time * 1000)) \n",
    "\n",
    "    count = 0\n",
    "    docs = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        doc = {}  #dict 선언\n",
    "        doc['rfile_name'] = hit[\"_source\"][\"rfile_name\"]      # contextid 담음\n",
    "        doc['rfile_text'] = hit[\"_source\"][\"rfile_text\"]      # text 담음.\n",
    "        doc['score'] = hit[\"_score\"]\n",
    "        docs.append(doc) \n",
    "            \n",
    "    return docs\n",
    "#------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "219e7066-b17b-4eb1-8a44-b3332749e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "#--------------------------------------------\n",
    "# prompt 생성\n",
    "#--------------------------------------------\n",
    "def make_prompt(docs, query)->str:\n",
    "     # prompt 구성\n",
    "    context:str = ''\n",
    "\n",
    "    for doc in docs:\n",
    "        score = doc['score']\n",
    "        if score > MIN_SCORE:\n",
    "            rfile_text = doc['rfile_text']\n",
    "            if rfile_text:\n",
    "                context += rfile_text + '\\n\\n'\n",
    "                \n",
    "    if context:\n",
    "        prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "    else:\n",
    "        prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "                \n",
    "    # KoAlpaca 프롬프트\n",
    "    #prompt = f\"### 질문: {query}\\n질문에 대해 아래 내용을 바탕으로 간략히 답변해 주세요\\n\\n### 문맥: {context}\\n\\n### 답변:\" if context else f\"### 질문: {query}\\n질문에 대해 간략히 답변해 주세요\\n\\n### 답변:\"\n",
    "    \n",
    "    # llama 프롬프트\n",
    "    #prompt = f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {context}\\n{query} ### Response:\"\n",
    "\n",
    "    #print(prompt)\n",
    "    #print()\n",
    "    return prompt\n",
    "    \n",
    "#-----------------------------------------\n",
    "# sLLM 이용한 text 생성\n",
    "#-----------------------------------------\n",
    "def generate_text_sLLM(prompt):\n",
    "    \n",
    "    max_new_tokens = 256\n",
    "    eos_str = tokenizer.decode(tokenizer.eos_token_id)\n",
    "    start_time = time.time()\n",
    "\n",
    "    #prompt = query\n",
    "    #prompt = f\"### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {input_text}\\n\\n### 답변:\"\n",
    "    #prompt = f\"### 질문 : 간략히 답변해줘.{query}\\r\\n###답변:\"\n",
    "    #prompt = f\"### 질문: {query}\\n\\n### 답변:\"\n",
    "\n",
    "    #print(prompt)\n",
    "\n",
    "    # config 설정\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.5,\n",
    "        #top_p=0.75,\n",
    "        #top_k=40,\n",
    "        #num_beams=1,\n",
    "        bos_token_id=tokenizer.bos_token_id,  # 시작토큰 \n",
    "        eos_token_id=tokenizer.eos_token_id,  # end 토큰\n",
    "        pad_token_id=tokenizer.pad_token_id   # padding 토큰\n",
    "    )\n",
    "\n",
    "    # 프롬프트 tokenizer \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    #print(input_ids)\n",
    "\n",
    "    # Without streaming\n",
    "    # generate 처리\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    # 출력\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"*Text생성시간: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "    #print(output.replace(eos_str, ''))\n",
    "    #print()\n",
    "    return output.replace(eos_str, '')\n",
    "\n",
    "#-----------------------------------------\n",
    "# GPT를 이용한 text 생성\n",
    "#-----------------------------------------\n",
    "import openai\n",
    "\n",
    "# key 지정\n",
    "openai.api_key = \"sk-xxxx\"\n",
    "\n",
    "# 모델 - GPT 3.5 Turbo 지정\n",
    "# => 모델 목록은 : https://platform.openai.com/docs/models/gpt-4 참조\n",
    "model = \"gpt-3.5-turbo\"#\"gpt-4\"#\"gpt-3.5-turbo\" #gpt-4-0314\n",
    "'''\n",
    "ROLE_PROMPT = \"아래 내용을 요약해서 질문에 알맞게 간략히 답변해주세요\"\n",
    "\n",
    "# 메시지 설정\n",
    "MESSAGES = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helful assistant.\"},\n",
    "            #{\"role\": \"system\", \"content\": \"질문에 대해 요약해줘\"},\n",
    "            #{\"role\": \"user\", \"content\" : \"How are you?\"},\n",
    "            #{\"role\": \"assistant\", \"content\" : \"I am doing well\"},\n",
    "            {\"role\": \"user\", \"content\": ROLE_PROMPT}\n",
    "        ]\n",
    "'''\n",
    "MESSAGES:list = []\n",
    "#-----------------------------------------\n",
    "# GPT를 이용한 text 생성\n",
    "#-----------------------------------------\n",
    "def generate_text_GPT(prompt, messages):\n",
    "    \n",
    "    print(f'len(messages):{len(messages)}') \n",
    "    print()\n",
    "    \n",
    "    #-------------------------------------------------------\n",
    "    # *** gpt에 메시지는 계속 대화 내용이 유지가 되므로, 비용이 발생함.\n",
    "    # 따라서 최근 2개 대화만 유지함.\n",
    "    #if len(messages) >= 2:\n",
    "    #    messages = messages[len(messages)-2:]  # 최근 2개의 대화만 가져오기\n",
    "    messages = []  # 무조건 최근대화 초기화\n",
    "     #-------------------------------------------------------\n",
    "        \n",
    "    # 사용자 메시지 추가\n",
    "    messages.append( {\"role\": \"user\", \"content\": prompt})\n",
    "    print(messages)\n",
    "\n",
    "    # ChatGPT-API 호출하기\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=500, # 토큰 수 \n",
    "        temperature=1,  # temperature 0~2 범위 : 작을수록 정형화된 답변, 클수록 유연한 답변(2는 엉뚱한 답변을 하므로, 1.5정도가 좋은것 같음=기본값은=1)\n",
    "        top_p=0.1 # 기본값은 1 (0.1이라고 하면 10% 토큰들에서 출력 토큰들을 선택한다는 의미)\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    print()\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    return answer\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "# 구글 bard를 이용한 text 생성\n",
    "#\n",
    "# 세션키를 이용하여 구글 bard 테스트 예제\n",
    "# 출처 : https://github.com/dsdanielpark/Bard-API\n",
    "#\n",
    "# token 값얻기\n",
    "# https://bard.google.com/ 방문\n",
    "# 콘솔용 F12\n",
    "# 세션: 애플리케이션 → 쿠키 → 쿠키 값을 복사합니다 __Secure-1PSID.\n",
    "# -> 참고로 반드시 뒤에 .으로 끝나고 .포함해서 길이가 72자임.\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "from bardapi import Bard\n",
    "\n",
    "token = 'XwhPmzcV-xxxxx.'\n",
    "\n",
    "def generate_text_BARD(prompt):\n",
    "    \n",
    "    bard = Bard(token=token,timeout=30) # Set timeout in seconds\n",
    "    answer = bard.get_answer(prompt)['content']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf69ccc9-5870-4d7a-b047-8f128dfbc4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 쿼리 입력후 테스트\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "\n",
    "def handle_query():\n",
    "    # 쿼리는 일반 쿼리인 경우 '일반##제주도 면적은 얼마?'\n",
    "    #query = '일반##제주도 면적은 얼마?' \n",
    "    query = input(\"질문을 입력하세요\")\n",
    "    \n",
    "    query_split = query.split('##')\n",
    "    prefix = query_split[0]\n",
    "    #print(f'prefix: {prefix}')\n",
    "    \n",
    "    if prefix == '일반':\n",
    "        query1 = query_split[1]\n",
    "        prompt = make_prompt(docs='', query=query1)\n",
    "    else:\n",
    "        query1 = query\n",
    "        docs = es_embed_query(query1)\n",
    "        print(docs)\n",
    "        print()\n",
    "        prompt = make_prompt(docs=docs, query=query1)\n",
    "        \n",
    "    if LLM_MODEL_TYPE == 0:     # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델\n",
    "        print(generate_text_sLLM(prompt))\n",
    "    elif LLM_MODEL_TYPE == 1:     # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델:\n",
    "        print(generate_text_GPT(prompt=prompt, messages=MESSAGES))\n",
    "    else:\n",
    "        print(generate_text_BARD(prompt))\n",
    "        \n",
    "    print()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a0988be-a7f5-4ada-8d63-5184c7d80eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 입력하세요 1일 출장시 숙박비는 얼마인가요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17410/2780233136.py:46: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  response = es.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ES 검색시간: 31.32 ms\n",
      "[{'rfile_name': 44, 'rfile_text': '출장 여비 규정\\n별표1 국내출장여비 \\n임원 이상이면, 철도 운임은 실비(KTX특실), 선박운임은 실비(1등급), 항공운임은 실비(일반실), 자동차 운임은 실비로 지원함.\\n임원 이상이면, 1일비는 20,000원 지원함.\\n임원 이상이면, 숙박비는 1박당 실비로 지원함.\\n임원 이상이면, 1일 식비는 25,000원 지원함.', 'score': 1.5665728}, {'rfile_name': 44, 'rfile_text': '출장 여비 규정\\n4. 지급기준\\n4.1. 일수 계산 시, 기산은 오전 0시부터로 하며 1일 미만의 단수는 1일로 계산한다.\\n숙박비는 숙박일수에 따라 계산하며, 기산은 오전 0시를 경과함으로써 1박으로 하며, 숙박이 다른 출장목적지로의 이동 중 교통편 내에서 이루어졌을 경우 여행 목적지의 해당 숙박비만을 지급한다.', 'score': 1.5133901}, {'rfile_name': 44, 'rfile_text': '출장 여비 규정\\n6. 일비, 숙박비 및 식비\\n6.1. 일비, 숙박비, 식비의 지급\\n6.1.1. 국내 여행자의 일비, 숙박비 및 식비는 별표 1에 따라 지급하고, 국외 여행자의 경우는 별표2에 따라 지급한다. \\n다만, 부득이한 사유로 숙박비의 상한을 초과하는 여비 지출이 필요한 경우, 국내 여행의 경우에는 숙박비 상한액의 10분의 3을, 국외 여행의 경우 2분의 1을 넘지 아니하는 범위에서 여비를 추가로 지급할 수 있다. \\n이 경우, 국내 여행의 경우, 본부장의 사전 승인을 득해야 하고, 국외 여행의 경우, 대표이사의 사전 승인을 득해야 한다.\\n6.1.2. 일비는 여행일수에 따라 지급하되, 법인차량을 이용하는 경우에는 일비의 2분의 1을 지급한다.\\n6.1.3. 식비는 여행일수에 따라 지급한다. 다만, 수로여행과 항공여행에 는 따로 식비가 필요한 경우에만 식비를 지급한다.', 'score': 1.4632416}]\n",
      "\n",
      "1일 출장시 숙박비는 실비로 지급하며, 상한액은 50,000원입니다. 다만, 부득이한 사유로 숙박비의 상한을 초과하는 여비 지출이 필요한 경우, 국내 여행의 경우에는 숙박비 상한액의 10분의 3을, 국외 여행의 경우 2분의 1을 넘지 아니하는 범위에서 여비를 추가로 지급할 수 있습니다. 이 경우, 국내 여행의 경우, 본부장의 사전 승인을 득해야 하고, 국외 여행의 경우, 대표이사의 사전 승인을 득해야 합니다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 입력하세요 1일 식비는 얼마인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ES 검색시간: 42.43 ms\n",
      "[{'rfile_name': 44, 'rfile_text': '출장 여비 규정\\n6. 일비, 숙박비 및 식비\\n6.1. 일비, 숙박비, 식비의 지급\\n6.1.1. 국내 여행자의 일비, 숙박비 및 식비는 별표 1에 따라 지급하고, 국외 여행자의 경우는 별표2에 따라 지급한다. \\n다만, 부득이한 사유로 숙박비의 상한을 초과하는 여비 지출이 필요한 경우, 국내 여행의 경우에는 숙박비 상한액의 10분의 3을, 국외 여행의 경우 2분의 1을 넘지 아니하는 범위에서 여비를 추가로 지급할 수 있다. \\n이 경우, 국내 여행의 경우, 본부장의 사전 승인을 득해야 하고, 국외 여행의 경우, 대표이사의 사전 승인을 득해야 한다.\\n6.1.2. 일비는 여행일수에 따라 지급하되, 법인차량을 이용하는 경우에는 일비의 2분의 1을 지급한다.\\n6.1.3. 식비는 여행일수에 따라 지급한다. 다만, 수로여행과 항공여행에 는 따로 식비가 필요한 경우에만 식비를 지급한다.', 'score': 1.380271}, {'rfile_name': 6, 'rfile_text': '급여규정\\n4. 급여조정\\n급여의 인상은 년 1회 1월 1일부로 실시함을 원칙으로 한다.', 'score': 1.3736984}, {'rfile_name': 45, 'rfile_text': '급여규정\\n4. 급여조정\\n급여의 인상은 년 1회 1월 1일부로 실시함을 원칙으로 한다.', 'score': 1.3736984}]\n",
      "\n",
      "1일 식비는 국내 출장의 경우 1만 5천원, 국외 출장의 경우 2만 원입니다. 다만, 부득이한 사유로 숙박비의 상한을 초과하는 여비 지출이 필요한 경우, 국내 여행의 경우에는 숙박비 상한액의 10분의 3을, 국외 여행의 경우 2분의 1을 넘지 아니하는 범위에서 여비를 추가로 지급할 수 있습니다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 입력하세요 회사 비전이 있나요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ES 검색시간: 45.59 ms\n",
      "[{'rfile_name': 18, 'rfile_text': '모코엠시스 위임전결 규정\\n마. 영업조직\\n1. 영업계획\\n1) 사업계획 수립 \\n2) 사업계획 실적 분석', 'score': 1.5256176}, {'rfile_name': 101, 'rfile_text': '회사 개요\\n회사 비전\\n세계에서 인정받는 최고의 ITSolution Vendor, 최고 가치의 IT Service Vendor실현\\n인재양성\\n최고의 기술력\\n고객 감동 및 동반 성장\\n인간중심\\n혁신을 통한 최고지향\\n소통을 통한 합리적인 조직\\n교육과 지식공유를 통한 인재육성\\nR&D기술지원강화\\n서비스 중심구조탈피\\n기존고객관리 강화\\n협력관계를 통한 신규사업발굴', 'score': 1.5184085}, {'rfile_name': 101, 'rfile_text': '회사 개요\\n㈜모코엠시스는 대한민국을 대표하는 EAI/ESB솔루션, 보안솔루션,  모니터링/AI 솔루션 전문 업체로  e-Business 인프라스트럭처의 리더로서 변화를 주도하고 고객감동을 위해 경쟁력 있는 업체로 성장해 나갈 것 입니다.\\nEAI/ESB 분야:기업간 혹은 기업내 시스템 통합의 필요성에 대한 요구가 커지고 있으며 이러한 요구에 발맞춰 ㈜모코엠시스는 최적의 솔루션을 제시해 왔습니다.\\nIT 환경의 변화에 따른 다양한 고객 요구에 대응하고 최고의 기술을 제공하기 위해 모든 임직원은 지속적인 연구개발을 하고, 이를 바탕으로 국내 금융, 공공, 제조사 프로젝트를 성공적으로 수행합니다.\\n보안 분야 :최근 이슈가 되고 있는 기업용 보안과 관련하여 문서중앙화, 정보보호 분야에 다양한 보안 솔루션을 개발, 공급하고 있습니다.  10여년간의 다양한 구축 경험과 보안 기술을 바탕으로 대표적인 통합 PC보안 솔루션 전문기업으로 도약하고 있습니다. 다양한 보안 제품군으로 토탈보안 전문기업이 되도록 노력합니다.\\n\\u200b모니터링 및 AI 분야:모바일, 클라우드, 빅데이터, SOA 등 오늘날의 복잡한 IT 환경에서의 효율적인  모니터링 솔루션과 AI을 통한 업무 혁신의 중요도가 높아지고 있습니다. \\n(주) 모코엠시스는 오랜 APM 솔루션의 파트너 경험으로 신뢰도 있는 기술력을 가지고 있으며, 이를 기반하여 넓은 커버리지의 모니터링 솔루션과 업무 혁신을 위한 AI t솔루션 NO.1 전문 파트너 기업으로 자리매김하고 있습니다. \\nOpen API 분야:오픈 API는 공개된 API를 기반으로 데이터나 서비스를 통합하는 기술로 마이데이터사업 등의 기반 솔루션으로 활용되고 있습니다. 당사는 기존 EAI/ESB 기술을 바탕으로 IBM 솔루션 및 자체 Open API 솔루션을 개발, 핀테크 등 새로운 시장 활성화에 기여하고 있습니다.', 'score': 1.4715191}]\n",
      "\n",
      "네, 모코엠시스의 회사 비전은 \"세계에서 인정받는 최고의 IT Solution Vendor, 최고 가치의 IT Service Vendor 실현\"입니다. 이를 위해 모코엠시스는 최고의 기술력, 고객 감동, 인재 양성, 혁신, 소통, 합리적인 조직, 교육과 지식 공유, R&D 기술 지원 강화, 서비스 중심 구조 탈피, 기존 고객 관리 강화, 협력 관계를 통한 신규 사업 발굴 등의 핵심 가치를 추구하고 있습니다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 입력하세요 회사에 주요 제품에는 어떤것인 있나?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ES 검색시간: 44.64 ms\n",
      "[{'rfile_name': 44, 'rfile_text': '출장 여비 규정\\n18. 기타사항\\n기타 필요한 사항은 건별로 사장이 결정한다.', 'score': 1.4405224}, {'rfile_name': 101, 'rfile_text': '회사 개요\\n회사 주요 제품 \\nEAI/ESB 분야:  MI(Midas Integration, GS인증 보유), MTE , BM Integration Foundation(Integration Bus, Websphere MQ 등), Vitria BusinessWare\\n보안 분야 : Mpower EZis-C (로컬저장금지 및 문서중앙화), Mpower EZis-V (도면유출방지), Mpower B2B (대외 파일 전송), Mpower 보안파일서버 (정보 자산화 및 협업), Mpower Backup (PC백업), Mpower EZis-I (망분리), Mpower S-filter (서버용 개인정보 검출)\\n모니터링/AI 분야: dynaTrace, Catchpoint, RWS Language Weaver\\nOpen API 분야 : IBM API Connect', 'score': 1.4044698}, {'rfile_name': 14, 'rfile_text': '유형자산 관리 지침\\n5. 모니터의 구매\\n모니터는 아래 품목 및 규격으로 사용자가 신청, 부서의 결재를 득하여 경영지원본부에서 구매한다.\\n- DELL 24인치-권고모델은 DELL P2422H(권고 모델 이외의 제품을 구매할 경우 사업부장 승인 필요)\\n- 모니터 제품 선정은 기술 발전 상황 등을 고려하여 필요 시 조정될 수 있다. 기술부서에서 조정의 필요성을 판단, 제품을 선정하고 이를 관리규정에 반영한다.', 'score': 1.3894854}]\n",
      "\n",
      "회사의 주요 제품은 다음과 같습니다.\n",
      "\n",
      "* EAI/ESB 분야: MI(Midas Integration, GS인증 보유), MTE , BM Integration Foundation(Integration Bus, Websphere MQ 등), Vitria BusinessWare\n",
      "* 보안 분야 : Mpower EZis-C (로컬저장금지 및 문서중앙화), Mpower EZis-V (도면유출방지), Mpower B2B (대외 파일 전송), Mpower 보안파일서버 (정보 자산화 및 협업), Mpower Backup (PC백업), Mpower EZis-I (망분리), Mpower S-filter (서버용 개인정보 검출)\n",
      "* 모니터링/AI 분야: dynaTrace, Catchpoint, RWS Language Weaver\n",
      "* Open API 분야 : IBM API Connect\n",
      "\n",
      "이 외에도 회사는 필요에 따라 제품을 추가하거나 조정할 수 있습니다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 입력하세요 회사에 설치해도 되는 소프트웨어 종류에는 어떤것들이 있나?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ES 검색시간: 52.96 ms\n",
      "[{'rfile_name': 9, 'rfile_text': '소프트웨어 관리 규정\\n4. 주관부서 및 임무\\n회사가 소유한 모든 소프트웨어의 제반관리는 경영지원팀에서 총괄하며 주관부서의 임무는 다음과 같다.\\n4.1 소프트웨어의 구매계획 및 예산편성\\n4.2 소프트웨어의 도입평가, 선정, 구매, 보급, 설치, 확인\\n4.3 소프트웨어의 사용자 교육, 관리(정기점검)\\n4.4 소프트웨어의 유지, 보수계약 관리\\n4.5 소프트웨어의 VERSION UPGRADE 추진\\n4.6 기타, 소프트웨어 관련 업무', 'score': 1.6530732}, {'rfile_name': 8, 'rfile_text': '㈜모코엠시스 보안관리규정\\n제7조 (일반적인 소프트웨어 정책)\\n공개 소프트웨어 : 설치 책임이 있는 전산 책임자가 저자/소스의 무결성에 대해 확신하는 경우에 한해 설치/사용할 수 있다.\\n업무용 소프트웨어 \\n업무용 소프트웨어란 OS, Office, 백신, 개발용TOOL , 회계인사 System, KMS 등을 말한다.\\n라이센스를 받지 않은 소프트웨어를 사용해서는 안 된다.\\n회사의 정책상, 업무의 특성상 상용 소프트웨어의 사용을 제한 할 수 있다.\\n이는 미리 업무 책임자와의 협의에 의해 진행되어야 한다.\\n회사의 업무에 필요한 소프트웨어의 구매에 대하여 전산 책임자는 업무 책임자와의 협의 하에 구매하여야 한다.\\n기타 업무용으로 개발된 소프트웨어의 접근 권한을 업무 범위에 따라 시스템별로 접근을 제한할 수 있다. ', 'score': 1.5971236}, {'rfile_name': 50, 'rfile_text': '㈜모코엠시스 보안관리규정\\n제7조 (일반적인 소프트웨어 정책)\\n공개 소프트웨어 : 설치 책임이 있는 전산 책임자가 저자/소스의 무결성에 대해 확신하는 경우에 한해 설치/사용할 수 있다.\\n업무용 소프트웨어 \\n업무용 소프트웨어란 OS, Office, 백신, 개발용TOOL , 회계인사 System, KMS 등을 말한다.\\n라이센스를 받지 않은 소프트웨어를 사용해서는 안 된다.\\n회사의 정책상, 업무의 특성상 상용 소프트웨어의 사용을 제한 할 수 있다.\\n이는 미리 업무 책임자와의 협의에 의해 진행되어야 한다.\\n회사의 업무에 필요한 소프트웨어의 구매에 대하여 전산 책임자는 업무 책임자와의 협의 하에 구매하여야 한다.\\n기타 업무용으로 개발된 소프트웨어의 접근 권한을 업무 범위에 따라 시스템별로 접근을 제한할 수 있다. ', 'score': 1.5971236}]\n",
      "\n",
      "㈜모코엠시스의 소프트웨어 관리 규정에 따르면, 회사에 설치해도 되는 소프트웨어 종류는 다음과 같습니다.\n",
      "\n",
      "* 공개 소프트웨어: 설치 책임이 있는 전산 책임자가 저자/소스의 무결성에 대해 확신하는 경우에 한해 설치/사용할 수 있다.\n",
      "* 업무용 소프트웨어: OS, Office, 백신, 개발용 TOOL, 회계인사 System, KMS 등을 말한다.\n",
      "* 라이센스를 받지 않은 소프트웨어는 사용해서는 안 된다.\n",
      "* 회사의 정책상, 업무의 특성상 상용 소프트웨어의 사용을 제한할 수 있다. 이는 미리 업무 책임자와의 협의에 의해 진행되어야 한다.\n",
      "* 회사의 업무에 필요한 소프트웨어의 구매에 대하여 전산 책임자는 업무 책임자와의 협의 하에 구매하여야 한다.\n",
      "* 기타 업무용으로 개발된 소프트웨어의 접근 권한을 업무 범위에 따라 시스템별로 접근을 제한할 수 있다.\n",
      "\n",
      "이 외에도 회사의 정책에 따라 설치할 수 없는 소프트웨어가 있을 수 있으므로, 궁금한 사항이 있으면 전산 책임자와 협의하는 것이 좋습니다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문을 입력하세요 개인 보호법이라는게 뭔가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*ES 검색시간: 40.17 ms\n",
      "[{'rfile_name': 16, 'rfile_text': '개인정보 내부 관리계획\\n제7조(개인정보 보호책임자의 역할 및 책임) ① 개인정보 보호책임자는 다음 각 호의 업무를 수행한다.\\n1. 개인정보 보호 계획의 수립 및 시행\\n2. 개인정보 처리 실태 및 관행의 정기적인 조사 및 개선\\n3. 개인정보 처리와 관련한 불만의 처리 및 피해 구제\\n4. 개인정보 유출 및 오용․남용 방지를 위한 내부통제시스템의 구축\\n5. 개인정보 보호 교육 계획의 수립 및 시행\\n6. 개인정보파일의 보호 및 관리 감독\\n7.「개인정보 보호법」 제30조에 따른 개인정보 처리방침의 수립․변경 및 시행\\n8. 개인정보 보호 관련 자료의 관리\\n9. 처리 목적이 달성되거나 보유기간이 지난 개인정보의 파기\\n② 개인정보 보호책임자는 제1항의 업무를 수행함에 있어서 필요한 경우 개인정보의 처리 현황, 처리 체계 등에 대하여 수시로 조사하거나 관계 당사자로부터 보고를 받을 수 있다.\\n③ 개인정보 보호책임자는 개인정보 보호와 관련하여 이 법 및 다른 관계 법령의 위반 사실을 알게 된 경우에는 즉시 개선조치를 하여야 하며, 필요하면 대표이사에게 개선조치를 보고하여야 한다. ', 'score': 1.5931453}, {'rfile_name': 51, 'rfile_text': '\\n개인정보 내부 관리계획\\n제7조(개인정보 보호책임자의 역할 및 책임) ① 개인정보 보호책임자는 다음 각 호의 업무를 수행한다.\\n1. 개인정보 보호 계획의 수립 및 시행\\n2. 개인정보 처리 실태 및 관행의 정기적인 조사 및 개선\\n3. 개인정보 처리와 관련한 불만의 처리 및 피해 구제\\n4. 개인정보 유출 및 오용․남용 방지를 위한 내부통제시스템의 구축\\n5. 개인정보 보호 교육 계획의 수립 및 시행\\n6. 개인정보파일의 보호 및 관리 감독\\n7.「개인정보 보호법」 제30조에 따른 개인정보 처리방침의 수립․변경 및 시행\\n8. 개인정보 보호 관련 자료의 관리\\n9. 처리 목적이 달성되거나 보유기간이 지난 개인정보의 파기\\n② 개인정보 보호책임자는 제1항의 업무를 수행함에 있어서 필요한 경우 개인정보의 처리 현황, 처리 체계 등에 대하여 수시로 조사하거나 관계 당사자로부터 보고를 받을 수 있다.\\n③ 개인정보 보호책임자는 개인정보 보호와 관련하여 이 법 및 다른 관계 법령의 위반 사실을 알게 된 경우에는 즉시 개선조치를 하여야 하며, 필요하면 대표이사에게 개선조치를 보고하여야 한다. ', 'score': 1.5931453}, {'rfile_name': 16, 'rfile_text': '개인정보 내부 관리계획\\n제25조(개인정보 유출사고 대응절차 및 비상연락체계 구축) ① 개인정보 유출사고에 따른 대응절차는 다음과 같다.', 'score': 1.5236596}]\n",
      "\n",
      "개인정보 보호법은 개인정보의 수집ㆍ유출ㆍ오용ㆍ남용으로부터 사생활의 비밀 등을 보호하기 위하여 제정된 법률입니다. 이 법은 개인정보의 처리 목적을 명확하게 하여야 하고 그 목적에 필요한 범위에서 최소한의 개인정보만을 적법하고 정당하게 수집하여야 하며, 개인정보를 안전하게 보관하고 관리하여야 한다고 규정하고 있습니다. 또한, 개인정보 유출 사고 발생 시 신속하게 대응하도록 의무를 부과하고 있습니다.\n",
      "\n",
      "개인정보 보호법은 개인의 사생활을 보호하고 개인의 권리를 보장하는 데 중요한 역할을 합니다. 이 법을 통해 개인은 자신의 개인정보에 대한 권리를 행사할 수 있으며, 기업은 개인정보를 안전하게 보호할 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39157fe-2ec6-488a-be20-dc46e095b8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
