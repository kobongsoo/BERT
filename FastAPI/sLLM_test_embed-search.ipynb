{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820aa697-ef0d-42e5-8f0f-04852a14e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# BERT와 sLLM 모델을 이용한 질의응답 서비스 구축 예\n",
    "# - sLLM_test_embed.ipynb에서 임베딩된 문서들을 이용하여 프롬프트 생성 및 입력하는 예시임.\n",
    "#\n",
    "# 질의 응답 시스템 과정\n",
    "# 문서들 전처리 : \n",
    "#    단락별루 분할(\\n\\n) - 불용어 제거 -문장별루 분할.\n",
    "# 임베딩 : \n",
    "#    kpf-sbert-v1.1로  문장 평균 임베딩벡터 구함 - es에 문장별루 단락text와 평균벡터 저장.\n",
    "# 프롬프트생성 및 입력 : \n",
    "#   검색어 입력(회사:과장일때 휴가 일수는 얼마?)-bert로 임베딩 검색(*스코어가 0.6이상인 경우 체택)-sLLM에 검색된 단락 text를 문맥으로 해서 prompt 구성\n",
    "#   sLLM에 prompt 입력-응답 결과 출력\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "LOGGER = mlogging(loggername=\"sllm-test\", logfilename='../../log/sll-test.txt') # 로그\n",
    "\n",
    "# param-----------------------------------\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "DEVICE = GPU_info() # GPU 혹은 CPU\n",
    "OUT_DIMENSION = 128   # 출력 dimension 128 혹은 0(768)\n",
    "EMBEDDING_METHOD=1 # 1=평균\n",
    "NUM_CLUSTERS=10\n",
    "NUM_CLUSTERS_VARIABLE=False\n",
    "   \n",
    "BATCH_SIZE=20\n",
    "FLOAT_TYPE = 'float16'\n",
    "\n",
    "# ES 접속\n",
    "ES_URL = 'http://10.10.4.10:9200/'             # elasticsearch 접속 url\n",
    "ES_INDEX_NAME = 'mpower_doc_128d_1'            # elasticsearch 인덱스명\n",
    "ES_INDEX_FILE = './data/mpower10u_128d_1.json' # 인덱스 생성 파일\n",
    "SEARCH_SIZE = 5                                # 검색 계수\n",
    "MIN_SCORE = 1.4                                # 검색 1.4 스코어 이하면 제거\n",
    "\n",
    "# 임베딩 모델 param\n",
    "MODEL_PATH = '../../data11/model/kpf-sbert-128d-v1'\n",
    "POLLING_MODE = 'mean'\n",
    "\n",
    "# LLM 모델 param\n",
    "lora_weights:str = '../../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'      # lora weight 경로\n",
    "llm_model_path:str ='../../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'     # llm 모델경로(KoAlpaca-Polyglot-5.8B 모델이 가장 속도가 빠르고, 잘 응답하는것 같음.)\n",
    "uselora_weight = False # Lora 사용하는 경우 True\n",
    "load_8bit = True       # 8bit 로딩 \n",
    "\n",
    "# prompt_template(모델에 따라 변경)\n",
    "PROMPT_DICT = {\n",
    "    #\"prompt_context\":(\"아래 내용을 가지고 질문에 대해 간략히 답변해 주세요\\n\\n### 내용: {context}\\n\\n### 질문: {query}\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"질문에 대해 간략히 답변해 주세요\\n\\n### 질문: {query}\\n\\n### 답변:\")\n",
    "     \n",
    "    \"prompt_context\":(\"### 질문: {query}\\n\\n아래 내용을 가지고 질문에 대해 간략히 답변해 주세요\\n\\n### 내용: {context}\\n\\n### 답변:\"),\n",
    "    \"prompt_no_context\":(\"### 질문: {query}\\n\\n질문에 대해 간략히 답변해 주세요\\n\\n### 답변:\")\n",
    "}\n",
    "#-------------------------------------------\n",
    "\n",
    "# prompt 테스트 \n",
    "print(f'\\n\\nprompt 테스트----------------------\\n')\n",
    "query='제주도의 크기는 얼마?'\n",
    "#context = ''\n",
    "context = '제주도는 최남단에 있는 섬으로, 길이는 40km가 되는 대한민국에서 가장큰 섬이다.'\n",
    "if context:\n",
    "    prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "else:\n",
    "    prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "    \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b593a7b-4d34-4c08-b625-d5a7cf45d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# 임베딩 모델 로딩\n",
    "#----------------------------------------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "from myutils import embed_text, bi_encoder, mpower_index_batch\n",
    "\n",
    "\n",
    "WORD_EMBDDING_MODEL1, BI_ENCODER1 = bi_encoder(model_path=MODEL_PATH, max_seq_len=512, do_lower_case=True, \n",
    "                                               pooling_mode=POLLING_MODE, out_dimension=OUT_DIMENSION, device=DEVICE)\n",
    "\n",
    "print(BI_ENCODER1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990059ea-bdc3-4328-9e0d-c2653cee6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# sLLM 모델 로딩\n",
    "#----------------------------------------------------------------------\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# tokenizer 로딩\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model_path)\n",
    "\n",
    "# 원본 모델 로딩\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(llm_model_path, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "if uselora_weight:\n",
    "    model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16) # loRA 모델 로딩\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ed00d-724c-49c0-8504-a1cd1a5834ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES 관련\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "#--------------------------------------------\n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "#--------------------------------------------\n",
    "def embedding(paragraphs:list)->list:\n",
    "    # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "    embeddings = embed_text(model=BI_ENCODER1, paragraphs=paragraphs, return_tensor=False).astype(FLOAT_TYPE)    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# ES임베딩 쿼리 벡터\n",
    "# -쿼리 임베딩 벡터를 구하고, es에 접속해서 rfile_text 뽑아냄\n",
    "#------------------------------------------------------------------------\n",
    "def es_embed_query(query:str):\n",
    "    assert query, f'query is empty'\n",
    "    \n",
    "    # 1.elasticsearch 접속\n",
    "    es = Elasticsearch(ES_URL)  \n",
    "\n",
    "    # 임베딩 구함.\n",
    "    start_time = time.time()\n",
    "    embed_query = embedding([query])[0]\n",
    "    \n",
    "    #print(len(embed_query))\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector1']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴\n",
    "                \"params\": {\"query_vector\": embed_query}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print(script_query)\n",
    "\n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=ES_INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\", \"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"*ES 검색시간: {:.2f} ms\".format(end_time * 1000)) \n",
    "\n",
    "    count = 0\n",
    "    docs = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        doc = {}  #dict 선언\n",
    "        doc['rfile_name'] = hit[\"_source\"][\"rfile_name\"]      # contextid 담음\n",
    "        doc['rfile_text'] = hit[\"_source\"][\"rfile_text\"]      # text 담음.\n",
    "        doc['score'] = hit[\"_score\"]\n",
    "        docs.append(doc) \n",
    "            \n",
    "    return docs\n",
    "#------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e7066-b17b-4eb1-8a44-b3332749e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "#--------------------------------------------\n",
    "# prompt 생성\n",
    "#--------------------------------------------\n",
    "def make_prompt(docs, query)->str:\n",
    "     # prompt 구성\n",
    "    context:str = ''\n",
    "\n",
    "    for doc in docs:\n",
    "        score = doc['score']\n",
    "        if score > MIN_SCORE:\n",
    "            rfile_text = doc['rfile_text']\n",
    "            if rfile_text:\n",
    "                context += rfile_text + '\\n\\n'\n",
    "                \n",
    "    if context:\n",
    "        prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "    else:\n",
    "        prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "                \n",
    "    # KoAlpaca 프롬프트\n",
    "    #prompt = f\"### 질문: {query}\\n질문에 대해 아래 내용을 바탕으로 간략히 답변해 주세요\\n\\n### 문맥: {context}\\n\\n### 답변:\" if context else f\"### 질문: {query}\\n질문에 대해 간략히 답변해 주세요\\n\\n### 답변:\"\n",
    "    \n",
    "    # llama 프롬프트\n",
    "    #prompt = f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {context}\\n{query} ### Response:\"\n",
    "\n",
    "    #print(prompt)\n",
    "    #print()\n",
    "    return prompt\n",
    "    \n",
    "#-----------------------------------------\n",
    "# text 생성\n",
    "#-----------------------------------------\n",
    "def generate_text(prompt):\n",
    "    \n",
    "    max_new_tokens = 256\n",
    "    eos_str = tokenizer.decode(tokenizer.eos_token_id)\n",
    "    start_time = time.time()\n",
    "\n",
    "    #prompt = query\n",
    "    #prompt = f\"### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {input_text}\\n\\n### 답변:\"\n",
    "    #prompt = f\"### 질문 : 간략히 답변해줘.{query}\\r\\n###답변:\"\n",
    "    #prompt = f\"### 질문: {query}\\n\\n### 답변:\"\n",
    "\n",
    "    #print(prompt)\n",
    "\n",
    "    # config 설정\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.5,\n",
    "        #top_p=0.75,\n",
    "        #top_k=40,\n",
    "        #num_beams=1,\n",
    "        bos_token_id=tokenizer.bos_token_id,  # 시작토큰 \n",
    "        eos_token_id=tokenizer.eos_token_id,  # end 토큰\n",
    "        pad_token_id=tokenizer.pad_token_id   # padding 토큰\n",
    "    )\n",
    "\n",
    "    # 프롬프트 tokenizer \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    #print(input_ids)\n",
    "\n",
    "    # Without streaming\n",
    "    # generate 처리\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    # 출력\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"*Text생성시간: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "    #print(output.replace(eos_str, ''))\n",
    "    #print()\n",
    "    return output.replace(eos_str, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69ccc9-5870-4d7a-b047-8f128dfbc4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 쿼리 입력후 테스트\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "\n",
    "def handle_query():\n",
    "    # 쿼리는 일반 쿼리인 경우 '일반##제주도 면적은 얼마?'\n",
    "    #query = '일반##제주도 면적은 얼마?' \n",
    "\n",
    "    query = input(\"질문:\")\n",
    "    \n",
    "    query_split = query.split('##')\n",
    "    prefix = query_split[0]\n",
    "    #print(f'prefix: {prefix}')\n",
    "    if prefix == '일반':\n",
    "        query1 = query_split[1]\n",
    "        prompt = make_prompt(docs='', query=query1)\n",
    "        print(generate_text(prompt))\n",
    "        print()\n",
    "    else:\n",
    "        query1 = query\n",
    "        docs = es_embed_query(query1)\n",
    "        print(docs)\n",
    "        print()\n",
    "        prompt = make_prompt(docs=docs, query=query1)\n",
    "        \n",
    "        print(generate_text(prompt))\n",
    "        print()\n",
    "        \n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0988be-a7f5-4ada-8d63-5184c7d80eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
