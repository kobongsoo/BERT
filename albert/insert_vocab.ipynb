{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e19a40-cd09-44b7-9008-922dadf1d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================================================================\n",
    "# albert-base-v2 토큰(sentenctpiece)에 신규 vocab을 추가하는 예\n",
    "#\n",
    "# 1) 말뭉치를 로딩하여, mecab(은전한닢)으로 형태소 분리 후 vocab.txt 파일로 저장함.\n",
    "#      => 이때 단어는 __ prefix 붙이고, subword는 안붙임.\n",
    "#      => 빈도수가 가장높은 단어들을 선정함\n",
    "#\n",
    "# 2) 기존 sentencepiece 모델(이하:sp)을 불러와서, 새롭게 생성한 vocab.txt에 단어들을 추가하여, 신규 sp 생성함\n",
    "#      => 이때 단어가 기존에 있는 단어인지 중복검사 하고, 혹시 단어가 null인지도 체크하여,\n",
    "#         신규 단어들을 기존 sp에 추가함.\n",
    "# \n",
    "# 3) (옵션) 필요에 따라 수동으로 필요한 단어들을 신규 sp에 재 추가함.\n",
    "#============================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa3418d3-705a-4eb4-b927-45096cde85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM, AlbertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b097bc-35aa-4918-ade4-46ee52d4a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "# 파일들 병합 하기 \n",
    "filenames = [\n",
    "    '../nlp_corpus/noxlsx2_dump/0000.txt',\n",
    "    '../nlp_corpus/noxlsx2_dump/0001.txt',\n",
    "    '../nlp_corpus/noxlsx2_dump/0002.txt'\n",
    "          ]\n",
    "\n",
    "out_file = '../data11/korpora/nlp_corpus_merge.txt'\n",
    "\n",
    "with open(out_file, 'w') as outfile:\n",
    "    for filename in filenames:\n",
    "        with open(filename) as file:\n",
    "            for line in file:\n",
    "                outfile.write(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668776a8-8f85-4a0b-9209-f744628f1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================\n",
    "# 1. 말뭉치를 로딩하여, mecab(은전한닢)으로 형태소 분리 후 vocab.txt 파일로 저장함.\n",
    "#=======================================================================================================\n",
    "# 말뭉치들을 불러옴\n",
    "corpora = [\n",
    "    '../nlp_corpus/noxlsx2_dump/0000.txt',\n",
    "    '../nlp_corpus/noxlsx2_dump/0001.txt',\n",
    "    '../nlp_corpus/noxlsx2_dump/0002.txt'\n",
    "          ]\n",
    "data = []\n",
    "for corpus in corpora:\n",
    "    with open(corpus, 'r', encoding='utf-8') as f:\n",
    "        data += f.read().split(' ') # 공백으로 구분해서 단어들을 추출함\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08664b33-e160-4ef8-b1e4-7b0ed23f92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[20000:20020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434c5f7-5d34-4e66-a041-183bf3d002a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 형태소 분석기를 이용하여, 읽어온 말뭉치를 단어,조사등으로 분리함\n",
    "# => 불용어는 제거함\n",
    "# => mecab으로 형태소 분할하면서, word 앞에는 prefix '__' 추가함\n",
    "\n",
    "# Mecab 선언\n",
    "mecab = Mecab()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords=['이','가','께서','에서','이다','의','을','를','에','에게','께','와','에서', \n",
    "           '라고', '과','은', '는', '부터','.',',']\n",
    "\n",
    "# Ture = nouns(명사)만 추출, False=형태소 추출\n",
    "nouns = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6059168-5551-4f32-a2c6-79f10d872acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab으로 형태소 혹은 명사만 분할할때, word 앞에는 prefix '▁' 추가함\n",
    "total_words=[]\n",
    "\n",
    "# 명사만 추출하는 경우\n",
    "if nouns == True:\n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.nouns(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count == 0:\n",
    "                    tmp = \"▁\" + tmp  #word 앞에는 prefix '▁' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "# 형태소도 포함하여 추출하는 경우\n",
    "else:\n",
    "    \n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.morphs(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count == 0:\n",
    "                    tmp = \"▁\" + tmp  ## word 앞에는 prefix '▁' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "                \n",
    "print(total_words[:20])\n",
    "print(f'총 단어 수: {len(total_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c981b0-c12e-44fe-9b63-f26b5296e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreqDist를 이용하여 빈도수 계산(*오래걸림)\n",
    "vocab = FreqDist(np.hstack(total_words))\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "# 최대 빈도수 가장높은 500개만 뽑아봄\n",
    "print(vocab.most_common(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec7f03-77f6-4ed5-864d-c80d9635bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 30000 개만 보존\n",
    "vocab_size = 30000\n",
    "vocab1 = vocab.most_common(vocab_size)\n",
    "\n",
    "vocab_len = len(vocab1)\n",
    "print('*단어 집합의 크기 : {}'.format(vocab_len))\n",
    "print('*마지막 단어 정보 : {}'.format(vocab1[vocab_len-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f217c-568d-4d26-bb8a-d563a9b41803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab을 list로 만듬\n",
    "new_vocab = []\n",
    "for index, word in tqdm(enumerate(vocab1)):\n",
    "    new_vocab.append(word[0])  # fword[0] 하면 단어만 추출\n",
    "    \n",
    "# 리스트 출력해봄\n",
    "print(new_vocab[50:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a53cb5-0ccd-47cd-a506-391e598d33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_vocab을 파일로 저장함\n",
    "new_vocab_out = '../data11/my/sp_vocab_2.txt'\n",
    "with open(new_vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(new_vocab):\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5d7383-a3d9-4ff2-a6b4-1e9e5250f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================\n",
    "# 2. 기존 sentencepiece 모델(이하:sp)을 불러와서, 새롭게 생성한 vocab.txt에 단어들을 추가하여, 신규 sp 생성함\n",
    "#=======================================================================================================\n",
    "\n",
    "# 기존 albert-base-v2 에 tokenizer(sentencepiece) 불러옴.\n",
    "import sentencepiece as spm\n",
    "import sentencepiece.sentencepiece_model_pb2 as spmodel\n",
    "\n",
    "smodel_path = '../data11/model/bert/albert-base-v2/spiece.model'\n",
    "m = spmodel.ModelProto()\n",
    "m.ParseFromString(open(smodel_path, 'rb').read())\n",
    "\n",
    "# 신규 단어 추가시, 중복 검사를 위해..\n",
    "# 기존 sentenctpiece vocab 목록을 리스트에 저장해 둠.\n",
    "vocab_list = []\n",
    "count = 0\n",
    "for i, piece in enumerate(m.pieces):\n",
    "    #print(piece.piece)\n",
    "    vocab_list.append(piece.piece)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8498da-dea8-4609-854e-2abbda1b2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_out = '../data11/my/sp_vocab_2.txt'\n",
    "with open(new_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    new_vocab = f.read().split('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc257f6a-83d7-4642-a46f-4d1f5340ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30001\n"
     ]
    }
   ],
   "source": [
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfdb9816-5bce-4c3a-9289-fea763f8ad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁국내',\n",
       " '▁매핑',\n",
       " '▁generated',\n",
       " '▁LDAP',\n",
       " '▁Transfer',\n",
       " '▁Rational',\n",
       " '▁errors',\n",
       " '▁Open',\n",
       " '▁15',\n",
       " '▁clients']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847a6085-fb26-42c5-a394-95dadb2bb939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca4e1b5-d773-4b33-b0d3-8d359d6c8fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9785e95fd544413485fa86b7e33a52a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "       \n",
    "# for문을 돌면서, 기존 vocab에 있는 단어인지 검사 후,\n",
    "# 없는 단어들만 추가함.\n",
    "for idx, vocab in tqdm(enumerate(new_vocab)):\n",
    "    \n",
    "    # 기존 vocab에 없는 단어들만 추가 \n",
    "    if vocab not in vocab_list:\n",
    "        #print(f'idx:{idx}, vocab:{vocab}')\n",
    "        # 공백이 아닌 vocab에 대해서만 입력\n",
    "        if vocab:\n",
    "            new_piece = type(m.pieces[0])()\n",
    "            new_piece.piece = vocab\n",
    "            new_piece.score = 0.0\n",
    "            new_piece.type = 1\n",
    "\n",
    "            m.pieces.append(new_piece)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e157deec-f2d5-4df4-93f8-761e538b90e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 54146\n",
      "▁함부로\n"
     ]
    }
   ],
   "source": [
    "# 추가해서 새롭게 생성된 vocab 마지막 단어를 출력해 봄\n",
    "pieces_len = len(m.pieces)\n",
    "print(f'len: {pieces_len}')\n",
    "print(m.pieces[pieces_len-1].piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed7c3b6-f5f6-431f-9e07-57870fa153cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델에 serialize 함.(저장함)\n",
    "new_smodel_path = '../data11/model/bert/albert-base-v2/spiece_new.model'\n",
    "with open(new_smodel_path, 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011a2e42-a14e-456b-9cc9-e03041d000c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁모코', '엠시스에서는', '▁문서', '중앙화', '▁및', '▁보안', '파일서버', '▁솔루션', '인', '▁엠', '파워를', '▁출시', '하였다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 새로운 spmodel 테스트 \n",
    "# => 추가된 단어들 별루 분리가 잘 된다.\n",
    "text = \"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "sp_new = spm.SentencePieceProcessor(model_file=new_smodel_path)\n",
    "print(sp_new.encode(text, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d9aa559-38a7-41fc-bd53-cff41409840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================\n",
    "# 3. 새로운 단어들을 신규 sp에 새롭게 추가함.\n",
    "#=======================================================================================================\n",
    "\n",
    "# 새롭게 저장된 tokenizer(sentencepiece) 불러옴.\n",
    "m_new = spmodel.ModelProto()\n",
    "m_new.ParseFromString(open(new_smodel_path, 'rb').read())\n",
    "\n",
    "# 신규 단어 추가시, 중복 검사를 위해..\n",
    "# 기존 sentenctpiece vocab 목록을 리스트에 저장해 둠.\n",
    "new_sp_list = []\n",
    "for i, piece in enumerate(m_new.pieces):\n",
    "    #print(piece.piece)\n",
    "    new_sp_list.append(piece.piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf1dbcb-45d3-4e36-9950-1a670637803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 단어들 추가\n",
    "new_vocab = ['▁문서중앙화', '▁보안파일서버', '▁엠파워', '▁Mpower', '▁EZis-C', '▁모코엠시스', '▁M드라이브']\n",
    "\n",
    "# for문을 돌면서, 기존 vocab에 있는 단어인지 검사 후,\n",
    "# 없는 단어들만 추가함.\n",
    "for idx, vocab in enumerate(new_vocab):\n",
    "    \n",
    "    # 기존 vocab에 없는 단어들만 추가 \n",
    "    if vocab not in new_sp_list:\n",
    "        #print(f'idx:{idx}, vocab:{vocab}')\n",
    "        new_piece = type(m_new.pieces[0])()\n",
    "        new_piece.piece = vocab\n",
    "        new_piece.score = 0.0\n",
    "        new_piece.type = 1\n",
    "\n",
    "        m_new.pieces.append(new_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4a1b16e-66a7-494f-a55d-04ff18fca925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델에 serialize 함.(저장함)\n",
    "new_smodel_path1 = '../data11/model/bert/albert-base-v2/spiece_new1.model'\n",
    "with open(new_smodel_path1, 'wb') as f:\n",
    "    f.write(m_new.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99cb2907-dcb6-4765-b1e2-6b82ebfb1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁모코엠시스', '에서는', '▁문서중앙화', '▁및', '▁보안파일서버', '▁솔루션', '인', '▁엠파워', '를', '▁출시', '하였다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 새로운 spmodel 테스트 \n",
    "# => 추가된 단어들 별루 분리가 잘 된다.\n",
    "text = \"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "sp_new = spm.SentencePieceProcessor(model_file=new_smodel_path1)\n",
    "print(sp_new.encode(text, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65178c-1b61-4ed3-93ef-40cdda5c4bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
