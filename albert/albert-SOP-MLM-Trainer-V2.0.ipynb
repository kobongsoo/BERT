{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# 허깅페이스 Trainer를 이용하여 SOP+MLM 훈련시키기\n",
    "#\n",
    "# => load_dataset 으로 wiki 연속된 문장이 있는 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# 연속된 문장인지 아닌지 SOP 문장을 만들고, (Lable, sentence_order_label 필드 추가)\n",
    "# 해당 문장 input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 pre-kowiki-20220620-2줄.txt 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "\n",
    "# Albert config\n",
    "# => albert-base-v2기준으로 생성\n",
    "# => 참조: https://huggingface.co/albert-base-v2/blob/main/config.json \n",
    "#\n",
    "# AlbertTokenizerFast\n",
    "# => AlbertTokenizerFast 인데 한국어도 토큰화되도록 하려면, tokenizer_config.json에 masked_token-normalized:true 로 해줘야함.\n",
    "# => 그렇지 않으면 한글 단어들은 모두 [unk] 으로 되어 버림 인데 \n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AlbertTokenizer, AlbertTokenizerFast, BertTokenizerFast, AlbertConfig, AlbertForMaskedLM, AlbertForPreTraining\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/Albert-MLM-Trainer_2022-11-29.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치\n",
    "# => 한줄에 연속된 문장이 있어야함.\n",
    "# -> .으로 구분된 한줄 문자이 아니라. 한줄에 .로구분된 여러문장이 이어진 문장이어야 함\n",
    "# -> 예시:'제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
    "# => 사전 만들때 동일한 말뭉치 이용.\n",
    "sep_string = ';$;'  # 말뭉치 2문장 구분자 => 문장A+[SET_STR]+문장B\n",
    "input_corpus = '../../data11/ai_hub/tl1/tl1-2줄.txt' #'../../data11/ai_hub/tl1/tl1-2줄-test.txt' \n",
    "eval_corpus = '../../data11/ai_hub/vl1/vl1-2줄-eval.txt' #\"../../data11/ai_hub/vl1/vl1-2줄-eval.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "bispretrain = True       # 새로 pretrain 할꺼면 =True, 기존모델에 Further pretrain할꺼면 = False\n",
    "model_path = \"albert-base-v2\"  \n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "#vocab_path=\"../../data11/ai_hub/vocab/tl1-1줄-mecab-30000\"  # bert vocab\n",
    "vocab_path=\"../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M-vocab\" ##albert vocab\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../data11/model/albert/albert-aihub-SOP+MLM-checkout'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 128      # batch_size\n",
    "token_max_len = 128   # token_seq_len\n",
    "epoch = 8             # epoch\n",
    "lr = 1e-4             # learning rate(기본:5e-5)\n",
    "weigth_decay = 0.01   # weigth_decay(기본:0.0, bert: 0.01)\n",
    "seed = 111\n",
    "do_lower_case_ = True # 1=영어 대문자를 소문자로 변경, 0=대.소문자 구분\n",
    "keep_acccents_ = False\n",
    "\n",
    "# 기본 <unk>, <pad> 인데, 변경된 경우에는 아래 값을 변경해주면됨\n",
    "unk_t ='[UNK]' #UNK 토큰 = <unk>\n",
    "pad_t ='[PAD]' #PAD 토큰 = <pad>\n",
    "\n",
    "############################################################################\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"Albert-MLM-Trainer\", logfilename=\"../../log/Albert-MLM-Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M-vocab is_fast:True\n",
      "*tokenizer_len:30000, special_token_size: 5, *tokenizer.vocab_size: 30000\n",
      "*num_param: 11813810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForPreTraining(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=30000, bias=True)\n",
       "    (activation): NewGELUActivation()\n",
       "  )\n",
       "  (sop_classifier): AlbertSOPHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# => AlbertTokenizer, AlbertTokenizerFast 둘중 사용하면됨.\n",
    "# => AlbertTokenizerFast 인데 한국어도 토큰화되도록 하려면, tokenizer_config.json에 normalized:true 로 해줘야함.\n",
    "# => 그렇지 않으면 한글 단어들은 모두 [unk] 으로 되어 버림\n",
    "\n",
    "tokenizer = AlbertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=do_lower_case_, keep_acccents=keep_acccents_, unk_token=unk_t, pad_token=pad_t)\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=do_lower_case_)\n",
    "\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'*{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "print('*tokenizer_len:{}, special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer), len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "\n",
    "if bispretrain == True:\n",
    "    # Albert 껍데기 만들기\n",
    "    # => albert-base-v2기준으로 생성\n",
    "    # => 참조: https://huggingface.co/albert-base-v2/blob/main/config.json\n",
    "    config = AlbertConfig(    \n",
    "        vocab_size=len(tokenizer), # default는 영어 기준이므로 내가 만든 vocab size에 맞게 수정해줘야 함\n",
    "        hidden_size = 768,         # default는 4096인데, base는 768로 함\n",
    "        num_attention_heads = 12,  # default는 64인데, base는 12로함\n",
    "        intermediate_size = 3072,  # default는 16384인데 base는 3072로 함hidden_size * 4 = base는 3072임\n",
    "        num_hidden_layers = 6      # default, base는 12인데, small버전으로 6으로 줄여봄\n",
    "    )\n",
    "\n",
    "    model = AlbertForPreTraining(config=config)\n",
    "else:\n",
    "    # 모델 로딩 further pre-training \n",
    "    model = AlbertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "\n",
    "    #################################################################################\n",
    "    # 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "    # 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "    # CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "    #  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "    #\n",
    "    #     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "    #     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "    #################################################################################\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f'*num_param: {model.num_parameters()}')\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d9f8614839c15f8a\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/default-d9f8614839c15f8a/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c506ddfe94da4e99930aa91942701b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 29365626\n",
      "    })\n",
      "})\n",
      "['정부, 무선국 검사제도 손본다;$;정부가 lte(롱텀에볼루션) 서비스 도입 등 바뀐 이동통신환경을 고려해 무선국 검사제도 규제개선을 추진한다.', '미래창조과학부는 효율적 전파관리 체계구축과 전파 이용자 편익증진을 위해 전파관리제도 개선 연구반이 도출한 무선국 검사제도 개선 방안을 시행한다고 16일 밝혔다.;$;개선안 주요 내용은 △표본검사대상 확대 및 표본비율 축소 △수시검사제도 도입 추진 △무선국 검사수수료 부과체계 합리화 등이다.', '미래부는 우선 이동통신사의 무선국 검사 부담 완화를 위해 현 준공검사 시 광중계기지국에 (이름) 시행되고 있던 표본검사를 준공검사 대상 전체 무선국으로 확대하고, 표본비율을 현 30%에서 향후 표본검사 시 불합격률을 고려해 단계적으로 20%까지 축소할 계획이다.;$;또 무선국 검사를 시행하는 비율을 축소할 경우 혼·간섭 없는 깨끗한 전파를 공급하기 위한 이동통신사업자들의 무선국 관리노력이 약화될 도덕적 해이가 우려돼 표본검사를 받지 않는 무선국에 대한 사후관리 제도인 수시검사 제도를 도입할 예정이다.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b0e0740236a65728\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/default-b0e0740236a65728/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72560caa845449a2bc6c0a4ae8c066fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7127\n",
      "    })\n",
      "})\n",
      "[\"청호나이스, '먹는물·씻는물 안심' 정수기로 프리미엄시장 공략;$;청호나이스가 2가지 정수 시스템을 이용해 먹는 물과 씻는 물 모두 (이름)는 하이브리드 얼음정수기 ‘도도’를 출시했다.\", '먹는 물에는 자사의 가장 뛰어난 필터 기술을 적용하고 식재료를 닦는 물에도 직수 필터 기술을 적용해 정수기 본질을 지킨 프리미엄 제품이라는 주장이다..;$;청호나이스는 환경오염과 생활 건강에 대한 소비자들 관심이 늘어나는 만큼 프리미엄 전략을 유지하며 직수 방식이나 다른 보급형 정수기를 쓰고 있는 가정을 고객으로 적극 끌어들일 계획이다.', '해외시장에도 활발히 나서 매년 국내외 제품 매출을 2배 수준으로 키운다는 포부다..;$;청호나이스는 9일 서울시 중구 소재 서울 웨스턴(이름)텔에서 하이브리드 얼음정수기 ‘도도’ 출시 발표회를 열었다..']\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "eval_dataset = load_dataset(\"text\", data_files=eval_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset=======================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['train']['text'][0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633edc59-9b29-4841-9026-1e547f704c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5bf602206947c5ab757bde4c57bc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29366 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#=================================================================================================\n",
    "# SOP 문장 만들기 \n",
    "#\n",
    "# => SOP는 주어진 한 쌍의 문장이 positive인지 negative인지 분류하는 이진 분류 문제이다.\n",
    "# 문장 1: 그는 김치볶음밥을 요리했다.\n",
    "# 문장 2: 맛있었다.\n",
    "# =>주어진 한 쌍의 문장을 보면, 문장 2가 문장 1 다음에 온다는 것을 알 수 있다. 이때를 positive(0)라고 한다.\n",
    "\n",
    "# 문장 1: 맛있었다.\n",
    "# 문장 2: 그는 김치볶음밥을 요리했다.\n",
    "# =>위 경우는 문장 순서가 바뀐 경우 이고 negetive(1)이다.\n",
    "#=================================================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "import random\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def tokenizer_function_sop(examples):\n",
    "     \n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    order_label = []\n",
    "\n",
    "    count = 0\n",
    "    for paragraph in examples['text']:\n",
    "        count += 1\n",
    "        # 하나의 문장을 읽어와서 .기준으로 나눈다.\n",
    "        sentences = [sentence for sentence in paragraph.split(sep_string) if sentence != '']\n",
    "        num_sentences = len(sentences)\n",
    "         \n",
    "         # . 기준으로 나눈 문장이 1이상이면..\n",
    "        if num_sentences > 1:\n",
    "            # 문장 a 시작번지는 랜덤하게, 해당 문장 이후로 지정\n",
    "            start = random.randint(0, num_sentences-2)\n",
    "            # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "            # 0.5 이상 랜덤값이면, 연속적인 문장으로 만듬\n",
    "            if random.random() >= 0.5:\n",
    "                # this is IsNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(sentences[start+1])\n",
    "                order_label.append(0)  #label=0이면 연속적\n",
    "            # 0.5 이하 랜덤값이면  순서를 바꿈\n",
    "            else:\n",
    "                # this is NotNextSentence\n",
    "                sentence_a.append(sentences[start+1])\n",
    "                sentence_b.append(sentences[start])\n",
    "                order_label.append(1)  #label=1이면 비연속적\n",
    "    \n",
    "    # ** return_overflowing_tokenis = False로 해야, 긴 문장인 경우 잘리더라도 다시 이어서 문장을 만들지 않는다.\n",
    "    # => 입력 문장은 10개인데, 긴문장이 포함된 경우 10개를 넘는 출력이 나옴\n",
    "    #\n",
    "    # ** albert 토크너나이즈 이용할때 아래 경고창 나옴. truncation='only_second' 하면 경고 몇개 안나오는데, 훈련할때 tensor size 오류 남.\n",
    "    #Be aware, overflowing tokens are not returned for the setting you have chosen, \n",
    "    # i.e. sequence pairs with the 'longest_first' truncation strategy. \n",
    "    # So the returned list will always be empty even if some tokens have been removed.\n",
    "    #\n",
    "    result = tokenizer(sentence_a, sentence_b, max_length=token_max_len, padding=True, truncation=True, return_overflowing_tokens=False)\n",
    "   \n",
    "    # next_sentence_label next_label 복사(**deepcopy)해서 추가\n",
    "    result['sentence_order_label'] = copy.deepcopy(order_label)\n",
    "     \n",
    "    return result\n",
    "\n",
    "# 훈련 NSP 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "%time train_dataset_fast = train_dataset.map(tokenizer_function_sop, batched=True)\n",
    "\n",
    "# 평가 NSP 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function_sop, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(f'*train_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset_fast[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][1]))\n",
    "\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(f'*eval_len:{len(eval_dataset_fast[\"train\"])}, len:{len(eval_dataset_fast[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(eval_dataset_fast['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(eval_dataset_fast['train']['input_ids'][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:1])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\norg===========================================\\r\\n')\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][0]))\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(mlm_train_sample['input_ids'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "epochs = epoch          # epochs\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.02)            # 평가 스탭수(0.02)\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "#save_total_limit=2                                # 마지막 2개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    learning_rate=lr,                       # lr: 기본 5e-5\n",
    "    weight_decay=weigth_decay,              # weigth_decay : 기본 = 0.0\n",
    "    per_device_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_strategy=\"epoch\",                  # 저장 전략 (no, epoch, steps 기본=steps) \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 (no, epoch, steps 기본=no)  \n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "#  SOP 토크처리된 훈련 데이터셋\n",
    "train_dataset_input = train_dataset_fast['train']\n",
    "eval_dataset_input = eval_dataset_fast['train']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_input,   # SOP 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_input,     # SOP 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/albert/albert-aihub-SOP+MLM/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
