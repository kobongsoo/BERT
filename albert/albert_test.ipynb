{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07847e08-52dc-49fb-bcd7-683544ee7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/model/albert/albert-aihub-SOP+MLM-checkout/checkpoint-1605933 is_fast:False\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================\n",
    "# ALBERT MASKED 예시\n",
    "#\n",
    "# AlbertTokenizerFast\n",
    "# => AlbertTokenizerFast 인데 한국어도 토큰화되도록 하려면, tokenizer_config.json에 masked_token-normalized:true 로 해줘야함.\n",
    "# => 그렇지 않으면 한글 단어들은 모두 [unk] 으로 되어 버림 인데 \n",
    "# \n",
    "#===========================================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertTokenizerFast, AutoTokenizer, BertTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "    \n",
    "do_lower_case_ = True # 1=영어 대문자를 소문자로 변경, 0=대.소문자 구분\n",
    "keep_acccents_ = False\n",
    "\n",
    "# 기본 <unk>, <pad> 인데, 변경된 경우에는 아래 값을 변경해주면됨\n",
    "unk_t ='[UNK]' #UNK 토큰 = <unk>\n",
    "pad_t ='[PAD]' #PAD 토큰 = <pad>\n",
    "\n",
    "model_path = '../../data11/model/albert/albert-aihub-SOP+MLM-checkout/checkpoint-1605933'\n",
    "#model_path = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-char/spiece.model'\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_path, max_len=128, do_lower_case=do_lower_case_, keep_acccents=keep_acccents_, unk_token=unk_t, pad_token=pad_t)\n",
    "#tokenizer = AlbertTokenizerFast.from_pretrained(model_path, max_len=128, do_lower_case=do_lower_case_, keep_acccents=keep_acccents_, nk_token=unk_t, pad_token=pad_t)\n",
    "\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(model_path, max_len=128, do_lower_case=do_lower_case_param)\n",
    "\n",
    "#tokenizer = KoBERTTokenizer.from_pretrained(model_path,max_len=128, do_lower_case=do_lower_case_, keep_acccents=keep_acccents_, nk_token=unk_t, pad_token=pad_t)\n",
    "\n",
    "print(f'{model_path} is_fast:{tokenizer.is_fast}')\n",
    "print(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5479bf4f-538c-412b-a985-b444cc7a0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5, 2534, 7333, 7260, 6676, 6, 1619, 3786, 515, 1663, 6676, 167, 342, 9182, 29521, 136, 1247, 3164, 845, 3733, 447, 3518, 234, 15726, 4898, 670, 6676, 1639, 2422, 10449, 224, 3518, 326, 3164, 4105, 541, 1294, 117, 1512, 1854, 1096, 326, 3274, 1771, 427, 1901, 1854, 6758, 2696, 429, 213, 3772, 670, 6676, 6], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] i love you.[SEP] 난 널 사랑해. 코로나(covid 19)는 2019년도에 시작되었다.모코엠시스에서는 문서 중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.[SEP]\n",
      "[CLS]\n",
      "i\n",
      "love\n",
      "you\n",
      ".\n",
      "[SEP]\n",
      "난\n",
      "널\n",
      "사랑\n",
      "해\n",
      ".\n",
      "코로나\n",
      "(\n",
      "co\n",
      "vid\n",
      "19\n",
      ")\n",
      "는\n",
      "2019\n",
      "년\n",
      "도\n",
      "에\n",
      "시작\n",
      "되\n",
      "었\n",
      "다\n",
      ".\n",
      "모\n",
      "코\n",
      "엠\n",
      "시스\n",
      "에\n",
      "서\n",
      "는\n",
      "문서\n",
      "중앙\n",
      "화\n",
      "및\n",
      "보안\n",
      "파\n",
      "일\n",
      "서\n",
      "버\n",
      "솔루션\n",
      "인\n",
      "엠\n",
      "파\n",
      "워\n",
      "를\n",
      "출시\n",
      "하\n",
      "였\n",
      "다\n",
      ".\n",
      "[SEP]\n",
      "5\n",
      "6\n",
      "0\n",
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Be aware, overflowing tokens are not returned for the setting you have chosen, \n",
    "# i.e. sequence pairs with the 'longest_first' truncation strategy. \n",
    "# So the returned list will always be empty even if some tokens have been removed.\n",
    "\n",
    "sentence_a = \"I love you.\"\n",
    "sentence_b = \"난 널 사랑해. 코로나(COVID 19)는 2019년도에 시작되었다.모코엠시스에서는 문서 중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "\n",
    "result = tokenizer(sentence_a, sentence_b, max_length=128, padding=True, truncation=True, return_overflowing_tokens=False)\n",
    "\n",
    "# tokenizer 출력\n",
    "print(result)\n",
    "\n",
    "print(tokenizer.decode(result.input_ids))\n",
    "\n",
    "# 토큰별루 출력\n",
    "for idx in range(len(result.input_ids)):\n",
    "    print(tokenizer.decode(result.input_ids[idx]))\n",
    "    if tokenizer.decode(result.input_ids[idx]) == '[PAD]':\n",
    "        break\n",
    "        \n",
    "        \n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('<UNK>')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "\n",
    "print(CLStokenid)\n",
    "print(SEPtokenid)\n",
    "print(UNKtokenid)\n",
    "print(PADtokenid)\n",
    "print(MASKtokenid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46a6c14-cef0-44dc-8885-c2f426892b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../data11/model/albert/albert-aihub-SOP+MLM-checkout/checkpoint-1605933 were not used when initializing AlbertForMaskedLM: ['sop_classifier.classifier.bias', 'sop_classifier.classifier.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*Input: 한국의 수도는 [MASK] 이다\n",
      "*[MASK] : ▁조금 (1530)\n",
      "\n",
      "\n",
      "*Input: 프랑스의 수도는 [MASK]이다\n",
      "*[MASK] : ▁프랑스 (1458)\n",
      "\n",
      "\n",
      "*Input: 충무공 이순신은 [MASK]에 최고의 장수였다\n",
      "*[MASK] : ▁우리 (184)\n",
      "\n",
      "\n",
      "*Input: 배가 아프면 [MASK]에 가서 진찰 받아야 한다\n",
      "*[MASK] : ▁병원 (501)\n",
      "\n",
      "\n",
      "*Input: 코로나에 걸리면, [MASK] 아프다.\n",
      "*[MASK] : ▁더 (137)\n",
      "\n",
      "\n",
      "*Input: 비행기는 [MASK]를 난다.\n",
      "*[MASK] : ▁안 (130)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163846/436655627.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits_pred=torch.argmax(F.softmax(logits[idx]), dim=1)\n"
     ]
    }
   ],
   "source": [
    "# masked 예시\n",
    "\n",
    "from transformers import AlbertForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = AlbertForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "text = ['한국의 수도는 [MASK] 이다', '프랑스의 수도는 [MASK]이다', \n",
    "        '충무공 이순신은 [MASK]에 최고의 장수였다', '배가 아프면 [MASK]에 가서 진찰 받아야 한다',\n",
    "        '코로나에 걸리면, [MASK] 아프다.','비행기는 [MASK]를 난다.'\n",
    "       ]\n",
    "\n",
    "tokenized_input = tokenizer(text, max_length=128, truncation=True, padding='max_length', return_tensors='pt')\n",
    "outputs = model(**tokenized_input)\n",
    "logits = outputs.logits\n",
    "mask_idx_list = []\n",
    "\n",
    "for tokens in tokenized_input['input_ids'].tolist():\n",
    "    token_str = [tokenizer.convert_ids_to_tokens(s) for s in tokens]\n",
    "    \n",
    "    # **위 token_str리스트에서 [MASK] 인덱스를 구함\n",
    "    # => **해당 [MASK] 안덱스 값 mask_idx 에서는 아래 출력하는데 사용됨\n",
    "    mask_idx = token_str.index('[MASK]')\n",
    "    mask_idx_list.append(mask_idx)\n",
    "    \n",
    "for idx, mask_idx in enumerate(mask_idx_list):\n",
    "    \n",
    "    logits_pred=torch.argmax(F.softmax(logits[idx]), dim=1)\n",
    "    mask_logits_idx = int(logits_pred[mask_idx])\n",
    "    # [MASK]에 해당하는 token 구함\n",
    "    mask_logits_token = tokenizer.convert_ids_to_tokens(mask_logits_idx)\n",
    "    # 결과 출력 \n",
    "    print('\\n')\n",
    "    print('*Input: {}'.format(text[idx]))\n",
    "    print('*[MASK] : {} ({})'.format(mask_logits_token, mask_logits_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efe60a-0405-4694-bc41-e80ecf8c7a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
