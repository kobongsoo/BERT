{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07847e08-52dc-49fb-bcd7-683544ee7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================================================\n",
    "# ALBERT Futher-PreTraining 예시\n",
    "# => MLM(Masked Language Model)만으로 Further Pre-Train 시키는 예시\n",
    "# => AlbertForMaskedLM 이용\n",
    "#===========================================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertTokenizerFast, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8a5df1-90aa-4e61-a4bb-0a8ac7e12d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lower_case = 1 # 1=영어 대문자를 소문자로 변경, 0=대.소문자 구분\n",
    "\n",
    "# 기본 <unk>, <pad> 인데, 변경된 경우에는 아래 값을 변경해주면됨\n",
    "unk_token ='[UNK]' #UNK 토큰 = <unk>\n",
    "pad_token ='[PAD]' #PAD 토큰 = <pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a95ae7a-d4a4-4508-bfb0-0714016837b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M-vocab/spiece.model is_fast:False\n",
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1678: FutureWarning: Calling AlbertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vocab_out_path = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M-vocab/spiece.model'\n",
    "#vocab_out_path = '../../data11/model/albert/albert-base-v2-ftp-5/'\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(vocab_out_path, max_len=128, do_lower_case=do_lower_case, unk_token=unk_token, pad_token=pad_token)\n",
    "#tokenizer = AlbertTokenizerFast.from_pretrained(vocab_out_path, max_len=128, do_lower_case=do_lower_case, unk_token=unk_token, pad_token=pad_token)\n",
    "\n",
    "print(f'{vocab_out_path} is_fast:{tokenizer.is_fast}')\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0de5d09-31bf-4503-b549-4d9201ba5fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   5, 2534, 7333, 7260, 6676,    6, 1619, 3786,  515, 1663,    6,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "text1 = \"I love you. \"\n",
    "text2 = \"난 널 사랑해\"\n",
    "token_ids = tokenizer.encode_plus(text1, text2, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145bcbef-b866-490c-88a2-920adf7fd899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love you.[SEP] 난 널 사랑해[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad7cb9b-7bd7-48be-8094-aa1cc6175459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "i\n",
      "love\n",
      "you\n",
      ".\n",
      "[SEP]\n",
      "난\n",
      "널\n",
      "사랑\n",
      "해\n",
      "[SEP]\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(token_ids.input_ids[0])):\n",
    "    print(tokenizer.decode(token_ids.input_ids[0][idx]))\n",
    "    if tokenizer.decode(token_ids.input_ids[0][idx]) == '[PAD]':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08189ffe-7403-4822-8f8f-a719bba5a27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd32cb1-81ca-4ae7-8c55-1fd09b828580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f41c1-935d-465f-8524-dcf185905f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325583e0-a530-4c36-902f-187fbe411794",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M/spm_22M_token.model'\n",
    "\n",
    "\n",
    "# AlbertTokenizer, AlbertTokenizerFast\n",
    "tokenizer = AlbertTokenizer.from_pretrained(vocab_path, max_len=128, do_lower_case=do_lower_case, unk_token=unk_token, pad_token=pad_token)\n",
    "#tokenizer = AlbertTokenizerFast.from_pretrained(vocab_file=vocab_path, max_len=128, do_lower_case=do_lower_case, unk_token=unk_token, pad_token=pad_token)\n",
    "\n",
    "print(f'{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "print(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06debbe-fd0f-40ec-af39-2f7c5101b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "vocab_out_path = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M-vocab/'\n",
    "tokenizer.save_pretrained(vocab_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e27c0-efa5-442b-bc8e-c505a7b8a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_out_path = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22M-vocab/'\n",
    "vocab_out_path = '../../data11/model/albert/albert-base-v2-ftp-5/'\n",
    "\n",
    "tokenizer = AlbertTokenizerFast.from_pretrained(vocab_out_path, max_len=128, do_lower_case=True, unk_token=unk_token, pad_token=pad_token)\n",
    "print(f'{vocab_out_path} is_fast:{tokenizer.is_fast}')\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48129c-4634-4ad8-8576-79f03d5c93ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f526cc-c18d-4a0a-8001-53d43e738fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2278f-e82f-4a20-8457-334227141752",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"난 널 사랑해\"\n",
    "token_ids = tokenizer.encode_plus(text, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9d16b-e45c-4dab-b3f0-1f99473b716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(token_ids.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f7563-5d7d-43a9-8c09-b039408417e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"]\n",
    "tokenized_input = tokenizer(text, max_length=128, truncation=True, padding='max_length', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e81c0f-11e6-46e7-aa1a-6e85acaac72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04c9af-c251-40d2-a445-d323bf9d0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "token_ids = tokenizer.encode_plus(text, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40a04a-0fc7-4192-84a8-2164215ea0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271d502-03f3-4aff-b433-3ac7f5050e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(token_ids.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99e4bf-7325-4938-978a-5c62266b4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(token_ids.input_ids[0])):\n",
    "    print(tokenizer.decode(token_ids.input_ids[0][idx]))\n",
    "    if tokenizer.decode(token_ids.input_ids[0][idx]) == '[PAD]':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480a865-8328-47cb-959b-b3f50bafef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('<UNK>')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "\n",
    "print(CLStokenid)\n",
    "print(SEPtokenid)\n",
    "print(UNKtokenid)\n",
    "print(PADtokenid)\n",
    "print(MASKtokenid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d45e6e-cfca-4687-9bce-cd21d9925c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
