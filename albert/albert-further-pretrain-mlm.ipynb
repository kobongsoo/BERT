{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07847e08-52dc-49fb-bcd7-683544ee7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/Albertfpt_2022-06-16.log\n"
     ]
    }
   ],
   "source": [
    "#===========================================================================================\n",
    "# ALBERT Futher-PreTraining 예시\n",
    "# => MLM(Masked Language Model)만으로 Further Pre-Train 시키는 예시\n",
    "# => AlbertForMaskedLM 이용\n",
    "#===========================================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM, AlbertModel\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import GPU_info, seed_everything, mlogging, AccuracyForMaskedToken, SaveBERTModel\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(333)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"Albertfpt\", logfilename=\"../../log/Albertfpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325583e0-a530-4c36-902f-187fbe411794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForMaskedLM were not initialized from the model checkpoint at ../../data11/model/albert/albert-ts-2022-06-15 and are newly initialized: ['predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertForMaskedLM(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(76043, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictions): AlbertMLMHead(\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (decoder): Linear(in_features=128, out_features=76043, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_path = '../../data11/model/albert/albert-ts-2022-06-15' # 앞에서 만들어진 spiece_new.model 이 있는 경로 지정해줌\n",
    "model_path = '../../data11/model/albert/albert-ts-2022-06-15'\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(vocab_path)\n",
    "print(len(tokenizer))\n",
    "\n",
    "model = AlbertForMaskedLM.from_pretrained(model_path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a04c9af-c251-40d2-a445-d323bf9d0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 54150,     1, 54146, 30010, 54147, 30219,     1, 54148,     1,\n",
      "         31858,     1,     9,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "token_ids = tokenizer.encode_plus(text, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c40a04a-0fc7-4192-84a8-2164215ea0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 54150,     1, 54146, 30010, 54147, 30219,     1, 54148,     1,\n",
       "         31858,     1,     9,     3,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4271d502-03f3-4aff-b433-3ac7f5050e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'모코엠시스 ⁇  문서중앙화 및 보안파일서버 솔루션 ⁇  엠파워 ⁇  출시 ⁇.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be30ff97-d35f-4e52-a368-a17255d094dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForMaskedLM(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(76043, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=76043, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신규 모델에 파일을 불러옴\n",
    "\n",
    "\n",
    "vocab_path = '../../data11/model/albert/albert-base-v2-ftp-4' # 앞에서 만들어진 spiece_new.model 이 있는 경로 지정해줌\n",
    "model_path = '../../data11/model/albert/albert-base-v2-ftp-4'\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(vocab_path)\n",
    "print(len(tokenizer))\n",
    "\n",
    "model = AlbertForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "# resize_token_embeddings 으로 신규 tokenizer 사이즈로 지정 해줌.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc9beef-e68e-4768-b255-91194d2f1836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17161227"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb381d56-ecb2-467a-b75f-107d1028db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 54150,     1, 54146, 30010, 54147, 30219,     1, 54148,     1,\n",
      "         31858,     1,     9,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# text tokenizer 해봄.\n",
    "text = \"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "token_ids = tokenizer.encode_plus(text, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2480a865-8328-47cb-959b-b3f50bafef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('<UNK>')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "\n",
    "print(CLStokenid)\n",
    "print(SEPtokenid)\n",
    "print(UNKtokenid)\n",
    "print(PADtokenid)\n",
    "print(MASKtokenid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9624f881-c469-4243-94d4-852e0c65f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:2, SEPid:3, UNKid:1, PADid:0, MASKid:4\n",
      "*maskvocablist_len: 51\n",
      "*maskvocablist: [30024, 30027, 30036, 30098, 30099, 30100, 30178, 30184, 30187, 30203, 30204, 30230, 30245, 30246, 30247, 30253, 30254, 30335, 30336, 30337, 30365, 30366, 2306, 30376, 30378, 829, 30495, 30496, 30653, 30654, 30655, 30666, 30667, 943, 33244, 34251, 34890, 35464, 36038, 36047, 36841, 37251, 37252, 37253, 37782, 38037, 38054, 38804, 38891, 38901, 52284]\n",
      "*corpus:../../data11/korpora/kowiki_20190620/wiki_20190620_mecab_false_0311.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:2, SEPtokenid:3, UNKtokenid:1, PADtokeinid:0, Masktokeid:4\n",
      "*total_line: 3748586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95f7c6c512848bfa0d69eff9033eb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3748586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf048c4b750428d8982ede8f607be19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3748586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*maskvocablist_len: 51\n",
      "*maskvocablist: [30024, 30027, 30036, 30098, 30099, 30100, 30178, 30184, 30187, 30203, 30204, 30230, 30245, 30246, 30247, 30253, 30254, 30335, 30336, 30337, 30365, 30366, 2306, 30376, 30378, 829, 30495, 30496, 30653, 30654, 30655, 30666, 30667, 943, 33244, 34251, 34890, 35464, 36038, 36047, 36841, 37251, 37252, 37253, 37782, 38037, 38054, 38804, 38891, 38901, 52284]\n",
      "*corpus:../../data11/korpora/kowiki_20190620/wiki_eval_test.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:2, SEPtokenid:3, UNKtokenid:1, PADtokeinid:0, Masktokeid:4\n",
      "*total_line: 114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806648e20bd240538643b4c422b35960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71b78740c724c22b0b9b5486d89b466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 54366, 35943,     4, 56862,     4, 57381,     4,    13,     1,\n",
      "        54270, 54170, 31074,  4890,     4, 30891, 37137,    13,     1, 30025,\n",
      "            4,     9,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([    2, 54366, 35943, 30034, 56862, 30034, 57381, 55303,    13,     1,\n",
      "        54270, 54170, 31074,  4890, 30405, 30891, 37137,    13,     1, 30025,\n",
      "           13,     9,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])}\n"
     ]
    }
   ],
   "source": [
    "input_corpus = '../../data11/korpora/kowiki_20190620/wiki_20190620_mecab_false_0311.txt'\n",
    "eval_corpus = '../../data11/korpora/kowiki_20190620/wiki_eval_test.txt'\n",
    "token_max_len = 128\n",
    "batch_size = 32\n",
    "#==============================================================================\n",
    "# DataLoader 설정으로 메모리 속도 계선방법\n",
    "#\n",
    "# 1) num_workers>0 설정\n",
    "# num_workers=0훈련 또는 이전 프로세스가 완료된 후에만 데이터 로드를 실행하는 반면,\n",
    "# num_workers > 0 값으로 설정하면 특히 I/O 및 대용량 데이터 증가에 대한 프로세스가 가속된다\n",
    "# => 따라서 Dataloader(dataset, num_workers=4*num_GPU) 식으로 설정함\n",
    "#\n",
    "# 2) pin_memory=True 설정\n",
    "# GPU는 CPU의 페이징 가능한 메모리에서 직접 데이터에 액세스할 수 없습니다. \n",
    "# 이 설정 pin_memory=True은 CPU 호스트의 데이터에 대한 스테이징 메모리를 직접 할당하고 \n",
    "# 페이징 가능 메모리에서 스테이징 메모리(즉, 고정된 메모리, 페이지 잠금 메모리)로 데이터를 \n",
    "# 전송하는 시간을 절약할 수 있다.\n",
    "# => Dataloader(dataset, pin_memory=True)\n",
    "#=========================================================================\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import MLMDataset\n",
    "\n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('<UNK>')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('<pad>')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "# mask 처리할 목록\n",
    "Maskvocab_list = [\n",
    "    '▁정보', '▁데이터', '▁서비스', '▁인터페이스', '▁환경', '▁통합', '▁이름', '▁저장', '▁성능', '▁연결', '▁테이블', \n",
    "    '▁웹', '▁Application', '▁소프트웨어', '▁관리자', '▁제품', '▁기간', '▁모듈', '▁구조', '▁네트워크', '▁호스트', \n",
    "    '▁정책', '▁software', '▁클라우드', '▁노드', '▁result', '▁명령', '▁시험', '▁업데이트', '▁Added', '▁로그인', \n",
    "    '▁클라이언트', '▁편집', '▁mark', '▁전파','▁다이얼로그', '▁디스플레이', '▁사무실', '▁오퍼레이션', '▁로그아웃', '▁유닉스', \n",
    "    '▁Soft', '▁관측', '▁윈도우즈', '▁시그널', '▁프린팅', '▁모바일게임', '▁생성자', '▁webserver', '▁카카오톡', '▁Keyboard'\n",
    "]\n",
    "\n",
    "train_dataset = MLMDataset(corpus_path = input_corpus,\n",
    "                           tokenizer = tokenizer, \n",
    "                           CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                           SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                           UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                           PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                           Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                           max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                           mlm_probability=0.15,\n",
    "                           overwrite_cache=False,\n",
    "                           Maskvocab_list = Maskvocab_list\n",
    "                          )\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          num_workers=4,   # *num_workers = 4*num_GPU 로 설정(프로세스가 빨라짐)\n",
    "                          pin_memory=True  # *pin_memory=True은 페이징 가능 메모리에서 스테이징 메모리로 전송하는 시간을 줄여줌\n",
    "                         )\n",
    "\n",
    "#===============================================================================\n",
    "# eval dataloader 생성\n",
    "eval_dataset = MLMDataset(corpus_path = eval_corpus,\n",
    "                          tokenizer = tokenizer, \n",
    "                          CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                          SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                          UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                          PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                          Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                          max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                          mlm_probability=0.15,\n",
    "                          overwrite_cache=False,\n",
    "                          Maskvocab_list = Maskvocab_list\n",
    "                          )\n",
    "\n",
    "\n",
    "# eval dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         #shuffle=True, # dataset을 섞음\n",
    "                         sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                         num_workers=4,   # *num_workers = 4*num_GPU 로 설정(프로세스가 빨라짐)\n",
    "                         pin_memory=True  # *pin_memory=True은 페이징 가능 메모리에서 스테이징 메모리로 전송하는 시간을 줄여줌\n",
    "                         )\n",
    "#===============================================================================\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977b20a-4df2-4eaf-b8f8-ad11a46f232f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953df4dc37c7419db25cdbcc4e443b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9c92b9d44f462286f46d44a6223b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 18:40:00,842 - Albertfpt - INFO - [Epoch 1/10] Iteration 22408 -> Train Loss: 0.0899, Train Acc: 0.5778, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-10 19:38:48,715 - Albertfpt - INFO - [Epoch 1/10] Iteration 44816 -> Train Loss: 0.0897, Train Acc: 0.5789, Val Acc:0.06181818181818182(17/275)\n",
      "2022-06-10 20:37:30,130 - Albertfpt - INFO - [Epoch 1/10] Iteration 67224 -> Train Loss: 0.0900, Train Acc: 0.5773, Val Acc:0.05818181818181818(16/275)\n",
      "2022-06-10 21:36:23,418 - Albertfpt - INFO - [Epoch 1/10] Iteration 89632 -> Train Loss: 0.0906, Train Acc: 0.5753, Val Acc:0.06909090909090909(19/275)\n",
      "2022-06-10 22:35:03,366 - Albertfpt - INFO - [Epoch 1/10] Iteration 112040 -> Train Loss: 0.0909, Train Acc: 0.5740, Val Acc:0.05090909090909091(14/275)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5026366c1d545c2bd0e641e3876e93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 23:34:04,715 - Albertfpt - INFO - [Epoch 2/10] Iteration 134448 -> Train Loss: 0.0891, Train Acc: 0.5787, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-11 00:32:39,884 - Albertfpt - INFO - [Epoch 2/10] Iteration 156856 -> Train Loss: 0.0890, Train Acc: 0.5788, Val Acc:0.06181818181818182(17/275)\n",
      "2022-06-11 01:31:21,423 - Albertfpt - INFO - [Epoch 2/10] Iteration 179264 -> Train Loss: 0.0883, Train Acc: 0.5809, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 02:30:08,686 - Albertfpt - INFO - [Epoch 2/10] Iteration 201672 -> Train Loss: 0.0879, Train Acc: 0.5825, Val Acc:0.06181818181818182(17/275)\n",
      "2022-06-11 03:28:58,640 - Albertfpt - INFO - [Epoch 2/10] Iteration 224080 -> Train Loss: 0.0873, Train Acc: 0.5837, Val Acc:0.05454545454545454(15/275)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da17b8bc73eb4caeb5abe3bd101eba62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 04:27:32,193 - Albertfpt - INFO - [Epoch 3/10] Iteration 246488 -> Train Loss: 0.0843, Train Acc: 0.5932, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 05:26:16,281 - Albertfpt - INFO - [Epoch 3/10] Iteration 268896 -> Train Loss: 0.0843, Train Acc: 0.5933, Val Acc:0.05090909090909091(14/275)\n",
      "2022-06-11 06:24:57,839 - Albertfpt - INFO - [Epoch 3/10] Iteration 291304 -> Train Loss: 0.0844, Train Acc: 0.5928, Val Acc:0.05818181818181818(16/275)\n",
      "2022-06-11 07:23:37,432 - Albertfpt - INFO - [Epoch 3/10] Iteration 313712 -> Train Loss: 0.0840, Train Acc: 0.5935, Val Acc:0.06909090909090909(19/275)\n",
      "2022-06-11 08:22:27,904 - Albertfpt - INFO - [Epoch 3/10] Iteration 336120 -> Train Loss: 0.0838, Train Acc: 0.5943, Val Acc:0.04363636363636364(12/275)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> save_model : ../../data11/model/albert/albert-base-v2-ftp-5batch:32-ep:10-lr:0.000030000-6m11d-8:22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387eafff20134e23874eed128d239969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 09:21:17,524 - Albertfpt - INFO - [Epoch 4/10] Iteration 358528 -> Train Loss: 0.0806, Train Acc: 0.6042, Val Acc:0.06181818181818182(17/275)\n",
      "2022-06-11 10:19:38,046 - Albertfpt - INFO - [Epoch 4/10] Iteration 380936 -> Train Loss: 0.0812, Train Acc: 0.6032, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 11:17:53,489 - Albertfpt - INFO - [Epoch 4/10] Iteration 403344 -> Train Loss: 0.0811, Train Acc: 0.6030, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 12:16:11,033 - Albertfpt - INFO - [Epoch 4/10] Iteration 425752 -> Train Loss: 0.0811, Train Acc: 0.6030, Val Acc:0.04363636363636364(12/275)\n",
      "2022-06-11 13:14:38,184 - Albertfpt - INFO - [Epoch 4/10] Iteration 448160 -> Train Loss: 0.0810, Train Acc: 0.6035, Val Acc:0.05454545454545454(15/275)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c867c7aea5434103a8b4b16f943cbeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 14:13:01,991 - Albertfpt - INFO - [Epoch 5/10] Iteration 470568 -> Train Loss: 0.0779, Train Acc: 0.6140, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 15:11:22,592 - Albertfpt - INFO - [Epoch 5/10] Iteration 492976 -> Train Loss: 0.0784, Train Acc: 0.6123, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-11 16:09:39,460 - Albertfpt - INFO - [Epoch 5/10] Iteration 515384 -> Train Loss: 0.0787, Train Acc: 0.6112, Val Acc:0.05090909090909091(14/275)\n",
      "2022-06-11 17:08:08,838 - Albertfpt - INFO - [Epoch 5/10] Iteration 537792 -> Train Loss: 0.0785, Train Acc: 0.6115, Val Acc:0.05090909090909091(14/275)\n",
      "2022-06-11 18:06:35,397 - Albertfpt - INFO - [Epoch 5/10] Iteration 560200 -> Train Loss: 0.0783, Train Acc: 0.6120, Val Acc:0.05818181818181818(16/275)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a4d25caf894e1e96611a68b6ff2b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 19:05:00,640 - Albertfpt - INFO - [Epoch 6/10] Iteration 582608 -> Train Loss: 0.0753, Train Acc: 0.6229, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-11 20:03:16,090 - Albertfpt - INFO - [Epoch 6/10] Iteration 605016 -> Train Loss: 0.0760, Train Acc: 0.6205, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 21:01:38,072 - Albertfpt - INFO - [Epoch 6/10] Iteration 627424 -> Train Loss: 0.0762, Train Acc: 0.6194, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-11 22:00:03,704 - Albertfpt - INFO - [Epoch 6/10] Iteration 649832 -> Train Loss: 0.0762, Train Acc: 0.6192, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-11 22:58:39,218 - Albertfpt - INFO - [Epoch 6/10] Iteration 672240 -> Train Loss: 0.0762, Train Acc: 0.6199, Val Acc:0.04363636363636364(12/275)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> save_model : ../../data11/model/albert/albert-base-v2-ftp-5batch:32-ep:10-lr:0.000030000-6m11d-22:58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc096303ad3c4418ad54e5462c944d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:57:34,988 - Albertfpt - INFO - [Epoch 7/10] Iteration 694648 -> Train Loss: 0.0732, Train Acc: 0.6303, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-12 00:56:29,935 - Albertfpt - INFO - [Epoch 7/10] Iteration 717056 -> Train Loss: 0.0737, Train Acc: 0.6285, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-12 01:55:23,208 - Albertfpt - INFO - [Epoch 7/10] Iteration 739464 -> Train Loss: 0.0739, Train Acc: 0.6279, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-12 02:54:16,254 - Albertfpt - INFO - [Epoch 7/10] Iteration 761872 -> Train Loss: 0.0740, Train Acc: 0.6273, Val Acc:0.05818181818181818(16/275)\n",
      "2022-06-12 03:53:04,741 - Albertfpt - INFO - [Epoch 7/10] Iteration 784280 -> Train Loss: 0.0742, Train Acc: 0.6270, Val Acc:0.05818181818181818(16/275)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7945bb28a24457bc83cd6e01dca6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 04:51:41,380 - Albertfpt - INFO - [Epoch 8/10] Iteration 806688 -> Train Loss: 0.0713, Train Acc: 0.6380, Val Acc:0.05454545454545454(15/275)\n",
      "2022-06-12 05:50:16,734 - Albertfpt - INFO - [Epoch 8/10] Iteration 829096 -> Train Loss: 0.0717, Train Acc: 0.6361, Val Acc:0.06181818181818182(17/275)\n",
      "2022-06-12 06:49:17,523 - Albertfpt - INFO - [Epoch 8/10] Iteration 851504 -> Train Loss: 0.0719, Train Acc: 0.6355, Val Acc:0.04363636363636364(12/275)\n",
      "2022-06-12 07:48:11,944 - Albertfpt - INFO - [Epoch 8/10] Iteration 873912 -> Train Loss: 0.0721, Train Acc: 0.6346, Val Acc:0.04727272727272727(13/275)\n",
      "2022-06-12 08:47:11,230 - Albertfpt - INFO - [Epoch 8/10] Iteration 896320 -> Train Loss: 0.0720, Train Acc: 0.6357, Val Acc:0.05818181818181818(16/275)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad35b0118bd48aa89514a02f8120a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################################\n",
    "epochs = 10            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "OUTPATH = '../../data11/model/albert/albert-base-v2-ftp-5'\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.2)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.3)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "total_test_correct = 0\n",
    "total_test_len = 0\n",
    "    \n",
    "list_train_loss = []\n",
    "list_train_acc = []\n",
    "list_validation_acc = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad(set_to_none=True)# 그래디언트 초기화(*set_to_none=True 로 설정하면, 그래디언트 업데이트시, 쓰기작업만 수행되어 속도가 빨라진다)\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "            \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "       \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            correct, masked_len = AccuracyForMaskedToken(logits, labels, input_ids, MASKtokenid)           \n",
    "            total_correct += correct.sum().item() \n",
    "            total_len += masked_len \n",
    "            #=========================================\n",
    "                \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                train_acc = total_correct/total_len\n",
    "                       \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "\n",
    "                #for data in tqdm(eval_loader):\n",
    "                for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data['input_ids'].to(device)\n",
    "                    attention_mask = data['attention_mask'].to(device)\n",
    "                    token_type_ids = data['token_type_ids'].to(device)       \n",
    "                    labels = data['labels'].to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       token_type_ids=token_type_ids,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        #loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "\n",
    "                        #===========================================\n",
    "                        # 정확도(Accurarcy) 계산\n",
    "                        correct, masked_len = AccuracyForMaskedToken(logits, labels, input_ids, MASKtokenid)           \n",
    "                        total_test_correct += correct.sum().item() \n",
    "                        total_test_len += masked_len \n",
    "                        #=========================================\n",
    "\n",
    "                val_acc = total_test_correct/total_test_len\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Acc:{}({}/{})'.format(epoch+1, epochs, itr, train_loss, train_acc, val_acc, total_test_correct, total_test_len))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_train_acc.append(train_acc)\n",
    "                list_validation_acc.append(val_acc)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "                total_test_correct = 0\n",
    "                total_test_len = 0\n",
    "                ####################################################################\n",
    "\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc6616-34d8-4df6-864d-f9f0fdc5ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a8046-22b0-47f2-81cf-92bec91f5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "#plt.plot(list_train_acc, label='Train Accuracy')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list_train_acc, label='Train Accuracy')\n",
    "plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d45e6e-cfca-4687-9bce-cd21d9925c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
