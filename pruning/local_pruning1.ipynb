{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b5dc02-9a9a-4467-a5cf-3a141a2c11be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#===================================================================================================\n",
    "# 로컬 pruning(가지치기)\n",
    "# - torch.nn.utils.prune 모듈을 활용한 BERT 모델에 대해 로컬 weight, bias들을 pruning 함\n",
    "#\n",
    "# 참고 자료 : https://huffon.github.io/2020/03/15/torch-pruning/\n",
    "# 참고 소스 : https://github.com/Huffon/nlp-various-tutorials/blob/master/pruning-bert.ipynb\n",
    "#===================================================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# pruning 을 위해서는 1.4.0 이상 torch가 필요\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536be8cf-a50f-45cf-a77a-c6503d004528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델 로딩\n",
    "from transformers import BertModel\n",
    "model_path = '../../../model/bert/bmc-fpt-bong_corpus_mecab-0428'\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97401449-d059-450e-b487-0671157ef7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (4): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (5): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (6): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (7): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (8): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (9): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (10): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (11): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 layer \n",
    "model.encoder.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2274f9d-2c5e-45be-a150-25e365fd5198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSelfAttention(\n",
       "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention 모듈만 불러옴\n",
    "model.encoder.layer[0].attention.self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6057b2f2-11ea-4374-911e-c1baf4ae4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention 모듈에서 key에 Pruning 적용 해볼것임\n",
    "# key에 named_parameters()에는 weigth, bias가 있음.\n",
    "module = model.encoder.layer[0].attention.self.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b17f6c-9bda-469e-bb22-5303f8ac35ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 8.0064e-03, -7.2079e-02, -4.5409e-02,  ...,  1.3022e-02,\n",
       "           -4.1921e-02, -1.8508e-02],\n",
       "          [ 4.7319e-02,  5.6065e-02,  7.8608e-03,  ..., -1.5596e-02,\n",
       "            1.1952e-02,  1.3859e-02],\n",
       "          [-1.3696e-02, -4.2909e-02,  4.8609e-03,  ..., -3.5625e-03,\n",
       "           -9.7081e-02, -5.2629e-02],\n",
       "          ...,\n",
       "          [-2.3917e-02, -3.2999e-02,  8.4614e-03,  ...,  8.6428e-05,\n",
       "            1.8967e-04,  1.8264e-03],\n",
       "          [-9.8836e-02, -8.1632e-02, -2.3575e-02,  ..., -1.8363e-02,\n",
       "           -5.1585e-02, -1.2072e-01],\n",
       "          [-1.6689e-02, -4.9597e-02, -2.4316e-02,  ...,  4.4092e-02,\n",
       "           -3.1870e-02,  3.8483e-02]], requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.0594e-03,  7.3360e-04,  5.4275e-03, -1.3735e-03,  2.9526e-03,\n",
       "           5.4779e-03,  2.9630e-03,  6.7515e-03,  4.7061e-03,  3.2703e-03,\n",
       "          -2.9103e-03,  1.1439e-03, -4.9434e-03,  4.8971e-03,  1.2137e-02,\n",
       "          -5.1321e-04, -5.4592e-03, -9.2707e-04,  5.4551e-03, -4.9731e-03,\n",
       "          -6.3760e-03, -2.2797e-03,  9.0375e-03,  6.7416e-03, -5.5720e-03,\n",
       "          -8.4298e-03,  2.1314e-03, -7.0118e-03,  1.4222e-03, -3.8760e-04,\n",
       "           4.2457e-04, -2.0165e-03,  4.2753e-03,  5.7197e-03, -7.3704e-03,\n",
       "           5.8238e-03, -3.5565e-03, -4.7996e-03, -4.2372e-03, -5.0620e-03,\n",
       "          -1.4990e-02, -1.2310e-03,  1.3331e-03, -4.7811e-03,  3.9264e-03,\n",
       "          -3.6671e-03, -5.8450e-04,  4.4344e-03, -7.1720e-03,  2.5168e-03,\n",
       "          -4.0643e-03,  4.9319e-03, -2.3885e-04, -5.4040e-04,  1.5722e-03,\n",
       "           7.4370e-03,  5.1830e-03, -8.6837e-03,  4.1268e-03, -1.6794e-03,\n",
       "          -2.8951e-03,  2.9707e-03,  7.9118e-03, -5.1855e-03,  2.5876e-03,\n",
       "           1.5904e-03,  7.5800e-03, -3.5597e-04, -5.9231e-03,  6.8127e-03,\n",
       "           2.4826e-03,  3.7108e-03,  2.9196e-03,  3.6116e-03,  7.9443e-03,\n",
       "           3.0862e-03, -8.6543e-03,  1.5662e-03,  2.4446e-04, -3.0337e-03,\n",
       "           1.9950e-03, -2.4247e-05,  1.0908e-03, -1.6533e-03,  1.6330e-03,\n",
       "           7.7977e-03,  1.7669e-03,  4.4636e-03,  1.0159e-03, -3.4746e-03,\n",
       "           2.4436e-03,  5.7131e-03,  4.2470e-03, -5.5416e-03, -5.7216e-03,\n",
       "          -4.3841e-03,  3.8319e-03,  5.9825e-03,  6.1899e-03,  4.9420e-03,\n",
       "          -4.0541e-03, -1.3266e-03,  4.5631e-03, -2.1548e-03, -3.9163e-03,\n",
       "          -3.1150e-03, -2.9764e-03,  5.0070e-04,  2.5498e-03, -6.9606e-03,\n",
       "          -7.9816e-04, -4.7175e-03, -1.3018e-03, -1.9629e-03, -3.5828e-03,\n",
       "          -5.3087e-03,  4.1970e-03,  2.5610e-03, -3.1208e-03,  1.0011e-03,\n",
       "           1.9064e-03, -3.0591e-03, -1.7905e-03, -9.0249e-04,  2.3697e-03,\n",
       "          -2.3032e-03, -4.0901e-03, -1.3419e-03, -8.8246e-03, -9.4603e-03,\n",
       "          -2.5765e-03, -1.8946e-03,  2.5719e-03, -6.7492e-05,  5.9619e-03,\n",
       "           2.7534e-03,  3.1956e-03, -1.1016e-02, -1.7439e-03, -6.5044e-04,\n",
       "          -2.6204e-03,  3.0428e-04, -5.8721e-03,  2.7891e-03, -3.1360e-04,\n",
       "          -8.8855e-03,  4.4342e-04, -2.2926e-03, -3.9622e-04, -5.2779e-07,\n",
       "           1.7250e-03,  5.3064e-03,  1.1023e-03, -6.9703e-03,  2.4717e-03,\n",
       "          -7.1629e-03,  7.6938e-04,  3.0848e-03, -3.8789e-04, -1.7353e-03,\n",
       "           9.7585e-04, -6.6693e-03, -5.4362e-03,  7.7898e-03,  6.0392e-04,\n",
       "           1.2048e-03,  6.6547e-03, -3.3710e-03,  4.2173e-04,  2.5011e-04,\n",
       "          -1.2513e-02,  1.1305e-03, -4.8391e-04, -2.6244e-03, -1.9154e-03,\n",
       "           9.1177e-04, -5.1075e-03,  2.2962e-03, -6.9221e-03,  1.9701e-03,\n",
       "          -4.4487e-03,  4.0503e-03, -1.6533e-03, -8.0324e-03,  9.8980e-03,\n",
       "           2.3873e-04, -7.0287e-03,  1.4790e-03, -9.0560e-03,  2.7626e-03,\n",
       "           7.6348e-03, -3.8236e-03,  2.1748e-03,  5.6715e-03, -4.1079e-03,\n",
       "          -8.6330e-03,  2.7571e-03,  1.0436e-04,  6.7715e-03, -7.0711e-03,\n",
       "           2.2036e-03, -7.8541e-03, -5.0218e-03, -2.9681e-03,  4.8014e-03,\n",
       "          -1.5437e-03, -2.7398e-03, -2.3554e-03,  2.5706e-04,  1.0561e-03,\n",
       "          -9.7760e-04, -3.3922e-03,  5.5928e-03, -6.7160e-03,  3.3686e-03,\n",
       "           4.3242e-03, -5.4408e-03,  1.5408e-03,  1.0058e-04, -7.9996e-04,\n",
       "          -3.2629e-03, -2.2161e-03, -1.3177e-03, -5.1807e-05,  2.3609e-03,\n",
       "           4.9950e-03, -2.0602e-03, -5.4615e-03,  2.8950e-04,  2.0184e-03,\n",
       "           4.7532e-03,  5.4658e-03,  4.3320e-03, -1.0169e-03,  1.0637e-02,\n",
       "          -4.3951e-03,  4.9507e-03,  8.5549e-03,  3.3624e-03, -7.0672e-03,\n",
       "           9.0887e-04,  1.4394e-03,  1.5079e-03,  5.6342e-03, -9.1224e-03,\n",
       "          -3.6396e-03, -2.1828e-03,  4.9893e-03, -3.6264e-03,  9.5204e-05,\n",
       "          -2.1416e-03, -3.7578e-03, -6.5791e-04, -2.7338e-03,  3.7211e-03,\n",
       "          -6.4095e-04,  6.2848e-03,  4.4657e-03, -8.9693e-04, -2.8121e-03,\n",
       "           4.7351e-03,  1.9476e-03,  1.6045e-03, -1.1358e-03,  4.0842e-03,\n",
       "          -9.0565e-03,  1.8678e-03,  8.2763e-03, -2.0891e-03, -6.1725e-03,\n",
       "          -3.8683e-03, -5.6557e-03,  3.6121e-04,  5.6306e-03, -1.0951e-03,\n",
       "           4.2146e-03, -4.4165e-04,  5.4636e-04,  8.3873e-03, -4.5464e-03,\n",
       "          -5.4504e-04,  1.1124e-03,  6.7319e-04,  3.3140e-03,  7.0477e-04,\n",
       "           4.5297e-03, -4.1698e-03,  1.6159e-03,  4.5990e-03,  1.7277e-04,\n",
       "          -1.8935e-03, -2.4641e-03,  2.4040e-03, -1.4627e-03, -4.0213e-03,\n",
       "           3.1960e-03,  4.2126e-03,  2.7681e-04,  4.9316e-03,  2.9775e-03,\n",
       "           2.0247e-04,  3.3949e-03, -3.0399e-03, -5.9004e-04, -1.5002e-03,\n",
       "           1.5973e-03,  4.3539e-03,  2.7664e-03,  6.4046e-04, -6.4387e-03,\n",
       "          -2.8438e-03,  7.9306e-03,  3.5898e-03, -2.4789e-03, -8.8152e-04,\n",
       "          -1.2068e-03, -9.2806e-04, -1.4852e-03, -7.5694e-03, -6.4826e-04,\n",
       "           2.6812e-03, -1.2762e-03, -5.3144e-03,  2.9498e-03, -3.1411e-03,\n",
       "          -1.2730e-03,  1.1371e-03,  5.1353e-03, -2.9256e-03, -7.5396e-03,\n",
       "           3.9064e-03, -4.7005e-03, -2.0528e-03,  1.2676e-04,  2.9950e-03,\n",
       "          -2.3699e-03,  3.1314e-03, -9.5319e-03,  4.2820e-04, -4.4032e-03,\n",
       "          -2.9295e-03, -4.2719e-03,  1.6849e-03, -1.1095e-03,  2.1484e-05,\n",
       "           9.6609e-05, -3.0547e-05, -7.5023e-04, -1.7178e-03,  1.1481e-02,\n",
       "          -3.4957e-03, -9.1051e-04, -7.6060e-03, -1.8178e-03,  4.8592e-03,\n",
       "          -7.6999e-04, -2.0050e-03,  8.9431e-04, -3.4881e-03,  4.0137e-04,\n",
       "           1.7924e-03,  1.3335e-04, -1.4130e-03, -3.0965e-03,  1.5006e-03,\n",
       "          -4.8251e-03,  1.0819e-02,  1.6536e-03, -2.2440e-03,  5.9803e-03,\n",
       "           5.9664e-04, -2.1570e-03, -2.5898e-03,  7.5594e-03, -1.3107e-03,\n",
       "          -4.2581e-03, -3.1261e-03,  2.2403e-03,  8.4835e-03, -2.2481e-03,\n",
       "          -1.3518e-03, -6.4194e-05, -2.6891e-03,  1.8407e-03,  8.2444e-03,\n",
       "          -1.6562e-03, -5.8601e-03, -5.1697e-03,  5.3567e-03, -1.0270e-02,\n",
       "          -5.2720e-03, -5.0548e-03,  7.9132e-03, -7.8214e-03,  1.3451e-03,\n",
       "           8.1410e-04,  1.7276e-03, -2.9032e-03, -1.2550e-02,  7.9872e-03,\n",
       "           3.6167e-03, -1.3335e-02,  7.5158e-04, -2.8252e-03, -2.7430e-03,\n",
       "           3.8431e-03,  4.6080e-04, -1.3090e-03,  5.2109e-03,  6.2680e-03,\n",
       "           1.9884e-03, -4.4133e-03,  7.9712e-03,  1.0295e-02,  1.5195e-02,\n",
       "           1.5326e-03,  2.9530e-04,  2.3002e-03,  3.9347e-03,  5.3343e-03,\n",
       "           1.8077e-03,  7.6865e-03, -1.0675e-03, -6.7227e-03,  9.4528e-03,\n",
       "          -8.0139e-03,  5.8060e-03, -1.6217e-03,  3.8840e-03,  4.2952e-03,\n",
       "           8.7806e-04, -1.0222e-03,  1.1811e-02, -5.7697e-03,  6.9922e-03,\n",
       "          -1.3790e-02,  7.2457e-04,  1.5484e-02,  2.7183e-03, -5.3224e-03,\n",
       "          -3.7296e-03, -2.6280e-03, -9.5657e-03,  1.3827e-02,  5.3420e-03,\n",
       "          -1.3356e-03,  3.6706e-03,  6.9164e-03,  4.3925e-03, -1.5419e-03,\n",
       "           4.1382e-03, -4.0516e-03, -4.1103e-03,  3.0949e-04, -9.4350e-05,\n",
       "          -6.9190e-03,  1.9689e-03,  2.2831e-03, -1.4378e-03, -6.8970e-04,\n",
       "           1.1415e-03,  9.4667e-04, -7.2704e-03, -2.9382e-03,  7.2932e-03,\n",
       "           3.1627e-04,  2.7267e-03,  4.2801e-03, -6.0559e-03, -5.5708e-03,\n",
       "           1.2757e-03, -3.5795e-03, -7.2389e-03, -1.7668e-03,  1.8800e-03,\n",
       "           3.0886e-03,  2.1105e-03,  1.5674e-03,  1.2828e-03,  5.9259e-03,\n",
       "           8.4010e-04, -1.6178e-03,  1.0639e-03,  3.5721e-03, -2.8771e-03,\n",
       "           3.1407e-03,  8.1162e-04, -9.5287e-03, -2.0651e-03,  4.2698e-03,\n",
       "          -2.0288e-03,  2.5148e-03,  1.6169e-03, -2.8961e-03, -1.0559e-03,\n",
       "          -4.0905e-03,  9.4554e-04,  3.0283e-03, -5.2308e-03,  2.0919e-03,\n",
       "           3.8192e-03, -1.2990e-03,  3.8938e-03, -1.5828e-03,  1.0480e-03,\n",
       "           2.5161e-03, -3.4312e-03,  3.8059e-03,  8.2166e-04,  4.0572e-03,\n",
       "           3.1652e-03,  2.1724e-03, -5.9991e-03,  5.7763e-03, -7.6956e-03,\n",
       "          -5.4374e-03, -3.1216e-04, -3.2782e-03, -4.3137e-03, -3.2482e-03,\n",
       "           2.7950e-03, -6.8682e-04, -1.6691e-03,  2.8336e-03, -2.0973e-03,\n",
       "          -6.6023e-03,  1.5846e-03, -5.0377e-03,  1.9074e-03,  2.4591e-03,\n",
       "          -3.9174e-03, -1.2733e-03,  1.0145e-03, -4.4710e-03,  2.9291e-03,\n",
       "          -9.5645e-04, -3.1429e-04, -2.2444e-03, -4.0813e-03, -3.7001e-03,\n",
       "           2.2773e-03, -2.2256e-03, -5.2029e-03,  7.8617e-03,  2.1254e-03,\n",
       "          -1.8045e-03, -3.9910e-03,  1.9054e-03, -9.5417e-03,  5.0948e-03,\n",
       "          -4.0267e-04,  3.8395e-03,  2.9889e-03,  8.9627e-03,  9.6131e-04,\n",
       "          -1.7270e-03, -2.3549e-03,  2.1966e-03,  3.3036e-03, -4.8108e-04,\n",
       "           3.8014e-03, -8.1695e-05,  2.3977e-04,  3.7977e-03,  3.2262e-03,\n",
       "          -5.8078e-03,  9.9914e-05,  1.7071e-03, -1.2467e-03,  3.1293e-03,\n",
       "           2.4645e-03, -2.5845e-03, -3.4412e-03, -2.7490e-03,  2.6711e-04,\n",
       "           1.1518e-03, -9.5652e-04,  8.7983e-03,  4.3791e-03, -6.1024e-03,\n",
       "          -1.9999e-02,  4.4730e-03, -7.9730e-04,  2.0613e-03, -1.2412e-02,\n",
       "           1.4591e-02,  1.2318e-02,  4.5111e-03,  4.7868e-03,  3.4282e-03,\n",
       "          -2.9516e-02, -6.3859e-03,  1.7355e-02,  1.2119e-02, -1.2059e-02,\n",
       "           1.0308e-02, -8.9580e-03,  1.3823e-02, -8.1396e-03,  7.3271e-03,\n",
       "           1.0888e-02, -2.8236e-03,  8.1908e-03,  1.2593e-02, -3.4558e-03,\n",
       "           7.6344e-04,  2.3814e-03, -1.2000e-02,  1.2937e-02, -9.3714e-03,\n",
       "          -6.4529e-03, -1.5321e-02,  1.0610e-02,  4.2802e-04,  1.3208e-02,\n",
       "           5.3129e-03, -6.5369e-03,  8.9730e-03, -3.8083e-03,  1.2664e-02,\n",
       "           6.9007e-03,  3.6855e-03,  1.1223e-02,  7.8671e-03,  1.2608e-03,\n",
       "          -3.5934e-03,  8.5362e-03,  7.7392e-03, -6.2439e-04,  6.8258e-03,\n",
       "           1.7344e-03, -4.5699e-03,  5.1521e-03,  7.7407e-03,  4.7372e-03,\n",
       "           4.2346e-03,  3.7088e-03,  9.2489e-03,  1.1715e-02, -4.3824e-03,\n",
       "          -3.4954e-03,  5.5534e-03,  8.3839e-04,  2.5224e-03,  9.5239e-03,\n",
       "          -4.8281e-03,  1.2804e-03,  7.7164e-04, -8.6801e-03, -1.1034e-03,\n",
       "          -5.4081e-03, -2.8767e-03, -1.1854e-03,  5.9572e-03,  6.5750e-03,\n",
       "           9.9296e-04,  2.2244e-03,  1.6213e-03, -6.3055e-03, -3.4300e-03,\n",
       "           1.4805e-03,  3.5275e-03, -1.5863e-03, -4.3710e-04, -5.3842e-04,\n",
       "           1.9117e-03, -9.1832e-03,  4.3838e-03, -6.9584e-04, -3.9889e-03,\n",
       "          -4.8672e-03,  7.2911e-03,  8.7121e-03, -4.5809e-03,  6.0049e-03,\n",
       "          -6.1273e-03, -1.2031e-04, -3.3488e-03, -1.2943e-02, -8.3492e-03,\n",
       "           3.5334e-03, -9.5223e-03, -2.1647e-03, -4.9706e-03, -6.3692e-03,\n",
       "           4.2424e-03,  4.3901e-03,  1.7415e-03,  1.7827e-03,  5.5266e-04,\n",
       "           6.0014e-03, -9.2381e-03,  1.3176e-03, -1.0151e-02, -6.6623e-03,\n",
       "           7.5403e-03,  1.2798e-02,  1.1754e-03,  3.1368e-03,  1.0764e-02,\n",
       "           1.1818e-02,  1.0479e-02,  3.5519e-03, -6.3688e-03, -5.3454e-03,\n",
       "           3.0223e-03,  2.2920e-03,  1.9793e-03,  2.6656e-03, -5.3251e-03,\n",
       "          -3.0441e-03,  1.6099e-03,  7.8349e-04,  3.9613e-03,  2.2048e-03,\n",
       "          -9.7030e-03,  9.0286e-03, -4.2312e-03, -4.9334e-03, -1.2192e-03,\n",
       "           1.0275e-03, -3.4095e-03,  2.2985e-03,  1.2645e-03, -2.2265e-03,\n",
       "          -7.4074e-03, -1.2246e-04, -3.4760e-03, -1.3233e-03, -7.3279e-03,\n",
       "          -2.1692e-03,  3.0799e-03, -3.4639e-03, -3.1355e-03,  2.9041e-03,\n",
       "           1.8976e-03, -1.3619e-03,  4.6187e-03,  1.1680e-03, -4.3926e-04,\n",
       "          -3.6108e-03, -2.8486e-03,  5.8254e-03,  5.5916e-03, -4.6303e-03,\n",
       "           1.7158e-03, -8.0431e-04,  5.0892e-03,  5.2776e-03, -1.1302e-03,\n",
       "          -2.6286e-03, -3.6092e-03, -6.5174e-03, -1.5916e-03,  1.6768e-03,\n",
       "           2.3379e-03, -2.4745e-03, -1.4414e-03,  1.2543e-03, -7.3630e-04,\n",
       "           4.3146e-03, -3.9160e-03, -7.9162e-04,  7.1797e-03, -3.1713e-03,\n",
       "           2.3081e-03,  6.8095e-03, -2.5026e-03], requires_grad=True))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfe1630-baba-4c21-b95d-6d656e5711e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# named_buffers 에는 아무것도 없음.\n",
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e535d507-5a62-4c29-ba09-38aed49b10b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruning 시작\n",
    "# key에서 weigth 중 임의로 30% 파라미터에 pruning 적용\n",
    "# => 첫번째인자 : 모듈명, name=모듈에 적용할 파라메터명(weight, bias등), amount=0~1 사이의 pruing할 퍼센테지\n",
    "prune.random_unstructured(module, name='weight', amount=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e21a33-38b3-43bb-8215-921001fad700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.0594e-03,  7.3360e-04,  5.4275e-03, -1.3735e-03,  2.9526e-03,\n",
       "           5.4779e-03,  2.9630e-03,  6.7515e-03,  4.7061e-03,  3.2703e-03,\n",
       "          -2.9103e-03,  1.1439e-03, -4.9434e-03,  4.8971e-03,  1.2137e-02,\n",
       "          -5.1321e-04, -5.4592e-03, -9.2707e-04,  5.4551e-03, -4.9731e-03,\n",
       "          -6.3760e-03, -2.2797e-03,  9.0375e-03,  6.7416e-03, -5.5720e-03,\n",
       "          -8.4298e-03,  2.1314e-03, -7.0118e-03,  1.4222e-03, -3.8760e-04,\n",
       "           4.2457e-04, -2.0165e-03,  4.2753e-03,  5.7197e-03, -7.3704e-03,\n",
       "           5.8238e-03, -3.5565e-03, -4.7996e-03, -4.2372e-03, -5.0620e-03,\n",
       "          -1.4990e-02, -1.2310e-03,  1.3331e-03, -4.7811e-03,  3.9264e-03,\n",
       "          -3.6671e-03, -5.8450e-04,  4.4344e-03, -7.1720e-03,  2.5168e-03,\n",
       "          -4.0643e-03,  4.9319e-03, -2.3885e-04, -5.4040e-04,  1.5722e-03,\n",
       "           7.4370e-03,  5.1830e-03, -8.6837e-03,  4.1268e-03, -1.6794e-03,\n",
       "          -2.8951e-03,  2.9707e-03,  7.9118e-03, -5.1855e-03,  2.5876e-03,\n",
       "           1.5904e-03,  7.5800e-03, -3.5597e-04, -5.9231e-03,  6.8127e-03,\n",
       "           2.4826e-03,  3.7108e-03,  2.9196e-03,  3.6116e-03,  7.9443e-03,\n",
       "           3.0862e-03, -8.6543e-03,  1.5662e-03,  2.4446e-04, -3.0337e-03,\n",
       "           1.9950e-03, -2.4247e-05,  1.0908e-03, -1.6533e-03,  1.6330e-03,\n",
       "           7.7977e-03,  1.7669e-03,  4.4636e-03,  1.0159e-03, -3.4746e-03,\n",
       "           2.4436e-03,  5.7131e-03,  4.2470e-03, -5.5416e-03, -5.7216e-03,\n",
       "          -4.3841e-03,  3.8319e-03,  5.9825e-03,  6.1899e-03,  4.9420e-03,\n",
       "          -4.0541e-03, -1.3266e-03,  4.5631e-03, -2.1548e-03, -3.9163e-03,\n",
       "          -3.1150e-03, -2.9764e-03,  5.0070e-04,  2.5498e-03, -6.9606e-03,\n",
       "          -7.9816e-04, -4.7175e-03, -1.3018e-03, -1.9629e-03, -3.5828e-03,\n",
       "          -5.3087e-03,  4.1970e-03,  2.5610e-03, -3.1208e-03,  1.0011e-03,\n",
       "           1.9064e-03, -3.0591e-03, -1.7905e-03, -9.0249e-04,  2.3697e-03,\n",
       "          -2.3032e-03, -4.0901e-03, -1.3419e-03, -8.8246e-03, -9.4603e-03,\n",
       "          -2.5765e-03, -1.8946e-03,  2.5719e-03, -6.7492e-05,  5.9619e-03,\n",
       "           2.7534e-03,  3.1956e-03, -1.1016e-02, -1.7439e-03, -6.5044e-04,\n",
       "          -2.6204e-03,  3.0428e-04, -5.8721e-03,  2.7891e-03, -3.1360e-04,\n",
       "          -8.8855e-03,  4.4342e-04, -2.2926e-03, -3.9622e-04, -5.2779e-07,\n",
       "           1.7250e-03,  5.3064e-03,  1.1023e-03, -6.9703e-03,  2.4717e-03,\n",
       "          -7.1629e-03,  7.6938e-04,  3.0848e-03, -3.8789e-04, -1.7353e-03,\n",
       "           9.7585e-04, -6.6693e-03, -5.4362e-03,  7.7898e-03,  6.0392e-04,\n",
       "           1.2048e-03,  6.6547e-03, -3.3710e-03,  4.2173e-04,  2.5011e-04,\n",
       "          -1.2513e-02,  1.1305e-03, -4.8391e-04, -2.6244e-03, -1.9154e-03,\n",
       "           9.1177e-04, -5.1075e-03,  2.2962e-03, -6.9221e-03,  1.9701e-03,\n",
       "          -4.4487e-03,  4.0503e-03, -1.6533e-03, -8.0324e-03,  9.8980e-03,\n",
       "           2.3873e-04, -7.0287e-03,  1.4790e-03, -9.0560e-03,  2.7626e-03,\n",
       "           7.6348e-03, -3.8236e-03,  2.1748e-03,  5.6715e-03, -4.1079e-03,\n",
       "          -8.6330e-03,  2.7571e-03,  1.0436e-04,  6.7715e-03, -7.0711e-03,\n",
       "           2.2036e-03, -7.8541e-03, -5.0218e-03, -2.9681e-03,  4.8014e-03,\n",
       "          -1.5437e-03, -2.7398e-03, -2.3554e-03,  2.5706e-04,  1.0561e-03,\n",
       "          -9.7760e-04, -3.3922e-03,  5.5928e-03, -6.7160e-03,  3.3686e-03,\n",
       "           4.3242e-03, -5.4408e-03,  1.5408e-03,  1.0058e-04, -7.9996e-04,\n",
       "          -3.2629e-03, -2.2161e-03, -1.3177e-03, -5.1807e-05,  2.3609e-03,\n",
       "           4.9950e-03, -2.0602e-03, -5.4615e-03,  2.8950e-04,  2.0184e-03,\n",
       "           4.7532e-03,  5.4658e-03,  4.3320e-03, -1.0169e-03,  1.0637e-02,\n",
       "          -4.3951e-03,  4.9507e-03,  8.5549e-03,  3.3624e-03, -7.0672e-03,\n",
       "           9.0887e-04,  1.4394e-03,  1.5079e-03,  5.6342e-03, -9.1224e-03,\n",
       "          -3.6396e-03, -2.1828e-03,  4.9893e-03, -3.6264e-03,  9.5204e-05,\n",
       "          -2.1416e-03, -3.7578e-03, -6.5791e-04, -2.7338e-03,  3.7211e-03,\n",
       "          -6.4095e-04,  6.2848e-03,  4.4657e-03, -8.9693e-04, -2.8121e-03,\n",
       "           4.7351e-03,  1.9476e-03,  1.6045e-03, -1.1358e-03,  4.0842e-03,\n",
       "          -9.0565e-03,  1.8678e-03,  8.2763e-03, -2.0891e-03, -6.1725e-03,\n",
       "          -3.8683e-03, -5.6557e-03,  3.6121e-04,  5.6306e-03, -1.0951e-03,\n",
       "           4.2146e-03, -4.4165e-04,  5.4636e-04,  8.3873e-03, -4.5464e-03,\n",
       "          -5.4504e-04,  1.1124e-03,  6.7319e-04,  3.3140e-03,  7.0477e-04,\n",
       "           4.5297e-03, -4.1698e-03,  1.6159e-03,  4.5990e-03,  1.7277e-04,\n",
       "          -1.8935e-03, -2.4641e-03,  2.4040e-03, -1.4627e-03, -4.0213e-03,\n",
       "           3.1960e-03,  4.2126e-03,  2.7681e-04,  4.9316e-03,  2.9775e-03,\n",
       "           2.0247e-04,  3.3949e-03, -3.0399e-03, -5.9004e-04, -1.5002e-03,\n",
       "           1.5973e-03,  4.3539e-03,  2.7664e-03,  6.4046e-04, -6.4387e-03,\n",
       "          -2.8438e-03,  7.9306e-03,  3.5898e-03, -2.4789e-03, -8.8152e-04,\n",
       "          -1.2068e-03, -9.2806e-04, -1.4852e-03, -7.5694e-03, -6.4826e-04,\n",
       "           2.6812e-03, -1.2762e-03, -5.3144e-03,  2.9498e-03, -3.1411e-03,\n",
       "          -1.2730e-03,  1.1371e-03,  5.1353e-03, -2.9256e-03, -7.5396e-03,\n",
       "           3.9064e-03, -4.7005e-03, -2.0528e-03,  1.2676e-04,  2.9950e-03,\n",
       "          -2.3699e-03,  3.1314e-03, -9.5319e-03,  4.2820e-04, -4.4032e-03,\n",
       "          -2.9295e-03, -4.2719e-03,  1.6849e-03, -1.1095e-03,  2.1484e-05,\n",
       "           9.6609e-05, -3.0547e-05, -7.5023e-04, -1.7178e-03,  1.1481e-02,\n",
       "          -3.4957e-03, -9.1051e-04, -7.6060e-03, -1.8178e-03,  4.8592e-03,\n",
       "          -7.6999e-04, -2.0050e-03,  8.9431e-04, -3.4881e-03,  4.0137e-04,\n",
       "           1.7924e-03,  1.3335e-04, -1.4130e-03, -3.0965e-03,  1.5006e-03,\n",
       "          -4.8251e-03,  1.0819e-02,  1.6536e-03, -2.2440e-03,  5.9803e-03,\n",
       "           5.9664e-04, -2.1570e-03, -2.5898e-03,  7.5594e-03, -1.3107e-03,\n",
       "          -4.2581e-03, -3.1261e-03,  2.2403e-03,  8.4835e-03, -2.2481e-03,\n",
       "          -1.3518e-03, -6.4194e-05, -2.6891e-03,  1.8407e-03,  8.2444e-03,\n",
       "          -1.6562e-03, -5.8601e-03, -5.1697e-03,  5.3567e-03, -1.0270e-02,\n",
       "          -5.2720e-03, -5.0548e-03,  7.9132e-03, -7.8214e-03,  1.3451e-03,\n",
       "           8.1410e-04,  1.7276e-03, -2.9032e-03, -1.2550e-02,  7.9872e-03,\n",
       "           3.6167e-03, -1.3335e-02,  7.5158e-04, -2.8252e-03, -2.7430e-03,\n",
       "           3.8431e-03,  4.6080e-04, -1.3090e-03,  5.2109e-03,  6.2680e-03,\n",
       "           1.9884e-03, -4.4133e-03,  7.9712e-03,  1.0295e-02,  1.5195e-02,\n",
       "           1.5326e-03,  2.9530e-04,  2.3002e-03,  3.9347e-03,  5.3343e-03,\n",
       "           1.8077e-03,  7.6865e-03, -1.0675e-03, -6.7227e-03,  9.4528e-03,\n",
       "          -8.0139e-03,  5.8060e-03, -1.6217e-03,  3.8840e-03,  4.2952e-03,\n",
       "           8.7806e-04, -1.0222e-03,  1.1811e-02, -5.7697e-03,  6.9922e-03,\n",
       "          -1.3790e-02,  7.2457e-04,  1.5484e-02,  2.7183e-03, -5.3224e-03,\n",
       "          -3.7296e-03, -2.6280e-03, -9.5657e-03,  1.3827e-02,  5.3420e-03,\n",
       "          -1.3356e-03,  3.6706e-03,  6.9164e-03,  4.3925e-03, -1.5419e-03,\n",
       "           4.1382e-03, -4.0516e-03, -4.1103e-03,  3.0949e-04, -9.4350e-05,\n",
       "          -6.9190e-03,  1.9689e-03,  2.2831e-03, -1.4378e-03, -6.8970e-04,\n",
       "           1.1415e-03,  9.4667e-04, -7.2704e-03, -2.9382e-03,  7.2932e-03,\n",
       "           3.1627e-04,  2.7267e-03,  4.2801e-03, -6.0559e-03, -5.5708e-03,\n",
       "           1.2757e-03, -3.5795e-03, -7.2389e-03, -1.7668e-03,  1.8800e-03,\n",
       "           3.0886e-03,  2.1105e-03,  1.5674e-03,  1.2828e-03,  5.9259e-03,\n",
       "           8.4010e-04, -1.6178e-03,  1.0639e-03,  3.5721e-03, -2.8771e-03,\n",
       "           3.1407e-03,  8.1162e-04, -9.5287e-03, -2.0651e-03,  4.2698e-03,\n",
       "          -2.0288e-03,  2.5148e-03,  1.6169e-03, -2.8961e-03, -1.0559e-03,\n",
       "          -4.0905e-03,  9.4554e-04,  3.0283e-03, -5.2308e-03,  2.0919e-03,\n",
       "           3.8192e-03, -1.2990e-03,  3.8938e-03, -1.5828e-03,  1.0480e-03,\n",
       "           2.5161e-03, -3.4312e-03,  3.8059e-03,  8.2166e-04,  4.0572e-03,\n",
       "           3.1652e-03,  2.1724e-03, -5.9991e-03,  5.7763e-03, -7.6956e-03,\n",
       "          -5.4374e-03, -3.1216e-04, -3.2782e-03, -4.3137e-03, -3.2482e-03,\n",
       "           2.7950e-03, -6.8682e-04, -1.6691e-03,  2.8336e-03, -2.0973e-03,\n",
       "          -6.6023e-03,  1.5846e-03, -5.0377e-03,  1.9074e-03,  2.4591e-03,\n",
       "          -3.9174e-03, -1.2733e-03,  1.0145e-03, -4.4710e-03,  2.9291e-03,\n",
       "          -9.5645e-04, -3.1429e-04, -2.2444e-03, -4.0813e-03, -3.7001e-03,\n",
       "           2.2773e-03, -2.2256e-03, -5.2029e-03,  7.8617e-03,  2.1254e-03,\n",
       "          -1.8045e-03, -3.9910e-03,  1.9054e-03, -9.5417e-03,  5.0948e-03,\n",
       "          -4.0267e-04,  3.8395e-03,  2.9889e-03,  8.9627e-03,  9.6131e-04,\n",
       "          -1.7270e-03, -2.3549e-03,  2.1966e-03,  3.3036e-03, -4.8108e-04,\n",
       "           3.8014e-03, -8.1695e-05,  2.3977e-04,  3.7977e-03,  3.2262e-03,\n",
       "          -5.8078e-03,  9.9914e-05,  1.7071e-03, -1.2467e-03,  3.1293e-03,\n",
       "           2.4645e-03, -2.5845e-03, -3.4412e-03, -2.7490e-03,  2.6711e-04,\n",
       "           1.1518e-03, -9.5652e-04,  8.7983e-03,  4.3791e-03, -6.1024e-03,\n",
       "          -1.9999e-02,  4.4730e-03, -7.9730e-04,  2.0613e-03, -1.2412e-02,\n",
       "           1.4591e-02,  1.2318e-02,  4.5111e-03,  4.7868e-03,  3.4282e-03,\n",
       "          -2.9516e-02, -6.3859e-03,  1.7355e-02,  1.2119e-02, -1.2059e-02,\n",
       "           1.0308e-02, -8.9580e-03,  1.3823e-02, -8.1396e-03,  7.3271e-03,\n",
       "           1.0888e-02, -2.8236e-03,  8.1908e-03,  1.2593e-02, -3.4558e-03,\n",
       "           7.6344e-04,  2.3814e-03, -1.2000e-02,  1.2937e-02, -9.3714e-03,\n",
       "          -6.4529e-03, -1.5321e-02,  1.0610e-02,  4.2802e-04,  1.3208e-02,\n",
       "           5.3129e-03, -6.5369e-03,  8.9730e-03, -3.8083e-03,  1.2664e-02,\n",
       "           6.9007e-03,  3.6855e-03,  1.1223e-02,  7.8671e-03,  1.2608e-03,\n",
       "          -3.5934e-03,  8.5362e-03,  7.7392e-03, -6.2439e-04,  6.8258e-03,\n",
       "           1.7344e-03, -4.5699e-03,  5.1521e-03,  7.7407e-03,  4.7372e-03,\n",
       "           4.2346e-03,  3.7088e-03,  9.2489e-03,  1.1715e-02, -4.3824e-03,\n",
       "          -3.4954e-03,  5.5534e-03,  8.3839e-04,  2.5224e-03,  9.5239e-03,\n",
       "          -4.8281e-03,  1.2804e-03,  7.7164e-04, -8.6801e-03, -1.1034e-03,\n",
       "          -5.4081e-03, -2.8767e-03, -1.1854e-03,  5.9572e-03,  6.5750e-03,\n",
       "           9.9296e-04,  2.2244e-03,  1.6213e-03, -6.3055e-03, -3.4300e-03,\n",
       "           1.4805e-03,  3.5275e-03, -1.5863e-03, -4.3710e-04, -5.3842e-04,\n",
       "           1.9117e-03, -9.1832e-03,  4.3838e-03, -6.9584e-04, -3.9889e-03,\n",
       "          -4.8672e-03,  7.2911e-03,  8.7121e-03, -4.5809e-03,  6.0049e-03,\n",
       "          -6.1273e-03, -1.2031e-04, -3.3488e-03, -1.2943e-02, -8.3492e-03,\n",
       "           3.5334e-03, -9.5223e-03, -2.1647e-03, -4.9706e-03, -6.3692e-03,\n",
       "           4.2424e-03,  4.3901e-03,  1.7415e-03,  1.7827e-03,  5.5266e-04,\n",
       "           6.0014e-03, -9.2381e-03,  1.3176e-03, -1.0151e-02, -6.6623e-03,\n",
       "           7.5403e-03,  1.2798e-02,  1.1754e-03,  3.1368e-03,  1.0764e-02,\n",
       "           1.1818e-02,  1.0479e-02,  3.5519e-03, -6.3688e-03, -5.3454e-03,\n",
       "           3.0223e-03,  2.2920e-03,  1.9793e-03,  2.6656e-03, -5.3251e-03,\n",
       "          -3.0441e-03,  1.6099e-03,  7.8349e-04,  3.9613e-03,  2.2048e-03,\n",
       "          -9.7030e-03,  9.0286e-03, -4.2312e-03, -4.9334e-03, -1.2192e-03,\n",
       "           1.0275e-03, -3.4095e-03,  2.2985e-03,  1.2645e-03, -2.2265e-03,\n",
       "          -7.4074e-03, -1.2246e-04, -3.4760e-03, -1.3233e-03, -7.3279e-03,\n",
       "          -2.1692e-03,  3.0799e-03, -3.4639e-03, -3.1355e-03,  2.9041e-03,\n",
       "           1.8976e-03, -1.3619e-03,  4.6187e-03,  1.1680e-03, -4.3926e-04,\n",
       "          -3.6108e-03, -2.8486e-03,  5.8254e-03,  5.5916e-03, -4.6303e-03,\n",
       "           1.7158e-03, -8.0431e-04,  5.0892e-03,  5.2776e-03, -1.1302e-03,\n",
       "          -2.6286e-03, -3.6092e-03, -6.5174e-03, -1.5916e-03,  1.6768e-03,\n",
       "           2.3379e-03, -2.4745e-03, -1.4414e-03,  1.2543e-03, -7.3630e-04,\n",
       "           4.3146e-03, -3.9160e-03, -7.9162e-04,  7.1797e-03, -3.1713e-03,\n",
       "           2.3081e-03,  6.8095e-03, -2.5026e-03], requires_grad=True)),\n",
       " ('weight_orig',\n",
       "  Parameter containing:\n",
       "  tensor([[ 8.0064e-03, -7.2079e-02, -4.5409e-02,  ...,  1.3022e-02,\n",
       "           -4.1921e-02, -1.8508e-02],\n",
       "          [ 4.7319e-02,  5.6065e-02,  7.8608e-03,  ..., -1.5596e-02,\n",
       "            1.1952e-02,  1.3859e-02],\n",
       "          [-1.3696e-02, -4.2909e-02,  4.8609e-03,  ..., -3.5625e-03,\n",
       "           -9.7081e-02, -5.2629e-02],\n",
       "          ...,\n",
       "          [-2.3917e-02, -3.2999e-02,  8.4614e-03,  ...,  8.6428e-05,\n",
       "            1.8967e-04,  1.8264e-03],\n",
       "          [-9.8836e-02, -8.1632e-02, -2.3575e-02,  ..., -1.8363e-02,\n",
       "           -5.1585e-02, -1.2072e-01],\n",
       "          [-1.6689e-02, -4.9597e-02, -2.4316e-02,  ...,  4.4092e-02,\n",
       "           -3.1870e-02,  3.8483e-02]], requires_grad=True))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈에서 weight 파라미터 제거후 weight_org로 대체됨.\n",
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05d966d-e32b-4c57-b185-c40e4a609d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_mask',\n",
       "  tensor([[1., 0., 1.,  ..., 0., 1., 0.],\n",
       "          [1., 0., 1.,  ..., 1., 1., 0.],\n",
       "          [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.]]))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# named_buffers 에는 pruning mask가 생성되어 있음.\n",
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b923fd6-8131-4852-84c5-21e57e9bb63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0080, -0.0000, -0.0454,  ...,  0.0000, -0.0419, -0.0000],\n",
       "        [ 0.0473,  0.0000,  0.0079,  ..., -0.0156,  0.0120,  0.0000],\n",
       "        [-0.0000, -0.0000,  0.0049,  ..., -0.0036, -0.0000, -0.0526],\n",
       "        ...,\n",
       "        [-0.0239, -0.0330,  0.0085,  ...,  0.0000,  0.0000,  0.0018],\n",
       "        [-0.0000, -0.0816, -0.0236,  ..., -0.0000, -0.0516, -0.0000],\n",
       "        [-0.0000, -0.0496, -0.0243,  ...,  0.0441, -0.0319,  0.0385]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈의 weight는 위 weight_org 와 weight_mask 을 적용해 계산됨.\n",
    "# 계산결과 weight는 속성(attribute)로 저장됨\n",
    "module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66e0dd5b-b398-457d-9ab4-4a1c05bc7620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight의 size\n",
    "module.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac9deeb-f195-475e-b484-17349f31f7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(176947)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pruning 후 weight가 0인 계수 출력\n",
    "(module.weight == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efb4148f-b933-4a82-8bc6-cd36ad192c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured at 0x7fe2aedc5520>)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruning을 순전파 이전에 적용하기 위해서는 forword_pre_hooks라는 속성 사용\n",
    "module._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d703789-f109-4a3d-860c-b63b7bb67637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key의 bias에도 Pruning 적용\n",
    "# => L1 노름 기준으로 가정 영향력이 작은 amount개의 파라메터 Pruning하도록 l1_unstructured 함수 사용\n",
    "prune.l1_unstructured(module, name='bias', amount=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a32c159-6488-4fd9-a7cc-3789e72a4aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_orig',\n",
       "  Parameter containing:\n",
       "  tensor([[ 8.0064e-03, -7.2079e-02, -4.5409e-02,  ...,  1.3022e-02,\n",
       "           -4.1921e-02, -1.8508e-02],\n",
       "          [ 4.7319e-02,  5.6065e-02,  7.8608e-03,  ..., -1.5596e-02,\n",
       "            1.1952e-02,  1.3859e-02],\n",
       "          [-1.3696e-02, -4.2909e-02,  4.8609e-03,  ..., -3.5625e-03,\n",
       "           -9.7081e-02, -5.2629e-02],\n",
       "          ...,\n",
       "          [-2.3917e-02, -3.2999e-02,  8.4614e-03,  ...,  8.6428e-05,\n",
       "            1.8967e-04,  1.8264e-03],\n",
       "          [-9.8836e-02, -8.1632e-02, -2.3575e-02,  ..., -1.8363e-02,\n",
       "           -5.1585e-02, -1.2072e-01],\n",
       "          [-1.6689e-02, -4.9597e-02, -2.4316e-02,  ...,  4.4092e-02,\n",
       "           -3.1870e-02,  3.8483e-02]], requires_grad=True)),\n",
       " ('bias_orig',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.0594e-03,  7.3360e-04,  5.4275e-03, -1.3735e-03,  2.9526e-03,\n",
       "           5.4779e-03,  2.9630e-03,  6.7515e-03,  4.7061e-03,  3.2703e-03,\n",
       "          -2.9103e-03,  1.1439e-03, -4.9434e-03,  4.8971e-03,  1.2137e-02,\n",
       "          -5.1321e-04, -5.4592e-03, -9.2707e-04,  5.4551e-03, -4.9731e-03,\n",
       "          -6.3760e-03, -2.2797e-03,  9.0375e-03,  6.7416e-03, -5.5720e-03,\n",
       "          -8.4298e-03,  2.1314e-03, -7.0118e-03,  1.4222e-03, -3.8760e-04,\n",
       "           4.2457e-04, -2.0165e-03,  4.2753e-03,  5.7197e-03, -7.3704e-03,\n",
       "           5.8238e-03, -3.5565e-03, -4.7996e-03, -4.2372e-03, -5.0620e-03,\n",
       "          -1.4990e-02, -1.2310e-03,  1.3331e-03, -4.7811e-03,  3.9264e-03,\n",
       "          -3.6671e-03, -5.8450e-04,  4.4344e-03, -7.1720e-03,  2.5168e-03,\n",
       "          -4.0643e-03,  4.9319e-03, -2.3885e-04, -5.4040e-04,  1.5722e-03,\n",
       "           7.4370e-03,  5.1830e-03, -8.6837e-03,  4.1268e-03, -1.6794e-03,\n",
       "          -2.8951e-03,  2.9707e-03,  7.9118e-03, -5.1855e-03,  2.5876e-03,\n",
       "           1.5904e-03,  7.5800e-03, -3.5597e-04, -5.9231e-03,  6.8127e-03,\n",
       "           2.4826e-03,  3.7108e-03,  2.9196e-03,  3.6116e-03,  7.9443e-03,\n",
       "           3.0862e-03, -8.6543e-03,  1.5662e-03,  2.4446e-04, -3.0337e-03,\n",
       "           1.9950e-03, -2.4247e-05,  1.0908e-03, -1.6533e-03,  1.6330e-03,\n",
       "           7.7977e-03,  1.7669e-03,  4.4636e-03,  1.0159e-03, -3.4746e-03,\n",
       "           2.4436e-03,  5.7131e-03,  4.2470e-03, -5.5416e-03, -5.7216e-03,\n",
       "          -4.3841e-03,  3.8319e-03,  5.9825e-03,  6.1899e-03,  4.9420e-03,\n",
       "          -4.0541e-03, -1.3266e-03,  4.5631e-03, -2.1548e-03, -3.9163e-03,\n",
       "          -3.1150e-03, -2.9764e-03,  5.0070e-04,  2.5498e-03, -6.9606e-03,\n",
       "          -7.9816e-04, -4.7175e-03, -1.3018e-03, -1.9629e-03, -3.5828e-03,\n",
       "          -5.3087e-03,  4.1970e-03,  2.5610e-03, -3.1208e-03,  1.0011e-03,\n",
       "           1.9064e-03, -3.0591e-03, -1.7905e-03, -9.0249e-04,  2.3697e-03,\n",
       "          -2.3032e-03, -4.0901e-03, -1.3419e-03, -8.8246e-03, -9.4603e-03,\n",
       "          -2.5765e-03, -1.8946e-03,  2.5719e-03, -6.7492e-05,  5.9619e-03,\n",
       "           2.7534e-03,  3.1956e-03, -1.1016e-02, -1.7439e-03, -6.5044e-04,\n",
       "          -2.6204e-03,  3.0428e-04, -5.8721e-03,  2.7891e-03, -3.1360e-04,\n",
       "          -8.8855e-03,  4.4342e-04, -2.2926e-03, -3.9622e-04, -5.2779e-07,\n",
       "           1.7250e-03,  5.3064e-03,  1.1023e-03, -6.9703e-03,  2.4717e-03,\n",
       "          -7.1629e-03,  7.6938e-04,  3.0848e-03, -3.8789e-04, -1.7353e-03,\n",
       "           9.7585e-04, -6.6693e-03, -5.4362e-03,  7.7898e-03,  6.0392e-04,\n",
       "           1.2048e-03,  6.6547e-03, -3.3710e-03,  4.2173e-04,  2.5011e-04,\n",
       "          -1.2513e-02,  1.1305e-03, -4.8391e-04, -2.6244e-03, -1.9154e-03,\n",
       "           9.1177e-04, -5.1075e-03,  2.2962e-03, -6.9221e-03,  1.9701e-03,\n",
       "          -4.4487e-03,  4.0503e-03, -1.6533e-03, -8.0324e-03,  9.8980e-03,\n",
       "           2.3873e-04, -7.0287e-03,  1.4790e-03, -9.0560e-03,  2.7626e-03,\n",
       "           7.6348e-03, -3.8236e-03,  2.1748e-03,  5.6715e-03, -4.1079e-03,\n",
       "          -8.6330e-03,  2.7571e-03,  1.0436e-04,  6.7715e-03, -7.0711e-03,\n",
       "           2.2036e-03, -7.8541e-03, -5.0218e-03, -2.9681e-03,  4.8014e-03,\n",
       "          -1.5437e-03, -2.7398e-03, -2.3554e-03,  2.5706e-04,  1.0561e-03,\n",
       "          -9.7760e-04, -3.3922e-03,  5.5928e-03, -6.7160e-03,  3.3686e-03,\n",
       "           4.3242e-03, -5.4408e-03,  1.5408e-03,  1.0058e-04, -7.9996e-04,\n",
       "          -3.2629e-03, -2.2161e-03, -1.3177e-03, -5.1807e-05,  2.3609e-03,\n",
       "           4.9950e-03, -2.0602e-03, -5.4615e-03,  2.8950e-04,  2.0184e-03,\n",
       "           4.7532e-03,  5.4658e-03,  4.3320e-03, -1.0169e-03,  1.0637e-02,\n",
       "          -4.3951e-03,  4.9507e-03,  8.5549e-03,  3.3624e-03, -7.0672e-03,\n",
       "           9.0887e-04,  1.4394e-03,  1.5079e-03,  5.6342e-03, -9.1224e-03,\n",
       "          -3.6396e-03, -2.1828e-03,  4.9893e-03, -3.6264e-03,  9.5204e-05,\n",
       "          -2.1416e-03, -3.7578e-03, -6.5791e-04, -2.7338e-03,  3.7211e-03,\n",
       "          -6.4095e-04,  6.2848e-03,  4.4657e-03, -8.9693e-04, -2.8121e-03,\n",
       "           4.7351e-03,  1.9476e-03,  1.6045e-03, -1.1358e-03,  4.0842e-03,\n",
       "          -9.0565e-03,  1.8678e-03,  8.2763e-03, -2.0891e-03, -6.1725e-03,\n",
       "          -3.8683e-03, -5.6557e-03,  3.6121e-04,  5.6306e-03, -1.0951e-03,\n",
       "           4.2146e-03, -4.4165e-04,  5.4636e-04,  8.3873e-03, -4.5464e-03,\n",
       "          -5.4504e-04,  1.1124e-03,  6.7319e-04,  3.3140e-03,  7.0477e-04,\n",
       "           4.5297e-03, -4.1698e-03,  1.6159e-03,  4.5990e-03,  1.7277e-04,\n",
       "          -1.8935e-03, -2.4641e-03,  2.4040e-03, -1.4627e-03, -4.0213e-03,\n",
       "           3.1960e-03,  4.2126e-03,  2.7681e-04,  4.9316e-03,  2.9775e-03,\n",
       "           2.0247e-04,  3.3949e-03, -3.0399e-03, -5.9004e-04, -1.5002e-03,\n",
       "           1.5973e-03,  4.3539e-03,  2.7664e-03,  6.4046e-04, -6.4387e-03,\n",
       "          -2.8438e-03,  7.9306e-03,  3.5898e-03, -2.4789e-03, -8.8152e-04,\n",
       "          -1.2068e-03, -9.2806e-04, -1.4852e-03, -7.5694e-03, -6.4826e-04,\n",
       "           2.6812e-03, -1.2762e-03, -5.3144e-03,  2.9498e-03, -3.1411e-03,\n",
       "          -1.2730e-03,  1.1371e-03,  5.1353e-03, -2.9256e-03, -7.5396e-03,\n",
       "           3.9064e-03, -4.7005e-03, -2.0528e-03,  1.2676e-04,  2.9950e-03,\n",
       "          -2.3699e-03,  3.1314e-03, -9.5319e-03,  4.2820e-04, -4.4032e-03,\n",
       "          -2.9295e-03, -4.2719e-03,  1.6849e-03, -1.1095e-03,  2.1484e-05,\n",
       "           9.6609e-05, -3.0547e-05, -7.5023e-04, -1.7178e-03,  1.1481e-02,\n",
       "          -3.4957e-03, -9.1051e-04, -7.6060e-03, -1.8178e-03,  4.8592e-03,\n",
       "          -7.6999e-04, -2.0050e-03,  8.9431e-04, -3.4881e-03,  4.0137e-04,\n",
       "           1.7924e-03,  1.3335e-04, -1.4130e-03, -3.0965e-03,  1.5006e-03,\n",
       "          -4.8251e-03,  1.0819e-02,  1.6536e-03, -2.2440e-03,  5.9803e-03,\n",
       "           5.9664e-04, -2.1570e-03, -2.5898e-03,  7.5594e-03, -1.3107e-03,\n",
       "          -4.2581e-03, -3.1261e-03,  2.2403e-03,  8.4835e-03, -2.2481e-03,\n",
       "          -1.3518e-03, -6.4194e-05, -2.6891e-03,  1.8407e-03,  8.2444e-03,\n",
       "          -1.6562e-03, -5.8601e-03, -5.1697e-03,  5.3567e-03, -1.0270e-02,\n",
       "          -5.2720e-03, -5.0548e-03,  7.9132e-03, -7.8214e-03,  1.3451e-03,\n",
       "           8.1410e-04,  1.7276e-03, -2.9032e-03, -1.2550e-02,  7.9872e-03,\n",
       "           3.6167e-03, -1.3335e-02,  7.5158e-04, -2.8252e-03, -2.7430e-03,\n",
       "           3.8431e-03,  4.6080e-04, -1.3090e-03,  5.2109e-03,  6.2680e-03,\n",
       "           1.9884e-03, -4.4133e-03,  7.9712e-03,  1.0295e-02,  1.5195e-02,\n",
       "           1.5326e-03,  2.9530e-04,  2.3002e-03,  3.9347e-03,  5.3343e-03,\n",
       "           1.8077e-03,  7.6865e-03, -1.0675e-03, -6.7227e-03,  9.4528e-03,\n",
       "          -8.0139e-03,  5.8060e-03, -1.6217e-03,  3.8840e-03,  4.2952e-03,\n",
       "           8.7806e-04, -1.0222e-03,  1.1811e-02, -5.7697e-03,  6.9922e-03,\n",
       "          -1.3790e-02,  7.2457e-04,  1.5484e-02,  2.7183e-03, -5.3224e-03,\n",
       "          -3.7296e-03, -2.6280e-03, -9.5657e-03,  1.3827e-02,  5.3420e-03,\n",
       "          -1.3356e-03,  3.6706e-03,  6.9164e-03,  4.3925e-03, -1.5419e-03,\n",
       "           4.1382e-03, -4.0516e-03, -4.1103e-03,  3.0949e-04, -9.4350e-05,\n",
       "          -6.9190e-03,  1.9689e-03,  2.2831e-03, -1.4378e-03, -6.8970e-04,\n",
       "           1.1415e-03,  9.4667e-04, -7.2704e-03, -2.9382e-03,  7.2932e-03,\n",
       "           3.1627e-04,  2.7267e-03,  4.2801e-03, -6.0559e-03, -5.5708e-03,\n",
       "           1.2757e-03, -3.5795e-03, -7.2389e-03, -1.7668e-03,  1.8800e-03,\n",
       "           3.0886e-03,  2.1105e-03,  1.5674e-03,  1.2828e-03,  5.9259e-03,\n",
       "           8.4010e-04, -1.6178e-03,  1.0639e-03,  3.5721e-03, -2.8771e-03,\n",
       "           3.1407e-03,  8.1162e-04, -9.5287e-03, -2.0651e-03,  4.2698e-03,\n",
       "          -2.0288e-03,  2.5148e-03,  1.6169e-03, -2.8961e-03, -1.0559e-03,\n",
       "          -4.0905e-03,  9.4554e-04,  3.0283e-03, -5.2308e-03,  2.0919e-03,\n",
       "           3.8192e-03, -1.2990e-03,  3.8938e-03, -1.5828e-03,  1.0480e-03,\n",
       "           2.5161e-03, -3.4312e-03,  3.8059e-03,  8.2166e-04,  4.0572e-03,\n",
       "           3.1652e-03,  2.1724e-03, -5.9991e-03,  5.7763e-03, -7.6956e-03,\n",
       "          -5.4374e-03, -3.1216e-04, -3.2782e-03, -4.3137e-03, -3.2482e-03,\n",
       "           2.7950e-03, -6.8682e-04, -1.6691e-03,  2.8336e-03, -2.0973e-03,\n",
       "          -6.6023e-03,  1.5846e-03, -5.0377e-03,  1.9074e-03,  2.4591e-03,\n",
       "          -3.9174e-03, -1.2733e-03,  1.0145e-03, -4.4710e-03,  2.9291e-03,\n",
       "          -9.5645e-04, -3.1429e-04, -2.2444e-03, -4.0813e-03, -3.7001e-03,\n",
       "           2.2773e-03, -2.2256e-03, -5.2029e-03,  7.8617e-03,  2.1254e-03,\n",
       "          -1.8045e-03, -3.9910e-03,  1.9054e-03, -9.5417e-03,  5.0948e-03,\n",
       "          -4.0267e-04,  3.8395e-03,  2.9889e-03,  8.9627e-03,  9.6131e-04,\n",
       "          -1.7270e-03, -2.3549e-03,  2.1966e-03,  3.3036e-03, -4.8108e-04,\n",
       "           3.8014e-03, -8.1695e-05,  2.3977e-04,  3.7977e-03,  3.2262e-03,\n",
       "          -5.8078e-03,  9.9914e-05,  1.7071e-03, -1.2467e-03,  3.1293e-03,\n",
       "           2.4645e-03, -2.5845e-03, -3.4412e-03, -2.7490e-03,  2.6711e-04,\n",
       "           1.1518e-03, -9.5652e-04,  8.7983e-03,  4.3791e-03, -6.1024e-03,\n",
       "          -1.9999e-02,  4.4730e-03, -7.9730e-04,  2.0613e-03, -1.2412e-02,\n",
       "           1.4591e-02,  1.2318e-02,  4.5111e-03,  4.7868e-03,  3.4282e-03,\n",
       "          -2.9516e-02, -6.3859e-03,  1.7355e-02,  1.2119e-02, -1.2059e-02,\n",
       "           1.0308e-02, -8.9580e-03,  1.3823e-02, -8.1396e-03,  7.3271e-03,\n",
       "           1.0888e-02, -2.8236e-03,  8.1908e-03,  1.2593e-02, -3.4558e-03,\n",
       "           7.6344e-04,  2.3814e-03, -1.2000e-02,  1.2937e-02, -9.3714e-03,\n",
       "          -6.4529e-03, -1.5321e-02,  1.0610e-02,  4.2802e-04,  1.3208e-02,\n",
       "           5.3129e-03, -6.5369e-03,  8.9730e-03, -3.8083e-03,  1.2664e-02,\n",
       "           6.9007e-03,  3.6855e-03,  1.1223e-02,  7.8671e-03,  1.2608e-03,\n",
       "          -3.5934e-03,  8.5362e-03,  7.7392e-03, -6.2439e-04,  6.8258e-03,\n",
       "           1.7344e-03, -4.5699e-03,  5.1521e-03,  7.7407e-03,  4.7372e-03,\n",
       "           4.2346e-03,  3.7088e-03,  9.2489e-03,  1.1715e-02, -4.3824e-03,\n",
       "          -3.4954e-03,  5.5534e-03,  8.3839e-04,  2.5224e-03,  9.5239e-03,\n",
       "          -4.8281e-03,  1.2804e-03,  7.7164e-04, -8.6801e-03, -1.1034e-03,\n",
       "          -5.4081e-03, -2.8767e-03, -1.1854e-03,  5.9572e-03,  6.5750e-03,\n",
       "           9.9296e-04,  2.2244e-03,  1.6213e-03, -6.3055e-03, -3.4300e-03,\n",
       "           1.4805e-03,  3.5275e-03, -1.5863e-03, -4.3710e-04, -5.3842e-04,\n",
       "           1.9117e-03, -9.1832e-03,  4.3838e-03, -6.9584e-04, -3.9889e-03,\n",
       "          -4.8672e-03,  7.2911e-03,  8.7121e-03, -4.5809e-03,  6.0049e-03,\n",
       "          -6.1273e-03, -1.2031e-04, -3.3488e-03, -1.2943e-02, -8.3492e-03,\n",
       "           3.5334e-03, -9.5223e-03, -2.1647e-03, -4.9706e-03, -6.3692e-03,\n",
       "           4.2424e-03,  4.3901e-03,  1.7415e-03,  1.7827e-03,  5.5266e-04,\n",
       "           6.0014e-03, -9.2381e-03,  1.3176e-03, -1.0151e-02, -6.6623e-03,\n",
       "           7.5403e-03,  1.2798e-02,  1.1754e-03,  3.1368e-03,  1.0764e-02,\n",
       "           1.1818e-02,  1.0479e-02,  3.5519e-03, -6.3688e-03, -5.3454e-03,\n",
       "           3.0223e-03,  2.2920e-03,  1.9793e-03,  2.6656e-03, -5.3251e-03,\n",
       "          -3.0441e-03,  1.6099e-03,  7.8349e-04,  3.9613e-03,  2.2048e-03,\n",
       "          -9.7030e-03,  9.0286e-03, -4.2312e-03, -4.9334e-03, -1.2192e-03,\n",
       "           1.0275e-03, -3.4095e-03,  2.2985e-03,  1.2645e-03, -2.2265e-03,\n",
       "          -7.4074e-03, -1.2246e-04, -3.4760e-03, -1.3233e-03, -7.3279e-03,\n",
       "          -2.1692e-03,  3.0799e-03, -3.4639e-03, -3.1355e-03,  2.9041e-03,\n",
       "           1.8976e-03, -1.3619e-03,  4.6187e-03,  1.1680e-03, -4.3926e-04,\n",
       "          -3.6108e-03, -2.8486e-03,  5.8254e-03,  5.5916e-03, -4.6303e-03,\n",
       "           1.7158e-03, -8.0431e-04,  5.0892e-03,  5.2776e-03, -1.1302e-03,\n",
       "          -2.6286e-03, -3.6092e-03, -6.5174e-03, -1.5916e-03,  1.6768e-03,\n",
       "           2.3379e-03, -2.4745e-03, -1.4414e-03,  1.2543e-03, -7.3630e-04,\n",
       "           4.3146e-03, -3.9160e-03, -7.9162e-04,  7.1797e-03, -3.1713e-03,\n",
       "           2.3081e-03,  6.8095e-03, -2.5026e-03], requires_grad=True))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias_orig 포함되더 있는지 확인\n",
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d975f0d2-d004-4f55-9af8-21ee577a7d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight_mask',\n",
       "  tensor([[1., 0., 1.,  ..., 0., 1., 0.],\n",
       "          [1., 0., 1.,  ..., 1., 1., 0.],\n",
       "          [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
       "          [0., 1., 1.,  ..., 1., 1., 1.]])),\n",
       " ('bias_mask',\n",
       "  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "          1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias_mask가 추가되어 있는지 확인\n",
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0cc8085-ebb5-4e38-9d42-449f3947231e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0011,  0.0007,  0.0054, -0.0014,  0.0030,  0.0055,  0.0030,  0.0068,\n",
       "         0.0047,  0.0033, -0.0029,  0.0011, -0.0049,  0.0049,  0.0121, -0.0005,\n",
       "        -0.0055, -0.0009,  0.0055, -0.0050, -0.0064, -0.0023,  0.0090,  0.0067,\n",
       "        -0.0056, -0.0084,  0.0021, -0.0070,  0.0014, -0.0000,  0.0000, -0.0020,\n",
       "         0.0043,  0.0057, -0.0074,  0.0058, -0.0036, -0.0048, -0.0042, -0.0051,\n",
       "        -0.0150, -0.0012,  0.0013, -0.0048,  0.0039, -0.0037, -0.0006,  0.0044,\n",
       "        -0.0072,  0.0025, -0.0041,  0.0049, -0.0000, -0.0005,  0.0016,  0.0074,\n",
       "         0.0052, -0.0087,  0.0041, -0.0017, -0.0029,  0.0030,  0.0079, -0.0052,\n",
       "         0.0026,  0.0016,  0.0076, -0.0000, -0.0059,  0.0068,  0.0025,  0.0037,\n",
       "         0.0029,  0.0036,  0.0079,  0.0031, -0.0087,  0.0016,  0.0000, -0.0030,\n",
       "         0.0020, -0.0000,  0.0011, -0.0017,  0.0016,  0.0078,  0.0018,  0.0045,\n",
       "         0.0010, -0.0035,  0.0024,  0.0057,  0.0042, -0.0055, -0.0057, -0.0044,\n",
       "         0.0038,  0.0060,  0.0062,  0.0049, -0.0041, -0.0013,  0.0046, -0.0022,\n",
       "        -0.0039, -0.0031, -0.0030,  0.0005,  0.0025, -0.0070, -0.0008, -0.0047,\n",
       "        -0.0013, -0.0020, -0.0036, -0.0053,  0.0042,  0.0026, -0.0031,  0.0010,\n",
       "         0.0019, -0.0031, -0.0018, -0.0009,  0.0024, -0.0023, -0.0041, -0.0013,\n",
       "        -0.0088, -0.0095, -0.0026, -0.0019,  0.0026, -0.0000,  0.0060,  0.0028,\n",
       "         0.0032, -0.0110, -0.0017, -0.0007, -0.0026,  0.0000, -0.0059,  0.0028,\n",
       "        -0.0000, -0.0089,  0.0004, -0.0023, -0.0000, -0.0000,  0.0017,  0.0053,\n",
       "         0.0011, -0.0070,  0.0025, -0.0072,  0.0008,  0.0031, -0.0000, -0.0017,\n",
       "         0.0010, -0.0067, -0.0054,  0.0078,  0.0006,  0.0012,  0.0067, -0.0034,\n",
       "         0.0000,  0.0000, -0.0125,  0.0011, -0.0005, -0.0026, -0.0019,  0.0009,\n",
       "        -0.0051,  0.0023, -0.0069,  0.0020, -0.0044,  0.0041, -0.0017, -0.0080,\n",
       "         0.0099,  0.0000, -0.0070,  0.0015, -0.0091,  0.0028,  0.0076, -0.0038,\n",
       "         0.0022,  0.0057, -0.0041, -0.0086,  0.0028,  0.0000,  0.0068, -0.0071,\n",
       "         0.0022, -0.0079, -0.0050, -0.0030,  0.0048, -0.0015, -0.0027, -0.0024,\n",
       "         0.0000,  0.0011, -0.0010, -0.0034,  0.0056, -0.0067,  0.0034,  0.0043,\n",
       "        -0.0054,  0.0015,  0.0000, -0.0008, -0.0033, -0.0022, -0.0013, -0.0000,\n",
       "         0.0024,  0.0050, -0.0021, -0.0055,  0.0000,  0.0020,  0.0048,  0.0055,\n",
       "         0.0043, -0.0010,  0.0106, -0.0044,  0.0050,  0.0086,  0.0034, -0.0071,\n",
       "         0.0009,  0.0014,  0.0015,  0.0056, -0.0091, -0.0036, -0.0022,  0.0050,\n",
       "        -0.0036,  0.0000, -0.0021, -0.0038, -0.0007, -0.0027,  0.0037, -0.0006,\n",
       "         0.0063,  0.0045, -0.0009, -0.0028,  0.0047,  0.0019,  0.0016, -0.0011,\n",
       "         0.0041, -0.0091,  0.0019,  0.0083, -0.0021, -0.0062, -0.0039, -0.0057,\n",
       "         0.0000,  0.0056, -0.0011,  0.0042, -0.0000,  0.0005,  0.0084, -0.0045,\n",
       "        -0.0005,  0.0011,  0.0007,  0.0033,  0.0007,  0.0045, -0.0042,  0.0016,\n",
       "         0.0046,  0.0000, -0.0019, -0.0025,  0.0024, -0.0015, -0.0040,  0.0032,\n",
       "         0.0042,  0.0000,  0.0049,  0.0030,  0.0000,  0.0034, -0.0030, -0.0006,\n",
       "        -0.0015,  0.0016,  0.0044,  0.0028,  0.0006, -0.0064, -0.0028,  0.0079,\n",
       "         0.0036, -0.0025, -0.0009, -0.0012, -0.0009, -0.0015, -0.0076, -0.0006,\n",
       "         0.0027, -0.0013, -0.0053,  0.0029, -0.0031, -0.0013,  0.0011,  0.0051,\n",
       "        -0.0029, -0.0075,  0.0039, -0.0047, -0.0021,  0.0000,  0.0030, -0.0024,\n",
       "         0.0031, -0.0095,  0.0000, -0.0044, -0.0029, -0.0043,  0.0017, -0.0011,\n",
       "         0.0000,  0.0000, -0.0000, -0.0008, -0.0017,  0.0115, -0.0035, -0.0009,\n",
       "        -0.0076, -0.0018,  0.0049, -0.0008, -0.0020,  0.0009, -0.0035,  0.0000,\n",
       "         0.0018,  0.0000, -0.0014, -0.0031,  0.0015, -0.0048,  0.0108,  0.0017,\n",
       "        -0.0022,  0.0060,  0.0006, -0.0022, -0.0026,  0.0076, -0.0013, -0.0043,\n",
       "        -0.0031,  0.0022,  0.0085, -0.0022, -0.0014, -0.0000, -0.0027,  0.0018,\n",
       "         0.0082, -0.0017, -0.0059, -0.0052,  0.0054, -0.0103, -0.0053, -0.0051,\n",
       "         0.0079, -0.0078,  0.0013,  0.0008,  0.0017, -0.0029, -0.0126,  0.0080,\n",
       "         0.0036, -0.0133,  0.0008, -0.0028, -0.0027,  0.0038,  0.0005, -0.0013,\n",
       "         0.0052,  0.0063,  0.0020, -0.0044,  0.0080,  0.0103,  0.0152,  0.0015,\n",
       "         0.0000,  0.0023,  0.0039,  0.0053,  0.0018,  0.0077, -0.0011, -0.0067,\n",
       "         0.0095, -0.0080,  0.0058, -0.0016,  0.0039,  0.0043,  0.0009, -0.0010,\n",
       "         0.0118, -0.0058,  0.0070, -0.0138,  0.0007,  0.0155,  0.0027, -0.0053,\n",
       "        -0.0037, -0.0026, -0.0096,  0.0138,  0.0053, -0.0013,  0.0037,  0.0069,\n",
       "         0.0044, -0.0015,  0.0041, -0.0041, -0.0041,  0.0000, -0.0000, -0.0069,\n",
       "         0.0020,  0.0023, -0.0014, -0.0007,  0.0011,  0.0009, -0.0073, -0.0029,\n",
       "         0.0073,  0.0000,  0.0027,  0.0043, -0.0061, -0.0056,  0.0013, -0.0036,\n",
       "        -0.0072, -0.0018,  0.0019,  0.0031,  0.0021,  0.0016,  0.0013,  0.0059,\n",
       "         0.0008, -0.0016,  0.0011,  0.0036, -0.0029,  0.0031,  0.0008, -0.0095,\n",
       "        -0.0021,  0.0043, -0.0020,  0.0025,  0.0016, -0.0029, -0.0011, -0.0041,\n",
       "         0.0009,  0.0030, -0.0052,  0.0021,  0.0038, -0.0013,  0.0039, -0.0016,\n",
       "         0.0010,  0.0025, -0.0034,  0.0038,  0.0008,  0.0041,  0.0032,  0.0022,\n",
       "        -0.0060,  0.0058, -0.0077, -0.0054, -0.0000, -0.0033, -0.0043, -0.0032,\n",
       "         0.0028, -0.0007, -0.0017,  0.0028, -0.0021, -0.0066,  0.0016, -0.0050,\n",
       "         0.0019,  0.0025, -0.0039, -0.0013,  0.0010, -0.0045,  0.0029, -0.0010,\n",
       "        -0.0000, -0.0022, -0.0041, -0.0037,  0.0023, -0.0022, -0.0052,  0.0079,\n",
       "         0.0021, -0.0018, -0.0040,  0.0019, -0.0095,  0.0051, -0.0000,  0.0038,\n",
       "         0.0030,  0.0090,  0.0010, -0.0017, -0.0024,  0.0022,  0.0033, -0.0005,\n",
       "         0.0038, -0.0000,  0.0000,  0.0038,  0.0032, -0.0058,  0.0000,  0.0017,\n",
       "        -0.0012,  0.0031,  0.0025, -0.0026, -0.0034, -0.0027,  0.0000,  0.0012,\n",
       "        -0.0010,  0.0088,  0.0044, -0.0061, -0.0200,  0.0045, -0.0008,  0.0021,\n",
       "        -0.0124,  0.0146,  0.0123,  0.0045,  0.0048,  0.0034, -0.0295, -0.0064,\n",
       "         0.0174,  0.0121, -0.0121,  0.0103, -0.0090,  0.0138, -0.0081,  0.0073,\n",
       "         0.0109, -0.0028,  0.0082,  0.0126, -0.0035,  0.0008,  0.0024, -0.0120,\n",
       "         0.0129, -0.0094, -0.0065, -0.0153,  0.0106,  0.0000,  0.0132,  0.0053,\n",
       "        -0.0065,  0.0090, -0.0038,  0.0127,  0.0069,  0.0037,  0.0112,  0.0079,\n",
       "         0.0013, -0.0036,  0.0085,  0.0077, -0.0006,  0.0068,  0.0017, -0.0046,\n",
       "         0.0052,  0.0077,  0.0047,  0.0042,  0.0037,  0.0092,  0.0117, -0.0044,\n",
       "        -0.0035,  0.0056,  0.0008,  0.0025,  0.0095, -0.0048,  0.0013,  0.0008,\n",
       "        -0.0087, -0.0011, -0.0054, -0.0029, -0.0012,  0.0060,  0.0066,  0.0010,\n",
       "         0.0022,  0.0016, -0.0063, -0.0034,  0.0015,  0.0035, -0.0016, -0.0000,\n",
       "        -0.0005,  0.0019, -0.0092,  0.0044, -0.0007, -0.0040, -0.0049,  0.0073,\n",
       "         0.0087, -0.0046,  0.0060, -0.0061, -0.0000, -0.0033, -0.0129, -0.0083,\n",
       "         0.0035, -0.0095, -0.0022, -0.0050, -0.0064,  0.0042,  0.0044,  0.0017,\n",
       "         0.0018,  0.0006,  0.0060, -0.0092,  0.0013, -0.0102, -0.0067,  0.0075,\n",
       "         0.0128,  0.0012,  0.0031,  0.0108,  0.0118,  0.0105,  0.0036, -0.0064,\n",
       "        -0.0053,  0.0030,  0.0023,  0.0020,  0.0027, -0.0053, -0.0030,  0.0016,\n",
       "         0.0008,  0.0040,  0.0022, -0.0097,  0.0090, -0.0042, -0.0049, -0.0012,\n",
       "         0.0010, -0.0034,  0.0023,  0.0013, -0.0022, -0.0074, -0.0000, -0.0035,\n",
       "        -0.0013, -0.0073, -0.0022,  0.0031, -0.0035, -0.0031,  0.0029,  0.0019,\n",
       "        -0.0014,  0.0046,  0.0012, -0.0000, -0.0036, -0.0028,  0.0058,  0.0056,\n",
       "        -0.0046,  0.0017, -0.0008,  0.0051,  0.0053, -0.0011, -0.0026, -0.0036,\n",
       "        -0.0065, -0.0016,  0.0017,  0.0023, -0.0025, -0.0014,  0.0013, -0.0007,\n",
       "         0.0043, -0.0039, -0.0008,  0.0072, -0.0032,  0.0023,  0.0068, -0.0025],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias 출력\n",
    "module.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7d63df3-7887-4c9b-98d7-4c8dec458b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bias 사이즈 \n",
    "module.bias.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f0dab8-0014-4e29-b89e-46ecd6d4fb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bias 중 0인 계수 합\n",
    "(module.bias==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb75020d-d954-4828-a1cb-e8eace95683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, <torch.nn.utils.prune.RandomUnstructured at 0x7fe2aedc5520>),\n",
       "             (1, <torch.nn.utils.prune.L1Unstructured at 0x7fe2aedc5a60>)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_forward_pre_hooks 출력해봄 => weight, biase 2개가 존재함.\n",
    "module._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae256c4a-57e2-49e5-8409-1bc8aa3c2a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중첩 적용\n",
    "# => 앞에서 적용한 weight 파라메터에 다른 Pruning을 중첩 적용함.\n",
    "# => weight에 dim=x 텐서에서 L2 노름을 기준으로 가장 영향력이 작은 파라메터를 Pruning 함.\n",
    "\n",
    "prune.ln_structured(module, name='weight', amount=0.3, n=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e08c465d-39ee-4a12-89e8-09cc1fd7a324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "        ...,\n",
       "        [1., 1., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 1., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight에 중첩 pruning 했으므로, mask 0의 계수가 많아졌다.\n",
    "list(module.named_buffers())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46783675-a3c8-4eec-95b7-51988d38bf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0080, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0473,  0.0000,  0.0000,  ..., -0.0156,  0.0000,  0.0000],\n",
       "        [-0.0000, -0.0000,  0.0000,  ..., -0.0036, -0.0000, -0.0526],\n",
       "        ...,\n",
       "        [-0.0239, -0.0330,  0.0000,  ...,  0.0000,  0.0000,  0.0018],\n",
       "        [-0.0000, -0.0816, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [-0.0000, -0.0496, -0.0000,  ...,  0.0441, -0.0000,  0.0385]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 weight에도 중첩 pruning 적용함으로 인해 0이 계수가 많아졌음.\n",
    "module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75c760c-0dc3-4ad5-9b88-edc9eeb6156f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.nn.utils.prune.PruningContainer at 0x7fe2aedc5700>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중첩 Pruning을 적용하는 객체는 torch.nn.utils.prune.PruningContainer 가됨.\n",
    "\n",
    "for hook in module._forward_pre_hooks.values():\n",
    "    if hook._tensor_name == 'weight':\n",
    "        break\n",
    "        \n",
    "hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bd3a37b-837c-46dc-9767-ce413f551f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.nn.utils.prune.RandomUnstructured at 0x7fe2aedc5520>,\n",
       " <torch.nn.utils.prune.LnStructured at 0x7fe2aedc50d0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight에 대해 2개의 중첩 pruning 기법이 적요되어 있음.\n",
    "list(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25605299-a618-46ac-87b4-54f8b6781339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['weight_orig', 'bias_orig', 'weight_mask', 'bias_mask'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruning 모델 적용\n",
    "# state_dict 에 모든 _mask, _orig 등이 있음.\n",
    "module.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5baeafc0-bba9-4628-9677-d5761060fe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bias_orig',\n",
       "  Parameter containing:\n",
       "  tensor([ 1.0594e-03,  7.3360e-04,  5.4275e-03, -1.3735e-03,  2.9526e-03,\n",
       "           5.4779e-03,  2.9630e-03,  6.7515e-03,  4.7061e-03,  3.2703e-03,\n",
       "          -2.9103e-03,  1.1439e-03, -4.9434e-03,  4.8971e-03,  1.2137e-02,\n",
       "          -5.1321e-04, -5.4592e-03, -9.2707e-04,  5.4551e-03, -4.9731e-03,\n",
       "          -6.3760e-03, -2.2797e-03,  9.0375e-03,  6.7416e-03, -5.5720e-03,\n",
       "          -8.4298e-03,  2.1314e-03, -7.0118e-03,  1.4222e-03, -3.8760e-04,\n",
       "           4.2457e-04, -2.0165e-03,  4.2753e-03,  5.7197e-03, -7.3704e-03,\n",
       "           5.8238e-03, -3.5565e-03, -4.7996e-03, -4.2372e-03, -5.0620e-03,\n",
       "          -1.4990e-02, -1.2310e-03,  1.3331e-03, -4.7811e-03,  3.9264e-03,\n",
       "          -3.6671e-03, -5.8450e-04,  4.4344e-03, -7.1720e-03,  2.5168e-03,\n",
       "          -4.0643e-03,  4.9319e-03, -2.3885e-04, -5.4040e-04,  1.5722e-03,\n",
       "           7.4370e-03,  5.1830e-03, -8.6837e-03,  4.1268e-03, -1.6794e-03,\n",
       "          -2.8951e-03,  2.9707e-03,  7.9118e-03, -5.1855e-03,  2.5876e-03,\n",
       "           1.5904e-03,  7.5800e-03, -3.5597e-04, -5.9231e-03,  6.8127e-03,\n",
       "           2.4826e-03,  3.7108e-03,  2.9196e-03,  3.6116e-03,  7.9443e-03,\n",
       "           3.0862e-03, -8.6543e-03,  1.5662e-03,  2.4446e-04, -3.0337e-03,\n",
       "           1.9950e-03, -2.4247e-05,  1.0908e-03, -1.6533e-03,  1.6330e-03,\n",
       "           7.7977e-03,  1.7669e-03,  4.4636e-03,  1.0159e-03, -3.4746e-03,\n",
       "           2.4436e-03,  5.7131e-03,  4.2470e-03, -5.5416e-03, -5.7216e-03,\n",
       "          -4.3841e-03,  3.8319e-03,  5.9825e-03,  6.1899e-03,  4.9420e-03,\n",
       "          -4.0541e-03, -1.3266e-03,  4.5631e-03, -2.1548e-03, -3.9163e-03,\n",
       "          -3.1150e-03, -2.9764e-03,  5.0070e-04,  2.5498e-03, -6.9606e-03,\n",
       "          -7.9816e-04, -4.7175e-03, -1.3018e-03, -1.9629e-03, -3.5828e-03,\n",
       "          -5.3087e-03,  4.1970e-03,  2.5610e-03, -3.1208e-03,  1.0011e-03,\n",
       "           1.9064e-03, -3.0591e-03, -1.7905e-03, -9.0249e-04,  2.3697e-03,\n",
       "          -2.3032e-03, -4.0901e-03, -1.3419e-03, -8.8246e-03, -9.4603e-03,\n",
       "          -2.5765e-03, -1.8946e-03,  2.5719e-03, -6.7492e-05,  5.9619e-03,\n",
       "           2.7534e-03,  3.1956e-03, -1.1016e-02, -1.7439e-03, -6.5044e-04,\n",
       "          -2.6204e-03,  3.0428e-04, -5.8721e-03,  2.7891e-03, -3.1360e-04,\n",
       "          -8.8855e-03,  4.4342e-04, -2.2926e-03, -3.9622e-04, -5.2779e-07,\n",
       "           1.7250e-03,  5.3064e-03,  1.1023e-03, -6.9703e-03,  2.4717e-03,\n",
       "          -7.1629e-03,  7.6938e-04,  3.0848e-03, -3.8789e-04, -1.7353e-03,\n",
       "           9.7585e-04, -6.6693e-03, -5.4362e-03,  7.7898e-03,  6.0392e-04,\n",
       "           1.2048e-03,  6.6547e-03, -3.3710e-03,  4.2173e-04,  2.5011e-04,\n",
       "          -1.2513e-02,  1.1305e-03, -4.8391e-04, -2.6244e-03, -1.9154e-03,\n",
       "           9.1177e-04, -5.1075e-03,  2.2962e-03, -6.9221e-03,  1.9701e-03,\n",
       "          -4.4487e-03,  4.0503e-03, -1.6533e-03, -8.0324e-03,  9.8980e-03,\n",
       "           2.3873e-04, -7.0287e-03,  1.4790e-03, -9.0560e-03,  2.7626e-03,\n",
       "           7.6348e-03, -3.8236e-03,  2.1748e-03,  5.6715e-03, -4.1079e-03,\n",
       "          -8.6330e-03,  2.7571e-03,  1.0436e-04,  6.7715e-03, -7.0711e-03,\n",
       "           2.2036e-03, -7.8541e-03, -5.0218e-03, -2.9681e-03,  4.8014e-03,\n",
       "          -1.5437e-03, -2.7398e-03, -2.3554e-03,  2.5706e-04,  1.0561e-03,\n",
       "          -9.7760e-04, -3.3922e-03,  5.5928e-03, -6.7160e-03,  3.3686e-03,\n",
       "           4.3242e-03, -5.4408e-03,  1.5408e-03,  1.0058e-04, -7.9996e-04,\n",
       "          -3.2629e-03, -2.2161e-03, -1.3177e-03, -5.1807e-05,  2.3609e-03,\n",
       "           4.9950e-03, -2.0602e-03, -5.4615e-03,  2.8950e-04,  2.0184e-03,\n",
       "           4.7532e-03,  5.4658e-03,  4.3320e-03, -1.0169e-03,  1.0637e-02,\n",
       "          -4.3951e-03,  4.9507e-03,  8.5549e-03,  3.3624e-03, -7.0672e-03,\n",
       "           9.0887e-04,  1.4394e-03,  1.5079e-03,  5.6342e-03, -9.1224e-03,\n",
       "          -3.6396e-03, -2.1828e-03,  4.9893e-03, -3.6264e-03,  9.5204e-05,\n",
       "          -2.1416e-03, -3.7578e-03, -6.5791e-04, -2.7338e-03,  3.7211e-03,\n",
       "          -6.4095e-04,  6.2848e-03,  4.4657e-03, -8.9693e-04, -2.8121e-03,\n",
       "           4.7351e-03,  1.9476e-03,  1.6045e-03, -1.1358e-03,  4.0842e-03,\n",
       "          -9.0565e-03,  1.8678e-03,  8.2763e-03, -2.0891e-03, -6.1725e-03,\n",
       "          -3.8683e-03, -5.6557e-03,  3.6121e-04,  5.6306e-03, -1.0951e-03,\n",
       "           4.2146e-03, -4.4165e-04,  5.4636e-04,  8.3873e-03, -4.5464e-03,\n",
       "          -5.4504e-04,  1.1124e-03,  6.7319e-04,  3.3140e-03,  7.0477e-04,\n",
       "           4.5297e-03, -4.1698e-03,  1.6159e-03,  4.5990e-03,  1.7277e-04,\n",
       "          -1.8935e-03, -2.4641e-03,  2.4040e-03, -1.4627e-03, -4.0213e-03,\n",
       "           3.1960e-03,  4.2126e-03,  2.7681e-04,  4.9316e-03,  2.9775e-03,\n",
       "           2.0247e-04,  3.3949e-03, -3.0399e-03, -5.9004e-04, -1.5002e-03,\n",
       "           1.5973e-03,  4.3539e-03,  2.7664e-03,  6.4046e-04, -6.4387e-03,\n",
       "          -2.8438e-03,  7.9306e-03,  3.5898e-03, -2.4789e-03, -8.8152e-04,\n",
       "          -1.2068e-03, -9.2806e-04, -1.4852e-03, -7.5694e-03, -6.4826e-04,\n",
       "           2.6812e-03, -1.2762e-03, -5.3144e-03,  2.9498e-03, -3.1411e-03,\n",
       "          -1.2730e-03,  1.1371e-03,  5.1353e-03, -2.9256e-03, -7.5396e-03,\n",
       "           3.9064e-03, -4.7005e-03, -2.0528e-03,  1.2676e-04,  2.9950e-03,\n",
       "          -2.3699e-03,  3.1314e-03, -9.5319e-03,  4.2820e-04, -4.4032e-03,\n",
       "          -2.9295e-03, -4.2719e-03,  1.6849e-03, -1.1095e-03,  2.1484e-05,\n",
       "           9.6609e-05, -3.0547e-05, -7.5023e-04, -1.7178e-03,  1.1481e-02,\n",
       "          -3.4957e-03, -9.1051e-04, -7.6060e-03, -1.8178e-03,  4.8592e-03,\n",
       "          -7.6999e-04, -2.0050e-03,  8.9431e-04, -3.4881e-03,  4.0137e-04,\n",
       "           1.7924e-03,  1.3335e-04, -1.4130e-03, -3.0965e-03,  1.5006e-03,\n",
       "          -4.8251e-03,  1.0819e-02,  1.6536e-03, -2.2440e-03,  5.9803e-03,\n",
       "           5.9664e-04, -2.1570e-03, -2.5898e-03,  7.5594e-03, -1.3107e-03,\n",
       "          -4.2581e-03, -3.1261e-03,  2.2403e-03,  8.4835e-03, -2.2481e-03,\n",
       "          -1.3518e-03, -6.4194e-05, -2.6891e-03,  1.8407e-03,  8.2444e-03,\n",
       "          -1.6562e-03, -5.8601e-03, -5.1697e-03,  5.3567e-03, -1.0270e-02,\n",
       "          -5.2720e-03, -5.0548e-03,  7.9132e-03, -7.8214e-03,  1.3451e-03,\n",
       "           8.1410e-04,  1.7276e-03, -2.9032e-03, -1.2550e-02,  7.9872e-03,\n",
       "           3.6167e-03, -1.3335e-02,  7.5158e-04, -2.8252e-03, -2.7430e-03,\n",
       "           3.8431e-03,  4.6080e-04, -1.3090e-03,  5.2109e-03,  6.2680e-03,\n",
       "           1.9884e-03, -4.4133e-03,  7.9712e-03,  1.0295e-02,  1.5195e-02,\n",
       "           1.5326e-03,  2.9530e-04,  2.3002e-03,  3.9347e-03,  5.3343e-03,\n",
       "           1.8077e-03,  7.6865e-03, -1.0675e-03, -6.7227e-03,  9.4528e-03,\n",
       "          -8.0139e-03,  5.8060e-03, -1.6217e-03,  3.8840e-03,  4.2952e-03,\n",
       "           8.7806e-04, -1.0222e-03,  1.1811e-02, -5.7697e-03,  6.9922e-03,\n",
       "          -1.3790e-02,  7.2457e-04,  1.5484e-02,  2.7183e-03, -5.3224e-03,\n",
       "          -3.7296e-03, -2.6280e-03, -9.5657e-03,  1.3827e-02,  5.3420e-03,\n",
       "          -1.3356e-03,  3.6706e-03,  6.9164e-03,  4.3925e-03, -1.5419e-03,\n",
       "           4.1382e-03, -4.0516e-03, -4.1103e-03,  3.0949e-04, -9.4350e-05,\n",
       "          -6.9190e-03,  1.9689e-03,  2.2831e-03, -1.4378e-03, -6.8970e-04,\n",
       "           1.1415e-03,  9.4667e-04, -7.2704e-03, -2.9382e-03,  7.2932e-03,\n",
       "           3.1627e-04,  2.7267e-03,  4.2801e-03, -6.0559e-03, -5.5708e-03,\n",
       "           1.2757e-03, -3.5795e-03, -7.2389e-03, -1.7668e-03,  1.8800e-03,\n",
       "           3.0886e-03,  2.1105e-03,  1.5674e-03,  1.2828e-03,  5.9259e-03,\n",
       "           8.4010e-04, -1.6178e-03,  1.0639e-03,  3.5721e-03, -2.8771e-03,\n",
       "           3.1407e-03,  8.1162e-04, -9.5287e-03, -2.0651e-03,  4.2698e-03,\n",
       "          -2.0288e-03,  2.5148e-03,  1.6169e-03, -2.8961e-03, -1.0559e-03,\n",
       "          -4.0905e-03,  9.4554e-04,  3.0283e-03, -5.2308e-03,  2.0919e-03,\n",
       "           3.8192e-03, -1.2990e-03,  3.8938e-03, -1.5828e-03,  1.0480e-03,\n",
       "           2.5161e-03, -3.4312e-03,  3.8059e-03,  8.2166e-04,  4.0572e-03,\n",
       "           3.1652e-03,  2.1724e-03, -5.9991e-03,  5.7763e-03, -7.6956e-03,\n",
       "          -5.4374e-03, -3.1216e-04, -3.2782e-03, -4.3137e-03, -3.2482e-03,\n",
       "           2.7950e-03, -6.8682e-04, -1.6691e-03,  2.8336e-03, -2.0973e-03,\n",
       "          -6.6023e-03,  1.5846e-03, -5.0377e-03,  1.9074e-03,  2.4591e-03,\n",
       "          -3.9174e-03, -1.2733e-03,  1.0145e-03, -4.4710e-03,  2.9291e-03,\n",
       "          -9.5645e-04, -3.1429e-04, -2.2444e-03, -4.0813e-03, -3.7001e-03,\n",
       "           2.2773e-03, -2.2256e-03, -5.2029e-03,  7.8617e-03,  2.1254e-03,\n",
       "          -1.8045e-03, -3.9910e-03,  1.9054e-03, -9.5417e-03,  5.0948e-03,\n",
       "          -4.0267e-04,  3.8395e-03,  2.9889e-03,  8.9627e-03,  9.6131e-04,\n",
       "          -1.7270e-03, -2.3549e-03,  2.1966e-03,  3.3036e-03, -4.8108e-04,\n",
       "           3.8014e-03, -8.1695e-05,  2.3977e-04,  3.7977e-03,  3.2262e-03,\n",
       "          -5.8078e-03,  9.9914e-05,  1.7071e-03, -1.2467e-03,  3.1293e-03,\n",
       "           2.4645e-03, -2.5845e-03, -3.4412e-03, -2.7490e-03,  2.6711e-04,\n",
       "           1.1518e-03, -9.5652e-04,  8.7983e-03,  4.3791e-03, -6.1024e-03,\n",
       "          -1.9999e-02,  4.4730e-03, -7.9730e-04,  2.0613e-03, -1.2412e-02,\n",
       "           1.4591e-02,  1.2318e-02,  4.5111e-03,  4.7868e-03,  3.4282e-03,\n",
       "          -2.9516e-02, -6.3859e-03,  1.7355e-02,  1.2119e-02, -1.2059e-02,\n",
       "           1.0308e-02, -8.9580e-03,  1.3823e-02, -8.1396e-03,  7.3271e-03,\n",
       "           1.0888e-02, -2.8236e-03,  8.1908e-03,  1.2593e-02, -3.4558e-03,\n",
       "           7.6344e-04,  2.3814e-03, -1.2000e-02,  1.2937e-02, -9.3714e-03,\n",
       "          -6.4529e-03, -1.5321e-02,  1.0610e-02,  4.2802e-04,  1.3208e-02,\n",
       "           5.3129e-03, -6.5369e-03,  8.9730e-03, -3.8083e-03,  1.2664e-02,\n",
       "           6.9007e-03,  3.6855e-03,  1.1223e-02,  7.8671e-03,  1.2608e-03,\n",
       "          -3.5934e-03,  8.5362e-03,  7.7392e-03, -6.2439e-04,  6.8258e-03,\n",
       "           1.7344e-03, -4.5699e-03,  5.1521e-03,  7.7407e-03,  4.7372e-03,\n",
       "           4.2346e-03,  3.7088e-03,  9.2489e-03,  1.1715e-02, -4.3824e-03,\n",
       "          -3.4954e-03,  5.5534e-03,  8.3839e-04,  2.5224e-03,  9.5239e-03,\n",
       "          -4.8281e-03,  1.2804e-03,  7.7164e-04, -8.6801e-03, -1.1034e-03,\n",
       "          -5.4081e-03, -2.8767e-03, -1.1854e-03,  5.9572e-03,  6.5750e-03,\n",
       "           9.9296e-04,  2.2244e-03,  1.6213e-03, -6.3055e-03, -3.4300e-03,\n",
       "           1.4805e-03,  3.5275e-03, -1.5863e-03, -4.3710e-04, -5.3842e-04,\n",
       "           1.9117e-03, -9.1832e-03,  4.3838e-03, -6.9584e-04, -3.9889e-03,\n",
       "          -4.8672e-03,  7.2911e-03,  8.7121e-03, -4.5809e-03,  6.0049e-03,\n",
       "          -6.1273e-03, -1.2031e-04, -3.3488e-03, -1.2943e-02, -8.3492e-03,\n",
       "           3.5334e-03, -9.5223e-03, -2.1647e-03, -4.9706e-03, -6.3692e-03,\n",
       "           4.2424e-03,  4.3901e-03,  1.7415e-03,  1.7827e-03,  5.5266e-04,\n",
       "           6.0014e-03, -9.2381e-03,  1.3176e-03, -1.0151e-02, -6.6623e-03,\n",
       "           7.5403e-03,  1.2798e-02,  1.1754e-03,  3.1368e-03,  1.0764e-02,\n",
       "           1.1818e-02,  1.0479e-02,  3.5519e-03, -6.3688e-03, -5.3454e-03,\n",
       "           3.0223e-03,  2.2920e-03,  1.9793e-03,  2.6656e-03, -5.3251e-03,\n",
       "          -3.0441e-03,  1.6099e-03,  7.8349e-04,  3.9613e-03,  2.2048e-03,\n",
       "          -9.7030e-03,  9.0286e-03, -4.2312e-03, -4.9334e-03, -1.2192e-03,\n",
       "           1.0275e-03, -3.4095e-03,  2.2985e-03,  1.2645e-03, -2.2265e-03,\n",
       "          -7.4074e-03, -1.2246e-04, -3.4760e-03, -1.3233e-03, -7.3279e-03,\n",
       "          -2.1692e-03,  3.0799e-03, -3.4639e-03, -3.1355e-03,  2.9041e-03,\n",
       "           1.8976e-03, -1.3619e-03,  4.6187e-03,  1.1680e-03, -4.3926e-04,\n",
       "          -3.6108e-03, -2.8486e-03,  5.8254e-03,  5.5916e-03, -4.6303e-03,\n",
       "           1.7158e-03, -8.0431e-04,  5.0892e-03,  5.2776e-03, -1.1302e-03,\n",
       "          -2.6286e-03, -3.6092e-03, -6.5174e-03, -1.5916e-03,  1.6768e-03,\n",
       "           2.3379e-03, -2.4745e-03, -1.4414e-03,  1.2543e-03, -7.3630e-04,\n",
       "           4.3146e-03, -3.9160e-03, -7.9162e-04,  7.1797e-03, -3.1713e-03,\n",
       "           2.3081e-03,  6.8095e-03, -2.5026e-03], requires_grad=True)),\n",
       " ('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0080, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0473,  0.0000,  0.0000,  ..., -0.0156,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0036, -0.0000, -0.0526],\n",
       "          ...,\n",
       "          [-0.0239, -0.0330,  0.0000,  ...,  0.0000,  0.0000,  0.0018],\n",
       "          [-0.0000, -0.0816, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0496, -0.0000,  ...,  0.0441, -0.0000,  0.0385]],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pruning 영구적용\n",
    "# =>Pruning 적용을 위해서는 weight_orig, weight_mask, bias_orig, bias_mask 등과 pre_hook 등 모듈들을 제거해야함\n",
    "# => remove 메소드 사용\n",
    "\n",
    "prune.remove(module, 'weight')\n",
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb9d48f0-ec01-403b-9d63-6d2fc1d4c639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0080, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0473,  0.0000,  0.0000,  ..., -0.0156,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0036, -0.0000, -0.0526],\n",
       "          ...,\n",
       "          [-0.0239, -0.0330,  0.0000,  ...,  0.0000,  0.0000,  0.0018],\n",
       "          [-0.0000, -0.0816, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0496, -0.0000,  ...,  0.0441, -0.0000,  0.0385]],\n",
       "         requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0011,  0.0007,  0.0054, -0.0014,  0.0030,  0.0055,  0.0030,  0.0068,\n",
       "           0.0047,  0.0033, -0.0029,  0.0011, -0.0049,  0.0049,  0.0121, -0.0005,\n",
       "          -0.0055, -0.0009,  0.0055, -0.0050, -0.0064, -0.0023,  0.0090,  0.0067,\n",
       "          -0.0056, -0.0084,  0.0021, -0.0070,  0.0014, -0.0000,  0.0000, -0.0020,\n",
       "           0.0043,  0.0057, -0.0074,  0.0058, -0.0036, -0.0048, -0.0042, -0.0051,\n",
       "          -0.0150, -0.0012,  0.0013, -0.0048,  0.0039, -0.0037, -0.0006,  0.0044,\n",
       "          -0.0072,  0.0025, -0.0041,  0.0049, -0.0000, -0.0005,  0.0016,  0.0074,\n",
       "           0.0052, -0.0087,  0.0041, -0.0017, -0.0029,  0.0030,  0.0079, -0.0052,\n",
       "           0.0026,  0.0016,  0.0076, -0.0000, -0.0059,  0.0068,  0.0025,  0.0037,\n",
       "           0.0029,  0.0036,  0.0079,  0.0031, -0.0087,  0.0016,  0.0000, -0.0030,\n",
       "           0.0020, -0.0000,  0.0011, -0.0017,  0.0016,  0.0078,  0.0018,  0.0045,\n",
       "           0.0010, -0.0035,  0.0024,  0.0057,  0.0042, -0.0055, -0.0057, -0.0044,\n",
       "           0.0038,  0.0060,  0.0062,  0.0049, -0.0041, -0.0013,  0.0046, -0.0022,\n",
       "          -0.0039, -0.0031, -0.0030,  0.0005,  0.0025, -0.0070, -0.0008, -0.0047,\n",
       "          -0.0013, -0.0020, -0.0036, -0.0053,  0.0042,  0.0026, -0.0031,  0.0010,\n",
       "           0.0019, -0.0031, -0.0018, -0.0009,  0.0024, -0.0023, -0.0041, -0.0013,\n",
       "          -0.0088, -0.0095, -0.0026, -0.0019,  0.0026, -0.0000,  0.0060,  0.0028,\n",
       "           0.0032, -0.0110, -0.0017, -0.0007, -0.0026,  0.0000, -0.0059,  0.0028,\n",
       "          -0.0000, -0.0089,  0.0004, -0.0023, -0.0000, -0.0000,  0.0017,  0.0053,\n",
       "           0.0011, -0.0070,  0.0025, -0.0072,  0.0008,  0.0031, -0.0000, -0.0017,\n",
       "           0.0010, -0.0067, -0.0054,  0.0078,  0.0006,  0.0012,  0.0067, -0.0034,\n",
       "           0.0000,  0.0000, -0.0125,  0.0011, -0.0005, -0.0026, -0.0019,  0.0009,\n",
       "          -0.0051,  0.0023, -0.0069,  0.0020, -0.0044,  0.0041, -0.0017, -0.0080,\n",
       "           0.0099,  0.0000, -0.0070,  0.0015, -0.0091,  0.0028,  0.0076, -0.0038,\n",
       "           0.0022,  0.0057, -0.0041, -0.0086,  0.0028,  0.0000,  0.0068, -0.0071,\n",
       "           0.0022, -0.0079, -0.0050, -0.0030,  0.0048, -0.0015, -0.0027, -0.0024,\n",
       "           0.0000,  0.0011, -0.0010, -0.0034,  0.0056, -0.0067,  0.0034,  0.0043,\n",
       "          -0.0054,  0.0015,  0.0000, -0.0008, -0.0033, -0.0022, -0.0013, -0.0000,\n",
       "           0.0024,  0.0050, -0.0021, -0.0055,  0.0000,  0.0020,  0.0048,  0.0055,\n",
       "           0.0043, -0.0010,  0.0106, -0.0044,  0.0050,  0.0086,  0.0034, -0.0071,\n",
       "           0.0009,  0.0014,  0.0015,  0.0056, -0.0091, -0.0036, -0.0022,  0.0050,\n",
       "          -0.0036,  0.0000, -0.0021, -0.0038, -0.0007, -0.0027,  0.0037, -0.0006,\n",
       "           0.0063,  0.0045, -0.0009, -0.0028,  0.0047,  0.0019,  0.0016, -0.0011,\n",
       "           0.0041, -0.0091,  0.0019,  0.0083, -0.0021, -0.0062, -0.0039, -0.0057,\n",
       "           0.0000,  0.0056, -0.0011,  0.0042, -0.0000,  0.0005,  0.0084, -0.0045,\n",
       "          -0.0005,  0.0011,  0.0007,  0.0033,  0.0007,  0.0045, -0.0042,  0.0016,\n",
       "           0.0046,  0.0000, -0.0019, -0.0025,  0.0024, -0.0015, -0.0040,  0.0032,\n",
       "           0.0042,  0.0000,  0.0049,  0.0030,  0.0000,  0.0034, -0.0030, -0.0006,\n",
       "          -0.0015,  0.0016,  0.0044,  0.0028,  0.0006, -0.0064, -0.0028,  0.0079,\n",
       "           0.0036, -0.0025, -0.0009, -0.0012, -0.0009, -0.0015, -0.0076, -0.0006,\n",
       "           0.0027, -0.0013, -0.0053,  0.0029, -0.0031, -0.0013,  0.0011,  0.0051,\n",
       "          -0.0029, -0.0075,  0.0039, -0.0047, -0.0021,  0.0000,  0.0030, -0.0024,\n",
       "           0.0031, -0.0095,  0.0000, -0.0044, -0.0029, -0.0043,  0.0017, -0.0011,\n",
       "           0.0000,  0.0000, -0.0000, -0.0008, -0.0017,  0.0115, -0.0035, -0.0009,\n",
       "          -0.0076, -0.0018,  0.0049, -0.0008, -0.0020,  0.0009, -0.0035,  0.0000,\n",
       "           0.0018,  0.0000, -0.0014, -0.0031,  0.0015, -0.0048,  0.0108,  0.0017,\n",
       "          -0.0022,  0.0060,  0.0006, -0.0022, -0.0026,  0.0076, -0.0013, -0.0043,\n",
       "          -0.0031,  0.0022,  0.0085, -0.0022, -0.0014, -0.0000, -0.0027,  0.0018,\n",
       "           0.0082, -0.0017, -0.0059, -0.0052,  0.0054, -0.0103, -0.0053, -0.0051,\n",
       "           0.0079, -0.0078,  0.0013,  0.0008,  0.0017, -0.0029, -0.0126,  0.0080,\n",
       "           0.0036, -0.0133,  0.0008, -0.0028, -0.0027,  0.0038,  0.0005, -0.0013,\n",
       "           0.0052,  0.0063,  0.0020, -0.0044,  0.0080,  0.0103,  0.0152,  0.0015,\n",
       "           0.0000,  0.0023,  0.0039,  0.0053,  0.0018,  0.0077, -0.0011, -0.0067,\n",
       "           0.0095, -0.0080,  0.0058, -0.0016,  0.0039,  0.0043,  0.0009, -0.0010,\n",
       "           0.0118, -0.0058,  0.0070, -0.0138,  0.0007,  0.0155,  0.0027, -0.0053,\n",
       "          -0.0037, -0.0026, -0.0096,  0.0138,  0.0053, -0.0013,  0.0037,  0.0069,\n",
       "           0.0044, -0.0015,  0.0041, -0.0041, -0.0041,  0.0000, -0.0000, -0.0069,\n",
       "           0.0020,  0.0023, -0.0014, -0.0007,  0.0011,  0.0009, -0.0073, -0.0029,\n",
       "           0.0073,  0.0000,  0.0027,  0.0043, -0.0061, -0.0056,  0.0013, -0.0036,\n",
       "          -0.0072, -0.0018,  0.0019,  0.0031,  0.0021,  0.0016,  0.0013,  0.0059,\n",
       "           0.0008, -0.0016,  0.0011,  0.0036, -0.0029,  0.0031,  0.0008, -0.0095,\n",
       "          -0.0021,  0.0043, -0.0020,  0.0025,  0.0016, -0.0029, -0.0011, -0.0041,\n",
       "           0.0009,  0.0030, -0.0052,  0.0021,  0.0038, -0.0013,  0.0039, -0.0016,\n",
       "           0.0010,  0.0025, -0.0034,  0.0038,  0.0008,  0.0041,  0.0032,  0.0022,\n",
       "          -0.0060,  0.0058, -0.0077, -0.0054, -0.0000, -0.0033, -0.0043, -0.0032,\n",
       "           0.0028, -0.0007, -0.0017,  0.0028, -0.0021, -0.0066,  0.0016, -0.0050,\n",
       "           0.0019,  0.0025, -0.0039, -0.0013,  0.0010, -0.0045,  0.0029, -0.0010,\n",
       "          -0.0000, -0.0022, -0.0041, -0.0037,  0.0023, -0.0022, -0.0052,  0.0079,\n",
       "           0.0021, -0.0018, -0.0040,  0.0019, -0.0095,  0.0051, -0.0000,  0.0038,\n",
       "           0.0030,  0.0090,  0.0010, -0.0017, -0.0024,  0.0022,  0.0033, -0.0005,\n",
       "           0.0038, -0.0000,  0.0000,  0.0038,  0.0032, -0.0058,  0.0000,  0.0017,\n",
       "          -0.0012,  0.0031,  0.0025, -0.0026, -0.0034, -0.0027,  0.0000,  0.0012,\n",
       "          -0.0010,  0.0088,  0.0044, -0.0061, -0.0200,  0.0045, -0.0008,  0.0021,\n",
       "          -0.0124,  0.0146,  0.0123,  0.0045,  0.0048,  0.0034, -0.0295, -0.0064,\n",
       "           0.0174,  0.0121, -0.0121,  0.0103, -0.0090,  0.0138, -0.0081,  0.0073,\n",
       "           0.0109, -0.0028,  0.0082,  0.0126, -0.0035,  0.0008,  0.0024, -0.0120,\n",
       "           0.0129, -0.0094, -0.0065, -0.0153,  0.0106,  0.0000,  0.0132,  0.0053,\n",
       "          -0.0065,  0.0090, -0.0038,  0.0127,  0.0069,  0.0037,  0.0112,  0.0079,\n",
       "           0.0013, -0.0036,  0.0085,  0.0077, -0.0006,  0.0068,  0.0017, -0.0046,\n",
       "           0.0052,  0.0077,  0.0047,  0.0042,  0.0037,  0.0092,  0.0117, -0.0044,\n",
       "          -0.0035,  0.0056,  0.0008,  0.0025,  0.0095, -0.0048,  0.0013,  0.0008,\n",
       "          -0.0087, -0.0011, -0.0054, -0.0029, -0.0012,  0.0060,  0.0066,  0.0010,\n",
       "           0.0022,  0.0016, -0.0063, -0.0034,  0.0015,  0.0035, -0.0016, -0.0000,\n",
       "          -0.0005,  0.0019, -0.0092,  0.0044, -0.0007, -0.0040, -0.0049,  0.0073,\n",
       "           0.0087, -0.0046,  0.0060, -0.0061, -0.0000, -0.0033, -0.0129, -0.0083,\n",
       "           0.0035, -0.0095, -0.0022, -0.0050, -0.0064,  0.0042,  0.0044,  0.0017,\n",
       "           0.0018,  0.0006,  0.0060, -0.0092,  0.0013, -0.0102, -0.0067,  0.0075,\n",
       "           0.0128,  0.0012,  0.0031,  0.0108,  0.0118,  0.0105,  0.0036, -0.0064,\n",
       "          -0.0053,  0.0030,  0.0023,  0.0020,  0.0027, -0.0053, -0.0030,  0.0016,\n",
       "           0.0008,  0.0040,  0.0022, -0.0097,  0.0090, -0.0042, -0.0049, -0.0012,\n",
       "           0.0010, -0.0034,  0.0023,  0.0013, -0.0022, -0.0074, -0.0000, -0.0035,\n",
       "          -0.0013, -0.0073, -0.0022,  0.0031, -0.0035, -0.0031,  0.0029,  0.0019,\n",
       "          -0.0014,  0.0046,  0.0012, -0.0000, -0.0036, -0.0028,  0.0058,  0.0056,\n",
       "          -0.0046,  0.0017, -0.0008,  0.0051,  0.0053, -0.0011, -0.0026, -0.0036,\n",
       "          -0.0065, -0.0016,  0.0017,  0.0023, -0.0025, -0.0014,  0.0013, -0.0007,\n",
       "           0.0043, -0.0039, -0.0008,  0.0072, -0.0032,  0.0023,  0.0068, -0.0025],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bias 도 remove 함\n",
    "prune.remove(module, \"bias\")\n",
    "list(module.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76f69eef-fee1-4400-837d-a5f4bd871fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# named_buffers() 출력 해봄\n",
    "list(module.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bf8c9bf-47dd-4614-861e-079331e63c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _forward_pre_hooks 출력해봄\n",
    "module._forward_pre_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae24f03e-3df1-4f99-b57b-f0e9af5db7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 저장해 봄\n",
    "import os\n",
    "out_path = '../../../model/bert/bmc-fpt-bong_corpus_mecab-0428-pruning1'\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "model.save_pretrained(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f8ef1-5f6f-4b38-815b-63c7dddd3f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
