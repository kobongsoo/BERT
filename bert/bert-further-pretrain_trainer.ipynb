{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2820d55-a439-416f-8287-1fcc660ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서는 기존 사전훈련된 bert 모델에 추가적으로 MLM 학습 시키는 과정을 설명한다.\n",
    "# => 사전은 기존 사전에 단어가 추가된 사전을 이용한다.(추가된 사전은 별도로 만들어야 함)\n",
    "#\n",
    "# 참고 사이트 \n",
    "# https://www.fatalerrors.org/a/further-pre-training-of-chinese-language-model-bert-roberta.html\n",
    "# 소스는 : https://github.com/zhusleep/pytorch_chinese_lm_pretrain 에 run_language_model_bert.py 참조함\n",
    "# MLM 다른 소스는 : https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c 참조 바람\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, TextDatasetForNextSentencePrediction, GPU_info, mlogging\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, BertConfig, AutoModelWithLMHead, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d6b3ba-8875-44e2-9fec-1125b6d01de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수들 설정\n",
    "\n",
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "input_corpus = \"../../data11/my_corpus/moco-corpus-kowiki2022.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "input_model_path = \"../../data11/model/bert/bert-multilingual-cased/\"\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_file=\"../tokenizer_sample/moco-vocab/moco-corpus-kowiki202206-42000/vocab.txt\"\n",
    "# 출력 모델 저장 경로\n",
    "ouput_model_dir = \"../../data11/model/bert/bmc-moco-corpus-kowiki202206-42000\"\n",
    "\n",
    "# 토큰활 할때 최대 길이 \n",
    "token_max_len = 130\n",
    "\n",
    "# 훈련용 변수\n",
    "batch_size = 32   # 64 하면, GPU Out of memory 발생함(=>**따라서 32 진행)\n",
    "train_epochs = 10\n",
    "learning_rate = 5e-5  # trainer로 훈련할때 기본이 5e-5임\n",
    " # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정(기본:512) 512, 1024, 2048 식으로 지정함, 엄청난 장문을 다룰경우 10124까지\n",
    "#max_position_embeddings = 128 \n",
    "logging_steps = 10000  # 훈련시, 로깅할 step 수 (크면 10000번 정도하고, 작으면 100번정도)\n",
    "save_steps = 50000     # 10000 step마다 모델 저장\n",
    "save_total_limit = 2 # 마지막 3개 모델 빼고 과거 모델은 삭제(100000번째마다 모델 저장하는데, 마지감 3개 빼고 나머지는 삭제)\n",
    "\n",
    "# NSP 관련 변수 (*여기서는 필요없음)\n",
    "#NSP_block_size = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1c681-0342-4212-8745-099ad9457966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertfpt_2022-09-07.log\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertfpt\", logfilename=\"bertfpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6914a0a9-1cd0-4fb6-9c55-8ce7c236ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_token_size: 5, tokenizer.vocab_size: 152537\n",
      "vocab_size: 152538\n",
      "tokenizer_len: 152537\n"
     ]
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "'''\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, \n",
    "                          max_len=token_max_len, \n",
    "                          do_lower_case=False)\n",
    "'''\n",
    "\n",
    "\n",
    "#tokenizer = BertTokenizerFast(vocab_speical_path)\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file=vocab_file,\n",
    "    max_len=token_max_len,\n",
    "    do_lower_case=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0145f5f5-119b-4042-97da-0ca91b58e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:998: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ../../data11/model/bert/bert-multilingual-cased/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "further pre-training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(152537, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=152537, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "#output_hidden_states = False # 기본은 False=>output 2개 출력됨, True로 지정하면 output이 3개 출력됨\n",
    "#return_dict = True   #False로 지정하는 경우 일반적인 tuple을 리턴, True인 경우는 transformers.file_utils.ModelOutput(ouput.logisc) 으로 리턴\n",
    "#model = BertModel.from_pretrained(input_model_path, output_hidden_states = output_hidden_states, return_dict = return_dict)\n",
    "\n",
    "# AutoModelWithLMHead 대신에 -> AutoModelForMaskedLM, BertForMaskedLM 해도됨\n",
    "#=>원래 AutoModelWithLMHead 하면 내부적으로 BertForMaskedLM 가 로딩됨\n",
    "\n",
    "if input_model_path:\n",
    "    # further pre-training 인 경우 (기존 거에 추가적으로 하는 경우)\n",
    "    config = BertConfig.from_pretrained(input_model_path)\n",
    "    \n",
    "    model = AutoModelWithLMHead.from_pretrained(input_model_path,\n",
    "                                                from_tf=bool(\".ckpt\" in input_model_path),\n",
    "                                                config=config) \n",
    "    print('further pre-training')\n",
    "else:\n",
    "    # Training new model from scratch 인 경우 (완전 새롭게 모델을 만드는 경우)\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    print('Training new model from scratch')\n",
    " \n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 훈련모드로 변경(평가모드 : model.eval())\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48949c94-965c-4192-b7b8-0db1f28c744b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203343833"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a9d7d0-46dc-449e-b220-32c9666d4cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at ../../data11/my_corpus/moco-corpus-kowiki2022.txt\n",
      "==>[Start] file read lines: ../../data11/my_corpus/moco-corpus-kowiki2022.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7cc31753b54319ac4a47f11b845c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7680008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<==[End] file read lines: ../../data11/my_corpus/moco-corpus-kowiki2022.txt\n",
      "==>[Start] tokenizer\n",
      "<==[End] tokenizer\n",
      "<myutils.bwpdataset.MyLineByLineTextDataset object at 0x7fc32bba4970>\n"
     ]
    }
   ],
   "source": [
    "# MLM(Markup Language Model), NSP(Next Sentence Prediction) 구성\n",
    "from transformers import DataCollatorForLanguageModeling, TextDataset\n",
    "from myutils import MyTextDataset, MyLineByLineTextDataset\n",
    "\n",
    "'''\n",
    "# NSP 만들기\n",
    "train_dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=input_corpus,\n",
    "    block_size=NSP_block_size,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.1,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "'''\n",
    "\n",
    "# further-pretrain 일때는 일단 NSP는 입력안함.\n",
    "# => 따라서 입력 corpus에 대해, NSP Dataset 이 아니라, TextDataset 으로 만듬\n",
    "# => 라인 구분된 말뭉치는 MyLineByLineTextDataset 사용, 라인 구분없는 말뭉치는 MyTextDataset 이용, \n",
    "\n",
    "#train_dataset = MyTextDataset(tokenizer=tokenizer, file_path=input_corpus, block_size=token_max_len, overwrite_cache=True)\n",
    "train_dataset = MyLineByLineTextDataset(tokenizer=tokenizer, file_path=input_corpus, block_size=token_max_len)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c83c94-2e9e-48de-9485-6eba3e382606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'Refer', 'to', 'the', 'V', '$', 'SYSTEM', '_', 'EVENT', 'view', 'for', 'time', 'wait', '##ed', 'and', 'average', 'waits', 'for', 'thefollowing', 'actions', ':', '[SEP]']]\n",
      "count:0=>{'input_ids': tensor([   101, 142678,  10114,  10105,    159,    109, 122321,    168, 149821,\n",
      "         17904,  10142,  10635,  83279,  10336,  10111,  13551, 126341,  10142,\n",
      "        124429,  22115,    131,    102])}\n",
      "[['[CLS]', 'To', 'estimate', 'the', 'time', 'wait', '##ed', 'for', 'reads', 'in', '##cu', '##rre', '##d', 'by', 're', '##read', '##ing', 'data', 'blocks', 'that', 'had', 'tobe', 'written', 'to', 'disk', 'because', 'of', 'a', 'request', 'from', 'another', 'instance', ',', 'multi', '##ply', 'the', 'stati', '##stic', '(', 'for', 'example', ',', 'the', 'time', 'wait', '##ed', 'for', 'db', 'ﬁ', '##le', 'sequential', 'reads', ')', 'by', 'the', 'percentage', 'of', 'read', '##I', '/', 'O', 'caused', 'by', 'previous', 'cache', 'ﬂ', '##ush', '##es', 'as', 'shown', 'in', 'this', 'formula', ':', '[SEP]']]\n",
      "count:1=>{'input_ids': tensor([   101,  11469,  78059,  10105,  10635,  83279,  10336,  10142,  91160,\n",
      "         10106,  12352,  19243,  10162,  10155,  11639,  66058,  10230,  11165,\n",
      "         47352,  10189,  10374, 136004,  13398,  10114,  50169,  12373,  10108,\n",
      "           169,  37449,  10188,  12864,  34469,    117,  21247,  59146,  10105,\n",
      "         17431,  26666,    113,  10142,  14351,    117,  10105,  10635,  83279,\n",
      "         10336,  10142,  49625, 119855,  10284, 134312,  91160,    114,  10155,\n",
      "         10105,  46971,  10108,  24944,  11281,    120,    152,  19513,  10155,\n",
      "         16741,  62070, 126429,  37026,  10171,  10146,  19989,  10106,  10531,\n",
      "         29659,    131,    102])}\n",
      "[['[CLS]', 'Where', '\"', 'lock', 'buffers', 'for', 'read', '\"', 'is', 'the', 'value', 'for', 'lock', 'converts', 'from', 'N', 'to', 'S', 'derived', 'from', '##V', '$', 'L', '##OCK', '_', 'ACT', '##IV', '##IT', '##Y', 'and', '\"', 'physical', 'reads', '\"', 'is', 'from', 'the', 'V', '$', 'SYS', '##ST', '##AT', 'view', '.', '[SEP]']]\n",
      "count:2=>{'input_ids': tensor([   101,  23525,    107,  79601, 125495,  10142,  24944,    107,  10124,\n",
      "         10105,  19211,  10142,  79601, 127838,  10188,    151,  10114,    156,\n",
      "         29369,  10188,  11779,    109,    149,  96608,    168,  90119,  91238,\n",
      "         37611,  14703,  10111,    107,  22899,  91160,    107,  10124,  10188,\n",
      "         10105,    159,    109, 125166,  32995,  32071,  17904,    119,    102])}\n",
      "[['[CLS]', 'Similarly', ',', 'the', 'proportion', 'of', 'the', 'time', 'wait', '##ed', 'for', 'database', 'ﬁ', '##le', 'parallel', 'writes', 'caused', '##by', 'ping', '##s', 'can', 'be', 'estimated', 'by', 'multi', '##ply', '##ing', 'db', 'ﬁ', '##le', 'parallel', 'write', 'time', 'as', 'found', 'in', '##V', '$', 'SYSTEM', '_', 'EVENT', '##S', 'by', ':', '[SEP]']]\n",
      "count:3=>{'input_ids': tensor([   101,  89126,    117,  10105,  66410,  10108,  10105,  10635,  83279,\n",
      "         10336,  10142,  11254, 119855,  10284,  26280,  49501,  19513,  11530,\n",
      "        133837,  10107,  10944,  10347,  25267,  10155,  21247,  59146,  10230,\n",
      "         49625, 119855,  10284,  26280,  28685,  10635,  10146,  11823,  10106,\n",
      "         11779,    109, 122321,    168, 149821,  10731,  10155,    131,    102])}\n",
      "[['[CLS]', 'Table', '11', '-', '1', 'describes', 'some', 'global', 'cache', 'co', '##here', '##nce', '-', 'related', 'views', 'and', 'the', 'types', 'of', '##stati', '##stic', '##s', 'they', 'contain', '.', '[SEP]']]\n",
      "count:4=>{'input_ids': tensor([  101, 34421, 10193,   118,   122, 34797, 11152, 18331, 62070, 11170,\n",
      "        57204, 12150,   118, 16382, 33396, 10111, 10105, 19164, 10108, 70796,\n",
      "        26666, 10107, 10689, 36003,   119,   102])}\n",
      "[['[CLS]', 'Refer', 'to', 'V', '$', 'SYS', '##ST', '##AT', 'to', 'count', '##request', '##s', 'for', 'the', 'actions', 'shown', '##to', 'the', 'right', '.', '[SEP]']]\n",
      "count:5=>{'input_ids': tensor([   101, 142678,  10114,    159,    109, 125166,  32995,  32071,  10114,\n",
      "         47777, 137691,  10107,  10142,  10105,  22115,  19989,  10340,  10105,\n",
      "         13448,    119,    102])}\n"
     ]
    }
   ],
   "source": [
    "# NSP 출력 해보기\n",
    "# => MyLineByLineTextDataset 인 경우에는 example['input_ids'] 로 MyTextDataset 인 경우엔는 example로 해줘야 함.\n",
    "count = 0\n",
    "for example in train_dataset:\n",
    "    token_str = [[tokenizer.convert_ids_to_tokens(s) for s in example['input_ids'].tolist()]]\n",
    "    print(token_str)\n",
    "    print('count:{}=>{}'.format(count,example))\n",
    "    count +=1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c85fa2-1007-4766-8c50-6f2921059ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[[['[CLS]', 'Where', '\"', 'lock', 'buffers', 'for', 'read', '\"', 'is', '[MASK]', '[MASK]', 'for', 'lock', 'converts', 'from', 'N', 'to', 'S', 'derived', 'from', '##V', '$', 'L', '##OCK', '_', 'ACT', '##IV', '##IT', '##Y', '[MASK]', '\"', '[MASK]', '[MASK]', '\"', 'is', 'from', 'the', 'V', '$', 'SYS', '##ST', '##AT', 'view', '.', '[SEP]']]]\n",
      "{'input_ids': tensor([[   101,  23525,    107,  79601, 125495,  10142,  24944,    107,  10124,\n",
      "            103,    103,  10142,  79601, 127838,  10188,    151,  10114,    156,\n",
      "          29369,  10188,  11779,    109,    149,  96608,    168,  90119,  91238,\n",
      "          37611,  14703,    103,    107,    103,    103,    107,  10124,  10188,\n",
      "          10105,    159,    109, 125166,  32995,  32071,  17904,    119,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10105,\n",
      "         19211,  -100,  -100,  -100,  -100,   151,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 10111,\n",
      "          -100, 22899, 91160,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "# MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "'''\n",
    "# MLM 출력 해보기\n",
    "for example in train_dataset:\n",
    "    print(type(example))\n",
    "    mlm_sample = data_collator(example['input_ids'])\n",
    "    token_str = [[tokenizer.convert_ids_to_tokens(s) for s in mlm_sample['input_ids']]]\n",
    "    print(token_str)\n",
    "    print('count:{}=>{}'.format(count,mlm_sample))\n",
    "    count +=1\n",
    "    if count > 5:\n",
    "        break\n",
    "'''        \n",
    "# MLM 출력 해보기\n",
    "mlm_sample = data_collator(train_dataset.examples[2:3])\n",
    "print(type(mlm_sample))\n",
    "print(mlm_sample.keys())\n",
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in mlm_sample['input_ids']]]\n",
    "print(token_str)\n",
    "print(mlm_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa6438-60bc-429a-a3d9-bda2fd9bf4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ouput_model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=train_epochs,\n",
    "    per_gpu_train_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,                      # lr: 기본 5e-5\n",
    "    save_steps=save_steps,    # step 수마다 모델을 저장\n",
    "    save_total_limit=save_total_limit, # 마지막 두 모델 빼고 과거 모델은 삭제\n",
    "    logging_steps=logging_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset   #TEXT 혹은 NSP(Next Setence Predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00887e4f-a3ba-4aef-a7c5-f89d998d36b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7679990\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2400000\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='608' max='2400000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    608/2400000 01:39 < 109:04:04, 6.11 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1495\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1497\u001b[0m )\n\u001b[0;32m-> 1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/trainer.py:1742\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1740\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1743\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1746\u001b[0m ):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938aa698-638b-46d5-8f88-4ae20050dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델 저장\n",
    "trainer.save_model(ouput_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafe127-4f99-4f7d-8931-ebe471fa9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeinizer 파일 저장(vocab)\n",
    "tokenizer.save_pretrained(ouput_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e17ed6-8b4a-4d76-969a-cd3a3dd0c415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
