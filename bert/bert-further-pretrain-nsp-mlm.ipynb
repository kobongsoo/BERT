{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2643afa8-68ce-4896-a1c4-ecf9f4ac43a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-10.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertfpt2_2022-03-10.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#========================================================================\n",
    "# MLM(Masked Language Model) 과 NSP(Next Sentence Predict) 로 Further Pre-Train 시키는 예시\n",
    "# 참고예제 : https://towardsdatascience.com/how-to-train-bert-aaad00533168\n",
    "#========================================================================\n",
    "\n",
    "from transformers import BertTokenizer, BertForPreTraining, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertfptnspmlm\", logfilname=\"bertfptnspmlm\")\n",
    "\n",
    "# tokenizer와 모델 로딩\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-multilingual-cased')\n",
    "model.to(device)\n",
    "\n",
    "# test Data 불러옴.\n",
    "# test data는 .으로 구분된 한줄 문자이 아니라. 한줄에 .로구분된 여러문장이 이어진 문장이어야 함\n",
    "# 예시:'제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
    "with open('my_data/data.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3b637d-4cb4-4d63-9c32-e127c1217d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.조지아 공과대학교를 졸업하였다.그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다.',\n",
       " '1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다.그의 별명이 \"땅콩 농부\" 로 알려졌다.1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다.',\n",
       " '대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다.조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ad5f01-8e70-44fe-9af4-02707d547c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "# NSP를 만들기 위해, .을 기준으로 문장들을 나눈 후 길이를 얻어 둔다.\n",
    "bag = [item for sentence in text for item in sentence.split('.') if item != '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6907f679-eee7-4087-9dcf-12583bed9510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'또한 소련과 제2차 전략 무기 제한 협상에 조인했다'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd0da17-b161-4522-a413-7a09145712e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NSP는 50:50으로 랜덤한 값, 랜덤하지 않은 문장으로 만든다.\n",
    "import random\n",
    "\n",
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for paragraph in text:\n",
    "    # 하나의 문장을 읽어와서 .기준으로 나눈다.\n",
    "    sentences = [sentence for sentence in paragraph.split('.') if sentence != '']\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "     # . 기준으로 나눈 문장이 1이상이면..\n",
    "    if num_sentences > 1:\n",
    "        # 문장 a 시작번지는 랜덤하게, 해당 문장 이후로 지정\n",
    "        start = random.randint(0, num_sentences-2)\n",
    "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "        # 0.5 이상 랜덤값이면, 연속적인 문장으로 만듬\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(sentences[start+1])\n",
    "            label.append(0)  #label=0이면 연속적\n",
    "        # 0.5 이하 랜덤값이면  연속적이 아닌 문장으로 만듬\n",
    "        else:\n",
    "            index = random.randint(0, bag_size-1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(bag[index])\n",
    "            label.append(1)  #label=1이면 비연속적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cca212f-e97b-48af-9187-f52885252cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다\n",
      "---\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다\n",
      "\n",
      "1\n",
      "그의 별명이 \"땅콩 농부\" 로 알려졌다\n",
      "---\n",
      "수학의 기초에 대한 위기는 그 당시 수많은 논쟁에 의해 촉발되었으며, 그 논쟁에는 칸토어의 집합론과 브라우어-힐베르트 논쟁이 포함되었다\n",
      "\n",
      "0\n",
      "조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다\n",
      "---\n",
      "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print(sentence_a[i] + '\\n---')\n",
    "    print(sentence_b[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be82e8fd-8932-4365-8882-8ece0825471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "{'input_ids': tensor([[  101,  9672, 36240,  ...,     0,     0,     0],\n",
      "        [  101, 21555,  9353,  ...,     0,     0,     0],\n",
      "        [  101,  9678, 12508,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  9638,  9284,  ...,     0,     0,     0],\n",
      "        [  101, 39671,   117,  ...,     0,     0,     0],\n",
      "        [  101, 19789,   117,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# 위 NSP 리스트 들을 tokenizer 함\n",
    "\n",
    "# max_length = 256으로 함, 512 하면 GPU Memory 오류 발생함\n",
    "max_length = 256\n",
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',\n",
    "                   max_length=max_length, truncation=True, padding='max_length')\n",
    "\n",
    "print(inputs.keys())\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500bc7b0-330a-422d-ae22-8ceae7834c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 한 NSP 에 'next_sentence_label' 값(0,1) 추가함\n",
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "\n",
    "print(inputs.next_sentence_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412eecc1-c7f6-436b-98cd-2c58d8960e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# MLM 만들기\n",
    "\n",
    "# labels에는 inputs_id를 복사해서 추가\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afcaae51-44cf-4072-a931-cc6bcb3a1a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "[[5, 11, 22, 27, 39, 41], [3, 28, 29, 39, 40, 43, 57, 59]]\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])\n",
      "tensor([[  101,  9672, 36240,  ...,     0,     0,     0],\n",
      "        [  101, 21555,  9353,  ...,     0,     0,     0],\n",
      "        [  101,  9678, 12508,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  9638,  9284,  ...,     0,     0,     0],\n",
      "        [  101, 39671,   117,  ...,     0,     0,     0],\n",
      "        [  101,   103,   117,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != CLStokenid) * (inputs.input_ids != SEPtokenid) * \\\n",
    "           (inputs.input_ids != UNKtokenid) * (inputs.input_ids != PADtokenid) * \\\n",
    "           (inputs.input_ids != MASKtokenid)\n",
    "\n",
    "selection = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "    \n",
    "print(selection[:2])\n",
    "\n",
    "# inputs_ids 에 [MASK] 추가시킴\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = MASKtokenid\n",
    "    \n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7423b552-05f3-46b5-a1af-01d4d7149977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 만듬 \n",
    "batch_size = 16\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "train_dataset = OurDataset(inputs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56840b9-3c59-4009-9e75-003426226ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_98499/3355184558.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|███████████████████████████| 4/4 [00:01<00:00,  2.21it/s, loss=10]\n",
      "Epoch 1: 100%|█████████████████████████| 4/4 [00:00<00:00,  4.53it/s, loss=8.12]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "\n",
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "epochs = 2\n",
    "learning_rate = 2e-5  # 학습률\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "    \n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        next_sentence_label = batch['next_sentence_label'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # process\n",
    "        outputs = model(input_ids = input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        next_sentence_label=next_sentence_label,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea128a3-c440-4fe4-9d44-5979273283a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
