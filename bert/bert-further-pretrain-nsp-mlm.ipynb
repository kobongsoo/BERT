{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643afa8-68ce-4896-a1c4-ecf9f4ac43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# MLM(Masked Language Model) 과 NSP(Next Sentence Predict) 로 Further Pre-Train 시키는 예시\n",
    "# 참고예제 : https://towardsdatascience.com/how-to-train-bert-aaad00533168\n",
    "#========================================================================\n",
    "\n",
    "import os\n",
    "from os import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForPreTraining, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import GPU_info, seed_everything, mlogging, AccuracyForMaskedToken, SaveBERTModel\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertfptnspmlm\", logfilename=\"../../log/bertfptnspmlm\")\n",
    "\n",
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "# 훈련시킬 말뭉치는  .으로 구분된 한줄 문자이 아니라. 한줄에 .로구분된 여러문장이 이어진 문장이어야 함\n",
    "# 예시:'제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
    "input_corpus = \"../../data/my_data/data.txt\"\n",
    "\n",
    "# eval 말뭉치 \n",
    "eval_corpus = \"../../korpora/kowiki_20190620/wiki_eval_test.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../model/bert/bert-multilingual-cased\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "#vocab_path=\"tokenizer/wiki_20190620_false_0311_speical/bmc_add_wiki_20190620_false_0311.txt\"\n",
    "vocab_path=\"../../tokenizer/my_bong_vocab/\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../model/bert/bert-multilingual-cased-0418/'\n",
    "\n",
    "batch_size = 32\n",
    "token_max_len = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcdd01-e816-4730-8f5d-738921c5d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer와 모델 로딩\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "#model = BertForPreTraining.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForPreTraining.from_pretrained(model_path)    \n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b637d-4cb4-4d63-9c32-e127c1217d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Data 불러옴.\n",
    "# test data는 .으로 구분된 한줄 문자이 아니라. 한줄에 .로구분된 여러문장이 이어진 문장이어야 함\n",
    "# 예시:'제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
    "with open(input_corpus, 'r') as fp:\n",
    "    text = fp.read().split('\\n')\n",
    "    \n",
    "print(text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad5f01-8e70-44fe-9af4-02707d547c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP를 만들기 위해, .을 기준으로 문장들을 나눈 후 길이를 얻어 둔다.\n",
    "bag = [item for sentence in text for item in sentence.split('.') if item != '']\n",
    "bag_size = len(bag)\n",
    "print(bag_size)\n",
    "print(bag[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0da17-b161-4522-a413-7a09145712e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NSP는 50:50으로 랜덤한 값, 랜덤하지 않은 문장으로 만든다.\n",
    "import random\n",
    "\n",
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "\n",
    "for paragraph in text:\n",
    "    # 하나의 문장을 읽어와서 .기준으로 나눈다.\n",
    "    sentences = [sentence for sentence in paragraph.split('.') if sentence != '']\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "     # . 기준으로 나눈 문장이 1이상이면..\n",
    "    if num_sentences > 1:\n",
    "        # 문장 a 시작번지는 랜덤하게, 해당 문장 이후로 지정\n",
    "        start = random.randint(0, num_sentences-2)\n",
    "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "        # 0.5 이상 랜덤값이면, 연속적인 문장으로 만듬\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(sentences[start+1])\n",
    "            label.append(0)  #label=0이면 연속적\n",
    "        # 0.5 이하 랜덤값이면  연속적이 아닌 문장으로 만듬\n",
    "        else:\n",
    "            index = random.randint(0, bag_size-1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(bag[index])\n",
    "            label.append(1)  #label=1이면 비연속적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca212f-e97b-48af-9187-f52885252cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = 0 이면, 연속적인 문장, 1이면 연속적인 문장이 아님\n",
    "for i in range(3):\n",
    "    print(label[i])\n",
    "    print(sentence_a[i] + '\\n---')\n",
    "    print(sentence_b[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82e8fd-8932-4365-8882-8ece0825471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 NSP 리스트 들을 tokenizer 함\n",
    "\n",
    "# max_length = 512 하면 GPU Memory 오류 발생함\n",
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',\n",
    "                   max_length=token_max_len, truncation=True, padding='max_length')\n",
    "\n",
    "print(inputs.keys())\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bc7b0-330a-422d-ae22-8ceae7834c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 한 NSP 에 'next_sentence_label' 값(0,1) 추가함\n",
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "\n",
    "print(inputs.next_sentence_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412eecc1-c7f6-436b-98cd-2c58d8960e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM 만들기\n",
    "\n",
    "# labels에는 inputs_id를 복사해서 추가\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcaae51-44cf-4072-a931-cc6bcb3a1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != CLStokenid) * (inputs.input_ids != SEPtokenid) * \\\n",
    "           (inputs.input_ids != UNKtokenid) * (inputs.input_ids != PADtokenid) * \\\n",
    "           (inputs.input_ids != MASKtokenid)\n",
    "\n",
    "selection = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "    \n",
    "print(selection[:2])\n",
    "\n",
    "# inputs_ids 에 [MASK] 추가시킴\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = MASKtokenid\n",
    "    \n",
    "\n",
    "print(inputs.keys())\n",
    "print(inputs.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423b552-05f3-46b5-a1af-01d4d7149977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 dataloader 만듬 \n",
    "#batch_size = 16\n",
    "\n",
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "train_dataset = OurDataset(inputs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True,\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16cacf-02fa-4ada-8a3c-f2ab260324ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import MLMDataset\n",
    "\n",
    "#===============================================================================\n",
    "# eval dataloader 생성\n",
    "eval_dataset = MLMDataset(corpus_path = eval_corpus,\n",
    "                          tokenizer = tokenizer, \n",
    "                          CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                          SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                          UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                          PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                          Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                          max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                          mlm_probability=0.15,\n",
    "                          overwrite_cache=False\n",
    "                          )\n",
    "\n",
    "\n",
    "# eval dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         #shuffle=True, # dataset을 섞음\n",
    "                         sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                         num_workers=3\n",
    "                         )\n",
    "#===============================================================================\n",
    "\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56840b9-3c59-4009-9e75-003426226ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "##################################################\n",
    "epochs = 5\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = int(total_steps * 0.1) #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "if p_itr <= 0:\n",
    "    p_itr = 1\n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "\n",
    "logger.info('*epchos:{}, lr:{:.9f}, total_steps: {}, warmup_steps:{}, p_itr:{}, save_steps:{}'.format(epochs, learning_rate, total_steps, warmup_steps, p_itr, save_steps))\n",
    "               \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "total_test_correct = 0\n",
    "total_test_len = 0\n",
    "    \n",
    "list_train_loss = []\n",
    "list_train_acc = []\n",
    "list_validation_acc = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    #loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    #for batch in loop:\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "    \n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        next_sentence_label = data['next_sentence_label'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        \n",
    "        # process\n",
    "        outputs = model(input_ids = input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        next_sentence_label=next_sentence_label,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.prediction_logits # torch.Size([32, 128, 157660]) => [batch_size, sequence_length, vocab_size]\n",
    "        #print(logits.shape)\n",
    "        \n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)      \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        \n",
    "        # print relevant info to progress bar\n",
    "        #loop.set_description(f'Epoch {epoch}')\n",
    "        #loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            correct, masked_len = AccuracyForMaskedToken(logits, labels, input_ids, MASKtokenid)           \n",
    "            total_correct += correct.sum().item() \n",
    "            total_len += masked_len \n",
    "            #=========================================\n",
    "                \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                train_acc = total_correct/total_len\n",
    "                       \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "\n",
    "                #for data in tqdm(eval_loader):\n",
    "                for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data['input_ids'].to(device)\n",
    "                    attention_mask = data['attention_mask'].to(device)\n",
    "                    token_type_ids = data['token_type_ids'].to(device)       \n",
    "                    labels = data['labels'].to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       token_type_ids=token_type_ids,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        #loss = outputs.loss\n",
    "                        logits = outputs.prediction_logits \n",
    "\n",
    "                        #===========================================\n",
    "                        # 정확도(Accurarcy) 계산\n",
    "                        correct, masked_len = AccuracyForMaskedToken(logits, labels, input_ids, MASKtokenid)           \n",
    "                        total_test_correct += correct.sum().item() \n",
    "                        total_test_len += masked_len \n",
    "                        #=========================================\n",
    "\n",
    "                val_acc = total_test_correct/total_test_len\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Acc:{}({}/{})'.format(epoch+1, epochs, itr, train_loss, train_acc, val_acc, total_test_correct, total_test_len))\n",
    "      \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_train_acc.append(train_acc)\n",
    "                list_validation_acc.append(val_acc)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "                total_test_correct = 0\n",
    "                total_test_len = 0\n",
    "                ####################################################################\n",
    "\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "                \n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea128a3-c440-4fe4-9d44-5979273283a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "#plt.plot(list_train_acc, label='Train Accuracy')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list_train_acc, label='Train Accuracy')\n",
    "plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3966d8-80f0-4170-8962-90c7490e16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "#save_model(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
