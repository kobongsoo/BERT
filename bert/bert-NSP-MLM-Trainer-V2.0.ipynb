{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# 허깅페이스 Trainer를 이용하여 NSP+MLM 훈련시키기\n",
    "#\n",
    "# => load_dataset 으로 wiki 연속된 문장이 있는 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# 연속된 문장인지 아닌지 NSP 문장을 만들고, (Lable, next_sentence_label 필드 추가)\n",
    "# 해당 문장 input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 pre-kowiki-20220620-2줄.txt 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/bert-NSP-MLM-Trainer_2022-12-26.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치\n",
    "# => 한줄에 연속된 문장이 있어야함.\n",
    "# -> .으로 구분된 한줄 문자이 아니라. 한줄에 .로구분된 여러문장이 이어진 문장이어야 함\n",
    "# -> 예시:'제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
    "# => 사전 만들때 동일한 말뭉치 이용.\n",
    "sep_string = ';$;'  # 말뭉치 2문장 구분자 => 문장A+[SET_STR]+문장B\n",
    "input_corpus = \"../../data11/ai_hub/tl1/tl1-1줄.txt\" # test-1줄.txt, tl1-1줄.txt, tl1-2줄-test.txt, tl1-2줄.txt\n",
    "eval_corpus = \"../../data11/ai_hub/vl1/test-1줄.txt\"  # test-1줄.txt, vl1-2줄-test.txt, vl1-2줄-eval.txt\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "bispretrain = False       # 새로 pretrain 할꺼면 =True, 기존모델에 Further pretrain할꺼면 = False\n",
    "model_path = \"../../data11/model/bert/bert-small-kor-v1/\"  #bert-base-multilingual-cased, ../../data11/model/bert/mbertV2.0\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_path=\"../../data11/model/bert/bert-small-kor-v1/\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../data11/model/bert/bert-small-kor-v1.1-checkout/'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 256       # batch_size\n",
    "token_max_len = 160   # token_seq_len\n",
    "epoch = 8             # epoch\n",
    "lr = 1e-4             # learning rate(기본:5e-5)\n",
    "seed = 111\n",
    "do_lower_case=True   # true=영문일때 모두 소문자로 변환(한국어일때는 false)\n",
    "############################################################################\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bert-NSP-MLM-Trainer\", logfilename=\"../../log/bert-NSP-MLM-Trainer\")\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*../../data11/model/bert/bert-small-kor-v1/ is_fast:True\n",
      "*tokenizer_len:10022, special_token_size: 27, *tokenizer.vocab_size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../data11/model/bert/bert-small-kor-v1/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(10022, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=512, out_features=10022, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨. 아니면 AutoTokenizer 사용\n",
    "tokenizer = BertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=do_lower_case)\n",
    "\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'*{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "print('*tokenizer_len:{}, special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer), len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "\n",
    "if bispretrain == True:\n",
    "    # BERT 껍데기 만들기\n",
    "\n",
    "    config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
    "        vocab_size=len(tokenizer), # default는 영어 기준이므로 내가 만든 vocab size에 맞게 수정해줘야 함\n",
    "        hidden_size=512,           # hidden_layer 임베딩수=> 기본 bae=768, small=512\n",
    "        num_hidden_layers=4,       # layer 수=>base=12개, large=24개, small=4개\n",
    "        num_attention_heads=8,     # transformer attention head number, base=12개, small=8개\n",
    "        intermediate_size=2048,    # transformer 내에 있는 feed-forward network의 dimension size=>base=3072, small=2048\n",
    "        # hidden_act=\"gelu\",\n",
    "        # hidden_dropout_prob=0.1,\n",
    "        # attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정(기본:512) 512, 1024, 2048 식으로 지정함, 엄청난 장문을 다룰경우 10124까지\n",
    "        # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
    "        # pad_token_id=0,\n",
    "        # position_embedding_type=\"absolute\"\n",
    "    )\n",
    "\n",
    "    model = BertForPreTraining(config=config)\n",
    "    model.num_parameters()\n",
    "else:\n",
    "    # 모델 로딩 further pre-training \n",
    "    model = BertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "\n",
    "    #################################################################################\n",
    "    # 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "    # 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "    # CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "    #  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "    #\n",
    "    #     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "    #     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "    #################################################################################\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ab3e293bd281e2b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /MOCOMSYS/.cache/huggingface/datasets/text/default-ab3e293bd281e2b0/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec8ddd7a9f54ab7beb348402856bd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe82c5c32504a3a951df764ead0cfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /MOCOMSYS/.cache/huggingface/datasets/text/default-ab3e293bd281e2b0/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21ecd74b8c740e9913e4c4a0305d93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 54944002\n",
      "    })\n",
      "})\n",
      "['정부, 무선국 검사제도 손본다', '정부가 lte(롱텀에볼루션) 서비스 도입 등 바뀐 이동통신환경을 고려해 무선국 검사제도 규제개선을 추진한다.', '미래창조과학부는 효율적 전파관리 체계구축과 전파 이용자 편익증진을 위해 전파관리제도 개선 연구반이 도출한 무선국 검사제도 개선 방안을 시행한다고 16일 밝혔다.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-daf01fff38c3fc3c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /MOCOMSYS/.cache/huggingface/datasets/text/default-daf01fff38c3fc3c/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cc481570b94c679ce5e4094ba10ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef67932cd064961a0b098c72aa6fe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /MOCOMSYS/.cache/huggingface/datasets/text/default-daf01fff38c3fc3c/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7bf59bd4774d239fda6ff3e8f20b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1667\n",
      "    })\n",
      "})\n",
      "['헬기 동체 잔해물과 부유물 등은 발견되고 있지만 정작 실종자들은 발견하지 못해 수색이 장기화될 것이라는 우려가 현실이 될 조짐을 보여 실종자 가족들의 애를 태우고 있다', '범정부현장수습지원단(지원단)은 10일 오전 10시 브리핑에서 이날 오전까지 독도해역 수색 결과 4점의 부유물을 추가 발견, 인양했다고 밝혔다', '지원단에 따르면 오전 8시32분쯤 해경 1511함이 동체로부터 2']\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "eval_dataset = load_dataset(\"text\", data_files=eval_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset=======================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['train']['text'][0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633edc59-9b29-4841-9026-1e547f704c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb31143a3d747d3951614bc6c4191dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54945 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 17min 52s, sys: 5h 25min 31s, total: 9h 43min 24s\n",
      "Wall time: 47min 37s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b27e1c1f61447488e9e89a29375316d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 1.13 s, total: 2.14 s\n",
      "Wall time: 264 ms\n"
     ]
    }
   ],
   "source": [
    "# NSP 훈련 데이터 만들기 \n",
    "# NSP 문장 만들기 \n",
    "import random\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def tokenizer_function_nsp(examples):\n",
    "    \n",
    "    bag = [item for sentence in examples['text'] for item in sentence.split(sep_string) if item != '']\n",
    "    bag_size = len(bag)\n",
    "   \n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    next_label = []\n",
    "\n",
    "    count = 0\n",
    "    for paragraph in examples['text']:\n",
    "        count += 1\n",
    "        # 하나의 문장을 읽어와서 .기준으로 나눈다.\n",
    "        sentences = [sentence for sentence in paragraph.split(sep_string) if sentence != '']\n",
    "        num_sentences = len(sentences)\n",
    "         \n",
    "         # . 기준으로 나눈 문장이 1이상이면..\n",
    "        if num_sentences > 1:\n",
    "            # 문장 a 시작번지는 랜덤하게, 해당 문장 이후로 지정\n",
    "            start = random.randint(0, num_sentences-2)\n",
    "            # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "            # 0.5 이상 랜덤값이면, 연속적인 문장으로 만듬\n",
    "            if random.random() >= 0.5:\n",
    "                # this is IsNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(sentences[start+1])\n",
    "                next_label.append(0)  #label=0이면 연속적\n",
    "            # 0.5 이하 랜덤값이면  연속적이 아닌 문장으로 만듬\n",
    "            else:\n",
    "                index = random.randint(0, bag_size-1)\n",
    "                # this is NotNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(bag[index])\n",
    "                next_label.append(1)  #label=1이면 비연속적\n",
    "    \n",
    "    # ** return_overflowing_tokenis = False로 해야, 긴 문장인 경우 잘리더라도 다시 이어서 문장을 만들지 않는다.\n",
    "    # => 입력 문장은 10개인데, 긴문장이 포함된 경우 10개를 넘는 출력이 나옴\n",
    "    result = tokenizer(sentence_a, sentence_b, max_length=token_max_len, truncation=True, return_overflowing_tokens=False)\n",
    "    \n",
    "    # next_sentence_label next_label 복사(**deepcopy)해서 추가\n",
    "    result['next_sentence_label'] = copy.deepcopy(next_label)\n",
    "        \n",
    "    # labels에는 inputs_id를 복사(**deepcopy)해서 추가\n",
    "    #result['labels'] = copy.deepcopy(result.input_ids)\n",
    "     \n",
    "    return result\n",
    "\n",
    "def tokenizer_function(examples):\n",
    "    result =  tokenizer(examples['text'], truncation=True, max_length=token_max_len, return_overflowing_tokens=True)\n",
    "    \n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "# pretrain 훈련인 경우\n",
    "if bispretrain == True:\n",
    "    # 훈련 NSP+MLM 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "    %time train_dataset_fast = train_dataset.map(tokenizer_function_nsp, batched=True)\n",
    "\n",
    "    # 평가 NSP+MLM 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "    %time eval_dataset_fast = eval_dataset.map(tokenizer_function_nsp, batched=True)\n",
    "\n",
    "# mlm 훈련인 경우\n",
    "else:\n",
    "     # 훈련 MLM 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "    %time train_dataset_fast = train_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "    # 평가 MLM 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "    %time eval_dataset_fast = eval_dataset.map(tokenizer_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast=======================================\n",
      "*train_len:55009116, len:55009116\n",
      "{'text': ['정부, 무선국 검사제도 손본다', '정부가 lte(롱텀에볼루션) 서비스 도입 등 바뀐 이동통신환경을 고려해 무선국 검사제도 규제개선을 추진한다.'], 'input_ids': [[2, 2002, 15, 5105, 1051, 2213, 1074, 1216, 559, 1254, 1081, 3], [2, 2002, 1174, 6916, 11, 368, 1747, 1104, 1611, 3402, 12, 2048, 2461, 299, 1, 3141, 1356, 1438, 3331, 1412, 2513, 1364, 5105, 1051, 2213, 1074, 1216, 2518, 1431, 1196, 1412, 2090, 3140, 17, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "decode===========================================\n",
      "\n",
      "[CLS] 정부, 무선국 검사제도 손본다 [SEP]\n",
      "[CLS] 정부가 lte ( 롱텀에볼루션 ) 서비스 도입 등 [UNK] 이동통신환경을 고려해 무선국 검사제도 규제개선을 추진한다. [SEP]\n",
      "eval_dataset_fast=======================================\n",
      "*eval_len:1822, len:1822\n",
      "{'text': ['헬기 동체 잔해물과 부유물 등은 발견되고 있지만 정작 실종자들은 발견하지 못해 수색이 장기화될 것이라는 우려가 현실이 될 조짐을 보여 실종자 가족들의 애를 태우고 있다', '범정부현장수습지원단(지원단)은 10일 오전 10시 브리핑에서 이날 오전까지 독도해역 수색 결과 4점의 부유물을 추가 발견, 인양했다고 밝혔다'], 'input_ids': [[2, 6409, 276, 1457, 712, 1364, 1142, 1106, 492, 1157, 1142, 299, 1114, 2876, 1440, 1041, 707, 2193, 7962, 9383, 1173, 1114, 2876, 1062, 1236, 5042, 5694, 1090, 2633, 1286, 1840, 108, 1090, 1108, 1077, 2348, 1174, 3029, 1090, 281, 9230, 1412, 2430, 9383, 2318, 1173, 1366, 617, 1178, 870, 1136, 1041, 707, 1081, 3], [2, 472, 1243, 1166, 1150, 1099, 1043, 1504, 9111, 1271, 11, 1994, 1271, 12, 692, 1976, 1258, 2121, 1976, 1131, 3748, 1104, 1068, 1992, 2121, 1143, 1236, 6563, 1364, 1436, 5694, 2078, 23, 1379, 1366, 492, 1157, 1142, 1412, 2135, 2876, 15, 9993, 1405, 3837, 1972, 1081, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "decode===========================================\n",
      "\n",
      "[CLS] 헬기 동체 잔해물과 부유물 등은 발견되고 있지만 정작 실종자들은 발견하지 못해 수색이 장기화될 것이라는 우려가 현실이 될 조짐을 보여 실종자 가족들의 애를 태우고 있다 [SEP]\n",
      "[CLS] 범정부현장수습지원단 ( 지원단 ) 은 10일 오전 10시 브리핑에서 이날 오전까지 독도해역 수색 결과 4점의 부유물을 추가 발견, 인양했다고 밝혔다 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(f'*train_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset_fast[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][1]))\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(f'*eval_len:{len(eval_dataset_fast[\"train\"])}, len:{len(eval_dataset_fast[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(eval_dataset_fast['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(eval_dataset_fast['train']['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast(MLM)=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 55009116\n",
      "    })\n",
      "})\n",
      "tensor([   2, 2002,   15, 5105, 1051, 2213, 1074, 1216,  559, 1254, 1081,    3])\n",
      "{'text': '정부, 무선국 검사제도 손본다', 'input_ids': [2, 2002, 15, 5105, 1051, 2213, 1074, 1216, 559, 1254, 1081, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "org===========================================\n",
      "\n",
      "[CLS] 정부, 무선국 검사제도 손본다 [SEP]\n",
      "\n",
      "decode===========================================\n",
      "\n",
      "[CLS] 정부, 무선국 검사제도 손본다 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:1])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\norg===========================================\\r\\n')\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][0]))\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(mlm_train_sample['input_ids'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*total_optim_steps: 1719034, *eval_steps:34380, *logging_steps:34380, *save_steps:171903\n",
      "*no_cuda: False\n"
     ]
    }
   ],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "epochs = epoch          # epochs\n",
    "#lr = 3e-5  # 학습률\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.02)           # 평가 스탭수\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "#save_total_limit=2                                # 마지막 2개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    learning_rate=lr,                      # lr: 기본 5e-5\n",
    "    per_gpu_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_strategy=\"epoch\",                  # 저장 전략 (no, epoch, steps 기본=steps) \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 (no, epoch, steps 기본=no)  \n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "#  NSP 토크처리된 훈련 데이터셋\n",
    "train_dataset_input = train_dataset_fast['train']\n",
    "eval_dataset_input = eval_dataset_fast['train']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_input,   # NSP 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_input,     # NSP 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 55009116\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1719040\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1030420' max='1719040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1030420/1719040 88:02:30 < 58:50:15, 3.25 it/s, Epoch 4.80/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>34380</td>\n",
       "      <td>1.691100</td>\n",
       "      <td>1.477869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68760</td>\n",
       "      <td>1.688700</td>\n",
       "      <td>1.490825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103140</td>\n",
       "      <td>1.679400</td>\n",
       "      <td>1.478756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137520</td>\n",
       "      <td>1.669700</td>\n",
       "      <td>1.448872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171900</td>\n",
       "      <td>1.660400</td>\n",
       "      <td>1.452738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206280</td>\n",
       "      <td>1.652500</td>\n",
       "      <td>1.477634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240660</td>\n",
       "      <td>1.643600</td>\n",
       "      <td>1.426226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275040</td>\n",
       "      <td>1.636600</td>\n",
       "      <td>1.432696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309420</td>\n",
       "      <td>1.629800</td>\n",
       "      <td>1.413609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343800</td>\n",
       "      <td>1.623200</td>\n",
       "      <td>1.425812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378180</td>\n",
       "      <td>1.617100</td>\n",
       "      <td>1.407653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412560</td>\n",
       "      <td>1.611700</td>\n",
       "      <td>1.416884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446940</td>\n",
       "      <td>1.604900</td>\n",
       "      <td>1.374513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481320</td>\n",
       "      <td>1.599700</td>\n",
       "      <td>1.428996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515700</td>\n",
       "      <td>1.595400</td>\n",
       "      <td>1.394340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550080</td>\n",
       "      <td>1.589700</td>\n",
       "      <td>1.404301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584460</td>\n",
       "      <td>1.584500</td>\n",
       "      <td>1.381827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618840</td>\n",
       "      <td>1.580200</td>\n",
       "      <td>1.381472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653220</td>\n",
       "      <td>1.575600</td>\n",
       "      <td>1.406882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687600</td>\n",
       "      <td>1.570800</td>\n",
       "      <td>1.374381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721980</td>\n",
       "      <td>1.566700</td>\n",
       "      <td>1.362598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756360</td>\n",
       "      <td>1.563300</td>\n",
       "      <td>1.372161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790740</td>\n",
       "      <td>1.559100</td>\n",
       "      <td>1.392441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825120</td>\n",
       "      <td>1.555100</td>\n",
       "      <td>1.359810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859500</td>\n",
       "      <td>1.550800</td>\n",
       "      <td>1.338439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893880</td>\n",
       "      <td>1.546500</td>\n",
       "      <td>1.316561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928260</td>\n",
       "      <td>1.542400</td>\n",
       "      <td>1.361896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962640</td>\n",
       "      <td>1.539200</td>\n",
       "      <td>1.394360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997020</td>\n",
       "      <td>1.535000</td>\n",
       "      <td>1.321062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-214880\n",
      "Configuration saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-214880/config.json\n",
      "Model weights saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-214880/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-429760\n",
      "Configuration saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-429760/config.json\n",
      "Model weights saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-429760/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-644640\n",
      "Configuration saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-644640/config.json\n",
      "Model weights saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-644640/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-859520\n",
      "Configuration saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-859520/config.json\n",
      "Model weights saved in ../../data11/model/bert/bert-small-kor-v1.1-checkout/checkpoint-859520/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text. If text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1822\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/bert/bert-small-kor-v1.1/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8a66c-831a-44f4-a063-9932aeaf7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
