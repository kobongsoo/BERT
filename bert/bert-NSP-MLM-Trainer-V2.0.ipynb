{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# 허깅페이스 Trainer를 이용하여 NSP+MLM 훈련시키기\n",
    "#\n",
    "# => load_dataset 으로 wiki 연속된 문장이 있는 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# 연속된 문장인지 아닌지 NSP 문장을 만들고, (Lable, next_sentence_label 필드 추가)\n",
    "# 해당 문장 input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 pre-kowiki-20220620-2줄.txt 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, BertConfig, BertForMaskedLM, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시킬 말뭉치\n",
    "# => 한줄에 연속된 문장이 있어야함.\n",
    "# -> .으로 구분된 한줄 문자이 아니라. 한줄에 .로구분된 여러문장이 이어진 문장이어야 함\n",
    "# -> 예시:'제임스 얼 \"지미\" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다.지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\n",
    "# => 사전 만들때 동일한 말뭉치 이용.\n",
    "sep_string = ';$;'  # 말뭉치 2문장 구분자 => 문장A+[SET_STR]+문장B\n",
    "input_corpus = \"../../data11/ai_hub/tl1/tl1-2줄.txt\"\n",
    "eval_corpus = \"../../data11/ai_hub/vl1/vl1-2줄-eval.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "bispretrain = True       # 새로 pretrain 할꺼면 =True, 기존모델에 Further pretrain할꺼면 = False\n",
    "model_path = \"bert-base-multilingual-cased\"  #bert-base-multilingual-cased, ../../data11/model/bert/mbertV2.0\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_path=\"../../data11/ai_hub/vocab/tl1-1줄-mecab-30000\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 64       # batch_size\n",
    "token_max_len = 128   # token_seq_len\n",
    "epoch = 10            # epoch\n",
    "lr = 5e-5             # learning rate(기본:5e-5)\n",
    "seed = 111\n",
    "############################################################################\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bert-MLM-Trainer\", logfilename=\"../../log/bert-MLM-Trainer\")\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeinzier 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨. 아니면 AutoTokenizer 사용\n",
    "tokenizer = BertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'*{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "print('*tokenizer_len:{}, special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer), len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "\n",
    "if bispretrain == True:\n",
    "    # BERT 껍데기 만들기\n",
    "\n",
    "    config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
    "        vocab_size=len(tokenizer), # default는 영어 기준이므로 내가 만든 vocab size에 맞게 수정해줘야 함\n",
    "        # hidden_size=768, # hidden_layer 임베딩수(768)\n",
    "        # num_hidden_layers=12,    # layer 수(base=12개, large=24개)\n",
    "        # num_attention_heads=12,    # transformer attention head number\n",
    "        # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
    "        # hidden_act=\"gelu\",\n",
    "        # hidden_dropout_prob=0.1,\n",
    "        # attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정(기본:512) 512, 1024, 2048 식으로 지정함, 엄청난 장문을 다룰경우 10124까지\n",
    "        # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
    "        # pad_token_id=0,\n",
    "        # position_embedding_type=\"absolute\"\n",
    "    )\n",
    "\n",
    "    model = BertForPreTraining(config=config)\n",
    "    model.num_parameters()\n",
    "else:\n",
    "    # 모델 로딩 further pre-training \n",
    "    model = BertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "\n",
    "    #################################################################################\n",
    "    # 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "    # 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "    # CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "    #  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "    #\n",
    "    #     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "    #     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "    #################################################################################\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "eval_dataset = load_dataset(\"text\", data_files=eval_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset=======================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['train']['text'][0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633edc59-9b29-4841-9026-1e547f704c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP 훈련 데이터 만들기 \n",
    "# NSP 문장 만들기 \n",
    "import random\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def tokenizer_function_nsp(examples):\n",
    "    \n",
    "    bag = [item for sentence in examples['text'] for item in sentence.split('.') if item != '']\n",
    "    bag_size = len(bag)\n",
    "   \n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    next_label = []\n",
    "\n",
    "    count = 0\n",
    "    for paragraph in examples['text']:\n",
    "        count += 1\n",
    "        # 하나의 문장을 읽어와서 .기준으로 나눈다.\n",
    "        sentences = [sentence for sentence in paragraph.split(sep_string) if sentence != '']\n",
    "        num_sentences = len(sentences)\n",
    "         \n",
    "         # . 기준으로 나눈 문장이 1이상이면..\n",
    "        if num_sentences > 1:\n",
    "            # 문장 a 시작번지는 랜덤하게, 해당 문장 이후로 지정\n",
    "            start = random.randint(0, num_sentences-2)\n",
    "            # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "            # 0.5 이상 랜덤값이면, 연속적인 문장으로 만듬\n",
    "            if random.random() >= 0.5:\n",
    "                # this is IsNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(sentences[start+1])\n",
    "                next_label.append(0)  #label=0이면 연속적\n",
    "            # 0.5 이하 랜덤값이면  연속적이 아닌 문장으로 만듬\n",
    "            else:\n",
    "                index = random.randint(0, bag_size-1)\n",
    "                # this is NotNextSentence\n",
    "                sentence_a.append(sentences[start])\n",
    "                sentence_b.append(bag[index])\n",
    "                next_label.append(1)  #label=1이면 비연속적\n",
    "    \n",
    "    # ** return_overflowing_tokenis = False로 해야, 긴 문장인 경우 잘리더라도 다시 이어서 문장을 만들지 않는다.\n",
    "    # => 입력 문장은 10개인데, 긴문장이 포함된 경우 10개를 넘는 출력이 나옴\n",
    "    result = tokenizer(sentence_a, sentence_b, max_length=token_max_len, truncation=True, return_overflowing_tokens=False)\n",
    "    \n",
    "    # next_sentence_label next_label 복사(**deepcopy)해서 추가\n",
    "    result['next_sentence_label'] = copy.deepcopy(next_label)\n",
    "        \n",
    "    # labels에는 inputs_id를 복사(**deepcopy)해서 추가\n",
    "    #result['labels'] = copy.deepcopy(result.input_ids)\n",
    "     \n",
    "    return result\n",
    "\n",
    "# 훈련 NSP 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "%time train_dataset_fast = train_dataset.map(tokenizer_function_nsp, batched=True)\n",
    "\n",
    "# 평가 NSP 데이터셋은 빠른 기본 toeknzier_function 이용하여 만듬\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function_nsp, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(f'*train_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset_fast[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][1]))\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(f'*eval_len:{len(eval_dataset_fast[\"train\"])}, len:{len(eval_dataset_fast[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(eval_dataset_fast['train']['input_ids'][0]))\n",
    "print(tokenizer.decode(eval_dataset_fast['train']['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:1])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\norg===========================================\\r\\n')\n",
    "print(tokenizer.decode(train_dataset_fast['train']['input_ids'][0]))\n",
    "\n",
    "print(f'\\r\\ndecode===========================================\\r\\n')\n",
    "print(tokenizer.decode(mlm_train_sample['input_ids'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "epochs = epoch          # epochs\n",
    "#lr = 3e-5  # 학습률\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.02)           # 평가 스탭수\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "#save_total_limit=2                                # 마지막 2개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    learning_rate=lr,                      # lr: 기본 5e-5\n",
    "    per_gpu_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_strategy=\"epoch\",                  # 저장 전략 (no, epoch, steps 기본=steps) \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 (no, epoch, steps 기본=no)  \n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "#  NSP 토크처리된 훈련 데이터셋\n",
    "train_dataset_input = train_dataset_fast['train']\n",
    "eval_dataset_input = eval_dataset_fast['train']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_input,   # NSP 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_input,     # NSP 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/bert/mbertV3.0-aihub-NSPMLM/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8a66c-831a-44f4-a063-9932aeaf7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
