{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9ba1c7-26e3-426e-a5b0-773f20ca3807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /MOCOMSYS/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# *WandB 툴을 이용하여, loss를 시각화함(http://wandb.ai 에 회원가입 해야 함)\n",
    "# => !pip install wandb -qqq \n",
    "#\n",
    "#\n",
    "#======================================================================================================\n",
    "#!pip install wandb -qqq\n",
    "import wandb\n",
    "wandb.login()  # 로그인 => wandb.ai 사이트에 로그인 후, API Key 입력해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3423f4ca-cb1f-410b-9a53-403cd78e1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/bertftmultitrain_2022-04-14.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# bert BertForSequenceClassification finetuning 예제\n",
    "# 참고 사이트 : https://zzaebok.github.io/deep_learning/nlp/Bert-for-classification/\n",
    "#\n",
    "#======================================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "logger = mlogging(loggername=\"bertfttrain\", logfilename=\"../../log/bertftmultitrain\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1171a43-c3d9-460e-9795-7360948a1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327/ and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(167550, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################################################\n",
    "# 변수들 설정\n",
    "# - model_path : from_pretrained() 로 호출하는 경우에는 모델파일이 있는 폴더 경로나 \n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')\n",
    "#          torch.load(model)로 로딩하는 경우에는 모델 파일 풀 경로\n",
    "#\n",
    "# - vocab_path : from_pretrained() 호출하는 경우에는 모델파일이 있는 폴더 경로나\n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')   \n",
    "#          BertTokenizer() 로 호출하는 경우에는 vocab.txt 파일 풀 경로,\n",
    "#\n",
    "# - OUTPATH : 출력 모델, vocab 저장할 폴더 경로\n",
    "#############################################################################################\n",
    "\n",
    "#model_path = 'model/kobertmodel/'\n",
    "#vocab_path = 'model/kobertmodel/vocab/vocab.txt'\n",
    "#OUTPATH = 'model/classification/kobert-ft-es-cfmodel/'\n",
    "\n",
    "model_path = '../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327/'\n",
    "vocab_path = '../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327/'\n",
    "OUTPATH = '../../model/classification/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327-nscm-0414/'\n",
    "\n",
    "# tokeniaer 및 model 설정\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# strip_accents=False : True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "# do_lower_case=False : # 소문자 입력 사용 안함(한국어에서는 반드시 False)\n",
    "#tokenizer = BertTokenizer(vocab_file=vocab_path, strip_accents=False, do_lower_case=False) \n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path, do_lower_case=False)\n",
    "\n",
    "# 레벨을 1개만 선택하는 경우\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)\n",
    "\n",
    "# 레벨을 멀티로 선택해야 하는 경우\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, problem_type=\"multi_label_classification\",num_labels=6)\n",
    "                   \n",
    "#기존 모델 파일을 로딩하는 경우    \n",
    "#model = torch.load(model_path) \n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6814c46-b6cd-4fd9-aac9-eb1d5ef123da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214721282"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3ae263-b8b0-4160-a4e9-11e0370f4f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobongsoo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/MOCOMSYS/dev/bong/BERT/bert/wandb/run-20220414_093713-q7pczmlo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kobongsoo/bert-ft-multiclassification/runs/q7pczmlo\" target=\"_blank\">misunderstood-wind-1</a></strong> to <a href=\"https://wandb.ai/kobongsoo/bert-ft-multiclassification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb 관련 hyper parameter들 설정\n",
    "wandb.init(\n",
    "        project='bert-ft-multiclassification',\n",
    "        config={\n",
    "            \"epochs\": 3,\n",
    "            \"batch_size\": 32,\n",
    "            \"lr\": 2e-5,\n",
    "        })\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160454e1-7845-452c-ac74-4d7e39fbcc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 09:39:03,424 - bwpdataset - INFO - Loading features from cached file ../../korpora/nsmc/cached_BertTokenizer_128_ratings_train.txt [took 8.132 s]\n",
      "2022-04-14 09:39:05,308 - bwpdataset - INFO - Loading features from cached file ../../korpora/nsmc/cached_BertTokenizer_128_ratings_test.txt [took 1.450 s]\n"
     ]
    }
   ],
   "source": [
    "# 학습 data loader 생성\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import ClassificationCSVCorpus, ClassificationDataset, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = 128   # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "#batch_size = 32        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "\n",
    "# 훈련할 csv 파일\n",
    "#file_fpath = 'korpora/감성대화말뭉치/감성대화말뭉치(최종데이터)_renew_labelenc_Training.csv'\n",
    "file_fpath = '../../korpora/nsmc/ratings_train.txt'\n",
    "column_num = 3           # .csv 파일에 컬럼수(예: text, label만 있으면 =2)\n",
    "csvfile = 0              # 0:tsv 파일, 1: csv 파일\n",
    "label_list = [\"0\", \"1\"]  # .csv 파일에 레벨 목록( list로 입력해야 함)\n",
    "#label_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]  # .csv 파일에 레벨 목록( list로 입력해야 함)\n",
    "cache = False   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 설정\n",
    "corpus = ClassificationCSVCorpus(column_num=column_num, iscsvfile=csvfile, label_list=label_list)\n",
    "\n",
    "# 학습 dataset 생성\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size=config.batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "# 평가 dataset 생성\n",
    "#file_fpath = 'korpora/감성대화말뭉치/감성대화말뭉치(최종데이터)_renew_labelenc_Validation.csv'\n",
    "file_fpath = '../../korpora/nsmc/ratings_test.txt'\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "eval_loader = DataLoader(dataset, \n",
    "                          batch_size=config.batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff76051d-a8f2-4bdf-9f24-a9ebc94742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167550\n",
      "[101, 9034, 10530, 124997, 11018, 125215, 10739, 69708, 42428, 10459, 10020, 129937, 10892, 132489, 12508, 49137, 102]\n",
      "৭\n",
      "123665\n"
     ]
    }
   ],
   "source": [
    "# tokenier 테스트\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.encode(\"눈에 보이는 반전이었지만 영화의 흡인력은 사라지지 않았다\"))\n",
    "print(tokenizer.convert_ids_to_tokens(1000))\n",
    "print(tokenizer.convert_tokens_to_ids('날씨'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2af4d68-796d-4d39-9735-7c8021dfa42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd95b46b034516a668af7115afb04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e113719157c24a9496a658c5e103a4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110173/830095837.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-04-14 09:48:55,961 - bertfttrain - INFO - [Epoch 1/3] Iteration 400 -> Train Loss: 0.4004, Train Accuracy: 0.820\n",
      "2022-04-14 09:48:55,961 - bertfttrain - INFO - [Epoch 1/3] Iteration 400 -> Train Loss: 0.4004, Train Accuracy: 0.820\n",
      "2022-04-14 09:49:53,234 - bertfttrain - INFO - [Epoch 1/3] Iteration 800 -> Train Loss: 0.3688, Train Accuracy: 0.836\n",
      "2022-04-14 09:49:53,234 - bertfttrain - INFO - [Epoch 1/3] Iteration 800 -> Train Loss: 0.3688, Train Accuracy: 0.836\n",
      "2022-04-14 09:50:52,316 - bertfttrain - INFO - [Epoch 1/3] Iteration 1200 -> Train Loss: 0.3576, Train Accuracy: 0.841\n",
      "2022-04-14 09:50:52,316 - bertfttrain - INFO - [Epoch 1/3] Iteration 1200 -> Train Loss: 0.3576, Train Accuracy: 0.841\n",
      "2022-04-14 09:51:49,786 - bertfttrain - INFO - [Epoch 1/3] Iteration 1600 -> Train Loss: 0.3581, Train Accuracy: 0.843\n",
      "2022-04-14 09:51:49,786 - bertfttrain - INFO - [Epoch 1/3] Iteration 1600 -> Train Loss: 0.3581, Train Accuracy: 0.843\n",
      "2022-04-14 09:52:49,665 - bertfttrain - INFO - [Epoch 1/3] Iteration 2000 -> Train Loss: 0.3388, Train Accuracy: 0.852\n",
      "2022-04-14 09:52:49,665 - bertfttrain - INFO - [Epoch 1/3] Iteration 2000 -> Train Loss: 0.3388, Train Accuracy: 0.852\n",
      "2022-04-14 09:53:48,357 - bertfttrain - INFO - [Epoch 1/3] Iteration 2400 -> Train Loss: 0.3337, Train Accuracy: 0.853\n",
      "2022-04-14 09:53:48,357 - bertfttrain - INFO - [Epoch 1/3] Iteration 2400 -> Train Loss: 0.3337, Train Accuracy: 0.853\n",
      "2022-04-14 09:54:47,273 - bertfttrain - INFO - [Epoch 1/3] Iteration 2800 -> Train Loss: 0.3295, Train Accuracy: 0.856\n",
      "2022-04-14 09:54:47,273 - bertfttrain - INFO - [Epoch 1/3] Iteration 2800 -> Train Loss: 0.3295, Train Accuracy: 0.856\n",
      "2022-04-14 09:55:45,713 - bertfttrain - INFO - [Epoch 1/3] Iteration 3200 -> Train Loss: 0.3195, Train Accuracy: 0.860\n",
      "2022-04-14 09:55:45,713 - bertfttrain - INFO - [Epoch 1/3] Iteration 3200 -> Train Loss: 0.3195, Train Accuracy: 0.860\n",
      "2022-04-14 09:56:44,637 - bertfttrain - INFO - [Epoch 1/3] Iteration 3600 -> Train Loss: 0.3195, Train Accuracy: 0.860\n",
      "2022-04-14 09:56:44,637 - bertfttrain - INFO - [Epoch 1/3] Iteration 3600 -> Train Loss: 0.3195, Train Accuracy: 0.860\n",
      "2022-04-14 09:57:41,878 - bertfttrain - INFO - [Epoch 1/3] Iteration 4000 -> Train Loss: 0.3138, Train Accuracy: 0.863\n",
      "2022-04-14 09:57:41,878 - bertfttrain - INFO - [Epoch 1/3] Iteration 4000 -> Train Loss: 0.3138, Train Accuracy: 0.863\n",
      "2022-04-14 09:58:39,052 - bertfttrain - INFO - [Epoch 1/3] Iteration 4400 -> Train Loss: 0.3107, Train Accuracy: 0.866\n",
      "2022-04-14 09:58:39,052 - bertfttrain - INFO - [Epoch 1/3] Iteration 4400 -> Train Loss: 0.3107, Train Accuracy: 0.866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbe062635f845d880a3b63d971ba6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110173/830095837.py:126: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-04-14 10:00:14,774 - bertfttrain - INFO - [Epoch 1/3] Validatation Accuracy:0.86842\n",
      "2022-04-14 10:00:14,774 - bertfttrain - INFO - [Epoch 1/3] Validatation Accuracy:0.86842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6465847608ec47628bdea7313d55a7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 10:00:31,120 - bertfttrain - INFO - [Epoch 2/3] Iteration 4800 -> Train Loss: 0.2954, Train Accuracy: 0.874\n",
      "2022-04-14 10:00:31,120 - bertfttrain - INFO - [Epoch 2/3] Iteration 4800 -> Train Loss: 0.2954, Train Accuracy: 0.874\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "2022-04-14 10:01:29,899 - bertfttrain - INFO - [Epoch 2/3] Iteration 5200 -> Train Loss: 0.2530, Train Accuracy: 0.898\n",
      "2022-04-14 10:01:29,899 - bertfttrain - INFO - [Epoch 2/3] Iteration 5200 -> Train Loss: 0.2530, Train Accuracy: 0.898\n",
      "2022-04-14 10:02:27,041 - bertfttrain - INFO - [Epoch 2/3] Iteration 5600 -> Train Loss: 0.2512, Train Accuracy: 0.895\n",
      "2022-04-14 10:02:27,041 - bertfttrain - INFO - [Epoch 2/3] Iteration 5600 -> Train Loss: 0.2512, Train Accuracy: 0.895\n",
      "2022-04-14 10:03:26,956 - bertfttrain - INFO - [Epoch 2/3] Iteration 6000 -> Train Loss: 0.2488, Train Accuracy: 0.902\n",
      "2022-04-14 10:03:26,956 - bertfttrain - INFO - [Epoch 2/3] Iteration 6000 -> Train Loss: 0.2488, Train Accuracy: 0.902\n",
      "2022-04-14 10:04:30,220 - bertfttrain - INFO - [Epoch 2/3] Iteration 6400 -> Train Loss: 0.2360, Train Accuracy: 0.905\n",
      "2022-04-14 10:04:30,220 - bertfttrain - INFO - [Epoch 2/3] Iteration 6400 -> Train Loss: 0.2360, Train Accuracy: 0.905\n",
      "2022-04-14 10:05:27,733 - bertfttrain - INFO - [Epoch 2/3] Iteration 6800 -> Train Loss: 0.2406, Train Accuracy: 0.900\n",
      "2022-04-14 10:05:27,733 - bertfttrain - INFO - [Epoch 2/3] Iteration 6800 -> Train Loss: 0.2406, Train Accuracy: 0.900\n",
      "2022-04-14 10:06:25,326 - bertfttrain - INFO - [Epoch 2/3] Iteration 7200 -> Train Loss: 0.2372, Train Accuracy: 0.904\n",
      "2022-04-14 10:06:25,326 - bertfttrain - INFO - [Epoch 2/3] Iteration 7200 -> Train Loss: 0.2372, Train Accuracy: 0.904\n",
      "2022-04-14 10:07:22,894 - bertfttrain - INFO - [Epoch 2/3] Iteration 7600 -> Train Loss: 0.2424, Train Accuracy: 0.898\n",
      "2022-04-14 10:07:22,894 - bertfttrain - INFO - [Epoch 2/3] Iteration 7600 -> Train Loss: 0.2424, Train Accuracy: 0.898\n",
      "2022-04-14 10:08:23,933 - bertfttrain - INFO - [Epoch 2/3] Iteration 8000 -> Train Loss: 0.2436, Train Accuracy: 0.901\n",
      "2022-04-14 10:08:23,933 - bertfttrain - INFO - [Epoch 2/3] Iteration 8000 -> Train Loss: 0.2436, Train Accuracy: 0.901\n",
      "2022-04-14 10:09:26,415 - bertfttrain - INFO - [Epoch 2/3] Iteration 8400 -> Train Loss: 0.2311, Train Accuracy: 0.904\n",
      "2022-04-14 10:09:26,415 - bertfttrain - INFO - [Epoch 2/3] Iteration 8400 -> Train Loss: 0.2311, Train Accuracy: 0.904\n",
      "2022-04-14 10:10:24,737 - bertfttrain - INFO - [Epoch 2/3] Iteration 8800 -> Train Loss: 0.2307, Train Accuracy: 0.907\n",
      "2022-04-14 10:10:24,737 - bertfttrain - INFO - [Epoch 2/3] Iteration 8800 -> Train Loss: 0.2307, Train Accuracy: 0.907\n",
      "2022-04-14 10:11:24,002 - bertfttrain - INFO - [Epoch 2/3] Iteration 9200 -> Train Loss: 0.2378, Train Accuracy: 0.899\n",
      "2022-04-14 10:11:24,002 - bertfttrain - INFO - [Epoch 2/3] Iteration 9200 -> Train Loss: 0.2378, Train Accuracy: 0.899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1de1309de14f0691dc501403af8ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 10:12:45,035 - bertfttrain - INFO - [Epoch 2/3] Validatation Accuracy:0.8761\n",
      "2022-04-14 10:12:45,035 - bertfttrain - INFO - [Epoch 2/3] Validatation Accuracy:0.8761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251f2cce93eb4d92b882ce1fdf1c81bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 10:13:18,815 - bertfttrain - INFO - [Epoch 3/3] Iteration 9600 -> Train Loss: 0.2049, Train Accuracy: 0.917\n",
      "2022-04-14 10:13:18,815 - bertfttrain - INFO - [Epoch 3/3] Iteration 9600 -> Train Loss: 0.2049, Train Accuracy: 0.917\n",
      "2022-04-14 10:14:16,470 - bertfttrain - INFO - [Epoch 3/3] Iteration 10000 -> Train Loss: 0.1816, Train Accuracy: 0.931\n",
      "2022-04-14 10:14:16,470 - bertfttrain - INFO - [Epoch 3/3] Iteration 10000 -> Train Loss: 0.1816, Train Accuracy: 0.931\n",
      "2022-04-14 10:15:17,660 - bertfttrain - INFO - [Epoch 3/3] Iteration 10400 -> Train Loss: 0.1716, Train Accuracy: 0.933\n",
      "2022-04-14 10:15:17,660 - bertfttrain - INFO - [Epoch 3/3] Iteration 10400 -> Train Loss: 0.1716, Train Accuracy: 0.933\n",
      "2022-04-14 10:16:18,831 - bertfttrain - INFO - [Epoch 3/3] Iteration 10800 -> Train Loss: 0.1810, Train Accuracy: 0.930\n",
      "2022-04-14 10:16:18,831 - bertfttrain - INFO - [Epoch 3/3] Iteration 10800 -> Train Loss: 0.1810, Train Accuracy: 0.930\n",
      "2022-04-14 10:17:16,421 - bertfttrain - INFO - [Epoch 3/3] Iteration 11200 -> Train Loss: 0.1774, Train Accuracy: 0.929\n",
      "2022-04-14 10:17:16,421 - bertfttrain - INFO - [Epoch 3/3] Iteration 11200 -> Train Loss: 0.1774, Train Accuracy: 0.929\n",
      "2022-04-14 10:18:15,257 - bertfttrain - INFO - [Epoch 3/3] Iteration 11600 -> Train Loss: 0.1710, Train Accuracy: 0.934\n",
      "2022-04-14 10:18:15,257 - bertfttrain - INFO - [Epoch 3/3] Iteration 11600 -> Train Loss: 0.1710, Train Accuracy: 0.934\n",
      "2022-04-14 10:19:14,494 - bertfttrain - INFO - [Epoch 3/3] Iteration 12000 -> Train Loss: 0.1751, Train Accuracy: 0.932\n",
      "2022-04-14 10:19:14,494 - bertfttrain - INFO - [Epoch 3/3] Iteration 12000 -> Train Loss: 0.1751, Train Accuracy: 0.932\n",
      "2022-04-14 10:20:12,121 - bertfttrain - INFO - [Epoch 3/3] Iteration 12400 -> Train Loss: 0.1727, Train Accuracy: 0.933\n",
      "2022-04-14 10:20:12,121 - bertfttrain - INFO - [Epoch 3/3] Iteration 12400 -> Train Loss: 0.1727, Train Accuracy: 0.933\n",
      "2022-04-14 10:21:11,330 - bertfttrain - INFO - [Epoch 3/3] Iteration 12800 -> Train Loss: 0.1662, Train Accuracy: 0.936\n",
      "2022-04-14 10:21:11,330 - bertfttrain - INFO - [Epoch 3/3] Iteration 12800 -> Train Loss: 0.1662, Train Accuracy: 0.936\n",
      "2022-04-14 10:22:09,304 - bertfttrain - INFO - [Epoch 3/3] Iteration 13200 -> Train Loss: 0.1731, Train Accuracy: 0.933\n",
      "2022-04-14 10:22:09,304 - bertfttrain - INFO - [Epoch 3/3] Iteration 13200 -> Train Loss: 0.1731, Train Accuracy: 0.933\n",
      "2022-04-14 10:23:07,039 - bertfttrain - INFO - [Epoch 3/3] Iteration 13600 -> Train Loss: 0.1752, Train Accuracy: 0.932\n",
      "2022-04-14 10:23:07,039 - bertfttrain - INFO - [Epoch 3/3] Iteration 13600 -> Train Loss: 0.1752, Train Accuracy: 0.932\n",
      "2022-04-14 10:24:06,744 - bertfttrain - INFO - [Epoch 3/3] Iteration 14000 -> Train Loss: 0.1741, Train Accuracy: 0.936\n",
      "2022-04-14 10:24:06,744 - bertfttrain - INFO - [Epoch 3/3] Iteration 14000 -> Train Loss: 0.1741, Train Accuracy: 0.936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c35ff6eb704c9eb345d07d4cda588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 10:25:10,843 - bertfttrain - INFO - [Epoch 3/3] Validatation Accuracy:0.87786\n",
      "2022-04-14 10:25:10,843 - bertfttrain - INFO - [Epoch 3/3] Validatation Accuracy:0.87786\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "   \n",
    "    ##################################################\n",
    "    # 변수 설정\n",
    "    ##################################################\n",
    "    epochs = config.epochs            # epochs\n",
    "    learning_rate = config.lr  # 학습률\n",
    "    p_itr = 400           # 손실률 보여줄 step 수\n",
    "    ##################################################\n",
    "\n",
    "    # optimizer 적용\n",
    "    optimizer = AdamW(model.parameters(), \n",
    "                     lr=learning_rate, \n",
    "                     eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "    # 총 훈련과정에서 반복할 스탭\n",
    "    total_steps = len(train_loader)*epochs\n",
    "\n",
    "    # 스캐줄러 생성\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    itr = 1\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    list_training_loss = []\n",
    "    list_acc_loss = []\n",
    "    list_validation_acc_loss = []\n",
    "\n",
    "    model.zero_grad()# 그래디언트 초기화\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        model.train() # 훈련모드로 변환\n",
    "        for data in tqdm(train_loader):\n",
    "\n",
    "            #optimizer.zero_grad()\n",
    "            model.zero_grad()# 그래디언트 초기화\n",
    "\n",
    "            # 입력 값 설정\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)       \n",
    "            labels = data['labels'].to(device)\n",
    "            #print('Labels:{}'.format(labels))\n",
    "\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=labels)\n",
    "\n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "\n",
    "            # optimizer 과 scheduler 업데이트 시킴\n",
    "            loss.backward()   # backward 구함\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "            optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "            scheduler.step()  # 학습률 감소\n",
    "\n",
    "            # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "            # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "            # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "            with torch.no_grad():\n",
    "                # 정확도와 총 손실률 계산\n",
    "                pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "                correct = pred.eq(labels)\n",
    "                total_correct += correct.sum().item()\n",
    "                total_len += len(labels)    \n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                if itr % p_itr == 0:\n",
    "\n",
    "                    logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                    # wandb 로그 기록\n",
    "                    wandb.log({\"Loss\": total_loss/p_itr,\n",
    "                              \"Accuracy\": total_correct/total_len\n",
    "                              })\n",
    "\n",
    "                    list_training_loss.append(total_loss/p_itr)\n",
    "                    list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                    total_loss = 0\n",
    "                    total_len = 0\n",
    "                    total_correct = 0\n",
    "\n",
    "\n",
    "            itr+=1\n",
    "\n",
    "        ####################################################################\n",
    "        # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "        # 평가 시작\n",
    "        model.eval()\n",
    "\n",
    "        total_test_correct = 0\n",
    "        total_test_len = 0\n",
    "\n",
    "        for data in tqdm(eval_loader):\n",
    "            # 입력 값 설정\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            token_type_ids = data['token_type_ids'].to(device)       \n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "            # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "            # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "            with torch.no_grad():\n",
    "                # 모델 실행\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,\n",
    "                                labels=labels)\n",
    "\n",
    "                # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                #loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # 총 손실류 구함\n",
    "                pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "                correct = pred.eq(labels)\n",
    "                total_test_correct += correct.sum().item()\n",
    "                total_test_len += len(labels)\n",
    "\n",
    "        list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "        logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "\n",
    "        # wandb 로그 기록\n",
    "        wandb.log({\"Validatation Accurac\": total_test_correct / total_test_len\n",
    "                  })\n",
    "\n",
    "        ####################################################################\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdbe8b51-19eb-4562-a5be-317702cfcb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▂▂▂▃▃▃▃▃▄▄▄▆▆▆▆▆▆▆▆▆▆▆▇███████████</td></tr><tr><td>Loss</td><td>█▇▇▇▆▆▆▆▆▅▅▅▄▄▃▃▃▃▃▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validatation Accurac</td><td>▁▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.93617</td></tr><tr><td>Loss</td><td>0.17405</td></tr><tr><td>Validatation Accurac</td><td>0.87786</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">misunderstood-wind-1</strong>: <a href=\"https://wandb.ai/kobongsoo/bert-ft-multiclassification/runs/q7pczmlo\" target=\"_blank\">https://wandb.ai/kobongsoo/bert-ft-multiclassification/runs/q7pczmlo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220414_093713-q7pczmlo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wnandb 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b0f930-7f94-4acb-ab54-9e57985f513d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e39d088-5f2a-48c3-99fc-d9648ae5e336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e0678a-51be-4335-8435-b5e75dc97b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/classification/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327-nscm-0329/vocab/tokenizer_config.json',\n",
       " 'model/classification/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327-nscm-0329/vocab/special_tokens_map.json',\n",
       " 'model/classification/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327-nscm-0329/vocab/vocab.txt',\n",
       " 'model/classification/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327-nscm-0329/vocab/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체모델 저장\n",
    "os.makedirs(OUTPATH)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH + 'vocab'\n",
    "os.makedirs(VOCAB_PATH)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fac7a89-f56e-42ea-a56d-2931254df36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03a0993ec34443fb8934bcba03fc886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83495/4265055506.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-03-29 13:16:10,863 - bertfttrain - INFO - Test-accuracy: 0.87684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-accuracy:  0.87684\n"
     ]
    }
   ],
   "source": [
    "# 최종 평가 시작\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "for data in tqdm(eval_loader):\n",
    "   \n",
    "    # 입력 값 설정\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    token_type_ids = data['token_type_ids'].to(device)       \n",
    "    labels = data['labels'].to(device)\n",
    " \n",
    "    # 모델 실행\n",
    "    outputs = model(input_ids=input_ids, \n",
    "                   attention_mask=attention_mask,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   labels=labels)\n",
    "    \n",
    "     # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # 총 손실류 구함\n",
    "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "    correct = pred.eq(labels)\n",
    "    total_correct += correct.sum().item()\n",
    "    total_len += len(labels)\n",
    "\n",
    "print('Test-accuracy: ', total_correct / total_len)\n",
    "logger.info(f\"Test-accuracy: {total_correct / total_len}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa8e8c5-c642-468f-940c-630083165c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntorch.save({\\n    'model': model.state_dict(),\\n    'optimizer': optimizer.state_dict()\\n}, OUTPATH + 'all.tar')  \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능\n",
    "'''\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, OUTPATH + 'all.tar')  \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
