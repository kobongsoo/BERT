{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd66b12-3efc-49a7-bdeb-739ff11bd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/bwdataset_2022-04-18.log\n",
      "logfilepath:../../log/qnadataset_2022-04-18.log\n"
     ]
    }
   ],
   "source": [
    "# MLM 방식을 이용한 Further pre-traning 방식 구현 예제\n",
    "# 참고 소스 : https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c 참조 바람\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import GPU_info, seed_everything, mlogging, AccuracyForMLM, SaveBERTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3321a0d-57d3-400b-be05-619e1bf63842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertfpt3_2022-04-18.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "#input_corpus = \"../korpora/kowiki_20190620/wiki_20190620_mecab_false_0311.txt\"\n",
    "input_corpus = \"../../korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "\n",
    "# eval 말뭉치 \n",
    "eval_corpus = \"../../korpora/kowiki_20190620/wiki_eval_test.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../model/bert/bert-multilingual-cased\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "#vocab_path=\"tokenizer/wiki_20190620_false_0311_speical/bmc_add_wiki_20190620_false_0311.txt\"\n",
    "vocab_path=\"../../tokenizer/my_bong_vocab/\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../model/bert/bert-multilingual-cased-0418/'\n",
    "\n",
    "batch_size = 32\n",
    "token_max_len = 128\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(222)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertfpt3\", logfilename=\"bertfpt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6fad4a-7b5b-4502-93bc-7564f197398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_token_size: 27, tokenizer.vocab_size: 167537\n",
      "vocab_size: 167560\n",
      "tokenizer_len: 167550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(167550, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=167550, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "\n",
    "#tokenizer = BertTokenizer(vocab_file=vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# tokenizer = BertTokenizerFast(vocab_file=vocab_file, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "#config = BertConfig.from_pretrained(model_path)\n",
    "#model = BertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path), config=config) \n",
    "model = BertForMaskedLM.from_pretrained(model_path)    \n",
    "\n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e41cf1f-4010-4348-8921-67d0f21ea09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:29:15,012 - bwpdataset - INFO - ==>[Start] cached file read: ../../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_20190620_small.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "*corpus:../../korpora/kowiki_20190620/wiki_20190620_small.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:29:15,358 - bwpdataset - INFO - <==[End] Loading features from cached file ../../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_20190620_small.txt [took 0.345 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*corpus:../../korpora/kowiki_20190620/wiki_eval_test.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n",
      "*total_line: 114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fa21d56c58413aa0ae2965e3bf4ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16378509d7f4f098b8de28542e2e20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   101, 120501,   9551,    107, 125318,    103, 125598, 122449,  11018,\n",
      "        120397,    103,  23545,  11303,  48506,  70672,    103,    119,    102,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   101, 120501,   9551,    107, 125318,    107, 125598, 122449,  11018,\n",
      "        120397, 119606,  23545,  11303,  48506,  70672,  30919,    119,    102,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import MLMDataset\n",
    "\n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "\n",
    "train_dataset = MLMDataset(corpus_path = input_corpus,\n",
    "                           tokenizer = tokenizer, \n",
    "                           CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                           SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                           UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                           PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                           Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                           max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                           mlm_probability=0.15,\n",
    "                           overwrite_cache=False\n",
    "                          )\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          num_workers=3\n",
    "                         )\n",
    "\n",
    "#===============================================================================\n",
    "# eval dataloader 생성\n",
    "eval_dataset = MLMDataset(corpus_path = eval_corpus,\n",
    "                          tokenizer = tokenizer, \n",
    "                          CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                          SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                          UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                          PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                          Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                          max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                          mlm_probability=0.15,\n",
    "                          overwrite_cache=False\n",
    "                          )\n",
    "\n",
    "\n",
    "# eval dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         #shuffle=True, # dataset을 섞음\n",
    "                         sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                         num_workers=3\n",
    "                         )\n",
    "#===============================================================================\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89756e9c-7002-4d63-b2ef-a3431486fb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038c3d4b9224466ba90b43e776f47352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d027a63aa84946bf5875c45d41ea7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:33:30,243 - bertfpt3 - INFO - [Epoch 1/4] Iteration 30 -> Train Loss: 0.3514, Train Acc: 0.8101, Val Acc:0.8231531819635434\n",
      "2022-04-18 10:33:39,138 - bertfpt3 - INFO - [Epoch 1/4] Iteration 60 -> Train Loss: 0.2840, Train Acc: 0.8306, Val Acc:0.8391429485129517\n",
      "2022-04-18 10:33:47,971 - bertfpt3 - INFO - [Epoch 1/4] Iteration 90 -> Train Loss: 0.2421, Train Acc: 0.8443, Val Acc:0.8554525103933482\n",
      "2022-04-18 10:33:56,897 - bertfpt3 - INFO - [Epoch 1/4] Iteration 120 -> Train Loss: 0.2308, Train Acc: 0.8575, Val Acc:0.8724016629357212\n",
      "2022-04-18 10:34:05,646 - bertfpt3 - INFO - [Epoch 1/4] Iteration 150 -> Train Loss: 0.1965, Train Acc: 0.8776, Val Acc:0.8883914294851295\n",
      "2022-04-18 10:34:14,504 - bertfpt3 - INFO - [Epoch 1/4] Iteration 180 -> Train Loss: 0.1866, Train Acc: 0.8894, Val Acc:0.8922289734569875\n",
      "2022-04-18 10:34:23,382 - bertfpt3 - INFO - [Epoch 1/4] Iteration 210 -> Train Loss: 0.1754, Train Acc: 0.8958, Val Acc:0.8960665174288456\n",
      "2022-04-18 10:34:32,300 - bertfpt3 - INFO - [Epoch 1/4] Iteration 240 -> Train Loss: 0.1621, Train Acc: 0.8989, Val Acc:0.9011832427246562\n",
      "2022-04-18 10:34:41,249 - bertfpt3 - INFO - [Epoch 1/4] Iteration 270 -> Train Loss: 0.1566, Train Acc: 0.9017, Val Acc:0.9047009913655261\n",
      "2022-04-18 10:34:50,147 - bertfpt3 - INFO - [Epoch 1/4] Iteration 300 -> Train Loss: 0.1484, Train Acc: 0.9027, Val Acc:0.9053405820275024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9592969f6bf49b6ad419b15485a01f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:34:59,502 - bertfpt3 - INFO - [Epoch 2/4] Iteration 330 -> Train Loss: 0.2094, Train Acc: 0.8585, Val Acc:0.870163095618804\n",
      "2022-04-18 10:35:08,432 - bertfpt3 - INFO - [Epoch 2/4] Iteration 360 -> Train Loss: 0.1298, Train Acc: 0.9129, Val Acc:0.9047009913655261\n",
      "2022-04-18 10:35:17,374 - bertfpt3 - INFO - [Epoch 2/4] Iteration 390 -> Train Loss: 0.1104, Train Acc: 0.9227, Val Acc:0.9056603773584906\n",
      "2022-04-18 10:35:26,291 - bertfpt3 - INFO - [Epoch 2/4] Iteration 420 -> Train Loss: 0.1091, Train Acc: 0.9239, Val Acc:0.9082187400063959\n",
      "2022-04-18 10:35:35,244 - bertfpt3 - INFO - [Epoch 2/4] Iteration 450 -> Train Loss: 0.1106, Train Acc: 0.9260, Val Acc:0.9091781259993604\n",
      "2022-04-18 10:35:44,220 - bertfpt3 - INFO - [Epoch 2/4] Iteration 480 -> Train Loss: 0.1077, Train Acc: 0.9253, Val Acc:0.9104573073233131\n",
      "2022-04-18 10:35:53,192 - bertfpt3 - INFO - [Epoch 2/4] Iteration 510 -> Train Loss: 0.1009, Train Acc: 0.9286, Val Acc:0.9114166933162776\n",
      "2022-04-18 10:36:02,155 - bertfpt3 - INFO - [Epoch 2/4] Iteration 540 -> Train Loss: 0.1052, Train Acc: 0.9255, Val Acc:0.9107771026543012\n",
      "2022-04-18 10:36:11,118 - bertfpt3 - INFO - [Epoch 2/4] Iteration 570 -> Train Loss: 0.0995, Train Acc: 0.9287, Val Acc:0.9123760793092421\n",
      "2022-04-18 10:36:20,088 - bertfpt3 - INFO - [Epoch 2/4] Iteration 600 -> Train Loss: 0.1009, Train Acc: 0.9295, Val Acc:0.9114166933162776\n",
      "2022-04-18 10:36:25,599 - bertfpt3 - INFO - Iteration 614 -> save model:../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02eb81331d524176a886f95a08510ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:36:31,355 - bertfpt3 - INFO - [Epoch 3/4] Iteration 630 -> Train Loss: 0.1432, Train Acc: 0.8986, Val Acc:0.8967061080908219\n",
      "2022-04-18 10:36:40,338 - bertfpt3 - INFO - [Epoch 3/4] Iteration 660 -> Train Loss: 0.0932, Train Acc: 0.9399, Val Acc:0.9104573073233131\n",
      "2022-04-18 10:36:49,299 - bertfpt3 - INFO - [Epoch 3/4] Iteration 690 -> Train Loss: 0.0742, Train Acc: 0.9477, Val Acc:0.9126958746402303\n",
      "2022-04-18 10:36:58,284 - bertfpt3 - INFO - [Epoch 3/4] Iteration 720 -> Train Loss: 0.0736, Train Acc: 0.9480, Val Acc:0.9114166933162776\n",
      "2022-04-18 10:37:07,204 - bertfpt3 - INFO - [Epoch 3/4] Iteration 750 -> Train Loss: 0.0707, Train Acc: 0.9488, Val Acc:0.9120562839782539\n",
      "2022-04-18 10:37:16,223 - bertfpt3 - INFO - [Epoch 3/4] Iteration 780 -> Train Loss: 0.0709, Train Acc: 0.9488, Val Acc:0.9136552606331948\n",
      "2022-04-18 10:37:25,193 - bertfpt3 - INFO - [Epoch 3/4] Iteration 810 -> Train Loss: 0.0723, Train Acc: 0.9479, Val Acc:0.9139750559641829\n",
      "2022-04-18 10:37:34,012 - bertfpt3 - INFO - [Epoch 3/4] Iteration 840 -> Train Loss: 0.0739, Train Acc: 0.9477, Val Acc:0.9133354653022066\n",
      "2022-04-18 10:37:42,998 - bertfpt3 - INFO - [Epoch 3/4] Iteration 870 -> Train Loss: 0.0700, Train Acc: 0.9491, Val Acc:0.9123760793092421\n",
      "2022-04-18 10:37:52,002 - bertfpt3 - INFO - [Epoch 3/4] Iteration 900 -> Train Loss: 0.0708, Train Acc: 0.9475, Val Acc:0.9114166933162776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871c9dd94bf940489765e98fb0701eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 10:38:01,430 - bertfpt3 - INFO - [Epoch 4/4] Iteration 930 -> Train Loss: 0.1000, Train Acc: 0.9263, Val Acc:0.9114166933162776\n",
      "2022-04-18 10:38:10,407 - bertfpt3 - INFO - [Epoch 4/4] Iteration 960 -> Train Loss: 0.0602, Train Acc: 0.9581, Val Acc:0.9123760793092421\n",
      "2022-04-18 10:38:19,240 - bertfpt3 - INFO - [Epoch 4/4] Iteration 990 -> Train Loss: 0.0594, Train Acc: 0.9584, Val Acc:0.9133354653022066\n",
      "2022-04-18 10:38:28,317 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1020 -> Train Loss: 0.0598, Train Acc: 0.9592, Val Acc:0.9123760793092421\n",
      "2022-04-18 10:38:37,270 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1050 -> Train Loss: 0.0552, Train Acc: 0.9609, Val Acc:0.9123760793092421\n",
      "2022-04-18 10:38:46,276 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1080 -> Train Loss: 0.0588, Train Acc: 0.9606, Val Acc:0.9120562839782539\n",
      "2022-04-18 10:38:55,223 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1110 -> Train Loss: 0.0545, Train Acc: 0.9618, Val Acc:0.9123760793092421\n",
      "2022-04-18 10:39:04,141 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1140 -> Train Loss: 0.0560, Train Acc: 0.9592, Val Acc:0.9120562839782539\n",
      "2022-04-18 10:39:13,136 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1170 -> Train Loss: 0.0576, Train Acc: 0.9621, Val Acc:0.9117364886472658\n",
      "2022-04-18 10:39:22,122 - bertfpt3 - INFO - [Epoch 4/4] Iteration 1200 -> Train Loss: 0.0545, Train Acc: 0.9621, Val Acc:0.9120562839782539\n",
      "2022-04-18 10:39:31,394 - bertfpt3 - INFO - Iteration 1228 -> save model:../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0418/1228\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "epochs = 4            # epochs\n",
    "learning_rate = 2e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.5)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "total_test_correct = 0\n",
    "total_test_len = 0\n",
    "    \n",
    "list_train_loss = []\n",
    "list_train_acc = []\n",
    "list_validation_acc = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            correct = AccuracyForMLM(logits, labels, attention_mask)\n",
    "            total_correct += correct.sum().item() \n",
    "            total_len += attention_mask.sum().item() # 단어 총 수는 attension_mask가 1(True) 인 것들의 합\n",
    "            #=========================================\n",
    "                \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                train_acc = total_correct/total_len\n",
    "                       \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "\n",
    "                #for data in tqdm(eval_loader):\n",
    "                for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data['input_ids'].to(device)\n",
    "                    attention_mask = data['attention_mask'].to(device)\n",
    "                    token_type_ids = data['token_type_ids'].to(device)       \n",
    "                    labels = data['labels'].to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       token_type_ids=token_type_ids,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        #loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "\n",
    "                        #===========================================\n",
    "                        # 정확도(Accurarcy) 계산\n",
    "                        correct = AccuracyForMLM(logits, labels, attention_mask)\n",
    "                        total_test_correct += correct.sum().item() \n",
    "                        total_test_len += attention_mask.sum().item()  # 단어 총 수는 attension_mask가 1(True) 인 것들의 합\n",
    "                        #=========================================\n",
    "\n",
    "                val_acc = total_test_correct/total_test_len\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Acc:{}'.format(epoch+1, epochs, itr, train_loss, train_acc, val_acc))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_train_acc.append(train_acc)\n",
    "                list_validation_acc.append(val_acc)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "                total_test_correct = 0\n",
    "                total_test_len = 0\n",
    "                ####################################################################\n",
    "\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517c6d1-246c-417a-92e8-b310dd4c2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "#plt.plot(list_train_acc, label='Train Accuracy')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list_train_acc, label='Train Accuracy')\n",
    "plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59891576-ff4e-4826-b5b3-90468005a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "#save_model(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fac71-a6d8-4eb6-bc77-e1bca6b19225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
