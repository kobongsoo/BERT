{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd66b12-3efc-49a7-bdeb-739ff11bd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/bwdataset_2022-04-22.log\n",
      "logfilepath:../../log/qnadataset_2022-04-22.log\n"
     ]
    }
   ],
   "source": [
    "# MLM 방식을 이용한 Further pre-traning 방식 구현 예제\n",
    "# 참고 소스 : https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c 참조 바람\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import GPU_info, seed_everything, mlogging, AccuracyForMLM, SaveBERTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3321a0d-57d3-400b-be05-619e1bf63842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertfpt3_2022-04-22.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "#input_corpus = \"../korpora/kowiki_20190620/wiki_20190620_mecab_false_0311.txt\"\n",
    "#input_corpus = \"../../korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "input_corpus = \"../../korpora/mycorpus/bong_corpus_mecab.txt\"\n",
    "\n",
    "# eval 말뭉치 \n",
    "eval_corpus = \"../../korpora/kowiki_20190620/wiki_eval_test.txt\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../model/bert/bert-multilingual-cased/\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "#vocab_path=\"../../tokenizer/wiki_20190620_nouns_0324\"\n",
    "vocab_path=\"../../tokenizer/my_vocab\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../model/bert/bmc-fpt-bong_corpus_mecab-0424/'\n",
    "\n",
    "batch_size = 32\n",
    "token_max_len = 128\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertfpt3\", logfilename=\"bertfpt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6fad4a-7b5b-4502-93bc-7564f197398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_token_size: 5, tokenizer.vocab_size: 149793\n",
      "vocab_size: 149794\n",
      "tokenizer_len: 149793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../model/bert/bert-multilingual-cased/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(149793, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=149793, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "\n",
    "#tokenizer = BertTokenizer(vocab_file=vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# tokenizer = BertTokenizerFast(vocab_file=vocab_file, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "#config = BertConfig.from_pretrained(model_path)\n",
    "#model = BertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path), config=config) \n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased', cache_dir='test')    \n",
    "model = BertForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "#167550\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e41cf1f-4010-4348-8921-67d0f21ea09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "*corpus:../../korpora/mycorpus/bong_corpus_mecab.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n",
      "*total_line: 3339430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d20006da40485bb5e579e818f0f21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3339430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2024c967e35e485fb95efff263528a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3339430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 06:21:59,963 - bwpdataset - INFO - ==>[Start] cached file create: ../../korpora/mycorpus/cached_lm_BertTokenizer_128_bong_corpus_mecab.txt\n",
      "2022-04-23 06:22:54,698 - bwpdataset - INFO - <==[End] Saving features into cached file ../../korpora/mycorpus/cached_lm_BertTokenizer_128_bong_corpus_mecab.txt [took 54.734 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*corpus:../../korpora/kowiki_20190620/wiki_eval_test.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n",
      "*total_line: 114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7cc94b5e5c47e4ae42340977abda1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3a94aa9151429e9b48f58a089d7e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 06:22:54,786 - bwpdataset - INFO - ==>[Start] cached file create: ../../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_eval_test.txt\n",
      "2022-04-23 06:22:54,788 - bwpdataset - INFO - <==[End] Saving features into cached file ../../korpora/kowiki_20190620/cached_lm_BertTokenizer_128_wiki_eval_test.txt [took 0.001 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   101,  24895,  37010,    103,   9632, 123360,   9637, 144398, 136581,\n",
      "        110148,    103, 119796,   9955,   9460,   9647,   9043, 122873,   9245,\n",
      "          9536,   9645,  48345,    102,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   101,  24895,  37010,  10230,   9632, 123360,   9637, 144398, 136581,\n",
      "        110148,   9233, 119796,   9955,   9460,   9647,   9043, 122873,   9245,\n",
      "          9536,   9645,  48345,    102,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import MLMDataset\n",
    "\n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "\n",
    "train_dataset = MLMDataset(corpus_path = input_corpus,\n",
    "                           tokenizer = tokenizer, \n",
    "                           CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                           SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                           UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                           PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                           Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                           max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                           mlm_probability=0.15,\n",
    "                           overwrite_cache=True\n",
    "                          )\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          num_workers=3\n",
    "                         )\n",
    "\n",
    "#===============================================================================\n",
    "# eval dataloader 생성\n",
    "eval_dataset = MLMDataset(corpus_path = eval_corpus,\n",
    "                          tokenizer = tokenizer, \n",
    "                          CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                          SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                          UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                          PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                          Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                          max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                          mlm_probability=0.15,\n",
    "                          overwrite_cache=True\n",
    "                          )\n",
    "\n",
    "\n",
    "# eval dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         #shuffle=True, # dataset을 섞음\n",
    "                         sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                         num_workers=3\n",
    "                         )\n",
    "#===============================================================================\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89756e9c-7002-4d63-b2ef-a3431486fb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62dd608ab324f51a1b8720841911727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a240561ec648beae22e21c404d580b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 07:08:59,234 - bertfpt3 - INFO - [Epoch 1/4] Iteration 10435 -> Train Loss: 0.6883, Train Acc: 0.8651, Val Acc:0.8629056718228693\n",
      "2022-04-23 07:54:40,955 - bertfpt3 - INFO - [Epoch 1/4] Iteration 20870 -> Train Loss: 0.1359, Train Acc: 0.9180, Val Acc:0.8586593873218077\n",
      "2022-04-23 08:40:09,419 - bertfpt3 - INFO - [Epoch 1/4] Iteration 31305 -> Train Loss: 0.1175, Train Acc: 0.9247, Val Acc:0.848650288140734\n",
      "2022-04-23 09:25:30,669 - bertfpt3 - INFO - [Epoch 1/4] Iteration 41740 -> Train Loss: 0.1075, Train Acc: 0.9282, Val Acc:0.8498635122838945\n",
      "2022-04-23 10:11:04,389 - bertfpt3 - INFO - [Epoch 1/4] Iteration 52175 -> Train Loss: 0.1000, Train Acc: 0.9309, Val Acc:0.845920533818623\n",
      "2022-04-23 10:56:30,370 - bertfpt3 - INFO - [Epoch 1/4] Iteration 62610 -> Train Loss: 0.0949, Train Acc: 0.9332, Val Acc:0.8392478010312405\n",
      "2022-04-23 11:41:49,345 - bertfpt3 - INFO - [Epoch 1/4] Iteration 73045 -> Train Loss: 0.0912, Train Acc: 0.9348, Val Acc:0.8428874734607219\n",
      "2022-04-23 12:27:31,550 - bertfpt3 - INFO - [Epoch 1/4] Iteration 83480 -> Train Loss: 0.0884, Train Acc: 0.9360, Val Acc:0.8343949044585988\n",
      "2022-04-23 12:27:34,726 - bwpdataset - INFO - ==> save_model : ../../model/bert/bmc-fpt-bong_corpus_mecab-0424/batch:32-ep:4-lr:0.000030000-4m23d-12:27\n",
      "2022-04-23 13:12:58,605 - bertfpt3 - INFO - [Epoch 1/4] Iteration 93915 -> Train Loss: 0.0860, Train Acc: 0.9372, Val Acc:0.8316651501364877\n",
      "2022-04-23 13:58:31,584 - bertfpt3 - INFO - [Epoch 1/4] Iteration 104350 -> Train Loss: 0.0843, Train Acc: 0.9380, Val Acc:0.8334849863512284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2854fc317842ec8259cb7a22880c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 14:44:35,625 - bertfpt3 - INFO - [Epoch 2/4] Iteration 114785 -> Train Loss: 0.0822, Train Acc: 0.9386, Val Acc:0.8334849863512284\n",
      "2022-04-23 15:29:42,332 - bertfpt3 - INFO - [Epoch 2/4] Iteration 125220 -> Train Loss: 0.0754, Train Acc: 0.9427, Val Acc:0.8346982104943889\n",
      "2022-04-23 16:15:00,993 - bertfpt3 - INFO - [Epoch 2/4] Iteration 135655 -> Train Loss: 0.0750, Train Acc: 0.9429, Val Acc:0.8280254777070064\n",
      "2022-04-23 17:00:07,454 - bertfpt3 - INFO - [Epoch 2/4] Iteration 146090 -> Train Loss: 0.0746, Train Acc: 0.9431, Val Acc:0.8313618441006976\n",
      "2022-04-23 17:45:36,635 - bertfpt3 - INFO - [Epoch 2/4] Iteration 156525 -> Train Loss: 0.0739, Train Acc: 0.9436, Val Acc:0.8180163785259327\n",
      "2022-04-23 18:31:05,073 - bertfpt3 - INFO - [Epoch 2/4] Iteration 166960 -> Train Loss: 0.0735, Train Acc: 0.9438, Val Acc:0.8143767060964513\n",
      "2022-04-23 18:31:09,621 - bwpdataset - INFO - ==> save_model : ../../model/bert/bmc-fpt-bong_corpus_mecab-0424/batch:32-ep:4-lr:0.000030000-4m23d-18:31\n",
      "2022-04-23 19:16:27,646 - bertfpt3 - INFO - [Epoch 2/4] Iteration 177395 -> Train Loss: 0.0729, Train Acc: 0.9441, Val Acc:0.8095238095238095\n",
      "2022-04-23 20:01:36,735 - bertfpt3 - INFO - [Epoch 2/4] Iteration 187830 -> Train Loss: 0.0722, Train Acc: 0.9445, Val Acc:0.8010312405216864\n",
      "2022-04-23 20:46:54,203 - bertfpt3 - INFO - [Epoch 2/4] Iteration 198265 -> Train Loss: 0.0716, Train Acc: 0.9448, Val Acc:0.8070973612374887\n",
      "2022-04-23 21:32:20,268 - bertfpt3 - INFO - [Epoch 2/4] Iteration 208700 -> Train Loss: 0.0712, Train Acc: 0.9450, Val Acc:0.8028510767364271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e25bc6c4aa4a7390a67f5819aac326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 22:18:20,734 - bertfpt3 - INFO - [Epoch 3/4] Iteration 219135 -> Train Loss: 0.0691, Train Acc: 0.9455, Val Acc:0.805277525022748\n",
      "2022-04-23 23:04:00,116 - bertfpt3 - INFO - [Epoch 3/4] Iteration 229570 -> Train Loss: 0.0603, Train Acc: 0.9515, Val Acc:0.8028510767364271\n",
      "2022-04-23 23:49:25,895 - bertfpt3 - INFO - [Epoch 3/4] Iteration 240005 -> Train Loss: 0.0603, Train Acc: 0.9516, Val Acc:0.7979981801637852\n",
      "2022-04-24 00:34:59,166 - bertfpt3 - INFO - [Epoch 3/4] Iteration 250440 -> Train Loss: 0.0603, Train Acc: 0.9516, Val Acc:0.794661813770094\n",
      "2022-04-24 00:35:05,336 - bwpdataset - INFO - ==> save_model : ../../model/bert/bmc-fpt-bong_corpus_mecab-0424/batch:32-ep:4-lr:0.000030000-4m24d-0:35\n",
      "2022-04-24 01:20:18,077 - bertfpt3 - INFO - [Epoch 3/4] Iteration 260875 -> Train Loss: 0.0602, Train Acc: 0.9517, Val Acc:0.7928419775553533\n",
      "2022-04-24 02:05:36,136 - bertfpt3 - INFO - [Epoch 3/4] Iteration 271310 -> Train Loss: 0.0600, Train Acc: 0.9518, Val Acc:0.7898089171974523\n",
      "2022-04-24 02:51:03,204 - bertfpt3 - INFO - [Epoch 3/4] Iteration 281745 -> Train Loss: 0.0599, Train Acc: 0.9518, Val Acc:0.7882923870185017\n",
      "2022-04-24 03:36:31,646 - bertfpt3 - INFO - [Epoch 3/4] Iteration 292180 -> Train Loss: 0.0599, Train Acc: 0.9519, Val Acc:0.7882923870185017\n",
      "2022-04-24 04:21:57,115 - bertfpt3 - INFO - [Epoch 3/4] Iteration 302615 -> Train Loss: 0.0594, Train Acc: 0.9522, Val Acc:0.7876857749469215\n",
      "2022-04-24 05:07:16,344 - bertfpt3 - INFO - [Epoch 3/4] Iteration 313050 -> Train Loss: 0.0592, Train Acc: 0.9524, Val Acc:0.7855626326963907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42431f8220eb4798ad532befb9909d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 05:53:14,167 - bertfpt3 - INFO - [Epoch 4/4] Iteration 323485 -> Train Loss: 0.0613, Train Acc: 0.9501, Val Acc:0.7907188353048226\n",
      "2022-04-24 06:38:44,120 - bertfpt3 - INFO - [Epoch 4/4] Iteration 333920 -> Train Loss: 0.0489, Train Acc: 0.9595, Val Acc:0.786472550803761\n",
      "2022-04-24 06:38:50,918 - bwpdataset - INFO - ==> save_model : ../../model/bert/bmc-fpt-bong_corpus_mecab-0424/batch:32-ep:4-lr:0.000030000-4m24d-6:38\n",
      "2022-04-24 07:24:01,212 - bertfpt3 - INFO - [Epoch 4/4] Iteration 344355 -> Train Loss: 0.0486, Train Acc: 0.9597, Val Acc:0.7840461025174401\n",
      "2022-04-24 08:09:09,194 - bertfpt3 - INFO - [Epoch 4/4] Iteration 354790 -> Train Loss: 0.0486, Train Acc: 0.9598, Val Acc:0.7855626326963907\n",
      "2022-04-24 08:54:28,412 - bertfpt3 - INFO - [Epoch 4/4] Iteration 365225 -> Train Loss: 0.0485, Train Acc: 0.9599, Val Acc:0.7849560206248104\n",
      "2022-04-24 09:39:39,377 - bertfpt3 - INFO - [Epoch 4/4] Iteration 375660 -> Train Loss: 0.0484, Train Acc: 0.9601, Val Acc:0.78374279648165\n",
      "2022-04-24 10:25:10,839 - bertfpt3 - INFO - [Epoch 4/4] Iteration 386095 -> Train Loss: 0.0483, Train Acc: 0.9601, Val Acc:0.7849560206248104\n",
      "2022-04-24 11:10:39,832 - bertfpt3 - INFO - [Epoch 4/4] Iteration 396530 -> Train Loss: 0.0481, Train Acc: 0.9603, Val Acc:0.7828328783742796\n",
      "2022-04-24 11:56:04,606 - bertfpt3 - INFO - [Epoch 4/4] Iteration 406965 -> Train Loss: 0.0481, Train Acc: 0.9604, Val Acc:0.7840461025174401\n",
      "2022-04-24 12:41:33,096 - bertfpt3 - INFO - [Epoch 4/4] Iteration 417400 -> Train Loss: 0.0480, Train Acc: 0.9604, Val Acc:0.7840461025174401\n",
      "2022-04-24 12:41:41,587 - bwpdataset - INFO - ==> save_model : ../../model/bert/bmc-fpt-bong_corpus_mecab-0424/batch:32-ep:4-lr:0.000030000-4m24d-12:41\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "epochs = 4            # epochs\n",
    "learning_rate = 3e-5  # 학습률\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 손실률 보여줄 step 수\n",
    "p_itr = int(len(train_loader)*0.1)  \n",
    "    \n",
    "# step마다 모델 저장\n",
    "save_steps = int(total_steps * 0.2)\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "total_test_correct = 0\n",
    "total_test_len = 0\n",
    "    \n",
    "list_train_loss = []\n",
    "list_train_acc = []\n",
    "list_validation_acc = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "            \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        \n",
    "       \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "       \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # ***further pretrain 에는 손실률 계산을 넣지 않음\n",
    "        # 정확도 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        \n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # 손실률 계산\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            correct = AccuracyForMLM(logits, labels, attention_mask)\n",
    "            total_correct += correct.sum().item() \n",
    "            total_len += attention_mask.sum().item() # 단어 총 수는 attension_mask가 1(True) 인 것들의 합\n",
    "            #=========================================\n",
    "                \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                \n",
    "                train_loss = total_loss/p_itr\n",
    "                train_acc = total_correct/total_len\n",
    "                       \n",
    "                ####################################################################\n",
    "                # 주기마다 eval(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "                # 평가 시작\n",
    "                model.eval()\n",
    "\n",
    "                #for data in tqdm(eval_loader):\n",
    "                for data in eval_loader:\n",
    "                    # 입력 값 설정\n",
    "                    input_ids = data['input_ids'].to(device)\n",
    "                    attention_mask = data['attention_mask'].to(device)\n",
    "                    token_type_ids = data['token_type_ids'].to(device)       \n",
    "                    labels = data['labels'].to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # 모델 실행\n",
    "                        outputs = model(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       token_type_ids=token_type_ids,\n",
    "                                       labels=labels)\n",
    "\n",
    "                        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "                        #loss = outputs.loss\n",
    "                        logits = outputs.logits\n",
    "\n",
    "                        #===========================================\n",
    "                        # 정확도(Accurarcy) 계산\n",
    "                        correct = AccuracyForMLM(logits, labels, attention_mask)\n",
    "                        total_test_correct += correct.sum().item() \n",
    "                        total_test_len += attention_mask.sum().item()  # 단어 총 수는 attension_mask가 1(True) 인 것들의 합\n",
    "                        #=========================================\n",
    "\n",
    "                val_acc = total_test_correct/total_test_len\n",
    "                    \n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Acc: {:.4f}, Val Acc:{}'.format(epoch+1, epochs, itr, train_loss, train_acc, val_acc))\n",
    "                    \n",
    "                list_train_loss.append(train_loss)\n",
    "                list_train_acc.append(train_acc)\n",
    "                list_validation_acc.append(val_acc)\n",
    "                 \n",
    "                # 변수들 초기화    \n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "                total_test_correct = 0\n",
    "                total_test_len = 0\n",
    "                ####################################################################\n",
    "\n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)\n",
    "\n",
    "        itr+=1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59891576-ff4e-4826-b5b3-90468005a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 08:27:49,379 - bwpdataset - INFO - ==> save_model : ../../model/bert/bmc-fpt-bong_corpus_mecab-0424/batch:32-ep:4-lr:0.000030000-4m25d-8:27\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "SaveBERTModel(model, tokenizer, OUTPATH, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b517c6d1-246c-417a-92e8-b310dd4c2f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_train_loss, label='Train Loss')\n",
    "#plt.plot(list_train_acc, label='Train Accuracy')\n",
    "#plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list_train_acc, label='Train Accuracy')\n",
    "plt.plot(list_validation_acc, label='Eval Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fac71-a6d8-4eb6-bc77-e1bca6b19225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
