{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f1e4a-09f6-4253-b1ae-4cc9193711f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 사이트 \n",
    "# 참고소스1 : transformers huggingface mlm 사전 학습 예제\n",
    "#   - https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
    "#\n",
    "# 참고소스2  : bert를 가지고 중국어 corpus로 추가학습시킨 예제\n",
    "#    - https://github.com/zhusleep/pytorch_chinese_lm_pretrain 에 run_language_model_bert.py \n",
    "#      https://www.fatalerrors.org/a/further-pre-training-of-chinese-language-model-bert-roberta.html\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, TextDatasetForNextSentencePrediction, GPU_info, mlogging, MyTextDataset\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, BertConfig, AutoModelWithLMHead, BertForMaskedLM\n",
    "from transformers import AutoTokenizer, DistilBertTokenizerFast, DistilBertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267fd3e-7443-43a2-83ae-ff3ccbb9dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger=mlogging(loggername=\"bertperplexityeval\", logfilename=\"bertperplexityeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a70767-9737-4081-8664-c8291f0289cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수들 설정\n",
    "#input_corpus = \"Korpora/kowikitext/kowikitext_20200920.test\"\n",
    "input_corpus = \"../../data11/my_corpus/bong_eval.txt\"\n",
    "\n",
    "\n",
    "input_model_path = \"../../data11/model/distilbert/mdistilbertV2.0\"\n",
    "vocab_file=\"../../data11/model/distilbert/mdistilbertV2.0/vocab.txt\"\n",
    "ouput_model_dir = \"../../data11/model/distilbert/mdistilbertV2.0-1\"\n",
    "\n",
    "'''\n",
    "input_model_path = \"model/bert-multilingual-cased\"\n",
    "vocab_file=\"model/bert-multilingual-cased/vocab/vocab.txt\"\n",
    "ouput_model_dir = \"model/bert-multilingual-cased\"\n",
    "'''\n",
    "'''\n",
    "input_model_path = \"model/bert-multilingual-cased_furter_pt_model_0216\"\n",
    "vocab_file=\"model/bert-multilingual-cased_furter_pt_model_0216/vocab/vocab.txt\"\n",
    "ouput_model_dir = \"model/bert-multilingual-cased_furter_pt_model_0216\"\n",
    "'''\n",
    "# 토큰활 할때 최대 길이 \n",
    "token_max_len = 128\n",
    "\n",
    "# 훈련용 변수\n",
    "batch_size = 32   # 128로 하면, GPU Out of memory 발생함(=>**따라서 32로 진행)\n",
    " # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정(기본:512) 512, 1024, 2048 식으로 지정함, 엄청난 장문을 다룰경우 10124까지\n",
    "max_position_embeddings = 128 \n",
    "logging_steps = 1  # 훈련시, 로깅할 step 수 (크면 10000번 정도하고, 작으면 100번정도)\n",
    "save_steps = 1     # 10000 step마다 모델 저장\n",
    "save_total_limit = 1 # 마지막 3개 모델 빼고 과거 모델은 삭제(100000번째마다 모델 저장하는데, 마지감 3개 빼고 나머지는 삭제)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417fe82-bcd5-4431-a7fd-2c7691008a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "'''\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, \n",
    "                          max_len=token_max_len, \n",
    "                          do_lower_case=False)\n",
    "'''\n",
    "'''\n",
    "#tokenizer = BertTokenizerFast(vocab_speical_path)\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file=vocab_file,\n",
    "    max_len=token_max_len,\n",
    "    do_lower_case=False,\n",
    "    )\n",
    "'''    \n",
    "tokenizer = DistilBertTokenizerFast(\n",
    "    vocab_file=vocab_file,\n",
    "    max_len=token_max_len,\n",
    "    do_lower_case=False,\n",
    "    )\n",
    "\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'{vocab_file} is_fast:{tokenizer.is_fast}')\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dc487-a740-4188-9525-2676e4078a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_hidden_states = False # 기본은 False=>output 2개 출력됨, True로 지정하면 output이 3개 출력됨\n",
    "#return_dict = True   #False로 지정하는 경우 일반적인 tuple을 리턴, True인 경우는 transformers.file_utils.ModelOutput(ouput.logisc) 으로 리턴\n",
    "#model = BertModel.from_pretrained(input_model_path, output_hidden_states = output_hidden_states, return_dict = return_dict)\n",
    "\n",
    "\n",
    "# AutoModelForMaskedLM, BertForMaskedLM \n",
    "# further pre-training 인 경우 (기존 거에 추가적으로 하는 경우)\n",
    "config = BertConfig.from_pretrained(input_model_path)\n",
    "#model = BertForMaskedLM.from_pretrained(input_model_path, from_tf=bool(\".ckpt\" in input_model_path), config=config)\n",
    "model = DistilBertForMaskedLM.from_pretrained(input_model_path, from_tf=bool(\".ckpt\" in input_model_path), config=config)\n",
    "\n",
    "# 모델 embedding 사이즈를 tokenizer +1 크기 만큼 재 설정함.\n",
    "# => 원래 크기를 tokenizer 사이즈 만큼만 설정하면되는데, kobert는 8002로 평가하면, 오류발생하므로 +1 정도 크게 설정함\n",
    "model.resize_token_embeddings(len(tokenizer) + 1)\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf3af8-5592-44f4-8a3a-cc2700587606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6321b2d-2517-4820-8c12-79ba7e2a0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 평가 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "eval_dataset = load_dataset(\"text\", data_files=input_corpus, split=\"train\") # text 로컬 파일 로딩\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset========================================\")\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719b1d8-20eb-4c50-b8eb-7dd326bbd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset========================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7abda-249d-4210-b14a-679827ea2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 처리\n",
    "def tokenizer_function(examples):\n",
    "    result =  tokenizer(examples['text'], truncation=True, max_length=token_max_len, return_overflowing_tokens=True)\n",
    "    \n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "# batched=True 하면 빠른 tokenizer 이용(Rust)\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "'''\n",
    "%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\n",
    "print(tokenized_dataset_fast['train']['text'][0:2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf08241-8bed-442b-9e5a-d4756331f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(eval_dataset_fast)\n",
    "print(f'*fast_len:{len(eval_dataset_fast)}, len:{len(eval_dataset)}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716b5dd-7a0d-4830-b7b5-25af92fdf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_eval_sample = data_collator(eval_dataset_fast['input_ids'][0:2])\n",
    "\n",
    "print(f\"eval_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_eval_sample['input_ids'][0])\n",
    "print(eval_dataset_fast[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a41e3-1a73-4b51-9d01-e37dad9ada32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_metric 사용을 위해서는 datasets, sklearn 패키지 설치해야함\n",
    "#!pip install datasets\n",
    "#!pip install sklearn\n",
    "\n",
    "# 참고 소스 : https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "def preprocess_logits_for_metrics1(logits, labels):\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics\n",
    "    labels = labels.reshape(-1)\n",
    "    preds = preds.reshape(-1)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184731d1-196f-4c01-a2f8-213263d7bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir=ouput_model_dir,\n",
    "    per_device_eval_batch_size=batch_size\n",
    "    #per_gpu_eval_batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset_fast_input_ids = eval_dataset_fast['input_ids']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #data\n",
    "    eval_dataset=eval_dataset_fast_input_ids,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3acac3-01de-48c2-9176-43a4d54b8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# 펄플렉서티(Perplexity, PPL) 로 성능 평가함 \n",
    "# => PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)입니다. \n",
    "#   PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미합니다. \n",
    "#  가령, 언어 모델에 어떤 테스트 데이터을 주고 측정했더니 PPL이 10이 나왔다고 해봅시다. \n",
    "#  그렇다면 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time step)마다 \n",
    "#  평균 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있습니다. \n",
    "#  같은 테스트 데이터에 대해서 두 언어 모델의 PPL을 각각 계산 후에 PPL의 값을 비교하면, \n",
    "#  두 언어 모델 중 PPL이 더 낮은 언어 모델의 성능이 더 좋다고 볼 수 있습니다.\n",
    "###################################################################\n",
    "eval_output = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86528d-8350-4328-acdb-2158d259ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "# Evaluation\n",
    "results = {}\n",
    "\n",
    "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "result = {\"perplexity\": perplexity}\n",
    "\n",
    "output_eval_file = os.path.join(ouput_model_dir, \"eval_results_lm.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"%s:  %s = %s\", input_model_path, key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "results.update(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278c226-29d2-42b8-afa0-d192f1e296fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT OF MEMORY 에러면 GPU 사용량 체크\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2960bfc-b612-4466-8180-fffef5e5e1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
