{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3141acc-e4a8-4dd4-be79-7b87a0465e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from myutils import seed_everything, TextDatasetForNextSentencePrediction, GPU_info, mlogging\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, BertConfig, AutoModelWithLMHead\n",
    "from transformers import DataCollatorForLanguageModeling, TextDataset\n",
    "import tokenizers\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f25df7-6092-407c-bf84-b05294419961",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_corpus = \"my_data/문서중앙화.txt\"\n",
    "input_model_path = \"model/kobertmodel\" \n",
    "vocab_file=\"Tokenizer/kobert/kobert_new_0208_1.model\"\n",
    "\n",
    "ouput_model_dir = \"model/model_0210_1\"\n",
    "\n",
    "# 훈련용 변수\n",
    "batch_size = 64   # 128로 하면, GPU Out of memory 발생함(=>**따라서 64로 진행)\n",
    "train_epochs = 10\n",
    " # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정(기본:512) 512, 1024, 2048 식으로 지정함, 엄청난 장문을 다룰경우 10124까지\n",
    "max_position_embeddings = 256 \n",
    "logging_steps = 1  # 훈련시, 로깅할 step 수 (크면 10000번 정도하고, 작으면 100번정도)\n",
    "save_steps = 1     # 10000 step마다 모델 저장\n",
    "save_total_limit = 1 # 마지막 3개 모델 빼고 과거 모델은 삭제(100000번째마다 모델 저장하는데, 마지감 3개 빼고 나머지는 삭제)\n",
    "\n",
    "# NSP 관련 변수\n",
    "NSP_block_size = 140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52beea6d-afae-4957-a66c-a2ac14ef443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger = mlogging(loggername=\"senfpt\", logfilname=\"senfpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d41636d-fcdc-4623-ad81-d9e8c96cb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(add_prefix_space = True,)  #add_prefix_space = True이면 문장 맨 앞 단어에도 공배 부여함\n",
    "sentencepiece_tokenizer.train(files = input_corpus, \n",
    "                             vocab_size = 50,\n",
    "                             min_frequency = 1,\n",
    "                             )\n",
    "\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "sorted(vocab, key=lambda x: vocab[x])\n",
    "\n",
    "out_dir = 'Tokenizer'\n",
    "out_name = 'testsp'\n",
    "sentencepiece_tokenizer.save_model(out_dir, out_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b98f733-3ac3-4b70-867f-4be843100930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '안', '<unk>', '하', '세', '요', '.', '.', '<unk>', '<unk>', '습', '니', '다']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file = 'Tokenizer/testsp-vocab.json'\n",
    "#json_file = 'Tokenizer/kobert/kobert_news_wiki_ko_cased-ae5711deb3.spiece'\n",
    "\n",
    "merge_file = 'Tokenizer/testsp-merges.txt'\n",
    "\n",
    "spload = SentencePieceBPETokenizer(vocab = json_file, merges = merge_file)\n",
    "\n",
    "spload.encode('안녕하세요..반갑습니다').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59b2e228-a9bb-4e27-a4f0-5050eb43a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "num_special_tokens_to_add() got an unexpected keyword argument 'pair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105522/864407601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input file path {file_path} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_special_tokens_to_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: num_special_tokens_to_add() got an unexpected keyword argument 'pair'"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(tokenizer=spload, file_path=input_corpus, block_size=128, overwrite_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a6e06-9602-45e9-80bb-8280564df1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_model_path:\n",
    "    # further pre-training 인 경우 (기존 거에 추가적으로 하는 경우)\n",
    "    config = BertConfig.from_pretrained(input_model_path)\n",
    "    model = AutoModelWithLMHead.from_pretrained(input_model_path,\n",
    "                                                from_tf=bool(\".ckpt\" in input_model_path),\n",
    "                                                config=config)\n",
    "else:\n",
    "    # Training new model from scratch 인 경우 (완전 새롭게 모델을 만드는 경우)\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    \n",
    "model.num_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbb5e8-51dc-4f61-aa0d-e0efe4a6f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file, padding_token=\"[PAD]\")\n",
    "tokenizer = nlp.data.BERTSPTokenizer(vocab_file, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7915e-9489-4eeb-bfc0-f3d3a74213cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "file_path=\"my_data/문서중앙화.txt\"\n",
    "datalist = []\n",
    "\n",
    "# [bong] total_line 수를 구함\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    total_line = len(f.readlines())\n",
    "    # [bong] 총 라인수 출력\n",
    "    print('*total_line: {}'.format(total_line))\n",
    "    \n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for idx in tqdm(range(total_line)): \n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            # [bong] 총 라인수 읽기 끝\n",
    "            print('*readline=>count: {} End!'.format(scount))\n",
    "            break\n",
    "        line = line.strip() # 필수!!!!\n",
    "        datalist.append([line]) #2차원으로 묶어야 함\n",
    "        \n",
    "print(datalist[0:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83cfe5b-e053-47ce-bcae-2329456281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len, pad, pair, Masktokenids=0):\n",
    "        \n",
    "        self.mydatalist = []\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "       \n",
    "        #self.sentences = [transform([i[sent_idx]]) for i in dataset]  #기존 코드\n",
    "        for i in dataset:\n",
    "            # transformedat 리턴값 으로는 token_ids, valid_length, token_type_id(segments_ids) 3개가 리턴됨\n",
    "            transformdata = transform([i[sent_idx]]) \n",
    "            #print(transformdata[0])\n",
    "            \n",
    "            # tokeni_ids, token_type_id, attentison_mask, label 등을 얻어오고, tensor로 변환\n",
    "            token_ids = transformdata[0]\n",
    "            valid_length = transformdata[1]\n",
    "            token_type_id = transformdata[2]\n",
    "            labels = copy.deepcopy(transformdata[0]) # *[bong] label를 token_ids 값을 deepcopy하여, 생성\n",
    "            \n",
    "            # token_ids 에 대해 MLM 생성\n",
    "            if Masktokenids != 0:\n",
    "                \n",
    "                #token_ids에 15% 확률로 [MASK] 붙임  \n",
    "                rand = torch.rand(token_ids.shape)\n",
    "                # 0,1,2,3 은 각각 특수 토큰이므로, 특수 토큰에는 [MASK]를 하지 않음\n",
    "                mask_arr = (rand < 0.15)*(token_ids != 0)*(token_ids != 1)*(token_ids != 2)*(token_ids != 3)\n",
    "                #print(mask_arr)\n",
    "\n",
    "                # MASK 붙일 위치를 배열로 변환\n",
    "                selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "                #print('MASK Position: {}'.format(selection))\n",
    "\n",
    "                # [MASK] 토큰(4) 으로 변경\n",
    "                token_ids[selection] = Masktokenids\n",
    "            \n",
    "              \n",
    "            token_ids_tensor = torch.Tensor([token_ids])\n",
    "            token_type_id_tensor = torch.Tensor([token_type_id])\n",
    "            labels_tensor = torch.Tensor([labels])\n",
    "            attention_mask_tensor = self.gen_attention_mask(token_ids_tensor, [valid_length])# attention_mask 설정\n",
    "            \n",
    "           \n",
    "            # dict 로 만듬.\n",
    "            mydict = {'inputs_ids':token_ids_tensor, 'token_type_ids':token_type_id_tensor, 'attention_mask':attention_mask_tensor, 'labels':labels_tensor} \n",
    "            #mydict = {'inputs_ids':token_ids_tensor, 'token_type_ids':token_type_id_tensor, 'attention_mask':attention_mask_tensor} \n",
    "            # list에 추가 \n",
    "            self.mydatalist.append(mydict)\n",
    "            \n",
    "    # attenton_mask 생성 함수\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "              attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.mydatalist[i] )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.mydatalist))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea507c27-8b33-4efb-9696-d3206ecaf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenidsDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len, pad, pair, Masktokenids=0):\n",
    "        \n",
    "        self.mydatalist = []\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "       \n",
    "        #self.sentences = [transform([i[sent_idx]]) for i in dataset]  #기존 코드\n",
    "        for i in dataset:\n",
    "            # transformedat 리턴값 으로는 token_ids, valid_length, token_type_id(segments_ids) 3개가 리턴됨\n",
    "            transformdata = transform([i[sent_idx]]) \n",
    "            #print(transformdata[0])\n",
    "            \n",
    "            # tokeni_ids, token_type_id, attentison_mask, label 등을 얻어오고, tensor로 변환\n",
    "            token_ids = transformdata[0]\n",
    "            labels = copy.deepcopy(transformdata[0]) # *[bong] label를 token_ids 값을 deepcopy하여, 생성   \n",
    "              \n",
    "            # token_ids 에 대해 MLM 생성\n",
    "            if Masktokenids != 0:\n",
    "                \n",
    "                #token_ids에 15% 확률로 [MASK] 붙임  \n",
    "                rand = torch.rand(token_ids.shape)\n",
    "                # 0,1,2,3 은 각각 특수 토큰이므로, 특수 토큰에는 [MASK]를 하지 않음\n",
    "                mask_arr = (rand < 0.15)*(token_ids != 0)*(token_ids != 1)*(token_ids != 2)*(token_ids != 3)\n",
    "                #print(mask_arr)\n",
    "\n",
    "                # MASK 붙일 위치를 배열로 변환\n",
    "                selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "                #print('MASK Position: {}'.format(selection))\n",
    "\n",
    "                # [MASK] 토큰(4) 으로 변경\n",
    "                token_ids[selection] = Masktokenids\n",
    "                \n",
    "            token_ids_tensor = torch.Tensor([token_ids])\n",
    "            labels_tensor = torch.Tensor([labels])\n",
    "                \n",
    "            # dict 로 만듬.\n",
    "            mydict = {'inputs_ids':token_ids_tensor, 'labels':labels_tensor} \n",
    "            # list에 추가 \n",
    "            self.mydatalist.append(mydict)\n",
    "            \n",
    "    # attenton_mask 생성 함수\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "              attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.mydatalist[i] )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.mydatalist))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc899f-76ca-4a3f-b996-3665873a9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=128\n",
    "datalen = len(datalist)\n",
    "#print(datalen)\n",
    "\n",
    "transform_data = MyTokenidsDataset(datalist[0:datalen], 0, tokenizer, max_len, True, False, 3) # 맨뒤에 3는 [MASK]토큰 id임\n",
    "#print(len(transform_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6dc44-b72c-4a4a-a072-0a7b3e06ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = MyTokenidsDataset(datalist[0:datalen], 0, tokenizer, max_len, True, False) # 맨뒤에 3는 [MASK]토큰 id임\n",
    "\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=input_corpus, block_size=128, overwrite_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709fc04-2426-4a24-b5de-1adc4210f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import copy\n",
    "for i in range(len(transform_data)):\n",
    "    token_ids = transform_data[i][0]\n",
    "    print(token_ids)\n",
    "    label = copy.deepcopy(token_ids)\n",
    "#    valid_length = transform_data[i][1]\n",
    "#    segment_ids = transform_data[i][2]\n",
    "    #label = trainsform_data[i][0]\n",
    "    #print(token_ids)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072a37f-c08e-4b80-a69c-d7cf50b788c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transform_data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a9e29-6786-470d-ad2f-02c1695d5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ouput_model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=train_epochs,\n",
    "    per_gpu_train_batch_size=batch_size,\n",
    "    save_steps=save_steps,    # step 수마다 모델을 저장\n",
    "    save_total_limit=save_total_limit, # 마지막 두 모델 빼고 과거 모델은 삭제\n",
    "    logging_steps=logging_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=transform_data,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset   #NSP(Next Setence Predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d600a0-652d-44a2-b42d-0f83e5f87ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafb572-37aa-4d0f-8c95-d58f5aa8168d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
