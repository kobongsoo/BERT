{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3141acc-e4a8-4dd4-be79-7b87a0465e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from myutils import seed_everything, TextDatasetForNextSentencePrediction, GPU_info, mlogging\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, BertConfig, AutoModelWithLMHead\n",
    "from transformers import DataCollatorForLanguageModeling, TextDataset\n",
    "import tokenizers\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        CharBPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f25df7-6092-407c-bf84-b05294419961",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_corpus = \"my_data/ë¬¸ì„œì¤‘ì•™í™”.txt\"\n",
    "input_model_path = \"model/kobertmodel\" \n",
    "vocab_file=\"Tokenizer/kobert/kobert_new_0208_1.model\"\n",
    "\n",
    "ouput_model_dir = \"model/model_0210_1\"\n",
    "\n",
    "# í›ˆë ¨ìš© ë³€ìˆ˜\n",
    "batch_size = 64   # 128ë¡œ í•˜ë©´, GPU Out of memory ë°œìƒí•¨(=>**ë”°ë¼ì„œ 64ë¡œ ì§„í–‰)\n",
    "train_epochs = 10\n",
    " # embedding size ìµœëŒ€ ëª‡ tokenê¹Œì§€ inputìœ¼ë¡œ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì§€ì •(ê¸°ë³¸:512) 512, 1024, 2048 ì‹ìœ¼ë¡œ ì§€ì •í•¨, ì—„ì²­ë‚œ ì¥ë¬¸ì„ ë‹¤ë£°ê²½ìš° 10124ê¹Œì§€\n",
    "max_position_embeddings = 256 \n",
    "logging_steps = 1  # í›ˆë ¨ì‹œ, ë¡œê¹…í•  step ìˆ˜ (í¬ë©´ 10000ë²ˆ ì •ë„í•˜ê³ , ì‘ìœ¼ë©´ 100ë²ˆì •ë„)\n",
    "save_steps = 1     # 10000 stepë§ˆë‹¤ ëª¨ë¸ ì €ì¥\n",
    "save_total_limit = 1 # ë§ˆì§€ë§‰ 3ê°œ ëª¨ë¸ ë¹¼ê³  ê³¼ê±° ëª¨ë¸ì€ ì‚­ì œ(100000ë²ˆì§¸ë§ˆë‹¤ ëª¨ë¸ ì €ì¥í•˜ëŠ”ë°, ë§ˆì§€ê° 3ê°œ ë¹¼ê³  ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œ)\n",
    "\n",
    "# NSP ê´€ë ¨ ë³€ìˆ˜\n",
    "NSP_block_size = 140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52beea6d-afae-4957-a66c-a2ac14ef443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu ê°œìˆ˜: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed ì„¤ì •\n",
    "seed_everything(111)\n",
    "\n",
    "#logging ì„¤ì •\n",
    "logger = mlogging(loggername=\"senfpt\", logfilname=\"senfpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d41636d-fcdc-4623-ad81-d9e8c96cb886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentencepiece_tokenizer = SentencePieceBPETokenizer(add_prefix_space = True,)  #add_prefix_space = Trueì´ë©´ ë¬¸ì¥ ë§¨ ì• ë‹¨ì–´ì—ë„ ê³µë°° ë¶€ì—¬í•¨\n",
    "sentencepiece_tokenizer.train(files = input_corpus, \n",
    "                             vocab_size = 50,\n",
    "                             min_frequency = 1,\n",
    "                             )\n",
    "\n",
    "vocab = sentencepiece_tokenizer.get_vocab()\n",
    "sorted(vocab, key=lambda x: vocab[x])\n",
    "\n",
    "out_dir = 'Tokenizer'\n",
    "out_name = 'testsp'\n",
    "sentencepiece_tokenizer.save_model(out_dir, out_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b98f733-3ac3-4b70-867f-4be843100930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['â–', 'ì•ˆ', '<unk>', 'í•˜', 'ì„¸', 'ìš”', '.', '.', '<unk>', '<unk>', 'ìŠµ', 'ë‹ˆ', 'ë‹¤']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file = 'Tokenizer/testsp-vocab.json'\n",
    "#json_file = 'Tokenizer/kobert/kobert_news_wiki_ko_cased-ae5711deb3.spiece'\n",
    "\n",
    "merge_file = 'Tokenizer/testsp-merges.txt'\n",
    "\n",
    "spload = SentencePieceBPETokenizer(vocab = json_file, merges = merge_file)\n",
    "\n",
    "spload.encode('ì•ˆë…•í•˜ì„¸ìš”..ë°˜ê°‘ìŠµë‹ˆë‹¤').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59b2e228-a9bb-4e27-a4f0-5050eb43a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "num_special_tokens_to_add() got an unexpected keyword argument 'pair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_105522/864407601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input file path {file_path} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_special_tokens_to_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: num_special_tokens_to_add() got an unexpected keyword argument 'pair'"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(tokenizer=spload, file_path=input_corpus, block_size=128, overwrite_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a6e06-9602-45e9-80bb-8280564df1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_model_path:\n",
    "    # further pre-training ì¸ ê²½ìš° (ê¸°ì¡´ ê±°ì— ì¶”ê°€ì ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°)\n",
    "    config = BertConfig.from_pretrained(input_model_path)\n",
    "    model = AutoModelWithLMHead.from_pretrained(input_model_path,\n",
    "                                                from_tf=bool(\".ckpt\" in input_model_path),\n",
    "                                                config=config)\n",
    "else:\n",
    "    # Training new model from scratch ì¸ ê²½ìš° (ì™„ì „ ìƒˆë¡­ê²Œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²½ìš°)\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    \n",
    "model.num_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbb5e8-51dc-4f61-aa0d-e0efe4a6f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file, padding_token=\"[PAD]\")\n",
    "tokenizer = nlp.data.BERTSPTokenizer(vocab_file, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7915e-9489-4eeb-bfc0-f3d3a74213cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "file_path=\"my_data/ë¬¸ì„œì¤‘ì•™í™”.txt\"\n",
    "datalist = []\n",
    "\n",
    "# [bong] total_line ìˆ˜ë¥¼ êµ¬í•¨\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    total_line = len(f.readlines())\n",
    "    # [bong] ì´ ë¼ì¸ìˆ˜ ì¶œë ¥\n",
    "    print('*total_line: {}'.format(total_line))\n",
    "    \n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for idx in tqdm(range(total_line)): \n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            # [bong] ì´ ë¼ì¸ìˆ˜ ì½ê¸° ë\n",
    "            print('*readline=>count: {} End!'.format(scount))\n",
    "            break\n",
    "        line = line.strip() # í•„ìˆ˜!!!!\n",
    "        datalist.append([line]) #2ì°¨ì›ìœ¼ë¡œ ë¬¶ì–´ì•¼ í•¨\n",
    "        \n",
    "print(datalist[0:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83cfe5b-e053-47ce-bcae-2329456281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len, pad, pair, Masktokenids=0):\n",
    "        \n",
    "        self.mydatalist = []\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "       \n",
    "        #self.sentences = [transform([i[sent_idx]]) for i in dataset]  #ê¸°ì¡´ ì½”ë“œ\n",
    "        for i in dataset:\n",
    "            # transformedat ë¦¬í„´ê°’ ìœ¼ë¡œëŠ” token_ids, valid_length, token_type_id(segments_ids) 3ê°œê°€ ë¦¬í„´ë¨\n",
    "            transformdata = transform([i[sent_idx]]) \n",
    "            #print(transformdata[0])\n",
    "            \n",
    "            # tokeni_ids, token_type_id, attentison_mask, label ë“±ì„ ì–»ì–´ì˜¤ê³ , tensorë¡œ ë³€í™˜\n",
    "            token_ids = transformdata[0]\n",
    "            valid_length = transformdata[1]\n",
    "            token_type_id = transformdata[2]\n",
    "            labels = copy.deepcopy(transformdata[0]) # *[bong] labelë¥¼ token_ids ê°’ì„ deepcopyí•˜ì—¬, ìƒì„±\n",
    "            \n",
    "            # token_ids ì— ëŒ€í•´ MLM ìƒì„±\n",
    "            if Masktokenids != 0:\n",
    "                \n",
    "                #token_idsì— 15% í™•ë¥ ë¡œ [MASK] ë¶™ì„  \n",
    "                rand = torch.rand(token_ids.shape)\n",
    "                # 0,1,2,3 ì€ ê°ê° íŠ¹ìˆ˜ í† í°ì´ë¯€ë¡œ, íŠ¹ìˆ˜ í† í°ì—ëŠ” [MASK]ë¥¼ í•˜ì§€ ì•ŠìŒ\n",
    "                mask_arr = (rand < 0.15)*(token_ids != 0)*(token_ids != 1)*(token_ids != 2)*(token_ids != 3)\n",
    "                #print(mask_arr)\n",
    "\n",
    "                # MASK ë¶™ì¼ ìœ„ì¹˜ë¥¼ ë°°ì—´ë¡œ ë³€í™˜\n",
    "                selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "                #print('MASK Position: {}'.format(selection))\n",
    "\n",
    "                # [MASK] í† í°(4) ìœ¼ë¡œ ë³€ê²½\n",
    "                token_ids[selection] = Masktokenids\n",
    "            \n",
    "              \n",
    "            token_ids_tensor = torch.Tensor([token_ids])\n",
    "            token_type_id_tensor = torch.Tensor([token_type_id])\n",
    "            labels_tensor = torch.Tensor([labels])\n",
    "            attention_mask_tensor = self.gen_attention_mask(token_ids_tensor, [valid_length])# attention_mask ì„¤ì •\n",
    "            \n",
    "           \n",
    "            # dict ë¡œ ë§Œë“¬.\n",
    "            mydict = {'inputs_ids':token_ids_tensor, 'token_type_ids':token_type_id_tensor, 'attention_mask':attention_mask_tensor, 'labels':labels_tensor} \n",
    "            #mydict = {'inputs_ids':token_ids_tensor, 'token_type_ids':token_type_id_tensor, 'attention_mask':attention_mask_tensor} \n",
    "            # listì— ì¶”ê°€ \n",
    "            self.mydatalist.append(mydict)\n",
    "            \n",
    "    # attenton_mask ìƒì„± í•¨ìˆ˜\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "              attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.mydatalist[i] )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.mydatalist))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea507c27-8b33-4efb-9696-d3206ecaf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenidsDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len, pad, pair, Masktokenids=0):\n",
    "        \n",
    "        self.mydatalist = []\n",
    "        \n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "       \n",
    "        #self.sentences = [transform([i[sent_idx]]) for i in dataset]  #ê¸°ì¡´ ì½”ë“œ\n",
    "        for i in dataset:\n",
    "            # transformedat ë¦¬í„´ê°’ ìœ¼ë¡œëŠ” token_ids, valid_length, token_type_id(segments_ids) 3ê°œê°€ ë¦¬í„´ë¨\n",
    "            transformdata = transform([i[sent_idx]]) \n",
    "            #print(transformdata[0])\n",
    "            \n",
    "            # tokeni_ids, token_type_id, attentison_mask, label ë“±ì„ ì–»ì–´ì˜¤ê³ , tensorë¡œ ë³€í™˜\n",
    "            token_ids = transformdata[0]\n",
    "            labels = copy.deepcopy(transformdata[0]) # *[bong] labelë¥¼ token_ids ê°’ì„ deepcopyí•˜ì—¬, ìƒì„±   \n",
    "              \n",
    "            # token_ids ì— ëŒ€í•´ MLM ìƒì„±\n",
    "            if Masktokenids != 0:\n",
    "                \n",
    "                #token_idsì— 15% í™•ë¥ ë¡œ [MASK] ë¶™ì„  \n",
    "                rand = torch.rand(token_ids.shape)\n",
    "                # 0,1,2,3 ì€ ê°ê° íŠ¹ìˆ˜ í† í°ì´ë¯€ë¡œ, íŠ¹ìˆ˜ í† í°ì—ëŠ” [MASK]ë¥¼ í•˜ì§€ ì•ŠìŒ\n",
    "                mask_arr = (rand < 0.15)*(token_ids != 0)*(token_ids != 1)*(token_ids != 2)*(token_ids != 3)\n",
    "                #print(mask_arr)\n",
    "\n",
    "                # MASK ë¶™ì¼ ìœ„ì¹˜ë¥¼ ë°°ì—´ë¡œ ë³€í™˜\n",
    "                selection = torch.flatten((mask_arr).nonzero()).tolist()\n",
    "                #print('MASK Position: {}'.format(selection))\n",
    "\n",
    "                # [MASK] í† í°(4) ìœ¼ë¡œ ë³€ê²½\n",
    "                token_ids[selection] = Masktokenids\n",
    "                \n",
    "            token_ids_tensor = torch.Tensor([token_ids])\n",
    "            labels_tensor = torch.Tensor([labels])\n",
    "                \n",
    "            # dict ë¡œ ë§Œë“¬.\n",
    "            mydict = {'inputs_ids':token_ids_tensor, 'labels':labels_tensor} \n",
    "            # listì— ì¶”ê°€ \n",
    "            self.mydatalist.append(mydict)\n",
    "            \n",
    "    # attenton_mask ìƒì„± í•¨ìˆ˜\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "              attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.mydatalist[i] )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.mydatalist))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc899f-76ca-4a3f-b996-3665873a9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=128\n",
    "datalen = len(datalist)\n",
    "#print(datalen)\n",
    "\n",
    "transform_data = MyTokenidsDataset(datalist[0:datalen], 0, tokenizer, max_len, True, False, 3) # ë§¨ë’¤ì— 3ëŠ” [MASK]í† í° idì„\n",
    "#print(len(transform_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6dc44-b72c-4a4a-a072-0a7b3e06ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = MyTokenidsDataset(datalist[0:datalen], 0, tokenizer, max_len, True, False) # ë§¨ë’¤ì— 3ëŠ” [MASK]í† í° idì„\n",
    "\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=input_corpus, block_size=128, overwrite_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709fc04-2426-4a24-b5de-1adc4210f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import copy\n",
    "for i in range(len(transform_data)):\n",
    "    token_ids = transform_data[i][0]\n",
    "    print(token_ids)\n",
    "    label = copy.deepcopy(token_ids)\n",
    "#    valid_length = transform_data[i][1]\n",
    "#    segment_ids = transform_data[i][2]\n",
    "    #label = trainsform_data[i][0]\n",
    "    #print(token_ids)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072a37f-c08e-4b80-a69c-d7cf50b788c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transform_data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a9e29-6786-470d-ad2f-02c1695d5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ouput_model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=train_epochs,\n",
    "    per_gpu_train_batch_size=batch_size,\n",
    "    save_steps=save_steps,    # step ìˆ˜ë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥\n",
    "    save_total_limit=save_total_limit, # ë§ˆì§€ë§‰ ë‘ ëª¨ë¸ ë¹¼ê³  ê³¼ê±° ëª¨ë¸ì€ ì‚­ì œ\n",
    "    logging_steps=logging_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=transform_data,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset   #NSP(Next Setence Predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d600a0-652d-44a2-b42d-0f83e5f87ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafb572-37aa-4d0f-8c95-d58f5aa8168d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
