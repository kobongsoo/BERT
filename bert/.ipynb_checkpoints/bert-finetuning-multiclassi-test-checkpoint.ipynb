{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b46ae84-209a-4a37-9ae5-a1669b3e81ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bertftmultitest_2022-03-02.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "#from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "#from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "\n",
    "logger = mlogging(loggername=\"bertfttest\", logfilname=\"bertftmultitest\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b88b2a-ef33-4750-a728-0494f9b85047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model/classification/kobertmodel-ft-nsmc-cfmodel/pytorch_model.bin'\n",
    "vocab_path = 'model/classification/kobertmodel-ft-nsmc-cfmodel/vocab/vocab.txt'\n",
    "\n",
    "\n",
    "# strip_accents=False : True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "# do_lower_case=False : # 소문자 입력 사용 안함(한국어에서는 반드시 False)\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_path, strip_accents=False, do_lower_case=False) \n",
    "\n",
    "model = torch.load(model_path) # 기존 모델 파일을 로딩하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bf1198-56b9-463e-b6e6-cedf0abfa709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 09:48:07,387 - bwpdataset - INFO - Creating features from dataset file at Korpora/nsmc/ratings_test.txt\n",
      "2022-02-25 09:48:07,388 - bwpdataset - INFO - loading data... LOOKING AT Korpora/nsmc/ratings_test.txt\n",
      "2022-02-25 09:48:07,388 - bwpdataset - INFO - tsv file open\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-02-25.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 09:48:07,631 - bwpdataset - INFO - tokenize sentences, it could take a lot of time...\n",
      "2022-02-25 09:48:15,614 - bwpdataset - INFO - tokenize sentences [took 7.982 s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4011650d9049f283075f8bd143733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 09:48:15,726 - bwpdataset - INFO - *** Example ***\n",
      "2022-02-25 09:48:15,727 - bwpdataset - INFO - sentence: 굳 ㅋ\n",
      "2022-02-25 09:48:15,728 - bwpdataset - INFO - tokens: [CLS] [UNK] [UNK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-02-25 09:48:15,728 - bwpdataset - INFO - label: 1\n",
      "2022-02-25 09:48:15,729 - bwpdataset - INFO - features: ClassificationFeatures(input_ids=[2, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "2022-02-25 09:48:15,731 - bwpdataset - INFO - Saving features into cached file, it could take a lot of time...\n",
      "2022-02-25 09:48:19,199 - bwpdataset - INFO - Saving features into cached file Korpora/nsmc/cached_BertTokenizer_128_ratings_test.txt [took 3.468 s]\n"
     ]
    }
   ],
   "source": [
    "# eval data loader 생성\n",
    "\n",
    "from bwpdataset import ClassificationCSVCorpus, ClassificationDataset, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = 256   # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "batch_size = 32        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "\n",
    "# 훈련할 csv 파일\n",
    "#file_fpath = 'Korpora/감성대화말뭉치/감성대화말뭉치(최종데이터)_renew_labelenc_Validation.csv'\n",
    "file_fpath = 'Korpora/nsmc/ratings_test.txt'\n",
    "column_num = 3           # .csv 파일에 컬럼수(예: text, label만 있으면 =2)\n",
    "csvfile = 0              # 0:tsv 파일, 1: csv 파일\n",
    "label_list = [\"0\",\"1\"]  # .csv 파일에 레벨 목록( list로 입력해야 함)\n",
    "cache = False   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 로딩\n",
    "corpus = ClassificationCSVCorpus(column_num=column_num, iscsvfile=csvfile, label_list=label_list)\n",
    "\n",
    "# 학습 dataset 생성\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "eval_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb220491-c79b-4284-ba20-af32fc8df217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdfbef9e91e49809f48af1bd8c8f89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136728/4231343293.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-02-25 09:49:13,140 - bertfttest - INFO - eval-accuracy: 0.72976\n"
     ]
    }
   ],
   "source": [
    "# 평가 시작\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "for data in tqdm(eval_loader):\n",
    "     # 입력 값 설정\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    token_type_ids = data['token_type_ids'].to(device)       \n",
    "    labels = data['labels'].to(device)\n",
    " \n",
    "    # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "    # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "    # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "    with torch.no_grad():\n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=None,\n",
    "                       labels=labels)\n",
    "    \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "\n",
    "logger.info(f\"eval-accuracy: {total_correct / total_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79fbbbc-53f2-4a94-9ddd-76ecbc4f17a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
