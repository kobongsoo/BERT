{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2820d55-a439-416f-8287-1fcc660ddc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서는 기존 사전훈련된 bert 모델에 추가적으로 MLM 학습 시키는 과정을 설명한다.\n",
    "# => 사전은 기존 사전에 단어가 추가된 사전을 이용한다.(추가된 사전은 별도로 만들어야 함)\n",
    "#\n",
    "# 참고 사이트 \n",
    "# https://www.fatalerrors.org/a/further-pre-training-of-chinese-language-model-bert-roberta.html\n",
    "# 소스는 : https://github.com/zhusleep/pytorch_chinese_lm_pretrain 에 run_language_model_bert.py 참조함\n",
    "# MLM 다른 소스는 : https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c 참조 바람\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from myutils import seed_everything, TextDatasetForNextSentencePrediction, GPU_info, mlogging\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, BertConfig, AutoModelWithLMHead, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d6b3ba-8875-44e2-9fec-1125b6d01de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수들 설정\n",
    "\n",
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "input_corpus = \"Korpora/kowikitext/kowikitext_20200920.train\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "input_model_path = \"model/bert-multilingual-cased/\"\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_file=\"Tokenizer/kowikitext_20200920.train_0216_false_speical/bert-multiligual-cased_add_kowikitext_20200920.train_0216_false.txt\"\n",
    "# 출력 모델 저장 경로\n",
    "ouput_model_dir = \"model/bmc_fpt_kowiki20200920.train_model_epoch10_226\"\n",
    "\n",
    "# 토큰활 할때 최대 길이 \n",
    "token_max_len = 130\n",
    "\n",
    "# 훈련용 변수\n",
    "batch_size = 32   # 64 하면, GPU Out of memory 발생함(=>**따라서 32 진행)\n",
    "train_epochs = 10\n",
    " # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정(기본:512) 512, 1024, 2048 식으로 지정함, 엄청난 장문을 다룰경우 10124까지\n",
    "#max_position_embeddings = 128 \n",
    "logging_steps = 10000  # 훈련시, 로깅할 step 수 (크면 10000번 정도하고, 작으면 100번정도)\n",
    "save_steps = 50000     # 10000 step마다 모델 저장\n",
    "save_total_limit = 2 # 마지막 3개 모델 빼고 과거 모델은 삭제(100000번째마다 모델 저장하는데, 마지감 3개 빼고 나머지는 삭제)\n",
    "\n",
    "# NSP 관련 변수 (*여기서는 필요없음)\n",
    "#NSP_block_size = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1c681-0342-4212-8745-099ad9457966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertfpt_2022-02-26.log\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertfpt\", logfilname=\"bertfpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6914a0a9-1cd0-4fb6-9c55-8ce7c236ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_token_size: 5, tokenizer.vocab_size: 143772\n",
      "vocab_size: 143773\n",
      "tokenizer_len: 143772\n"
     ]
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_file, \n",
    "                          max_len=token_max_len, \n",
    "                          do_lower_case=False)\n",
    "'''\n",
    "#tokenizer = BertTokenizerFast(vocab_speical_path)\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file=vocab_file,\n",
    "    max_len=token_max_len,\n",
    "    do_lower_case=False,\n",
    "    )\n",
    "'''\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('special_token_size: {}, tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('vocab_size: {}'.format(vocab_size))\n",
    "print('tokenizer_len: {}'.format(len(tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0145f5f5-119b-4042-97da-0ca91b58e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:783: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at model/bert-multilingual-cased/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "further pre-training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(143772, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=143772, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "#output_hidden_states = False # 기본은 False=>output 2개 출력됨, True로 지정하면 output이 3개 출력됨\n",
    "#return_dict = True   #False로 지정하는 경우 일반적인 tuple을 리턴, True인 경우는 transformers.file_utils.ModelOutput(ouput.logisc) 으로 리턴\n",
    "#model = BertModel.from_pretrained(input_model_path, output_hidden_states = output_hidden_states, return_dict = return_dict)\n",
    "\n",
    "# AutoModelWithLMHead 대신에 -> AutoModelForMaskedLM, BertForMaskedLM 해도됨\n",
    "#=>원래 AutoModelWithLMHead 하면 내부적으로 BertForMaskedLM 가 로딩됨\n",
    "\n",
    "if input_model_path:\n",
    "    # further pre-training 인 경우 (기존 거에 추가적으로 하는 경우)\n",
    "    config = BertConfig.from_pretrained(input_model_path)\n",
    "    \n",
    "    model = AutoModelWithLMHead.from_pretrained(input_model_path,\n",
    "                                                from_tf=bool(\".ckpt\" in input_model_path),\n",
    "                                                config=config) \n",
    "    print('further pre-training')\n",
    "else:\n",
    "    # Training new model from scratch 인 경우 (완전 새롭게 모델을 만드는 경우)\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    print('Training new model from scratch')\n",
    " \n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 훈련모드로 변경(평가모드 : model.eval())\n",
    "model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48949c94-965c-4192-b7b8-0db1f28c744b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196603548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a9d7d0-46dc-449e-b220-32c9666d4cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 16:09:05,287 - bwpdataset - INFO - Creating features from dataset file at Korpora/kowikitext\n",
      "2022-02-26 16:09:05,288 - bwpdataset - INFO - ==>[Start] file read: Korpora/kowikitext/kowikitext_20200920.train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-02-26.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 16:09:16,855 - bwpdataset - INFO - <==[End] file read: Korpora/kowikitext/kowikitext_20200920.train\n",
      "2022-02-26 16:09:16,858 - bwpdataset - INFO - ==>[Start] tokenizer convert_tokens_to_ids..wait max 30minute...\n",
      "2022-02-26 16:47:15,487 - bwpdataset - INFO - <==[End] tokenizer convert_tokens_to_ids\n",
      "2022-02-26 16:47:15,496 - bwpdataset - INFO - ==>[Start] tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4343d0fcc14c19a34d00936d983043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2957673 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 16:48:14,074 - bwpdataset - INFO - ==>[End] tokenizer\n",
      "2022-02-26 16:48:14,077 - bwpdataset - INFO - ==>[Start] cached file create: Korpora/kowikitext/cached_lm_BertTokenizer_128_kowikitext_20200920.train\n",
      "2022-02-26 16:48:23,380 - bwpdataset - INFO - <==[End] Saving features into cached file Korpora/kowikitext/cached_lm_BertTokenizer_128_kowikitext_20200920.train [took 9.303 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bwpdataset.MyTextDataset object at 0x7fdc59f812e0>\n",
      "[['[CLS]', '=', '분류', ':', '중화', '##인', '##민', '##공화국', '##의', '외교부', '##장', '=', '외교부', '##장', '외교부', '##장', '=', '분류', ':', '헝가리', '##의', '공원', '=', '공원', '공원', '=', '김세', '##권', '=', '김세', '##권', '(', '1931', '##년', '~', ',', '金', '世', '權', ')', '은', '제', '##16', '##대', '서울', '##고', '##등', '##검', '##찰청', '검사장', '##을', '역임', '##한', '법조인', '##이다', '.', '=', '=', '생애', '=', '=', '1931', '##년', '서울시', '##에서', '태어나', '경기', '##중', '##학교', ',', '1982년', '4월', '12일', '##자', '매일', '##경제', '서울', '##고', '##등학교', ',', '1981년', '4월', '25일', '##자', '동아일보', '1956', '##년', '서울대', '##학교', '법학', '##과', '##를', '나온', '후', '1956', '##년', '제', '##8', '##회', '고등', '##고시', '사법', '##과', '##에서', '합격', '##하였다', '.', '1958', '##년', '서울', '##지', '##방', '##검', '##찰청', '검사', '##에', '임용', '##되었다', '.', '김세', '##권', '##은', '두산', '##그룹', '창업', '##주', '##인', '박', '##두', '[SEP]']]\n",
      "count:0=>tensor([   101,    134, 119564,    131, 120053,  12030,  36553, 100084,  10459,\n",
      "        131634,  13890,    134, 131634,  13890, 131634,  13890,    134, 119564,\n",
      "           131, 121170,  10459, 120234,    134, 120234, 120234,    134, 127976,\n",
      "         25347,    134, 127976,  25347,    113,  11383,  10954,    198,    117,\n",
      "          7911,   2087,   4769,    114,   9632,   9672,  37301,  14423,  48253,\n",
      "         11664, 101322, 118625, 137233, 134176,  10622, 120478,  11102, 131092,\n",
      "         11925,    119,    134,    134, 120045,    134,    134,  11383,  10954,\n",
      "        123732,  11489, 122341,  86015,  41693,  46599,    117,  98063,  16464,\n",
      "         46026,  13764, 120169, 133540,  48253,  11664,  55511,    117,  96318,\n",
      "         16464,  43283,  13764, 110388,  10975,  10954, 124676,  46599, 122630,\n",
      "         11882,  11513, 120396,  10003,  10975,  10954,   9672,  11396,  14863,\n",
      "        119634, 134099, 121650,  11882,  11489, 121931,  12609,    119,  10947,\n",
      "         10954,  48253,  12508,  42337, 118625, 137233, 120095,  10530, 127094,\n",
      "         13628,    119, 127976,  25347,  10892, 122105, 126768, 124986,  16323,\n",
      "         12030,   9319, 118802,    102])\n",
      "[['[CLS]', '##병', '딸', '##인', '박용', '##언', '##과', '결혼', '##했다', '.', '김세', '##권', '##과', '박용', '##언', '##은', '아들', '##은', '1970', '##년대', '봉', '##제', '##업', '##으로', '성장', '##한', '태', '##흥', '##의', '창업', '##주', '권태', '##흥', '##의', '딸', '권', '##혜', '##경', '##과', '결혼', '##한', '김형', '##일', '일', '##경', '##산업', '##개발', '부회장', '##으로', '1990년', '##대', '초반', '대한민국', '##에', '게', '##스', '·', '폴로', '등을', '수입', '##해', '유명세', '##를', '탔', '##던', '기업가', '##다', '.', '딸', '김희', '##정', '##의', '남편', '##은', '최원', '##현', '케이', '##씨', '##엘', '대표', '##변', '##호사', '##다', '.', '박승', '##직', '박', '##두', '##병', '박용', '##곤', '박정', '##원', '재벌', '##가', '4', '##대', '33', '##명', '결혼', '스토리', '대검찰청', '차장', '##으로', '재직', '##하던', '1986년', '2월', '전국', '검사장', '회의', '##를', '주재', '##하면서', '\"', '국', '##법', '##질', '##서', '확립', '##을', '위한', '검찰', '##의', '과제', '\"', '라는', '주제', '##로', '[SEP]']]\n",
      "count:1=>tensor([   101,  73380,   9133,  12030, 126300,  48036,  11882, 119920,  12490,\n",
      "           119, 127976,  25347,  11882, 126300,  48036,  10892,  54991,  10892,\n",
      "         10607,  86181,   9362,  17730,  26784,  11467, 120310,  11102,   9854,\n",
      "        119466,  10459, 124986,  16323, 140576, 119466,  10459,   9133,   8917,\n",
      "        119437,  31720,  11882, 119920,  11102, 124551,  18392,   9641,  31720,\n",
      "        133479, 135295, 124625,  11467,  71767,  14423, 121434,  26168,  10530,\n",
      "          8872,  12605,    217, 134316,  33727, 121341,  14523, 139616,  11513,\n",
      "          9852,  23990, 132379,  11903,    119,   9133, 124585,  16605,  10459,\n",
      "        120876,  10892, 133904,  30842, 120165,  49212,  96720, 119597, 118985,\n",
      "        120609,  11903,    119, 129897,  33077,   9319, 118802,  73380, 126300,\n",
      "        118639, 120862,  14279, 129954,  11287,    125,  14423,  11000,  16758,\n",
      "        119920, 121214, 137320, 123262,  11467, 121707,  76424,  85485,  17520,\n",
      "        119863, 134176, 119999,  11513, 123678,  37341,    107,   8909,  33768,\n",
      "         48599,  12424, 123234,  10622,  28195, 120906,  10459, 124099,    107,\n",
      "         33292, 120147,  11261,    102])\n",
      "[['[CLS]', '토의', '##를', '하여', '국', '##법', '##질', '##서', '##와', '사회', '##기', '##강', '확립', ',', '경제', '##도', '##약', '##의', '전기', '##가', '될', '수', '있는', '현재의', '국내외', '경제', '##여', '##건', '##을', '최대한', '유지', ',', '활용', '##할', '수', '있도록', '경제', '##질', '##서', '##교', '##란', '##사', '##범', '엄', '##단', ',', '민생', '##을', '불안', '##하게', '하는', '반', '##윤', '##리', '##적인', '강력', '##사', '##범', '##에', '대한', '단호', '##한', '응', '##징', ',', '사회', '##적', '신분', '##과', '지위', '##의', '고하', '##를', '가리', '##지', '않는', '엄정', '공평', '##한', '법', '##집', '##행을', '1986년', '##도', '검찰', '##권', '행사', '##의', '방향', '##으로', '정했', '##다', '.', '1986년', '2월', '25일', '##자', '동아일보', '=', '=', '경력', '=', '=', '1958', '##년', '~', '1959', '##년', '서울', '##지', '##방', '##검', '##찰청', '검사', '1968', '##년', '법무부', '출입국', '##관리', '##과', '##장', '겸', '서울', '##고', '##등', '##검', '##찰청', '검사', '1970', '[SEP]']]\n",
      "count:2=>tensor([   101, 133163,  11513,  51076,   8909,  33768,  48599,  12424,  12638,\n",
      "        119600,  12310,  47181, 123234,    117, 119641,  12092,  47289,  10459,\n",
      "        119942,  11287,   9100,   9460,  13767,  79163, 127821, 119641,  29935,\n",
      "         71439,  10622, 125564, 119819,    117, 120691,  14843,   9460, 107931,\n",
      "        119641,  48599,  12424,  25242,  49919,  12945, 108056,   9553,  24989,\n",
      "           117, 135762,  10622, 123512,  17594,  23969,   9321, 119182,  12692,\n",
      "         15387, 120587,  12945, 108056,  10530,  18154, 136656,  11102,   9636,\n",
      "        119233,    117, 119600,  14801, 122079,  11882, 121209,  10459, 135618,\n",
      "         11513, 119947,  12508,  55698, 133044, 134273,  11102,   9341,  38696,\n",
      "         88904,  85485,  12092, 120906,  25347, 120243,  10459, 120037,  11467,\n",
      "        132446,  11903,    119,  85485,  17520,  43283,  13764, 110388,    134,\n",
      "           134, 119839,    134,    134,  10947,  10954,    198,  10980,  10954,\n",
      "         48253,  12508,  42337, 118625, 137233, 120095,  10698,  10954, 125904,\n",
      "        137558, 125776,  11882,  13890,   8882,  48253,  11664, 101322, 118625,\n",
      "        137233, 120095,  10607,    102])\n",
      "[['[CLS]', '##년', '~', '1971', '##년대', '##검', '##찰청', '수사', '##국', '제', '##3', '##과', '##장', '1971', '##년', '서울', '##지', '##방', '##검', '##찰청', '동부', '##지', '##청', '부장', '##검', '##사', '1975', '##년', '서울', '##고', '##등', '##검', '##찰청', '검사', '1977', '##년', '대전', '##지', '##방', '##검', '##찰청', '차장', '##검', '##사', '1978', '##년', '2월', '11일', '~', '1980년', '6월', '8일', '제', '##3', '##대', '서울', '##지', '##방', '##검', '##찰청', '성북', '##지', '##청', '지청', '##장', '##장', '1981년', '4월', '27일', '~', '1981년', '12월', '16일', '제', '##25', '##대', '대전', '##지', '##방', '##검', '##찰청', '검사장', '1981년', '법무부', '기획', '##관리', '##실', '##장', '1982년', '6월', '18일', '~', '1985년', '5월', '24일', '제', '##15', '##대', '광주', '##고', '##등', '##검', '##찰청', '검사장', '1985년', '5월', '25일', '~', '1986년', '4월', '29일', '대검찰청', '차장', '##검', '##사', '1986년', '5월', '2일', '~', '1987년', '6월', '7일', '제', '##16', '##대', '서울', '##고', '##등', '##검', '[SEP]']]\n",
      "count:3=>tensor([   101,  10954,    198,  10732,  86181, 118625, 137233, 120345,  20479,\n",
      "          9672,  10884,  11882,  13890,  10732,  10954,  48253,  12508,  42337,\n",
      "        118625, 137233, 120571,  12508,  40311, 120874, 118625,  12945,  10665,\n",
      "         10954,  48253,  11664, 101322, 118625, 137233, 120095,  10722,  10954,\n",
      "        103988,  12508,  42337, 118625, 137233, 123262, 118625,  12945,  10693,\n",
      "         10954,  17520,  46986,    198,  97748,  17253,  46265,   9672,  10884,\n",
      "         14423,  48253,  12508,  42337, 118625, 137233, 128885,  12508,  40311,\n",
      "        125361,  13890,  13890,  96318,  16464,  44514,    198,  96318,  16367,\n",
      "         44978,   9672,  69168,  14423, 103988,  12508,  42337, 118625, 137233,\n",
      "        134176,  96318, 125904, 119938, 125776,  31503,  13890,  98063,  17253,\n",
      "         45972,    198,  90584,  17162,  47251,   9672,  37462,  14423, 120008,\n",
      "         11664, 101322, 118625, 137233, 134176,  90584,  17162,  43283,    198,\n",
      "         85485,  16464,  45670, 137320, 123262, 118625,  12945,  85485,  17162,\n",
      "         39788,    198,  69610,  17253,  46659,   9672,  37301,  14423,  48253,\n",
      "         11664, 101322, 118625,    102])\n",
      "[['[CLS]', '##찰청', '검사장', '1987년', '8월', '서울시', '강남구', '변호사', '개업', '1988년', '~', '2001년', '재단법인', '연', '##강', '##재단', '이사장', '1994년', '~', '2008년', '법무법인', '케이', '##씨', '##엘', '대표', '변호사', '1995년', '~', '1998년', '한국', '##형', '##사', '##정책', '##연구원', '이사장', '2008년', '~', '법무법인', '케이', '##씨', '##엘', '고문', '변호사', '=', '=', '각주', '=', '=', '=', '소방', '##차', '(', '음악', '그룹', ')', '=', '소방', '##차', '(', '消', '防', '車', ')', '는', '대한민국의', '남성', '3', '##인', '##조', '댄스', '팝', '음악', '그룹', '##이다', '.', '=', '=', '개요', '=', '=', '김태', '##형', ',', '정원', '##관', ',', '이상', '##원으로', '구성', '##되', '##었', '##었다', '.', '소방', '##차', '##는', '현재', '한국', '아이돌', '그룹', '##의', '원형', '##으로', '평가', '##받', '##고', '있으며', '후에', '아이돌', '그룹', '##에', '많은', '영향을', '끼쳤', '##다고', '평가', '##받', '##고', '있다', '.', '원래', '그룹', '이름은', '사시', '##사', '##철', '영원', '##한', '인기', '[SEP]']]\n",
      "count:4=>tensor([   101, 137233, 134176,  69610,  17289, 123732, 127464, 121005, 122037,\n",
      "         75991,    198,  42299, 129188,   9568,  47181, 132768, 122294,  60762,\n",
      "           198,  20656, 134723, 120165,  49212,  96720, 119597, 121005,  54735,\n",
      "           198,  48983,  48556,  27506,  12945, 139315, 138256, 122294,  20656,\n",
      "           198, 134723, 120165,  49212,  96720, 121172, 121005,    134,    134,\n",
      "        119588,    134,    134,    134, 121112,  23466,    113,  74293, 119771,\n",
      "           114,    134, 121112,  23466,    113,   5010,   8198,   7596,    114,\n",
      "          9043,  27580, 120500,    124,  12030,  20626, 122774,   9906,  74293,\n",
      "        119771,  11925,    119,    134,    134, 120106,    134,    134, 121953,\n",
      "         27506,    117, 121078,  20595,    117,  66982,  78686, 119652, 118800,\n",
      "        119138,  17706,    119, 121112,  23466,  11018,  26565,  48556, 121632,\n",
      "        119771,  10459, 123536,  11467, 119791, 118965,  11664,  22634,  56528,\n",
      "        121632, 119771,  10530,  25685,  58088, 130331,  85634, 119791, 118965,\n",
      "         11664,  11506,    119,  83953, 119771,  78199, 133682,  12945,  47465,\n",
      "        122348,  11102, 120118,    102])\n",
      "[['[CLS]', '##를', '누리', '##라는', '뜻', '##의', '‘', '코스모스', '위에', '나비', '앉', '##았', '##네', '’', '인데', '팀', '##명이', '너무', '길', '##다는', '이유로', '밤', '##무', '##대', '공연', '당시', '업소', '사장', '##이', '즉', '##석', '##에서', '지금', '##의', '그룹', '이름을', '지', '##었다', '.', '주요', '##곡', '##으로', '<', '그녀', '##에게', '전해', '##주', '##오', '>', ',', '<', '[UNK]', '이야기', '>', ',', '<', '통화', '##중', '>', ',', '<', '일', '##급', '비밀', '>', ',', '<', '하얀', '바람', '>', ',', '<', '사랑', '##하고', '싶', '##어', '>', ',', '<', 'G', '##카', '##페', '>', '등이', '있다', '.', '1987년', '《', '그녀', '##에게', '전해', '##주', '##오', '》', '로', '데뷔', '하였다', '.', '1988년', '이상', '##원이', '탈퇴', '##하였다', '.', '1988년', '도', '##건', '##우', '##를', '영입', '##하였다', '.', '1990년', '베스트', '##앨', '##범', '##을', '끝', '##으로', '해체', '##했다', '.', '1994년', '원년', '멤버', '##로', '재결', '##성', '후', '[SEP]']]\n",
      "count:5=>tensor([   101,  11513, 121905,  60362,   9153,  10459, 119549, 142288,  98212,\n",
      "        125070,   9522, 119118,  77884, 119550, 120233,   9899,  66923, 120444,\n",
      "          8934,  82034,  89184,   9326,  32537,  14423, 119909,  24756, 136383,\n",
      "        120712,  10739,   9701,  40958,  11489, 119789,  10459, 119771,  56426,\n",
      "          9706,  17706,    119,  55368,  55670,  11467,    133, 119763,  26212,\n",
      "        120869,  16323,  28188,    135,    117,    133,    100, 110148,    135,\n",
      "           117,    133, 123267,  41693,    135,    117,    133,   9641,  37568,\n",
      "        120415,    135,    117,    133, 124782, 120427,    135,    117,    133,\n",
      "        119653,  12453,   9495,  12965,    135,    117,    133,    144,  24206,\n",
      "        119391,    135,  36322,  11506,    119,  69610,   1888, 119763,  26212,\n",
      "        120869,  16323,  28188,   1889,   9202, 119850,  28750,    119,  75991,\n",
      "         66982,  73295, 122536,  12609,    119,  75991,   9087,  71439,  27355,\n",
      "         11513, 121711,  12609,    119,  71767, 120733, 119124, 108056,  10622,\n",
      "          8977,  11467, 121519,  12490,    119,  60762, 122155, 120031,  11261,\n",
      "        132913,  17138,  10003,    102])\n"
     ]
    }
   ],
   "source": [
    "# MLM(Markup Language Model), NSP(Next Sentence Prediction) 구성\n",
    "from transformers import DataCollatorForLanguageModeling, TextDataset\n",
    "from bwpdataset import MyTextDataset\n",
    "\n",
    "'''\n",
    "# NSP 만들기\n",
    "train_dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=input_corpus,\n",
    "    block_size=NSP_block_size,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.1,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "'''\n",
    "\n",
    "# further-pretrain 일때는 일단 NSP는 입력안함.\n",
    "# => 따라서 입력 corpus에 대해, NSP Dataset 이 아니라, TextDataset 으로 만듬\n",
    "train_dataset = MyTextDataset(tokenizer=tokenizer, file_path=input_corpus, block_size=token_max_len, overwrite_cache=True)\n",
    "\n",
    "# NSP 출력 해보기\n",
    "print(train_dataset)\n",
    "count = 0\n",
    "for example in train_dataset:\n",
    "    token_str = [[tokenizer.convert_ids_to_tokens(s) for s in example.tolist()]]\n",
    "    print(token_str)\n",
    "    print('count:{}=>{}'.format(count,example))\n",
    "    count +=1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c85fa2-1007-4766-8c50-6f2921059ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForLanguageModeling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_185222/2640319822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# MLM 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlm_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m '''\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForLanguageModeling' is not defined"
     ]
    }
   ],
   "source": [
    "# MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "'''\n",
    "# MLM 출력 해보기\n",
    "for example in train_dataset:\n",
    "    print(type(example))\n",
    "    mlm_sample = data_collator(example['input_ids'])\n",
    "    token_str = [[tokenizer.convert_ids_to_tokens(s) for s in mlm_sample['input_ids']]]\n",
    "    print(token_str)\n",
    "    print('count:{}=>{}'.format(count,mlm_sample))\n",
    "    count +=1\n",
    "    if count > 5:\n",
    "        break\n",
    "'''        \n",
    "# MLM 출력 해보기\n",
    "mlm_sample = data_collator(train_dataset.examples[2:3])\n",
    "print(type(mlm_sample))\n",
    "print(mlm_sample.keys())\n",
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in mlm_sample['input_ids']]]\n",
    "print(token_str)\n",
    "print(mlm_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69aa6438-60bc-429a-a3d9-bda2fd9bf4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ouput_model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=train_epochs,\n",
    "    per_gpu_train_batch_size=batch_size,\n",
    "    save_steps=save_steps,    # step 수마다 모델을 저장\n",
    "    save_total_limit=save_total_limit, # 마지막 두 모델 빼고 과거 모델은 삭제\n",
    "    logging_steps=logging_steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset   #TEXT 혹은 NSP(Next Setence Predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00887e4f-a3ba-4aef-a7c5-f89d998d36b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 2957673\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 924280\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258578' max='924280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258578/924280 18:17:25 < 47:05:18, 3.93 it/s, Epoch 2.80/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>2.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>2.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>1.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>1.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>1.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>1.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>1.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>1.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>1.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>1.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>1.847600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>1.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>1.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>1.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>1.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>1.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>1.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>1.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>1.765600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>1.759100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-50000\n",
      "Configuration saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-50000/config.json\n",
      "Model weights saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-50000/pytorch_model.bin\n",
      "Saving model checkpoint to model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-100000\n",
      "Configuration saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-100000/config.json\n",
      "Model weights saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-100000/pytorch_model.bin\n",
      "Saving model checkpoint to model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-150000\n",
      "Configuration saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-150000/config.json\n",
      "Model weights saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-150000/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-200000\n",
      "Configuration saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-200000/config.json\n",
      "Model weights saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-200000/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-100000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-250000\n",
      "Configuration saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-250000/config.json\n",
      "Model weights saved in model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-250000/pytorch_model.bin\n",
      "Deleting older checkpoint [model/bmc_fpt_kowiki20200920.train_model_epoch10_226/checkpoint-150000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938aa698-638b-46d5-8f88-4ae20050dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델 저장\n",
    "trainer.save_model(ouput_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafe127-4f99-4f7d-8931-ebe471fa9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeinizer 파일 저장(vocab)\n",
    "tokenizer.save_pretrained(ouput_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e17ed6-8b4a-4d76-969a-cd3a3dd0c415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
