{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1801c86a-1498-4a64-b290-e4a22d36bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-28.log\n",
      "logfilepath:qnadataset_2022-03-28.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertembedding_2022-03-28.log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gluonnlp as nlp     # GluonNLP는 버트를 간단하게 로딩하는 인터페이스를 제공하는 API 임\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# myutils 패키지 import\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import seed_everything, GPU_info, pytorch_cos_sim, mlogging\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertembedding\", logfilename=\"bertembedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21830db-fcdf-4726-9dda-a8eda1bf7949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "#cache_dir = '../model/distilbert-base-multilingual-cased' \n",
    "#model_vocab_path = '../model/distilbert-base-multilingual-cased'\n",
    "model_vocab_path = '../model/distilbert/distilbert-0327-TS'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_vocab_path, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383fcf52-4387-469f-86bf-66381a4d245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ../model/distilbert/distilbert-0327-TS were not used when initializing BertModel: ['distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/distilbert/distilbert-0327-TS and are newly initialized: ['encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(167550, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 불러옴\n",
    "model = BertModel.from_pretrained(model_vocab_path)\n",
    "model.eval()\n",
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb75bfc-7651-4fd2-bd62-d5a02593c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 16:59:46,573 - bertembedding - INFO - === model: ../model/distilbert/distilbert-0327-TS ===\n",
      "2022-03-28 16:59:46,578 - bertembedding - INFO - num_parameters: 214719744\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"=== model: {model_vocab_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2761f02-bee7-4aa1-9888-62a5455e2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101, 122278,  10892,   9379,  11287,   9580,  11664, 123665,  11287,\n",
      "          10015,  12692, 118632,  11903,    102,      0,      0,      0,      0],\n",
      "        [   101, 122278,  10892,   9034,  10739,   9580,  11664, 123665,  11287,\n",
      "          10015,  12692, 118632,  11903,    102,      0,      0,      0,      0],\n",
      "        [   101, 122278,  10892,   8843, 118707,  10015,  62211,    117,   9034,\n",
      "          10739,   9583,  15891,  11506,    102,      0,      0,      0,      0],\n",
      "        [   101, 119676, 123323,  10892, 136093,  11287,   9254,  76820,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 122278, 140417,  11018, 131258,  11467, 121653,  28750,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 129345,  10892, 120578,  15303,  10015, 154112,  19105,    117,\n",
      "         120233,  15303, 123665,  11287,   9685, 118632,  11903,    102,      0],\n",
      "        [   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119581,\n",
      "         119603, 120640,  11925,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 136591, 119603, 119803,  10892,    125,    110,   9069, 119803,\n",
      "          10622,   9638, 118891,  41521,  17342, 120364,  22096,    102,      0],\n",
      "        [   101,  47364, 120035,  11018, 126336,  21611, 122626,  20173, 120558,\n",
      "           9737,  11018, 121362, 120466,  11925,    102,      0,      0,      0],\n",
      "        [   101, 128051,  15303,  70672, 119674,  12638, 105383, 121057,  11287,\n",
      "          58248,  66421,  11903,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 122278, 123665,  11018,   9379,  11287, 123658,  11664,  42608,\n",
      "            100,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9450, 119444, 159780,  50266, 121456,  37905,  81785,  10193,\n",
      "          48506,   8892,  10622,   9010,  17706,    102,      0,      0,      0],\n",
      "        [   101, 120824,  11102, 123665,  10530, 134288,  10622, 126551, 108436,\n",
      "          16139,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 120569,  37115,  18398,  10530, 121752, 119643,  10622, 119850,\n",
      "          28750,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 138367,  10892, 128051, 126550, 133359,  11513, 132086,  12490,\n",
      "            102,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 123665,  11287,   9685,  11903,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9521,   8924,  37388,  12092,  54780,  28911,   9303,    119,\n",
      "           8904, 119250,  16985,    119,    102,      0,      0,      0,      0],\n",
      "        [   101, 129345, 121492,  18382, 120426, 120053,  11287,   8977,  41919,\n",
      "          23925, 120364,  33188,  48345,    119,    102,      0,      0,      0],\n",
      "        [   101, 121467,  33188,  48345,    119, 127364,  40364, 119583,  10739,\n",
      "           9100, 136592,    119,    102,      0,      0,      0,      0,      0],\n",
      "        [   101,   8924,  30873,  14867, 120578,  10150, 131192, 119994,  31398,\n",
      "           9952, 119217,    119, 129345,    100,    119,    102,      0,      0],\n",
      "        [   101, 139896, 134170,  11287,   9056,   9141,  12965, 119210, 119081,\n",
      "          48345,    119,   9074, 121962,  14843, 118671,  48549,    136,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [\n",
    "    '오늘은 비가 오고 날씨가 흐리겠다',\n",
    "    '오늘은 눈이 오고 날씨가 흐리겠다',\n",
    "    '오늘은 가끔 흐리고, 눈이 올수 있다',\n",
    "    '여기 식당은 파스타가 맛있다',\n",
    "    '오늘 증시는 내림으로 마감 하였다',\n",
    "    '내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다',\n",
    "    '서울은 대한민국에 수도이며, 정치 경제 중심지이다',\n",
    "    '내년 경제 성장은 4%대 성장을 이룰거라 예상된다',\n",
    "    '프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다',\n",
    "    '올해에는 대통령 선거와 지방선거가 동시에 열린다',\n",
    "    '오늘 날씨는 비가 내리고 매우 춥다',\n",
    "    '손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다',\n",
    "    '건조한 날씨에 산불을 조심해야 한다',\n",
    "    '윈도우11 OS에 검색 기능을 강화 하였다',\n",
    "    '한국은행은 올해 하반기 금리를 동결했다',\n",
    "    '날씨가 좋다',\n",
    "    '안 그래도 되는데 뭐. 괜찮아.',\n",
    "    '내일 저녁까지 보수 공사가 끝날 것으로 예상합니다.',\n",
    "    '감사합니다. 후회 없는 결정이 될 겁니다.',\n",
    "    '그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다.',\n",
    "    '프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?'\n",
    "    ]\n",
    "\n",
    "# ** 멀티로 한번에 tokenizer 할때는 반드시 padding=True 해야 함.(그래야 최대 길이 token에 맞춰서 padding 됨)\n",
    "test = tokenizer(test_sentence, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=256, \n",
    "                 return_tensors=\"pt\")\n",
    "print(test)\n",
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bee70d-7945-4f93-9865-c83a42aa86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 17:00:08,948 - bertembedding - INFO - === model 처리시간: 0.290 초 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 18, 768])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "output = model(**test)\n",
    "\n",
    "logger.info(f'=== model 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "\n",
    "# sequence_state\n",
    "last_hidden_state = output[0]\n",
    "print(last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a0f0b2-c3e5-48ea-8292-be0585221910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 17:00:10,409 - bertembedding - INFO - ---------------------------------------------------------\n",
      "2022-03-28 17:00:10,411 - bertembedding - INFO - 오늘은 비가 오고 날씨가 흐리겠다, 유사도:0.9999995231628418\n",
      "2022-03-28 17:00:10,412 - bertembedding - INFO - 오늘은 눈이 오고 날씨가 흐리겠다, 유사도:0.9349409937858582\n",
      "2022-03-28 17:00:10,413 - bertembedding - INFO - 손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다, 유사도:0.9272699952125549\n",
      "2022-03-28 17:00:10,413 - bertembedding - INFO - 한국은행은 올해 하반기 금리를 동결했다, 유사도:0.926433801651001\n",
      "2022-03-28 17:00:10,414 - bertembedding - INFO - 감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.9235895872116089\n",
      "2022-03-28 17:00:10,415 - bertembedding - INFO - 프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다, 유사도:0.9220457077026367\n",
      "2022-03-28 17:00:10,416 - bertembedding - INFO - 그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.9216639399528503\n",
      "2022-03-28 17:00:10,417 - bertembedding - INFO - 오늘은 가끔 흐리고, 눈이 올수 있다, 유사도:0.9180310368537903\n",
      "2022-03-28 17:00:10,417 - bertembedding - INFO - 오늘 날씨는 비가 내리고 매우 춥다, 유사도:0.9126017689704895\n",
      "2022-03-28 17:00:10,418 - bertembedding - INFO - 윈도우11 OS에 검색 기능을 강화 하였다, 유사도:0.9065737128257751\n",
      "2022-03-28 17:00:10,419 - bertembedding - INFO - 오늘 증시는 내림으로 마감 하였다, 유사도:0.9047356843948364\n",
      "2022-03-28 17:00:10,420 - bertembedding - INFO - 날씨가 좋다, 유사도:0.9026564359664917\n",
      "2022-03-28 17:00:10,420 - bertembedding - INFO - 올해에는 대통령 선거와 지방선거가 동시에 열린다, 유사도:0.9013746976852417\n",
      "2022-03-28 17:00:10,421 - bertembedding - INFO - 프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:0.8983842134475708\n",
      "2022-03-28 17:00:10,422 - bertembedding - INFO - 여기 식당은 파스타가 맛있다, 유사도:0.8950197696685791\n",
      "2022-03-28 17:00:10,423 - bertembedding - INFO - 안 그래도 되는데 뭐. 괜찮아., 유사도:0.8934358954429626\n",
      "2022-03-28 17:00:10,426 - bertembedding - INFO - 서울은 대한민국에 수도이며, 정치 경제 중심지이다, 유사도:0.8920432329177856\n",
      "2022-03-28 17:00:10,427 - bertembedding - INFO - 건조한 날씨에 산불을 조심해야 한다, 유사도:0.8852701783180237\n",
      "2022-03-28 17:00:10,428 - bertembedding - INFO - 내년 경제 성장은 4%대 성장을 이룰거라 예상된다, 유사도:0.8701287508010864\n",
      "2022-03-28 17:00:10,428 - bertembedding - INFO - 내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다, 유사도:0.867485523223877\n",
      "2022-03-28 17:00:10,429 - bertembedding - INFO - 내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:0.866547167301178\n",
      "2022-03-28 17:00:10,430 - bertembedding - INFO - ---------------------------------------------------------\n",
      "2022-03-28 17:00:10,431 - bertembedding - INFO - === 유사도 처리시간: 0.028 초 ===\n",
      "2022-03-28 17:00:10,432 - bertembedding - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = torch.mean(last_hidden_state[0], dim=0)\n",
    "#print(in_mean_sequence.shape)\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(last_hidden_state):\n",
    "    out_mean_sequence = torch.mean(hidden, dim=0)\n",
    "    simul_score = pytorch_cos_sim(in_mean_sequence, out_mean_sequence)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    #print(test_sentence[index-1])\n",
    "    logger.info(f'{test_sentence[index-1]}, 유사도:{value[0][0]}')\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685ce89-c91a-47d1-bc27-daa1cf21fcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f6882-0772-4349-a2bf-21124ae6c095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
