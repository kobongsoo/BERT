{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ee1bd61-1480-4322-a670-d64bd88ff866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================\n",
    "# BERT에서 512가 넘는 문장은 어떻게 할까?\n",
    "# => BERT는 최대 토큰 입력 계수가 512이다. \n",
    "# => 여기서는 512 가 넘는 토큰입력인 경우, input_id_chunks 토큰(512 계수)들로 분할하여 처리하는 방법에 대해 설명한다.\n",
    "#\n",
    "# 과정\n",
    "# 1) 입력 문장에 대해 tokenizer 처리함(*이때 add_special_tokens=False로 하여, special_token은 포함되지 않도록 처리)\n",
    "# 2) 입력 token들을 510 씩 나눈다.(splits 함수 이용)\n",
    "# 3) 나눈 token들에 앞뒤로 [CLS] tokens [SEP] 붙임.\n",
    "# 4) 512 보다 작은 맨뒤에 남는 token에는 [PAD] 입력\n",
    "# 5) 분할된 input_id_chunks 들을 dict 형태로 만들고, 모델에 입력\n",
    "# 6) 이후 출력된 outputs 에 대해 평균(mean) 값을 구하면 됨\n",
    "\n",
    "# 참고 : https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f\n",
    "# 소스 참고 : https://github.com/jamescalam/transformers/blob/main/course/language_classification/04_window_method_in_pytorch.ipynb\n",
    "#===============================================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import seed_everything, GPU_info, pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b4d96e-7383-4801-a859-547ddfcac820",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"../model/classification/bmc-ft-nsmc-cfmodel/vocab\"\n",
    "\n",
    "seed = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3bd1a2-b925-4e69-a22c-d2e339d0112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a405d84a-462a-4724-a7b7-2dd830f3594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 설정\n",
    "tokenizer = BertTokenizerFast.from_pretrained(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "966abd8c-127e-4ee7-be39-1b8934e67c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708\n",
      "{'input_ids': tensor([[   146,  10894,  11850,  10114,  15329,  20442,  10435,  18957,  10107,\n",
      "          10135,  10105,  55185,  95757,  20299,  10531,  16118,    119,    146,\n",
      "          10392,  10472,  12796,  24874,  10336,  10978,  10105,  17313,  12935,\n",
      "          15698,  10115,  10473,  10105,  94994,  20299,  10106,  95757,  10107,\n",
      "            119,  10576,    123,    120,  10250,  10105,  10150,  10924,  93163,\n",
      "          95757,  10107,  19299,  10155,  17122,    130,  22362,  10111,  10135,\n",
      "            123,    120,  10270,  10105,  95757,  19299,  10155,  17122,    126,\n",
      "          22362,    119,  21663,  49544,  10188,  10105,  73067,  38964,  26295,\n",
      "            131,    115,    115,    115,  10117,    100,  54260,  10129,  14222,\n",
      "          12659,    100,  10106,  10207,  10134,    169,  94994,  32650,  21353,\n",
      "          10106,  77201,  95757,  10107,  10850,  10114,  17313,  97586,  10350,\n",
      "          10662,  10105,  14492,  23120,  13854,  10189,  10271,  10894,  16135,\n",
      "          54260,  13135,  10474,  67833,  11942,  22257,  16357,  12483,    119,\n",
      "            115,    115,    115,    115,    115,  14988,  12397,  41375,  12166,\n",
      "          10105,  11356,  10529,  21610,  17644,  38200,  10114,  23704,  15626,\n",
      "          10107,  10111,  18850,  10119,  30619, 104101,  10336,  28784,  17285,\n",
      "          10108,  13935,  10308,  37891,  10107,  10106,    169,  50385,  10114,\n",
      "          51433,  10741,  10105,  27570,  15916,  10105,  24960, 108986,  10350,\n",
      "            119,    115,    115,    115,    115,    115,  12209,    117,  10105,\n",
      "          19037,  28710,  10106,  95757,  10107,  40132,  10189,  11152,  83795,\n",
      "          10301,  21868,  10114,  33687,  60287,  10216,    169,  85202,  24428,\n",
      "          10108,  14368,  17864,  10165,  11084,  33687,  60287,  11912,  10114,\n",
      "          72992,    169,  23078,  28710,  10106, 105888,    119,    115,    115,\n",
      "          10117,  19037,  28710,  10106,  55185,  95757,  10107,  10111,    158,\n",
      "            119,    156,    119, 105888,  11419,  96815,  10393,  11152,  83795,\n",
      "          10338,  10157,  10189,    169, 107832,  10108,  10105,  10207,    100,\n",
      "          54260,  10129,  14222,  12659,    100,  12174,  10347,  10135,  10105,\n",
      "          56191,  22742,    119,  10117,  86696,  22719,    158,    119,    156,\n",
      "            119,  10150,    118,  10924,  77201,  19864,  93274,  10336,  16038,\n",
      "            122,    119,    124,    110,  10142,  10105,  10422,  10635,  11764,\n",
      "          11508,  23607,  18905,  10531,  16118,    117,  11371,  10105,  10244,\n",
      "            118,  10924,  55185,  10379,  14946,  10474,  18134,  13277,  10142,\n",
      "            169,  10924,    119,  40512,  24317,  10107,  18577,  69848,  10454,\n",
      "          10114,  55185,  57476,    119,  40512,  24317,  10107,  45415,  10114,\n",
      "          28710,  10106,  79601,  11157,  10410,  10169, 105888,  11419,  96815,\n",
      "            117,  10319,  10529,  17087,  10455,  18134,  21559,  10106,    169,\n",
      "          35858,  10106,  10105,    158,    119,    156,    119,    117,  42668,\n",
      "          10155,  19299,  11284, 105335,  10108,    169,  12077,  43645,  28780,\n",
      "          43197,  14075,  62432,    117,  40316,  10135,  10321,  28217,  10238,\n",
      "          25520,  72199,  10111,  66558,  10123,    118,  10741,  64580,  34394,\n",
      "            119,  10117,    100,  54260,  10129,  14222,  12659,    100,  10106,\n",
      "          10207,  10134,    169,  94994,  32650,  21353,  10106,  77201,  95757,\n",
      "          10107,  10850,  10114,  17313,  97586,  10350,  10662,  10105,  14492,\n",
      "          23120,  13854,  10189,  10271,  10894,  16135,  54260,  13135,  10474,\n",
      "          67833,  11942,  22257,  16357,  12483,    119,  14988,  12397,  41375,\n",
      "          12166,  10105,  11356,  10529,  21610,  17644,  38200,  10114,  23704,\n",
      "          15626,  10107,  10111,  18850,  10119,  30619, 104101,  10336,  28784,\n",
      "          17285,  10108,  13935,  10308,  37891,  10107,  10106,    169,  50385,\n",
      "          10114,  51433,  10741,  10105,  27570,  15916,  10105,  24960, 108986,\n",
      "          10350,    119,  10117,  91335,  10111,  14633,  10529,  32974,  13145,\n",
      "          11942,  43619,  10107,  10106,  19037,  14368,  47307,    117,  12556,\n",
      "          25649,  10114,  23819,  20570,  17315, 102920,  10146,  10105,  18331,\n",
      "          27570,  59148,  10114,  99467,  10188,  10105,  13098,  32194,    118,\n",
      "          10270,  24960, 108986,  10350,    119,  12209,    117,  10105,  19037,\n",
      "          28710,  10106,  95757,  10107,  40132,  10189,  11152,  83795,  10301,\n",
      "          21868,  10114,  33687,  60287,  10216,    169,  85202,  24428,  10108,\n",
      "          14368,  17864,  10165,  11084,  33687,  60287,  11912,  10114,  72992,\n",
      "            169,  23078,  28710,  10106, 105888,    119,  12613,  12397,  19423,\n",
      "          13145,  23898,    117,  93163,  15910,  18042,  10106,  30839,  10319,\n",
      "         104838,  95757,  10107,  17981,    119,  10747,  10944,  10379,  32650,\n",
      "          19503,  10491,  10708,  20511,  42893,  10146,  17981,  17644,  38200,\n",
      "          17574,  10798,  55869,  64803,  19113,  10142,  84459,    117,  34705,\n",
      "          18908,  10943,  10114,  36120,  13149,  10107,  10105,  10106,  63996,\n",
      "          10376,  26069,    119,    100,  10117,  13145,  11942,  45244,  10419,\n",
      "          10188,  14368,  65744,  11337,  22497,  25430,  10106,  11192,  11444,\n",
      "          10105,  10321,  28217,  11496,  10529, 102731,  10162,    169,  13170,\n",
      "          10114,  11152,  15079,  10114,  16626,  11949,    117,    100,  12415,\n",
      "          40429,  93352,  77639,  10147,    117,  19421,  37933,  19288,  10160,\n",
      "          64097,  63254,    117,  10106,    169,  14108,  19864,  10531,  16118,\n",
      "            119,    100,  12209,    117,  11155,  11337,  10347,    169,  24179,\n",
      "          10108,  12864,    100,  54260,  10129,  14222,  12659,    100,  13213,\n",
      "          10114,  10105,  10464,  11951,  82871,  10336,  10106,  10207,    117,\n",
      "          10111,  10531,  10124,  17446,  12126,  23195,  10142,  67267,    117,\n",
      "            100,  93352,  77639,  10147,  52452,  10162,    117,  14819,  14368,\n",
      "          65744,  16135,  10114,  10119,  47673,  10531,  28780,  43197,  14075,\n",
      "            119,  13307,    118,  13719,  55185,  95757,  10107,  10106,  11891,\n",
      "          10111,  11767,  15689,    158,    119,    156,    119,  77201,  10107,\n",
      "          17981,  32216,  10105,  11572,  10108,  10105,  16118,  10146,  55185,\n",
      "          75111,  81011,  10455, 101816,  10107,    119]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt = \"\"\"\n",
    "I would like to get your all  thoughts on the bond yield increase this week.  I am not worried about the market downturn but the sudden increase in yields. On 2/16 the 10 year bonds yields increased by almost  9 percent and on 2/19 the yield increased by almost 5 percent.\n",
    "\n",
    "Key Points from the CNBC Article:\n",
    "\n",
    "* **The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.**\n",
    "* **Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic.**\n",
    "* **However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.**\n",
    "\n",
    "The recent rise in bond yields and U.S. inflation expectations has some investors wary that a repeat of the 2013 “taper tantrum” could be on the horizon.\n",
    "\n",
    "The benchmark U.S. 10-year Treasury note climbed above 1.3% for the first time since February 2020 earlier this week, while the 30-year bond also hit its highest level for a year. Yields move inversely to bond prices.\n",
    "\n",
    "Yields tend to rise in lockstep with inflation expectations, which have reached their highest levels in a decade in the U.S., powered by increased prospects of a large fiscal stimulus package, progress on vaccine rollouts and pent-up consumer demand.\n",
    "\n",
    "The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.\n",
    "\n",
    "Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic. The Fed and others have maintained supportive tones in recent policy meetings, vowing to keep financial conditions loose as the global economy looks to emerge from the Covid-19 pandemic.\n",
    "\n",
    "However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.\n",
    "\n",
    "With central bank support removed, bonds usually fall in price which sends yields higher. This can also spill over into stock markets as higher interest rates means more debt servicing for firms, causing traders to reassess the investing environment.\n",
    "\n",
    "“The supportive stance from policymakers will likely remain in place until the vaccines have paved a way to some return to normality,” said Shane Balkham, chief investment officer at Beaufort Investment, in a research note this week.\n",
    "\n",
    "“However, there will be a risk of another ‘taper tantrum’ similar to the one we witnessed in 2013, and this is our main focus for 2021,” Balkham projected, should policymakers begin to unwind this stimulus.\n",
    "\n",
    "Long-term bond yields in Japan and Europe followed U.S. Treasurys higher toward the end of the week as bondholders shifted their portfolios.\n",
    "\"\"\"\n",
    "\n",
    "# 입력 문장길이가 708 즉 512를 넘으면, BERT 모델 에서 에러 발생한다.(최대 512까지만 지원함)\n",
    "# 따라서 512 + 198로 나눠서 2개의 input_ids를 만듬.\n",
    "# add_special_tokens=False로 해서, 일단 [CLS], [SEP]등이 포함되지 않느 input_ids를 구함.\n",
    "\n",
    "tokenized_input = tokenizer(txt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "#tokenized_input = tokenizer(txt, max_length=512, add_special_tokens=False, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "#tokenized_input = tokenizer.encode_plus(txt, add_special_tokens=False, return_tensors='pt')\n",
    "#tokenized_input = tokenizer.encode_plus(txt, add_special_tokens=True, max_length=512, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "print(len(tokenized_input.input_ids[0]))\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b53a6c7-4e53-4fb1-bd61-ee59f47fb78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "tensor([   101,    146,  10894,  11850,  10114,  15329,  20442,  10435,  18957,\n",
      "         10107,  10135,  10105,  55185,  95757,  20299,  10531,  16118,    119,\n",
      "           146,  10392,  10472,  12796,  24874,  10336,  10978,  10105,  17313,\n",
      "         12935,  15698,  10115,  10473,  10105,  94994,  20299,  10106,  95757,\n",
      "         10107,    119,  10576,    123,    120,  10250,  10105,  10150,  10924,\n",
      "         93163,  95757,  10107,  19299,  10155,  17122,    130,  22362,  10111,\n",
      "         10135,    123,    120,  10270,  10105,  95757,  19299,  10155,  17122,\n",
      "           126,  22362,    119,  21663,  49544,  10188,  10105,  73067,  38964,\n",
      "         26295,    131,    115,    115,    115,  10117,    100,  54260,  10129,\n",
      "         14222,  12659,    100,  10106,  10207,  10134,    169,  94994,  32650,\n",
      "         21353,  10106,  77201,  95757,  10107,  10850,  10114,  17313,  97586,\n",
      "         10350,  10662,  10105,  14492,  23120,  13854,  10189,  10271,  10894,\n",
      "         16135,  54260,  13135,  10474,  67833,  11942,  22257,  16357,  12483,\n",
      "           119,    115,    115,    115,    115,    115,  14988,  12397,  41375,\n",
      "         12166,  10105,  11356,  10529,  21610,  17644,  38200,  10114,  23704,\n",
      "         15626,  10107,  10111,  18850,  10119,  30619, 104101,  10336,  28784,\n",
      "         17285,  10108,  13935,  10308,  37891,  10107,  10106,    169,  50385,\n",
      "         10114,  51433,  10741,  10105,  27570,  15916,  10105,  24960, 108986,\n",
      "         10350,    119,    115,    115,    115,    115,    115,  12209,    117,\n",
      "         10105,  19037,  28710,  10106,  95757,  10107,  40132,  10189,  11152,\n",
      "         83795,  10301,  21868,  10114,  33687,  60287,  10216,    169,  85202,\n",
      "         24428,  10108,  14368,  17864,  10165,  11084,  33687,  60287,  11912,\n",
      "         10114,  72992,    169,  23078,  28710,  10106, 105888,    119,    115,\n",
      "           115,  10117,  19037,  28710,  10106,  55185,  95757,  10107,  10111,\n",
      "           158,    119,    156,    119, 105888,  11419,  96815,  10393,  11152,\n",
      "         83795,  10338,  10157,  10189,    169, 107832,  10108,  10105,  10207,\n",
      "           100,  54260,  10129,  14222,  12659,    100,  12174,  10347,  10135,\n",
      "         10105,  56191,  22742,    119,  10117,  86696,  22719,    158,    119,\n",
      "           156,    119,  10150,    118,  10924,  77201,  19864,  93274,  10336,\n",
      "         16038,    122,    119,    124,    110,  10142,  10105,  10422,  10635,\n",
      "         11764,  11508,  23607,  18905,  10531,  16118,    117,  11371,  10105,\n",
      "         10244,    118,  10924,  55185,  10379,  14946,  10474,  18134,  13277,\n",
      "         10142,    169,  10924,    119,  40512,  24317,  10107,  18577,  69848,\n",
      "         10454,  10114,  55185,  57476,    119,  40512,  24317,  10107,  45415,\n",
      "         10114,  28710,  10106,  79601,  11157,  10410,  10169, 105888,  11419,\n",
      "         96815,    117,  10319,  10529,  17087,  10455,  18134,  21559,  10106,\n",
      "           169,  35858,  10106,  10105,    158,    119,    156,    119,    117,\n",
      "         42668,  10155,  19299,  11284, 105335,  10108,    169,  12077,  43645,\n",
      "         28780,  43197,  14075,  62432,    117,  40316,  10135,  10321,  28217,\n",
      "         10238,  25520,  72199,  10111,  66558,  10123,    118,  10741,  64580,\n",
      "         34394,    119,  10117,    100,  54260,  10129,  14222,  12659,    100,\n",
      "         10106,  10207,  10134,    169,  94994,  32650,  21353,  10106,  77201,\n",
      "         95757,  10107,  10850,  10114,  17313,  97586,  10350,  10662,  10105,\n",
      "         14492,  23120,  13854,  10189,  10271,  10894,  16135,  54260,  13135,\n",
      "         10474,  67833,  11942,  22257,  16357,  12483,    119,  14988,  12397,\n",
      "         41375,  12166,  10105,  11356,  10529,  21610,  17644,  38200,  10114,\n",
      "         23704,  15626,  10107,  10111,  18850,  10119,  30619, 104101,  10336,\n",
      "         28784,  17285,  10108,  13935,  10308,  37891,  10107,  10106,    169,\n",
      "         50385,  10114,  51433,  10741,  10105,  27570,  15916,  10105,  24960,\n",
      "        108986,  10350,    119,  10117,  91335,  10111,  14633,  10529,  32974,\n",
      "         13145,  11942,  43619,  10107,  10106,  19037,  14368,  47307,    117,\n",
      "         12556,  25649,  10114,  23819,  20570,  17315, 102920,  10146,  10105,\n",
      "         18331,  27570,  59148,  10114,  99467,  10188,  10105,  13098,  32194,\n",
      "           118,  10270,  24960, 108986,  10350,    119,  12209,    117,  10105,\n",
      "         19037,  28710,  10106,  95757,  10107,  40132,  10189,  11152,  83795,\n",
      "         10301,  21868,  10114,  33687,  60287,  10216,    169,  85202,  24428,\n",
      "         10108,  14368,  17864,  10165,  11084,  33687,  60287,    102])\n",
      "512\n",
      "tensor([1.0100e+02, 1.1912e+04, 1.0114e+04, 7.2992e+04, 1.6900e+02, 2.3078e+04,\n",
      "        2.8710e+04, 1.0106e+04, 1.0589e+05, 1.1900e+02, 1.2613e+04, 1.2397e+04,\n",
      "        1.9423e+04, 1.3145e+04, 2.3898e+04, 1.1700e+02, 9.3163e+04, 1.5910e+04,\n",
      "        1.8042e+04, 1.0106e+04, 3.0839e+04, 1.0319e+04, 1.0484e+05, 9.5757e+04,\n",
      "        1.0107e+04, 1.7981e+04, 1.1900e+02, 1.0747e+04, 1.0944e+04, 1.0379e+04,\n",
      "        3.2650e+04, 1.9503e+04, 1.0491e+04, 1.0708e+04, 2.0511e+04, 4.2893e+04,\n",
      "        1.0146e+04, 1.7981e+04, 1.7644e+04, 3.8200e+04, 1.7574e+04, 1.0798e+04,\n",
      "        5.5869e+04, 6.4803e+04, 1.9113e+04, 1.0142e+04, 8.4459e+04, 1.1700e+02,\n",
      "        3.4705e+04, 1.8908e+04, 1.0943e+04, 1.0114e+04, 3.6120e+04, 1.3149e+04,\n",
      "        1.0107e+04, 1.0105e+04, 1.0106e+04, 6.3996e+04, 1.0376e+04, 2.6069e+04,\n",
      "        1.1900e+02, 1.0000e+02, 1.0117e+04, 1.3145e+04, 1.1942e+04, 4.5244e+04,\n",
      "        1.0419e+04, 1.0188e+04, 1.4368e+04, 6.5744e+04, 1.1337e+04, 2.2497e+04,\n",
      "        2.5430e+04, 1.0106e+04, 1.1192e+04, 1.1444e+04, 1.0105e+04, 1.0321e+04,\n",
      "        2.8217e+04, 1.1496e+04, 1.0529e+04, 1.0273e+05, 1.0162e+04, 1.6900e+02,\n",
      "        1.3170e+04, 1.0114e+04, 1.1152e+04, 1.5079e+04, 1.0114e+04, 1.6626e+04,\n",
      "        1.1949e+04, 1.1700e+02, 1.0000e+02, 1.2415e+04, 4.0429e+04, 9.3352e+04,\n",
      "        7.7639e+04, 1.0147e+04, 1.1700e+02, 1.9421e+04, 3.7933e+04, 1.9288e+04,\n",
      "        1.0160e+04, 6.4097e+04, 6.3254e+04, 1.1700e+02, 1.0106e+04, 1.6900e+02,\n",
      "        1.4108e+04, 1.9864e+04, 1.0531e+04, 1.6118e+04, 1.1900e+02, 1.0000e+02,\n",
      "        1.2209e+04, 1.1700e+02, 1.1155e+04, 1.1337e+04, 1.0347e+04, 1.6900e+02,\n",
      "        2.4179e+04, 1.0108e+04, 1.2864e+04, 1.0000e+02, 5.4260e+04, 1.0129e+04,\n",
      "        1.4222e+04, 1.2659e+04, 1.0000e+02, 1.3213e+04, 1.0114e+04, 1.0105e+04,\n",
      "        1.0464e+04, 1.1951e+04, 8.2871e+04, 1.0336e+04, 1.0106e+04, 1.0207e+04,\n",
      "        1.1700e+02, 1.0111e+04, 1.0531e+04, 1.0124e+04, 1.7446e+04, 1.2126e+04,\n",
      "        2.3195e+04, 1.0142e+04, 6.7267e+04, 1.1700e+02, 1.0000e+02, 9.3352e+04,\n",
      "        7.7639e+04, 1.0147e+04, 5.2452e+04, 1.0162e+04, 1.1700e+02, 1.4819e+04,\n",
      "        1.4368e+04, 6.5744e+04, 1.6135e+04, 1.0114e+04, 1.0119e+04, 4.7673e+04,\n",
      "        1.0531e+04, 2.8780e+04, 4.3197e+04, 1.4075e+04, 1.1900e+02, 1.3307e+04,\n",
      "        1.1800e+02, 1.3719e+04, 5.5185e+04, 9.5757e+04, 1.0107e+04, 1.0106e+04,\n",
      "        1.1891e+04, 1.0111e+04, 1.1767e+04, 1.5689e+04, 1.5800e+02, 1.1900e+02,\n",
      "        1.5600e+02, 1.1900e+02, 7.7201e+04, 1.0107e+04, 1.7981e+04, 3.2216e+04,\n",
      "        1.0105e+04, 1.1572e+04, 1.0108e+04, 1.0105e+04, 1.6118e+04, 1.0146e+04,\n",
      "        5.5185e+04, 7.5111e+04, 8.1011e+04, 1.0455e+04, 1.0182e+05, 1.0107e+04,\n",
      "        1.1900e+02, 1.0200e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# 참고 소스 :https://github.com/jamescalam/transformers/blob/main/course/language_classification/04_window_method_in_pytorch.ipynb\n",
    "\n",
    "# define target chunksize\n",
    "chunksize = 512\n",
    "\n",
    "# split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
    "# 510으로 큰 문장을 나누고\n",
    "input_id_chunks = list(tokenized_input['input_ids'][0].split(chunksize - 2))\n",
    "mask_chunks = list(tokenized_input['attention_mask'][0].split(chunksize - 2))\n",
    "\n",
    "# loop through each chunk\n",
    "for i in range(len(input_id_chunks)):\n",
    "    \n",
    "    # add CLS and SEP tokens to input IDs\n",
    "    # 나눈 input_id_chunks에 [CLS], [SEP]를 추가함 \n",
    "    input_id_chunks[i] = torch.cat([\n",
    "        torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
    "    ])\n",
    "    \n",
    "    # add attention tokens to attention mask\n",
    "    # attentionmask에도 [CLS], [SEP]에 1을 추가\n",
    "    mask_chunks[i] = torch.cat([\n",
    "        torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
    "    ])\n",
    "    \n",
    "    # get required padding length\n",
    "    # PAD 길이를 얻어와서, 512보다 작은곳에는 PAD 추가함\n",
    "    pad_len = chunksize - input_id_chunks[i].shape[0]\n",
    "    \n",
    "    # check if tensor length satisfies required chunk size\n",
    "    if pad_len > 0:\n",
    "        # if padding length is more than 0, we must add padding\n",
    "        input_id_chunks[i] = torch.cat([\n",
    "            input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
    "        ])\n",
    "        mask_chunks[i] = torch.cat([\n",
    "            mask_chunks[i], torch.Tensor([0] * pad_len)\n",
    "        ])\n",
    "\n",
    "# check length of each tensor\n",
    "for chunk in input_id_chunks:\n",
    "    print(len(chunk))\n",
    "    print(chunk)\n",
    "\n",
    "# print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
    "#print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08060b11-3009-42b4-b556-12dc10eb430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/bert-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True로 해야, hidden_states 가 출력됨\n",
    "output_hidden_states = True\n",
    "#False로 지정하는 경우 일반적인 tuple을 리턴, True인 경우는 transformers.file_utils.ModelOutput 으로 리턴\n",
    "return_dict = False\n",
    "\n",
    "model_path = '../model/bert-multilingual-cased' \n",
    "\n",
    "# model 불러옴\n",
    "model = BertModel.from_pretrained(model_path, \n",
    "                                  output_hidden_states=output_hidden_states,\n",
    "                                  return_dict=return_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eb9180f-4208-4103-88c9-adf3916a146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   146, 10894,  ..., 33687, 60287,   102],\n",
      "        [  101, 11912, 10114,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "# input_ids_chunks르 dict로 만듬\n",
    "input_ids = torch.stack(input_id_chunks)\n",
    "attention_mask = torch.stack(mask_chunks)\n",
    "\n",
    "input_dict = {\n",
    "    'input_ids': input_ids.long(),\n",
    "    'attention_mask': attention_mask.int()\n",
    "}\n",
    "\n",
    "print(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9925b71e-82b3-4ddf-b1a9-e551317dc4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 모델어 input_dict 넣고 출력 \n",
    "outputs = model(**input_dict)\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47426aee-c89d-4f19-8864-5bb23fe8cb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence type: <class 'torch.Tensor'>\n",
      "sequence 길이: torch.Size([2, 512, 768])\n",
      "\n",
      "\n",
      "pooled type: <class 'torch.Tensor'>\n",
      "pooled 길이:torch.Size([2, 768])\n",
      "\n",
      "\n",
      "hidden_states type: <class 'tuple'>\n",
      "hidden_states\n",
      "-레이어 수:13\n",
      "-배치 수: 2\n",
      "-토큰 수 : 512\n",
      "-hidden 유닛 수 : 768\n"
     ]
    }
   ],
   "source": [
    "sequence_output = outputs[0]\n",
    "print('sequence type: {}'.format(type(sequence_output)))\n",
    "print('sequence 길이: {}'.format(sequence_output.size()))\n",
    "print('\\n')\n",
    "            \n",
    "pooled_output = outputs[1]\n",
    "print('pooled type: {}'.format(type(pooled_output)))\n",
    "print('pooled 길이:{}'.format(pooled_output.size()))\n",
    "print('\\n')\n",
    "\n",
    "hidden_states = outputs[2]\n",
    "print('hidden_states type: {}'.format(type(hidden_states)))\n",
    "layer_idx = 0\n",
    "batch_idx = 0\n",
    "token_idx = 0\n",
    "print('hidden_states')\n",
    "print(\"-레이어 수:{}\".format(len(hidden_states)))\n",
    "print(\"-배치 수: {}\".format(len(hidden_states[layer_idx])))\n",
    "print(\"-토큰 수 : {}\".format(len(hidden_states[layer_idx][batch_idx])))\n",
    "print(\"-hidden 유닛 수 : {}\".format(len(hidden_states[layer_idx][batch_idx][token_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aad77fe1-ff57-47a7-955b-53f3e2e16c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "torch.Size([768])\n",
      "tensor([ 0.2065, -0.1807,  0.2499, -0.3467, -0.1316,  0.2024,  0.2877,  0.2766,\n",
      "        -0.4050,  0.2525, -0.2488, -0.3253, -0.1884, -0.3464,  0.2898, -0.2977,\n",
      "         0.6143,  0.3040,  0.0855, -0.1333, -0.9998, -0.4182, -0.2352, -0.2241,\n",
      "        -0.5292,  0.3844, -0.2604,  0.4396,  0.2811, -0.2399,  0.1456, -0.9998,\n",
      "         0.7237,  0.6057,  0.3402, -0.2484,  0.1226,  0.1253,  0.1584, -0.4195,\n",
      "        -0.1671,  0.1556, -0.1758,  0.1589, -0.2212, -0.2882, -0.3309,  0.2314,\n",
      "        -0.3749,  0.2454, -0.0100,  0.1833,  0.4785,  0.1836,  0.1699,  0.1592,\n",
      "         0.3646,  0.2219,  0.3616, -0.2915,  0.0265,  0.3371,  0.3091, -0.3351,\n",
      "        -0.2154, -0.3299,  0.2114, -0.1359,  0.2699, -0.3360, -0.3172, -0.2742,\n",
      "        -0.1866,  0.1523,  0.3244, -0.2772,  0.3102,  0.2228,  0.1599, -0.1991,\n",
      "        -0.3259, -0.3332, -0.3347,  0.2152, -0.1937,  0.2903,  0.2723, -0.3829,\n",
      "         0.3020, -0.2513,  0.0803,  0.4680, -0.2964,  0.4038, -0.2600, -0.0880,\n",
      "        -0.7526, -0.1678, -0.3243, -0.2442, -0.2528,  0.2656, -0.2874, -0.2967,\n",
      "        -0.2452, -0.3439,  0.3390,  0.1721, -0.2388,  0.4148,  0.2736, -0.4170,\n",
      "        -0.2872,  0.2312, -0.3359,  0.9536, -0.4699,  0.1923,  0.1808, -0.2016,\n",
      "        -0.3775,  0.9998,  0.2987, -0.3094,  0.3261,  0.2169, -0.5141,  0.3064,\n",
      "         0.1237,  0.2807,  0.1400, -0.1830, -0.1787, -0.3299, -0.7231, -0.2003,\n",
      "        -0.3674,  0.4218, -0.1248, -0.3189,  0.3061,  0.4241,  0.2166, -0.2504,\n",
      "        -0.1259, -0.2335,  0.2052, -0.1806,  0.9997,  0.4817, -0.2392, -0.3461,\n",
      "         0.6256, -0.5323, -0.2475, -0.3179, -0.2770, -0.3995,  0.3001,  0.2133,\n",
      "         0.1083, -0.1677, -0.2464, -0.1813,  0.0277, -0.4934, -0.1669,  0.2141,\n",
      "         0.3012,  0.2773, -0.3731,  0.3975,  0.2962, -0.2958, -0.1301,  0.2761,\n",
      "         0.1739, -0.2584, -0.3613, -0.2476,  0.2901, -0.2487, -0.5716,  0.2859,\n",
      "        -0.1790, -0.2326,  0.3559, -0.2897, -0.3643,  0.2850, -0.3856,  0.2657,\n",
      "        -0.2616,  0.2303,  0.4039,  0.2981, -0.3328,  0.2083,  0.2613,  0.2096,\n",
      "         0.3442,  0.0724,  0.2332,  0.2147, -0.3441, -0.7223,  0.3329,  0.2333,\n",
      "         0.4054, -0.2831, -0.5153, -0.4121,  0.4249,  0.3458, -0.3874,  0.2695,\n",
      "         0.3656, -0.2674, -0.2925,  0.2964, -0.2513, -0.3736, -0.2610, -0.4014,\n",
      "        -0.3172,  0.2723,  0.3527,  0.3292,  0.3585, -0.2336, -0.1487, -0.3327,\n",
      "         0.3110,  0.2450, -0.2413,  0.7251, -0.1312,  0.2857, -0.4488, -0.2631,\n",
      "         0.2234, -0.2367,  0.3588,  0.9014,  0.1807, -0.3046,  0.2898,  0.1151,\n",
      "         0.2486, -0.3274,  0.2057, -0.4479,  0.7251,  0.3099,  0.2429, -0.9998,\n",
      "         0.1744,  0.2171,  0.2875,  0.3124,  0.1405,  0.2103,  0.0649,  0.8064,\n",
      "        -0.2947, -0.3813, -0.3747, -0.3409, -0.2683, -0.0702, -0.2226, -0.3274,\n",
      "        -0.1747, -0.3229, -0.1942,  0.2744,  0.3818, -0.9863,  0.7682,  0.2738,\n",
      "        -0.3006,  0.0873,  0.3394, -0.9998,  0.2376, -0.1995, -0.3494,  0.2135,\n",
      "        -0.3015, -0.3891,  0.2884,  0.2582,  0.4263,  0.2362,  0.2482,  0.2665,\n",
      "        -0.2396,  0.1402,  0.2512, -0.2326,  0.4672, -0.1059,  0.4469,  0.2662,\n",
      "        -0.2493,  0.1879, -0.2458,  0.0893,  0.3507,  0.1788,  0.2050, -0.1826,\n",
      "         0.3079, -0.6742,  0.3259, -0.2968, -0.4461, -0.2867,  0.2036, -0.2221,\n",
      "        -0.2484,  0.3810, -0.3797,  0.9997,  0.2203, -0.3447, -0.3220,  0.4485,\n",
      "         0.5409, -0.2527, -0.8322, -0.2811,  0.3350,  0.3315,  0.3580,  0.1480,\n",
      "         0.2525,  0.2228, -0.2337, -0.2964,  0.2299, -0.2169,  0.3160, -0.1320,\n",
      "        -0.4359,  0.1921, -0.1763, -0.1704, -0.8679,  0.1853,  0.2031,  0.3677,\n",
      "         0.3046,  0.3154, -0.1819,  0.4350,  0.4539, -0.2835, -0.2594, -0.3840,\n",
      "        -0.3473,  0.2202, -0.1856, -0.4334,  0.3044, -0.5686,  0.1482, -0.2621,\n",
      "        -0.1692, -0.2862,  0.3068, -0.9998, -0.3267,  0.2921, -0.2394,  0.3399,\n",
      "        -0.4244, -0.2318,  0.3297,  0.2839, -0.1715,  0.2171, -0.3798,  0.3038,\n",
      "        -0.1915,  0.1619,  0.5958,  0.4417,  0.1982, -0.4140,  0.2844, -0.2692,\n",
      "        -0.3019,  0.2632,  0.1939, -0.2461,  0.3305,  0.2278,  0.2847, -0.2001,\n",
      "         0.2898, -0.3529, -0.4644,  0.2339, -0.1720, -0.3005, -0.2788,  0.1581,\n",
      "        -0.4320,  0.2298,  0.2949,  0.2777,  0.1656,  0.3363, -0.3147, -0.0971,\n",
      "        -0.1583, -0.0873, -0.3891, -0.3236, -0.2725,  0.9998,  0.4014,  0.3348,\n",
      "        -0.2805,  0.1887,  0.3129, -0.2143,  0.1386,  0.2858,  0.3479, -0.1858,\n",
      "         0.2254,  0.2994,  0.2902,  0.2557,  0.2463,  0.3864, -0.1812,  0.6893,\n",
      "        -0.3440, -0.3503, -0.9850,  0.2263,  0.5298, -0.2563, -0.7339,  0.2121,\n",
      "        -0.2357,  0.3715, -0.0623,  0.2166,  0.1526, -0.3033,  0.2562, -0.4447,\n",
      "         0.9998, -0.1244,  0.1908,  0.2886,  0.2110, -0.2762, -0.1175, -0.2321,\n",
      "         0.2606, -0.4124,  0.1840, -0.8752,  0.3638,  0.2434,  0.4065, -0.2028,\n",
      "         0.3117, -0.4514,  0.4154, -0.3321, -0.1993, -0.3636,  0.3008, -0.3900,\n",
      "         0.3560, -0.3074,  0.2952, -0.2515,  0.2485, -0.1751,  0.2502, -0.1716,\n",
      "         0.1878, -0.1640, -0.1929, -0.3793,  0.1661, -0.2630,  0.9998, -0.2196,\n",
      "         0.2829, -0.3664,  0.2930, -0.1748,  0.3347,  0.5997, -0.2930,  0.1903,\n",
      "         0.3917, -0.5017,  0.2478, -0.2286, -0.9076, -0.3296,  0.9237,  0.2318,\n",
      "         0.2442,  0.4566,  0.3444,  0.2382, -0.3228,  0.2759,  0.7423,  0.2715,\n",
      "         0.4315,  0.4176,  0.1733, -0.3993, -0.3221,  0.9998,  0.9997,  0.2624,\n",
      "         0.3220, -0.1498, -0.4330, -0.2478,  0.4184,  0.2440,  0.3166, -0.1846,\n",
      "         0.2981, -0.2431, -0.3120, -0.1708, -0.1986, -0.1992,  0.1779, -0.2461,\n",
      "         0.5134,  0.3426,  0.1460,  0.4589,  0.1509,  0.2191, -0.1756, -0.2662,\n",
      "         0.2923, -0.1976, -0.2612, -0.3204,  0.3499, -0.9997, -0.2071, -0.2318,\n",
      "        -0.3264,  0.4216,  0.1818,  0.1325, -0.3083, -0.1512, -0.2473,  0.0801,\n",
      "         0.1456,  0.2290, -0.2569, -0.2381,  0.1288, -0.3859,  0.2299, -0.2424,\n",
      "        -0.4984, -0.5293, -0.1657, -0.1838,  0.4280, -0.3796, -0.3027,  0.2096,\n",
      "         0.2564,  0.0237, -0.4273,  0.2517, -0.2905,  0.2070,  0.2631,  0.2802,\n",
      "         0.2390, -0.2896, -0.2352, -0.2634, -0.3046, -0.1571,  0.3147, -0.1722,\n",
      "         0.2575, -0.1029,  0.1575, -0.3130,  0.3407,  0.4112,  0.2646, -0.2236,\n",
      "         0.3073,  0.2884, -0.2047,  0.2970,  0.2042, -0.2652, -0.2788,  0.9998,\n",
      "         0.3044,  0.1580,  0.3127, -0.1496,  0.3806,  0.3700,  0.3571, -0.3177,\n",
      "         0.9476, -0.3228,  0.2252,  0.2851,  0.3482,  0.1942,  0.2488,  0.2324,\n",
      "         0.6898,  0.2777,  0.2721,  0.1486,  0.2967,  0.2360,  0.2460,  0.2575,\n",
      "         0.3822,  0.3269, -0.2442,  0.1330, -0.3552, -0.3271, -0.3420, -0.3357,\n",
      "        -0.3901,  0.1563, -0.1994, -0.2098, -0.3326,  0.2554, -0.1191,  0.0092,\n",
      "        -0.2662, -0.3194,  0.5838, -0.3230,  0.3167, -0.2256,  0.1927, -0.5896,\n",
      "         0.1358, -0.2664, -0.2643, -0.4035, -0.4756,  0.2748,  0.3246, -0.1614,\n",
      "         0.2428, -0.2476,  0.4000, -0.0881, -0.1546,  0.2821, -0.9998,  0.2125,\n",
      "         0.2432, -0.2758,  0.1604,  0.2907,  0.3197,  0.2181, -0.2657, -0.2640,\n",
      "        -0.2705,  0.1236, -0.3571,  0.1443,  0.2884, -0.1965, -0.3277,  0.1534,\n",
      "        -0.3064,  0.1742,  0.3314, -0.1890,  0.3818, -0.1501,  0.1423, -0.2154,\n",
      "         0.3378, -0.2600, -0.2238,  0.2856, -0.3209, -0.1082, -0.3482,  0.1977,\n",
      "        -0.3549,  0.2577,  0.3498, -0.1739,  0.3357, -0.2716,  0.3215, -0.1829,\n",
      "         0.1999, -0.7067, -0.4181, -0.1970, -0.3928,  0.0985,  0.4119,  0.1563,\n",
      "         0.2806, -0.3192,  0.2962, -0.2501,  0.1828,  0.4137, -0.3032,  0.0942,\n",
      "        -0.2644,  0.2490, -0.1886,  0.1627, -0.9817, -0.2323,  0.2523,  0.1966,\n",
      "         0.4240, -0.3609, -0.4053, -0.3355, -0.3950,  0.3277,  0.3509,  0.4696,\n",
      "         0.2892,  0.3370, -0.1286, -0.3757,  0.8516, -0.0880,  0.2614,  0.4480,\n",
      "         0.0712,  0.8900,  0.1993,  0.1719,  0.3583, -0.2761,  0.3553,  0.2908],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# pooled_out에 2개의 문장의 평균값으로 임베딩값 표현함\n",
    "mean_pooled = torch.mean(pooled_output, dim=0)\n",
    "print(len(mean_pooled))\n",
    "print(mean_pooled.size())\n",
    "print(mean_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70442c97-278a-468d-b4dc-e4c0769e9e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
