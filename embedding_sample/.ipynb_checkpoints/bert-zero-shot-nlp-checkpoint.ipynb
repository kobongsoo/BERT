{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee1bd61-1480-4322-a670-d64bd88ff866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-07.log\n"
     ]
    }
   ],
   "source": [
    "# NLP를 이용한 Zero-shot Classification 분류 예시\n",
    "# => 각 문장과 labels를 가지고, 문장(전제)과 labels(가설)간 관계가 최대 참(entailment) 일 확률을 구하는 방식\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import seed_everything, GPU_info, pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b4d96e-7383-4801-a859-547ddfcac820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_path = '../model/classification/bmc_fpt_kowiki20200920.train_model_0225-ft-klue-nli-0303' \n",
    "vocab_path = \"../model/classification/bmc_fpt_kowiki20200920.train_model_0225-ft-klue-nli-0303/vocab\"\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a405d84a-462a-4724-a7b7-2dd830f3594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 설정\n",
    "tokenizer = BertTokenizerFast.from_pretrained(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6a9c77-2854-4bbf-8fb4-81c587c9736e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(143772, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 불러옴\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, output_hidden_states=True, num_labels=3)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0903aac4-df6b-431b-a67a-ac8ed988f6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prameters:196460547\n"
     ]
    }
   ],
   "source": [
    "print('prameters:{}'.format(model.num_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966abd8c-127e-4ee7-be39-1b8934e67c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "input_texts = ['오늘은 오후 부터 춥고 비가 올것 같다.']\n",
    "labels = [\n",
    "        ['오늘은 가끔 흐리고, 눈이 올수 있다'],\n",
    "        ['여기 식당은 파스타가 맛있다'],\n",
    "        ['오늘 증시는 내림으로 마감 하였다'],\n",
    "        ['내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다']\n",
    "]\n",
    "'''\n",
    "\n",
    "'''\n",
    "#input_texts = ['오늘은 오전에는 흐리고 오후부터는 가끔씩 비가 오겠다']\n",
    "input_texts = ['즐겨볼만한 스포츠']\n",
    "labels = [['축구'],['야구'],['등산'],['낚시'],['날씨'],['상품권'],['교육']]\n",
    "'''\n",
    "\n",
    "labels = ['서울은 대한민국에 수도이며, 정치 경제 중심지이다',\n",
    "          '내년 경제 성장은 4%대 성장을 이룰거라 예상된다',\n",
    "          '프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다',\n",
    "          '올해에는 대통령 선거와 지방선거가 동시에 열린다',\n",
    "          '오늘 날씨는 비가 내리고 매우 춥다',\n",
    "          '손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다',\n",
    "          '건조한 날씨에 산불을 조심해야 한다',\n",
    "          '윈도우11 OS에 검색 기능을 강화 하였다',\n",
    "          '한국은행은 올해 하반기 금리를 동결했다']\n",
    "          \n",
    "\n",
    "input_texts = ['여행',\n",
    "           '투표',\n",
    "           '증권',\n",
    "           'IT']\n",
    "\n",
    "# labels이 문장이고, input_texts가 keyword(단어)인 경우에는 True로 해줌\n",
    "reverse_tokenizer = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "805c9d1c-622e-4bc1-8c49-a797ba90edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "내년 경제 성장은 4%대 성장을 이룰거라 예상된다\n",
      "4\n",
      "투표\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[1])\n",
    "print(len(input_texts))\n",
    "print(input_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfdcc7b0-b573-4afa-8c8a-cd9523f9c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', '여행', '[SEP]', '서울', '##은', '대한민국', '##에', '수도', '##이며', ',', '정치', '경제', '중심지', '##이다', '[SEP]']]\n",
      "{'input_ids': tensor([[   101, 120337,    102,  48253,  10892,  26168,  10530,  69283,  33542,\n",
      "            117, 119622, 119641, 122451,  11925,    102]]), 'token_type_ids': tensor([[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# NLP 문장을 테스트 : [CLS]input_text[SEP]labels[SEP] 형식으로 tokenizer 시킴 \n",
    "tokenized_input = tokenizer([input_texts[0]], [labels[0]], return_tensors=\"pt\")\n",
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "print(token_str)\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf7cecf1-2e5e-4158-8a57-97d6922c3583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19d66a75fe94ba782974a1b1df134d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119622,\n",
      "         119641, 122451,  11925,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 135331, 119641, 120310,  10892,    125,    110,   9069, 120310,\n",
      "          10622,   9638, 118891,  41521,  17342, 121583,  22096,    102, 120337,\n",
      "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,  47364, 120425,  11018, 130714,  21611, 126880,  20173, 121891,\n",
      "           9737,  11018, 120437,  12092,  14040,  11925,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121110,  15303,  70672, 119609,  12638, 105383, 127258,  11287,\n",
      "          58248, 131044,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 120059, 126746,  11018,   9379,  11287, 121654,  11664,  42608,\n",
      "            100,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,   9450, 119444,  36553,  10739,  50266, 122367,  37905,  81785,\n",
      "          10193,  48506,   8892,  10622,   9010,  17706,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 122493,  11102, 126746,  10530, 136973,  10622, 129169, 108436,\n",
      "          16139,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121649,  37115,  18398,  10530, 121178, 119787,  10622, 120249,\n",
      "          28750,    102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 138241,  10892, 121110, 131540, 136548,  11513, 138961,  12490,\n",
      "            102, 120337,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "여행\n",
      "\n",
      "프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다 (Score: 0.0053)\n",
      "건조한 날씨에 산불을 조심해야 한다 (Score: 0.0001)\n",
      "올해에는 대통령 선거와 지방선거가 동시에 열린다 (Score: 0.0001)\n",
      "윈도우11 OS에 검색 기능을 강화 하였다 (Score: 0.0001)\n",
      "한국은행은 올해 하반기 금리를 동결했다 (Score: 0.0001)\n",
      "오늘 날씨는 비가 내리고 매우 춥다 (Score: 0.0001)\n",
      "내년 경제 성장은 4%대 성장을 이룰거라 예상된다 (Score: 0.0001)\n",
      "손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다 (Score: 0.0001)\n",
      "서울은 대한민국에 수도이며, 정치 경제 중심지이다 (Score: 0.0001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50e8c3455824efd91722230d36160e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119622,\n",
      "         119641, 122451,  11925,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 135331, 119641, 120310,  10892,    125,    110,   9069, 120310,\n",
      "          10622,   9638, 118891,  41521,  17342, 121583,  22096,    102, 119907,\n",
      "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,  47364, 120425,  11018, 130714,  21611, 126880,  20173, 121891,\n",
      "           9737,  11018, 120437,  12092,  14040,  11925,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121110,  15303,  70672, 119609,  12638, 105383, 127258,  11287,\n",
      "          58248, 131044,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 120059, 126746,  11018,   9379,  11287, 121654,  11664,  42608,\n",
      "            100,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,   9450, 119444,  36553,  10739,  50266, 122367,  37905,  81785,\n",
      "          10193,  48506,   8892,  10622,   9010,  17706,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 122493,  11102, 126746,  10530, 136973,  10622, 129169, 108436,\n",
      "          16139,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121649,  37115,  18398,  10530, 121178, 119787,  10622, 120249,\n",
      "          28750,    102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 138241,  10892, 121110, 131540, 136548,  11513, 138961,  12490,\n",
      "            102, 119907,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "투표\n",
      "\n",
      "올해에는 대통령 선거와 지방선거가 동시에 열린다 (Score: 0.0015)\n",
      "내년 경제 성장은 4%대 성장을 이룰거라 예상된다 (Score: 0.0001)\n",
      "한국은행은 올해 하반기 금리를 동결했다 (Score: 0.0001)\n",
      "프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다 (Score: 0.0001)\n",
      "서울은 대한민국에 수도이며, 정치 경제 중심지이다 (Score: 0.0001)\n",
      "오늘 날씨는 비가 내리고 매우 춥다 (Score: 0.0000)\n",
      "손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다 (Score: 0.0000)\n",
      "건조한 날씨에 산불을 조심해야 한다 (Score: 0.0000)\n",
      "윈도우11 OS에 검색 기능을 강화 하였다 (Score: 0.0000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562c53ab8986439d9cd41de200f6831c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119622,\n",
      "         119641, 122451,  11925,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 135331, 119641, 120310,  10892,    125,    110,   9069, 120310,\n",
      "          10622,   9638, 118891,  41521,  17342, 121583,  22096,    102, 123542,\n",
      "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,  47364, 120425,  11018, 130714,  21611, 126880,  20173, 121891,\n",
      "           9737,  11018, 120437,  12092,  14040,  11925,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121110,  15303,  70672, 119609,  12638, 105383, 127258,  11287,\n",
      "          58248, 131044,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 120059, 126746,  11018,   9379,  11287, 121654,  11664,  42608,\n",
      "            100,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,   9450, 119444,  36553,  10739,  50266, 122367,  37905,  81785,\n",
      "          10193,  48506,   8892,  10622,   9010,  17706,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 122493,  11102, 126746,  10530, 136973,  10622, 129169, 108436,\n",
      "          16139,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121649,  37115,  18398,  10530, 121178, 119787,  10622, 120249,\n",
      "          28750,    102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 138241,  10892, 121110, 131540, 136548,  11513, 138961,  12490,\n",
      "            102, 123542,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "증권\n",
      "\n",
      "한국은행은 올해 하반기 금리를 동결했다 (Score: 0.0001)\n",
      "손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다 (Score: 0.0001)\n",
      "서울은 대한민국에 수도이며, 정치 경제 중심지이다 (Score: 0.0001)\n",
      "오늘 날씨는 비가 내리고 매우 춥다 (Score: 0.0001)\n",
      "프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다 (Score: 0.0000)\n",
      "올해에는 대통령 선거와 지방선거가 동시에 열린다 (Score: 0.0000)\n",
      "건조한 날씨에 산불을 조심해야 한다 (Score: 0.0000)\n",
      "내년 경제 성장은 4%대 성장을 이룰거라 예상된다 (Score: 0.0000)\n",
      "윈도우11 OS에 검색 기능을 강화 하였다 (Score: 0.0000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199122e578c54e34ad9691a93315d140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119622,\n",
      "         119641, 122451,  11925,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 135331, 119641, 120310,  10892,    125,    110,   9069, 120310,\n",
      "          10622,   9638, 118891,  41521,  17342, 121583,  22096,    102,  26956,\n",
      "            102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,  47364, 120425,  11018, 130714,  21611, 126880,  20173, 121891,\n",
      "           9737,  11018, 120437,  12092,  14040,  11925,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121110,  15303,  70672, 119609,  12638, 105383, 127258,  11287,\n",
      "          58248, 131044,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 120059, 126746,  11018,   9379,  11287, 121654,  11664,  42608,\n",
      "            100,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101,   9450, 119444,  36553,  10739,  50266, 122367,  37905,  81785,\n",
      "          10193,  48506,   8892,  10622,   9010,  17706,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 122493,  11102, 126746,  10530, 136973,  10622, 129169, 108436,\n",
      "          16139,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 121649,  37115,  18398,  10530, 121178, 119787,  10622, 120249,\n",
      "          28750,    102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[   101, 138241,  10892, 121110, 131540, 136548,  11513, 138961,  12490,\n",
      "            102,  26956,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "IT\n",
      "\n",
      "손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다 (Score: 0.0001)\n",
      "윈도우11 OS에 검색 기능을 강화 하였다 (Score: 0.0001)\n",
      "서울은 대한민국에 수도이며, 정치 경제 중심지이다 (Score: 0.0001)\n",
      "프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다 (Score: 0.0001)\n",
      "올해에는 대통령 선거와 지방선거가 동시에 열린다 (Score: 0.0001)\n",
      "오늘 날씨는 비가 내리고 매우 춥다 (Score: 0.0001)\n",
      "건조한 날씨에 산불을 조심해야 한다 (Score: 0.0000)\n",
      "한국은행은 올해 하반기 금리를 동결했다 (Score: 0.0000)\n",
      "내년 경제 성장은 4%대 성장을 이룰거라 예상된다 (Score: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 분류 문장을 불러와서, tokenize 한 다음, 모델에 넣고 출력값을 얻어옴\n",
    "out_dict = {}\n",
    "top_k = 5\n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    for idx, label in tqdm(enumerate(labels)):\n",
    "        if reverse_tokenizer == True:\n",
    "            tokenized_input = tokenizer(label, input_text, return_tensors=\"pt\")  #역으로 표현\n",
    "        else:\n",
    "            tokenized_input = tokenizer(input_text, label, return_tensors=\"pt\")\n",
    "       \n",
    "        token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "        #print(token_str)\n",
    "        #print(tokenized_input)\n",
    "\n",
    "        outputs = model(**tokenized_input)\n",
    "        #print(outputs)\n",
    "        logits = outputs.logits\n",
    "        #print(logits)\n",
    "\n",
    "        '''\n",
    "        # ouput_hidden_states = True일때 출력되는 hidden_state 값을 가지고 임베딩 구할수 있음\n",
    "        hidden_states = outputs.hidden_states\n",
    "        layer_idx = 0\n",
    "        batch_idx = 0\n",
    "        token_idx = 0\n",
    "        print('hidden_states')\n",
    "        print(\"-레이어 수:{}\".format(len(hidden_states)))\n",
    "        print(\"-배치 수: {}\".format(len(hidden_states[layer_idx])))\n",
    "        print(\"-토큰 수 : {}\".format(len(hidden_states[layer_idx][batch_idx])))\n",
    "        print(\"-hidden 유닛 수 : {}\".format(len(hidden_states[layer_idx][batch_idx][token_idx])))\n",
    "        '''\n",
    "\n",
    "        # logits 에 softmax 를 취해서 총합이 1이되는 확률 분포로 만듬\n",
    "        prob = logits.softmax(dim=1)\n",
    "        #print(prob)\n",
    "\n",
    "        # entailment(참) 일 확률 중에서 가장  높은 거 선택 \n",
    "        entailment_prob = prob[0][0]      # 참(수반)일 경우 확률\n",
    "        #contradiction_prob = prob[0][1]  #거짓(모순)일 경우 확률 \n",
    "        #netral_prob = prob[0][2]         # 중립일 경우 확률\n",
    "        #print(f'input:{input_text[0]}, label:{label[0]}, 참일 확률:{entailment_prob}')\n",
    "        \n",
    "              \n",
    "        # 사전 key로 순번으로 하고, entailment(참) 일 확률를 저장함\n",
    "        key = str(idx+1)\n",
    "        out_dict[key] = entailment_prob\n",
    "      \n",
    "        '''      \n",
    "        # 확률을 소수점 2자리에서 반올림\n",
    "        entailment_prob = round(prob[0][0].item(),2) # 참(수반)일 경우 확률\n",
    "        contradiction_prob = round(prob[0][1].item(),2) # 거짓(모순)일 경우 확률\n",
    "        netral_prob = round(prob[0][2].item(),2) # 중립일 경우 확률\n",
    "\n",
    "        print(f'input:{input_text}, label:{label}')\n",
    "        print(f'참:{entailment_prob}, 거짓:{contradiction_prob}, 중립:{netral_prob}')\n",
    "\n",
    "        if torch.argmax(prob) == 0:\n",
    "            pred = \"참(entailment)\"\n",
    "        elif torch.argmax(prob) == 1:\n",
    "            pred = \"거짓(contradiction)\"\n",
    "        else: \n",
    "            pred = \"중립(netral)\"\n",
    "\n",
    "        print(f'{pred}')\n",
    "        '''      \n",
    "\n",
    "    # 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "    sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "    #print(sorted_dict)\n",
    "\n",
    "    print(f'{input_text}\\n')\n",
    "\n",
    "    # 내립차순으로 정렬된 사전출력 \n",
    "    for count in (sorted_dict):\n",
    "        value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "        #print(value)\n",
    "        idx = int(count[0])\n",
    "        #print(idx)\n",
    "        #print(labels[idx-1][0])\n",
    "        print(labels[idx-1].strip(), \"(Score: %.4f)\" % (value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d32aef-a886-4445-952f-5e5b97e9f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
