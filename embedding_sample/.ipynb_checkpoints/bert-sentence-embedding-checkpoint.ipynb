{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee1bd61-1480-4322-a670-d64bd88ff866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gluonnlp as nlp     # GluonNLP는 버트를 간단하게 로딩하는 인터페이스를 제공하는 API 임\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel\n",
    "from utils import seed_everything, GPU_info, pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b4d96e-7383-4801-a859-547ddfcac820",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#config.json, pytorch_model.bin 같이 있는 폴더 지정\n",
    "model_path = 'model/bert-multilingual-cased_furter_pt_model_0216' \n",
    "\n",
    "# voab.txt, special_tokens_map.json, tokenizer_config.json,added_tokens.json 폴더 경로 지정\n",
    "vocab_path = \"model/bert-multilingual-cased_furter_pt_model_0216/vocab\"\n",
    "\n",
    "model_path = 'model/bert-multilingual-cased' \n",
    "vocab_path = \"model/bert-multilingual-cased/vocab\"\n",
    "\n",
    "model_path = 'model/my-model_kowikitext_0207' \n",
    "vocab_path = \"model/my-model_kowikitext_0207/vocab\"\n",
    "'''\n",
    "\n",
    "model_path = 'model/classification/bmc-ft-nsmc-cfmodel' \n",
    "vocab_path = \"model/classification/bmc-ft-nsmc-cfmodel/vocab\"\n",
    "\n",
    "# True로 해야, hidden_states 가 출력됨\n",
    "output_hidden_states = True\n",
    "#False로 지정하는 경우 일반적인 tuple을 리턴, True인 경우는 transformers.file_utils.ModelOutput 으로 리턴\n",
    "return_dict = False\n",
    "\n",
    "seed = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3bd1a2-b925-4e69-a22c-d2e339d0112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a405d84a-462a-4724-a7b7-2dd830f3594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 설정\n",
    "tokenizer = BertTokenizerFast.from_pretrained(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6a9c77-2854-4bbf-8fb4-81c587c9736e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 불러옴\n",
    "model = BertModel.from_pretrained(model_path, \n",
    "                                  output_hidden_states=output_hidden_states,\n",
    "                                  return_dict=return_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0903aac4-df6b-431b-a67a-ac8ed988f6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prameters:92186880\n"
     ]
    }
   ],
   "source": [
    "print('prameters:{}'.format(model.num_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966abd8c-127e-4ee7-be39-1b8934e67c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = ['오늘은 오후 부터 춥고 비가 올것 같다.']\n",
    "\n",
    "\n",
    "classification_sentence = [['오늘은 가끔 흐리고, 눈이 올수 있다'],\n",
    "                 ['여기 식당은 파스타가 맛있다'],\n",
    "                 ['오늘 증시는 내림으로 마감 하였다'],\n",
    "                 ['내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7cecf1-2e5e-4158-8a57-97d6922c3583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', '오늘', '##은', '오후', '부', '##터', '[UNK]', '비가', '올', '##것', '같다', '[UNK]', '[SEP]']]\n",
      "{'input_ids': tensor([[   2, 3419, 7086, 3434, 2423, 7609,    0, 2515, 3439, 5398,  831,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '오늘', '##은', '[UNK]', '흐', '##리', '##고', '[UNK]', '눈', '##이', '올', '##수', '있다', '[SEP]']]\n",
      "{'input_ids': tensor([[   2, 3419, 7086,    0, 5196, 6122, 5439,    0, 1535, 7096, 3439, 6629,\n",
      "         3862,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '여기', '식당', '##은', '파', '##스타', '##가', '맛', '##있다', '[SEP]']]\n",
      "{'input_ids': tensor([[   2, 3301, 3007, 7086, 4799, 6684, 5330, 1967, 7143,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '오늘', '증시', '##는', '내', '##림', '##으로', '마감', '하', '##였다', '[SEP]']]\n",
      "{'input_ids': tensor([[   2, 3419, 4294, 5760, 1434, 6136, 7078, 1908, 4924, 6946,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '내', '##일', '##은', '오전', '##에는', '흐', '##리지', '##만', '[UNK]', '오후', '##에는', '날씨', '##가', '좋겠다', '[SEP]']]\n",
      "{'input_ids': tensor([[   2, 1434, 7126, 7086, 3431, 6900, 5196, 6131, 6150,    0, 3434, 6900,\n",
      "         1408, 5330, 4205,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장을 불러와서 출력값들을 구함\n",
    "tokenized_input = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "print(token_str)\n",
    "print(tokenized_input)\n",
    "in_outputs = model(**tokenized_input)\n",
    "    \n",
    "# 분류 문장을 불러와서, tokenize 한 다음, 모델에 넣고 출력값을 얻어옴\n",
    "sequence_out_list = []\n",
    "pooled_out_list = []\n",
    "hidden_states_list = []\n",
    "\n",
    "for idx in range(len(classification_sentence)):\n",
    "    sentence = classification_sentence[idx][0]\n",
    "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "    print(token_str)\n",
    "    print(tokenized_input)\n",
    "    outputs = model(**tokenized_input)\n",
    "    sequence_out_list.append(outputs[0])\n",
    "    pooled_out_list.append(outputs[1])\n",
    "    hidden_states_list.append(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf0cf87-e592-4459-a6c3-a638c3315503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(pooled_out_list))\n",
    "print(pooled_out_list[0].shape)\n",
    "print(in_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8cf7b79-631a-4754-a053-93eb7cc50293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vs 0 유사도:tensor([[0.0127]])\n",
      "input vs 1 유사도:tensor([[0.4771]])\n",
      "input vs 2 유사도:tensor([[0.0577]])\n",
      "input vs 3 유사도:tensor([[0.5098]])\n"
     ]
    }
   ],
   "source": [
    "# case1) pooled_out 값으로 유사도 측정\n",
    "in_list = in_outputs[1].tolist()\n",
    "out_dict = {}\n",
    "\n",
    "for idx in range(len(pooled_out_list)):\n",
    "    out_list = pooled_out_list[idx][0].tolist()\n",
    "    simul_score = pytorch_cos_sim(in_list, out_list)\n",
    "    \n",
    "    # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    print(f'순번:{count[0]}, 유사도:{value[0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73e68ca-18fb-481b-939d-abaf92e9fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "# case 2) sequence_output 평균값으로 유사도 측정\n",
    "\n",
    "print(len(sequence_out_list))\n",
    "print(sequence_out_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9c29e6-675c-4a26-b272-076dface05a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "input vs 0 유사도:tensor([[0.8128]], grad_fn=<MmBackward0>)\n",
      "input vs 1 유사도:tensor([[0.6102]], grad_fn=<MmBackward0>)\n",
      "input vs 2 유사도:tensor([[0.6673]], grad_fn=<MmBackward0>)\n",
      "input vs 3 유사도:tensor([[0.8393]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in_mean_sequence = torch.mean(in_outputs[0], dim=1)\n",
    "print(in_mean_sequence.shape)\n",
    "out_dict = {}\n",
    "\n",
    "for idx in range(len(sequence_out_list)):\n",
    "    out_mean_sequence = torch.mean(sequence_out_list[idx], dim=1)\n",
    "    simul_score = pytorch_cos_sim(in_mean_sequence, out_mean_sequence)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    print(f'순번:{count[0]}, 유사도:{value[0][0]}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b47e17-e1c2-4f82-9079-be39f8d70936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vs 0 유사도:tensor([[0.8753]], grad_fn=<MmBackward0>)\n",
      "input vs 1 유사도:tensor([[0.7472]], grad_fn=<MmBackward0>)\n",
      "input vs 2 유사도:tensor([[0.7922]], grad_fn=<MmBackward0>)\n",
      "input vs 3 유사도:tensor([[0.8801]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# case 3) hidden_state 마지막 2번째 레이어의 평균값으로 유사도 측정\n",
    "out_dict = {}\n",
    "ebd = torch.stack(list(in_outputs[2]), dim=0)\n",
    "# 마지막 2번째(-2) 레이어에 평균을 구한다. \n",
    "in_hidden = torch.mean(ebd[-2][0], dim=0)\n",
    "  \n",
    "for idx,embedding in enumerate(hidden_states_list): # enumerate는 index, value 값이 리턴됨\n",
    "    #print('index: {:d}, type: {}'.format(idx, type(embedding)))\n",
    "    \n",
    "    # hidden_state_list[0]은 tuple 이므로 이를 tensor로 변환 하면서, dim=0 하여 레이어를 결합해서 큰 텐서[13,1,128,768] 를 만든다.\n",
    "    ebd = torch.stack(list(embedding), dim=0)\n",
    "    \n",
    "    # 마지막 2번째(-2) 레이어에 평균을 구한다. \n",
    "    out_hidden = torch.mean(ebd[-2][0], dim=0)\n",
    "    simul_score = pytorch_cos_sim(in_hidden, out_hidden)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    print(f'순번:{count[0]}, 유사도:{value[0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08060b11-3009-42b4-b556-12dc10eb430f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925b71e-82b3-4ddf-b1a9-e551317dc4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
