{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1801c86a-1498-4a64-b290-e4a22d36bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-21.log\n",
      "logfilepath:qnadataset_2022-03-21.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:bertembedding_2022-03-21.log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gluonnlp as nlp     # GluonNLP는 버트를 간단하게 로딩하는 인터페이스를 제공하는 API 임\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# myutils 패키지 import\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import seed_everything, GPU_info, pytorch_cos_sim, mlogging\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bertembedding\", logfilname=\"bertembedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21830db-fcdf-4726-9dda-a8eda1bf7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache_dir = '../model/distilbert-base-multilingual-cased' \n",
    "#model_vocab_path = '../model/distilbert-base-multilingual-cased'\n",
    "model_vocab_path = '../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_vocab_path, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383fcf52-4387-469f-86bf-66381a4d245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321-student were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(143773, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 불러옴\n",
    "model = BertModel.from_pretrained(model_vocab_path)\n",
    "model.eval()\n",
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb75bfc-7651-4fd2-bd62-d5a02593c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 08:53:07,535 - bertembedding - INFO - === model: ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321-student ===\n",
      "2022-03-22 08:53:07,538 - bertembedding - INFO - num_parameters: 153931776\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"=== model: {model_vocab_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2761f02-bee7-4aa1-9888-62a5455e2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101, 120060,  10892,   9379,  11287,   9580,  11664, 126747,  11287,\n",
      "         141714, 118632,  11903,    102,      0,      0,      0,      0],\n",
      "        [   101, 120060,  10892,   9034,  10739,   9580,  11664, 126747,  11287,\n",
      "         141714, 118632,  11903,    102,      0,      0,      0,      0],\n",
      "        [   101, 120060,  10892,   8843, 118707, 141714,  11664,    117,   9034,\n",
      "          10739,   9583,  15891,  11506,    102,      0,      0,      0],\n",
      "        [   101, 119778, 123967,  10892, 140367,  11287, 130647,  11903,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 120060,   9705,  14040,  11018, 132191,  11467, 124163,  28750,\n",
      "            102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 123491,  10892, 121409,  15303, 141714,  28578,    117, 120648,\n",
      "          15303, 126747,  11287,   9685, 118632,  11903,    102,      0],\n",
      "        [   101,  48253,  10892,  26168,  10530,  69283,  33542,    117, 119623,\n",
      "         119642, 122452,  11925,    102,      0,      0,      0,      0],\n",
      "        [   101, 135332, 119642, 120311,  10892,    125,    110,   9069, 120311,\n",
      "          10622,   9638, 118891,  41521,  17342, 121584,  22096,    102],\n",
      "        [   101,  47364, 120426,  11018, 130715,  21611, 126881,  20173, 121892,\n",
      "           9737,  11018, 120438,  12092,  14040,  11925,    102,      0],\n",
      "        [   101, 121111,  15303,  70672, 119610,  12638, 105383, 127259,  11287,\n",
      "          58248, 131045,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 120060, 126747,  11018,   9379,  11287, 121655,  11664,  42608,\n",
      "            100,    102,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9450, 119444,  36553,  10739,  50266, 122368,  37905,  81785,\n",
      "          10193,  48506,   8892,  10622,   9010,  17706,    102,      0],\n",
      "        [   101, 122494,  11102, 126747,  10530, 136974,  10622, 129170, 108436,\n",
      "          16139,    102,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 121650,  37115,  18398,  10530, 121179, 119788,  10622, 120250,\n",
      "          28750,    102,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 138242,  10892, 121111, 131541, 136549,  11513, 138962,  12490,\n",
      "            102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 126747,  11287,   9685,  11903,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9521, 125213,  54780,  28911,   9303,    119,   8904, 119250,\n",
      "          16985,    119,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 123491,   9663, 118738,  18382, 120567, 119962,  11287, 132180,\n",
      "          23925, 121584, 129727,    119,    102,      0,      0,      0],\n",
      "        [   101, 120376, 129727,    119, 131172,  40364, 119687,  10739,   9100,\n",
      "         123269,    119,    102,      0,      0,      0,      0,      0],\n",
      "        [   101, 129764, 121409,  10150,  14040,  10530, 120135,  31398,   9952,\n",
      "         119217,    119, 123491,    100,    119,    102,      0,      0],\n",
      "        [   101, 121809,  15184, 137386,  80795,   9056, 126034, 119081,  48345,\n",
      "            119,   9074, 122993,  14843, 121158,    136,    102,      0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [\n",
    "    '오늘은 비가 오고 날씨가 흐리겠다',\n",
    "    '오늘은 눈이 오고 날씨가 흐리겠다',\n",
    "    '오늘은 가끔 흐리고, 눈이 올수 있다',\n",
    "    '여기 식당은 파스타가 맛있다',\n",
    "    '오늘 증시는 내림으로 마감 하였다',\n",
    "    '내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다',\n",
    "    '서울은 대한민국에 수도이며, 정치 경제 중심지이다',\n",
    "    '내년 경제 성장은 4%대 성장을 이룰거라 예상된다',\n",
    "    '프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다',\n",
    "    '올해에는 대통령 선거와 지방선거가 동시에 열린다',\n",
    "    '오늘 날씨는 비가 내리고 매우 춥다',\n",
    "    '손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다',\n",
    "    '건조한 날씨에 산불을 조심해야 한다',\n",
    "    '윈도우11 OS에 검색 기능을 강화 하였다',\n",
    "    '한국은행은 올해 하반기 금리를 동결했다',\n",
    "    '날씨가 좋다',\n",
    "    '안 그래도 되는데 뭐. 괜찮아.',\n",
    "    '내일 저녁까지 보수 공사가 끝날 것으로 예상합니다.',\n",
    "    '감사합니다. 후회 없는 결정이 될 겁니다.',\n",
    "    '그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다.',\n",
    "    '프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?'\n",
    "    ]\n",
    "\n",
    "# ** 멀티로 한번에 tokenizer 할때는 반드시 padding=True 해야 함.(그래야 최대 길이 token에 맞춰서 padding 됨)\n",
    "test = tokenizer(test_sentence, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=256, \n",
    "                 return_tensors=\"pt\")\n",
    "print(test)\n",
    "print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bee70d-7945-4f93-9865-c83a42aa86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 08:53:09,350 - bertembedding - INFO - === model 처리시간: 0.152 초 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 17, 768])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "output = model(**test)\n",
    "\n",
    "logger.info(f'=== model 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "\n",
    "# sequence_state\n",
    "last_hidden_state = output[0]\n",
    "print(last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a0f0b2-c3e5-48ea-8292-be0585221910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 08:53:10,367 - bertembedding - INFO - ---------------------------------------------------------\n",
      "2022-03-22 08:53:10,368 - bertembedding - INFO - 오늘은 비가 오고 날씨가 흐리겠다, 유사도:0.9999995231628418\n",
      "2022-03-22 08:53:10,369 - bertembedding - INFO - 오늘은 눈이 오고 날씨가 흐리겠다, 유사도:0.9816093444824219\n",
      "2022-03-22 08:53:10,370 - bertembedding - INFO - 오늘 날씨는 비가 내리고 매우 춥다, 유사도:0.9459734559059143\n",
      "2022-03-22 08:53:10,371 - bertembedding - INFO - 오늘은 가끔 흐리고, 눈이 올수 있다, 유사도:0.921137273311615\n",
      "2022-03-22 08:53:10,371 - bertembedding - INFO - 내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다, 유사도:0.9105890989303589\n",
      "2022-03-22 08:53:10,372 - bertembedding - INFO - 건조한 날씨에 산불을 조심해야 한다, 유사도:0.9032831192016602\n",
      "2022-03-22 08:53:10,373 - bertembedding - INFO - 내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:0.902162492275238\n",
      "2022-03-22 08:53:10,374 - bertembedding - INFO - 오늘 증시는 내림으로 마감 하였다, 유사도:0.8849372267723083\n",
      "2022-03-22 08:53:10,374 - bertembedding - INFO - 올해에는 대통령 선거와 지방선거가 동시에 열린다, 유사도:0.8835422396659851\n",
      "2022-03-22 08:53:10,375 - bertembedding - INFO - 안 그래도 되는데 뭐. 괜찮아., 유사도:0.8791847825050354\n",
      "2022-03-22 08:53:10,376 - bertembedding - INFO - 그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.8769314289093018\n",
      "2022-03-22 08:53:10,377 - bertembedding - INFO - 감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.8752756714820862\n",
      "2022-03-22 08:53:10,377 - bertembedding - INFO - 날씨가 좋다, 유사도:0.8706161975860596\n",
      "2022-03-22 08:53:10,378 - bertembedding - INFO - 프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:0.8525866866111755\n",
      "2022-03-22 08:53:10,379 - bertembedding - INFO - 한국은행은 올해 하반기 금리를 동결했다, 유사도:0.850116491317749\n",
      "2022-03-22 08:53:10,380 - bertembedding - INFO - 여기 식당은 파스타가 맛있다, 유사도:0.8493297696113586\n",
      "2022-03-22 08:53:10,380 - bertembedding - INFO - 윈도우11 OS에 검색 기능을 강화 하였다, 유사도:0.8473197817802429\n",
      "2022-03-22 08:53:10,381 - bertembedding - INFO - 서울은 대한민국에 수도이며, 정치 경제 중심지이다, 유사도:0.8337460160255432\n",
      "2022-03-22 08:53:10,382 - bertembedding - INFO - 손홍민이 영국 프리미어 축구 경기에서 11번째 골을 넣었다, 유사도:0.8150359988212585\n",
      "2022-03-22 08:53:10,382 - bertembedding - INFO - 프랑스 파리는 전세계 관광객들이 매년 찾는 관광도시이다, 유사도:0.79215008020401\n",
      "2022-03-22 08:53:10,383 - bertembedding - INFO - 내년 경제 성장은 4%대 성장을 이룰거라 예상된다, 유사도:0.7890520095825195\n",
      "2022-03-22 08:53:10,384 - bertembedding - INFO - ---------------------------------------------------------\n",
      "2022-03-22 08:53:10,385 - bertembedding - INFO - === 유사도 처리시간: 0.029 초 ===\n",
      "2022-03-22 08:53:10,386 - bertembedding - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = torch.mean(last_hidden_state[0], dim=0)\n",
    "#print(in_mean_sequence.shape)\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(last_hidden_state):\n",
    "    out_mean_sequence = torch.mean(hidden, dim=0)\n",
    "    simul_score = pytorch_cos_sim(in_mean_sequence, out_mean_sequence)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    #print(test_sentence[index-1])\n",
    "    logger.info(f'{test_sentence[index-1]}, 유사도:{value[0][0]}')\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685ce89-c91a-47d1-bc27-daa1cf21fcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f6882-0772-4349-a2bf-21124ae6c095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
