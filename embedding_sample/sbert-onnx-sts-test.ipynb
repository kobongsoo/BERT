{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290d1743-5796-4f2c-8ee2-50fad01c6dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:sbert-optimized_2022-08-26.log\n"
     ]
    }
   ],
   "source": [
    "#===============================================================================================\n",
    "# onnx 모델 혹은 sbert 모델을 sts 말뭉치로 테스트 하는 예시\n",
    "# - sentence_transformer 에 EmbeddingSimilarityEvaluator 를 참조항\n",
    "# - 참고 : https://github.com/UKPLab/sentence-transformers/blob/957c87b3b4cabb96049e9991c7b77624736188af/sentence_transformers/evaluation/EmbeddingSimilarityEvaluator.py#L5\n",
    "#\n",
    "#\n",
    "#===============================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "logger = mlogging(loggername=\"sbert-optimized\", logfilename=\"sbert-optimized\")\n",
    "seed_everything(111)\n",
    "\n",
    "# sbert 모델 경로\n",
    "smodel_path = \"bongsoo/sentencebert_v1.2\"\n",
    "\n",
    "# onnx 모델 경로 \n",
    "onnxmodel_path = \"../../data11/model/onnx/sentencebert_v1.2-onnx\"\n",
    "\n",
    "# onnx 모델일때,  모델출력 embedding 값을 어떻게 만들어서 비교할지 type 설정값\n",
    "# 0=2차원으로 reshape 해서 비교, 1=평균값으로 비교 \n",
    "embed_type = 1  \n",
    "\n",
    "# 평가 sts 형태의 test 파일 \n",
    "test_file_type = 2  # 0이면 korsts, 1이면 kluests 파일, 2이면 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643a376f-e4aa-4d59-a1ce-32f8fa03e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<optimum.onnxruntime.modeling_ort.ORTModelForFeatureExtraction object at 0x7f6da03f5f40>\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# 양자화 모델 불러옴\n",
    "# => 양자화 모델은 model.eval() 하면 에러남.\n",
    "#============================================================\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnxmodel_path)\n",
    "\n",
    "# 문장임베딩이면, ORTModelForFeatureExtraction 호출\n",
    "onnxmodel = ORTModelForFeatureExtraction.from_pretrained(onnxmodel_path)\n",
    "print(onnxmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b392e24-ecc7-4f77-9a21-d60570cfc93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/huggingface_hub/file_download.py:621: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "#sentence bert 원래 모델 로딩 \n",
    "#============================================================\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# device='cpu'로 함\n",
    "smodel = SentenceTransformer(smodel_path, device='cpu')\n",
    "print(smodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99fc5d1-3844-4b3f-a8c0-b916ce614be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/korpora/korsts/tune_test.tsv-len: 1379\n",
      "s1: 한 소녀가 머리를 스타일링하고 있다.\n",
      "s2: 한 소녀가 머리를 빗고 있다.\n",
      "scores: 0.5\n",
      "../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json-len: 1898\n",
      "s1: 여느 포르투갈의 비앤비와 같이 엘리베이터는 없습니다.\n",
      "s2: 포르투의 거의 모든 숙박 시설은 엘리베이터는 없습니다.\n",
      "scores: 2.9\n"
     ]
    }
   ],
   "source": [
    "sentence1 = []\n",
    "sentence2 = []\n",
    "scores = [] \n",
    "    \n",
    "#============================================================\n",
    "# korsts 로딩\n",
    "#============================================================\n",
    "if test_file_type == 0 or test_file_type == 2:\n",
    "    \n",
    "    test_file1 = '../../data11/korpora/korsts/tune_test.tsv'\n",
    "\n",
    "    with open(test_file1, 'rt', encoding='utf-8') as fIn1:\n",
    "        lines = fIn1.readlines()\n",
    "        for line in lines:\n",
    "            s1, s2, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0\n",
    "\n",
    "            sentence1.append(s1)\n",
    "            sentence2.append(s2)\n",
    "            scores.append(score)\n",
    "\n",
    "    print(f'{test_file1}-len: {len(sentence1)}')\n",
    "    print(f's1: {sentence1[0]}')\n",
    "    print(f's2: {sentence2[0]}')\n",
    "    print(f'scores: {scores[0]}')\n",
    "\n",
    "#============================================================\n",
    "# kluests 로딩\n",
    "#============================================================\n",
    "if test_file_type == 1 or test_file_type == 2:\n",
    "    \n",
    "    test_file2 = '../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json'\n",
    "    import json\n",
    "\n",
    "    with open(test_file2, \"r\") as fIn2:\n",
    "        data = json.load(fIn2)\n",
    "        for el in data:\n",
    "            s1 = el[\"sentence1\"]\n",
    "            s2 = el[\"sentence2\"]\n",
    "            score = el[\"labels\"]['label']\n",
    "\n",
    "            sentence1.append(s1)\n",
    "            sentence2.append(s2)\n",
    "            scores.append(score)\n",
    "\n",
    "    print(f'{test_file2}-len: {len(sentence1)}')\n",
    "    print(f's1: {sentence1[-1]}')\n",
    "    print(f's2: {sentence2[-1]}')\n",
    "    print(f'scores: {scores[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2535c79-9f99-4f1d-bf9d-d40c9ec1eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3796\n",
      "{'input_ids': tensor([[   101,   9954, 121260,  ...,      0,      0,      0],\n",
      "        [   101,   9954, 120936,  ...,      0,      0,      0],\n",
      "        [   101,   9954, 100006,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   101, 120851, 119737,  ...,      0,      0,      0],\n",
      "        [   101, 119559, 135617,  ...,      0,      0,      0],\n",
      "        [   101, 123002,  10459,  ...,      0,      0,      0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([[   101,   9954, 121260,  ...,      0,      0,      0],\n",
      "        [   101,   9954, 120936,  ...,      0,      0,      0],\n",
      "        [   101,   9954, 100006,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   101, 120851, 119737,  ...,      0,      0,      0],\n",
      "        [   101, 119559, 135617,  ...,      0,      0,      0],\n",
      "        [   101, 123002,  10459,  ...,      0,      0,      0]])\n",
      "type:<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "torch.Size([3796, 54])\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# tokenize 처리 \n",
    "# ** 멀티로 한번에 tokenizer 할때는 반드시 padding=True 해야 함.(그래야 최대 길이 token에 맞춰서 padding 됨)\n",
    "#============================================================\n",
    "# sentence1 + sentence2를 묶어서 tokenizer 처리함\n",
    "corpus = sentence1 + sentence2\n",
    "print(len(corpus))\n",
    "\n",
    "corpus_inputs = tokenizer(corpus, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\")\n",
    "\n",
    "print(corpus_inputs)\n",
    "print(corpus_inputs['input_ids'])\n",
    "print(f'type:{type(corpus_inputs)}')\n",
    "print(corpus_inputs['input_ids'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7692eb76-c903-4369-a1e3-ea448ae2edfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 13:22:35,184 - sbert-optimized - INFO - *평균 embedding 사용\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_len:torch.Size([3796, 54, 768])\n",
      "1898\n",
      "torch.Size([1898, 54, 768])\n",
      "torch.Size([1898, 54, 768])\n",
      "(768,)\n",
      "(768,)\n",
      "<class 'numpy.ndarray'>\n",
      "1898\n",
      "0.851942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 13:22:35,390 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:22:35,390 - sbert-optimized - INFO - onnx모델: ../../data11/model/onnx/sentencebert_v1.2-onnx\n",
      "2022-08-26 13:22:35,391 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:22:35,392 - sbert-optimized - INFO - === pearson_cosine: 0.41840956189397926 ===\n",
      "2022-08-26 13:22:35,392 - sbert-optimized - INFO - === spearman_cosine: 0.6203559496512039 ===\n",
      "2022-08-26 13:22:35,393 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:22:35,393 - sbert-optimized - INFO - === pearson_manhattan: 0.4272547480861082 ===\n",
      "2022-08-26 13:22:35,393 - sbert-optimized - INFO - === spearman_manhattan: 0.6402805941144337 ===\n",
      "2022-08-26 13:22:35,394 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:22:35,394 - sbert-optimized - INFO - === pearson_euclidean: 0.42652507993926225 ===\n",
      "2022-08-26 13:22:35,395 - sbert-optimized - INFO - === spearman_euclidean: 0.6374318022910255 ===\n",
      "2022-08-26 13:22:35,396 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:22:35,397 - sbert-optimized - INFO - === pearson_dot: 0.2794458103742085 ===\n",
      "2022-08-26 13:22:35,397 - sbert-optimized - INFO - === spearman_dot: 0.4842905069473938 ===\n",
      "2022-08-26 13:22:35,398 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:22:35,399 - sbert-optimized - INFO - === 처리시간: 19.193 초 ===\n",
      "2022-08-26 13:22:35,399 - sbert-optimized - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#============================================================\n",
    "# onnx 모델 임베딩 구하기\n",
    "#============================================================\n",
    "outputs = onnxmodel(**corpus_inputs)\n",
    "embedding = outputs.last_hidden_state\n",
    "print(f'embed_len:{embedding.shape}')\n",
    "\n",
    "# 구한 embeding 값을 sentence1, sentence2 로 나눔.\n",
    "embed_len = len(embedding)//2\n",
    "print(embed_len)\n",
    "tembed1 = embedding[0:embed_len]\n",
    "tembed2 = embedding[embed_len:]\n",
    "\n",
    "print(tembed1.shape)\n",
    "print(tembed2.shape)\n",
    "\n",
    "if embed_type == 0:\n",
    "    logger.info(f'*2D reshape embedding 사용')\n",
    "        \n",
    "    # 3차원을 -> 2차원으로  reshape 시키고, numpy()로 만듬\n",
    "    # - 예: [1379, 52, 768] -> (1379, 39936)\n",
    "    embedlist1 = tembed1.reshape(embed_len, -1).numpy()\n",
    "    embedlist2 = tembed2.reshape(embed_len, -1).numpy() \n",
    "\n",
    "    print(embedlist1.shape)\n",
    "    print(embedlist1.shape)\n",
    "    \n",
    "elif embed_type == 1:\n",
    "    logger.info(f'*평균 embedding 사용')\n",
    "    \n",
    "    # 아래는 1차원 평균값으로 만드는 예시임\n",
    "    # => paired_cosine_distances 를 사용하려면, >= 2D numpy 배열로 만들어야 함.\n",
    "    # => embed1,2는 3차원->1차원 평균값으로 만듬\n",
    "    embedlist1 = []\n",
    "    for idx,embedding in enumerate(tembed1): # enumerate는 index, value 값이 리턴됨\n",
    "        embed = torch.mean(embedding, dim=0).numpy()\n",
    "        embedlist1.append(embed)\n",
    "\n",
    "    embedlist2 = []\n",
    "    for idx,embedding in enumerate(tembed2): # enumerate는 index, value 값이 리턴됨\n",
    "        embed = torch.mean(embedding, dim=0).numpy()\n",
    "        embedlist2.append(embed)\n",
    "\n",
    "    print(embedlist1[0].shape)\n",
    "    print(embedlist2[0].shape)\n",
    "\n",
    "\n",
    "# sklearn 을 이용하여 cosine_scores를 구함\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "\n",
    "cosine_scores = 1 - (paired_cosine_distances(embedlist1, embedlist2))\n",
    "\n",
    "manhattan_distances = -paired_manhattan_distances(embedlist1, embedlist2)\n",
    "\n",
    "euclidean_distances = -paired_euclidean_distances(embedlist1, embedlist2)\n",
    "\n",
    "dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(embedlist1, embedlist2)]\n",
    "        \n",
    "print(type(cosine_scores))\n",
    "print(len(cosine_scores))\n",
    "print(cosine_scores[0])\n",
    "\n",
    "# pearson 과 spearman 평균을 구함.\n",
    "# => 실제 sts문장들 scores와 모델에서 구한 cosine_scores를 비교하여 Acc 평균 값들을 구함\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "eval_pearson_cosine, _ = pearsonr(scores, cosine_scores)\n",
    "eval_spearman_cosine, _ = spearmanr(scores, cosine_scores)\n",
    "\n",
    "eval_pearson_manhattan, _ = pearsonr(scores, manhattan_distances)\n",
    "eval_spearman_manhattan, _ = spearmanr(scores, manhattan_distances)\n",
    "\n",
    "eval_pearson_euclidean, _ = pearsonr(scores, euclidean_distances)\n",
    "eval_spearman_euclidean, _ = spearmanr(scores, euclidean_distances)\n",
    "\n",
    "eval_pearson_dot, _ = pearsonr(scores, dot_products)\n",
    "eval_spearman_dot, _ = spearmanr(scores, dot_products)\n",
    "        \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'onnx모델: {onnxmodel_path}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_cosine: {eval_pearson_cosine} ===')\n",
    "logger.info(f'=== spearman_cosine: {eval_spearman_cosine} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_manhattan: {eval_pearson_manhattan} ===')\n",
    "logger.info(f'=== spearman_manhattan: {eval_spearman_manhattan} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_euclidean: {eval_pearson_euclidean} ===')\n",
    "logger.info(f'=== spearman_euclidean: {eval_spearman_euclidean} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_dot: {eval_pearson_dot} ===')\n",
    "logger.info(f'=== spearman_dot: {eval_spearman_dot} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "505bf672-c136-40cb-b5c6-8cff52c86d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 13:27:37,447 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:27:37,449 - sbert-optimized - INFO - sbert모델: bongsoo/sentencebert_v1.2\n",
      "2022-08-26 13:27:37,450 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:27:37,451 - sbert-optimized - INFO - === pearson_cosine: 0.42790362083793543 ===\n",
      "2022-08-26 13:27:37,452 - sbert-optimized - INFO - === spearman_cosine: 0.6305919207416886 ===\n",
      "2022-08-26 13:27:37,452 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:27:37,453 - sbert-optimized - INFO - === pearson_manhattan: 0.4538038691905121 ===\n",
      "2022-08-26 13:27:37,454 - sbert-optimized - INFO - === spearman_manhattan: 0.67585103663608 ===\n",
      "2022-08-26 13:27:37,455 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:27:37,456 - sbert-optimized - INFO - === pearson_euclidean: 0.4548321135918387 ===\n",
      "2022-08-26 13:27:37,457 - sbert-optimized - INFO - === spearman_euclidean: 0.6749659801283853 ===\n",
      "2022-08-26 13:27:37,458 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:27:37,459 - sbert-optimized - INFO - === pearson_dot: 0.28690556223566116 ===\n",
      "2022-08-26 13:27:37,460 - sbert-optimized - INFO - === spearman_dot: 0.5037139235690637 ===\n",
      "2022-08-26 13:27:37,461 - sbert-optimized - INFO - ---------------------------------------------------------\n",
      "2022-08-26 13:27:37,462 - sbert-optimized - INFO - === 처리시간: 46.445 초 ===\n",
      "2022-08-26 13:27:37,463 - sbert-optimized - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1898, 768)\n",
      "(1898, 768)\n",
      "<class 'numpy.ndarray'>\n",
      "1898\n",
      "0.7331225\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#============================================================\n",
    "# sbert 모델 임베딩 구하기\n",
    "#============================================================\n",
    "embeddings1 = smodel.encode(sentence1, batch_size = len(sentence1), convert_to_numpy=True)\n",
    "embeddings2 = smodel.encode(sentence2, batch_size = len(sentence2), convert_to_numpy=True)\n",
    "\n",
    "print(embeddings1.shape)\n",
    "print(embeddings2.shape)\n",
    "\n",
    "# sklearn 을 이용하여 cosine_scores를 구함\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "\n",
    "cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
    "\n",
    "manhattan_distances = -paired_manhattan_distances(embeddings1, embeddings2)\n",
    "\n",
    "euclidean_distances = -paired_euclidean_distances(embeddings1, embeddings2)\n",
    "\n",
    "dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(embeddings1, embeddings2)]\n",
    "\n",
    "print(type(cosine_scores))\n",
    "print(len(cosine_scores))\n",
    "print(cosine_scores[0])\n",
    "\n",
    "# pearson 과 spearman 평균을 구함.\n",
    "# => 실제 sts문장들 scores와 모델에서 구한 cosine_scores를 비교하여 Acc 평균 값들을 구함\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "eval_pearson_cosine, _ = pearsonr(scores, cosine_scores)\n",
    "eval_spearman_cosine, _ = spearmanr(scores, cosine_scores)\n",
    "\n",
    "eval_pearson_manhattan, _ = pearsonr(scores, manhattan_distances)\n",
    "eval_spearman_manhattan, _ = spearmanr(scores, manhattan_distances)\n",
    "\n",
    "eval_pearson_euclidean, _ = pearsonr(scores, euclidean_distances)\n",
    "eval_spearman_euclidean, _ = spearmanr(scores, euclidean_distances)\n",
    "\n",
    "eval_pearson_dot, _ = pearsonr(scores, dot_products)\n",
    "eval_spearman_dot, _ = spearmanr(scores, dot_products)\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'sbert모델: {smodel_path}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_cosine: {eval_pearson_cosine} ===')\n",
    "logger.info(f'=== spearman_cosine: {eval_spearman_cosine} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_manhattan: {eval_pearson_manhattan} ===')\n",
    "logger.info(f'=== spearman_manhattan: {eval_spearman_manhattan} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_euclidean: {eval_pearson_euclidean} ===')\n",
    "logger.info(f'=== spearman_euclidean: {eval_spearman_euclidean} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== pearson_dot: {eval_pearson_dot} ===')\n",
    "logger.info(f'=== spearman_dot: {eval_spearman_dot} ===')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836bfcd-fcff-448c-b10c-8348b9f32c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
