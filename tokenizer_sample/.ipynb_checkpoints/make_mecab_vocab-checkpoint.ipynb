{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6013926b-4331-43f9-9a93-13d0b3cb34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# 기존 bert vocab에 새로운 도메인 vocab만들어서 추가하기\n",
    "#\n",
    "# =>기존 pre-trained bert vocab에는 전문단어 domain이 없다(예:문서중앙화, COVID 등)  \n",
    "# 따라서 전문 domain을 기존 bert vocab에 추가해야 한다.\n",
    "# => 따라서 여기서는 mecab(한국어 형태로 분석기)를 이용하여, 추가 vocab을 만들고, \n",
    "# bert wordpiecetokenzer에 추가하는 방법에 대해 설명한다\n",
    "# \n",
    "# 참고 자료 \n",
    "# https://github.com/piegu/language-models/blob/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb\n",
    "# https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41\n",
    "# https://wikidocs.net/64517\n",
    "#\n",
    "# [과정]\n",
    "# 1) 도메인 말뭉치(예:kowiki.txt)에서 mecab을 이용하여 형태소 분석하여 단어들을 추출함.\n",
    "#   => mecab으로 형태소 혹은 명사만 분할하면서, subword 앞에는 '##' prefix 추가함(**ertwordpiece subword와 동일하게)\n",
    "# 2) NLTK 빈도수 계산하는 FreqDist()를 이용하여, 단어들이 빈도수 계산\n",
    "# 3) 상위 빈도수(예:30000)를 가지는 단어들만 add_vocab.txt 로 만듬\n",
    "# 4) 기존 bert vocab.txt 파일에 직접, add_vocab.txt 토큰들 추가(*이때 중복 제거함)\n",
    "# 5) 추가한 vocab을 가지고, tokenizer 생성하고, special 토큰 추가 후, 저장\n",
    "#\n",
    "# **원래 참고자료에는 tokenizer.add_tokens() 으로 추가하면, added_tokens.json 파일에 추가되는데, \n",
    "# 이때 BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름:엄청 느려지는것 같음)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#==============================================================================================\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db39d2f-f474-482b-95dd-8321d62d1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제임스', '얼', '\"지미\"']\n",
      "43783339\n"
     ]
    }
   ],
   "source": [
    "# wiki_20190620.txt 말뭉치 불러옴.\n",
    "corpus = '../korpora/kowiki_20190620/wiki_20190620.txt'\n",
    "#corpus = '../korpora/kowiki_20190620/wiki_20190620_small.txt'\n",
    "\n",
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split(' ') # 공백으로 구분해서 단어들을 추출함\n",
    "\n",
    "print(data[:3])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f16ffd7-2e89-4057-83d0-9db4a468ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 형태소 분석기를 이용하여, 읽어온 말뭉치를 단어,조사등으로 분리함\n",
    "# => 불용어는 제거함\n",
    "# => mecab으로 형태소 분할하면서, subword 앞에는 prefix '##' 추가함\n",
    "\n",
    "# Mecab 선언\n",
    "mecab = Mecab()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords=['이','가','께서','에서','이다','의','을','를','에','에게','께','와','에서', \n",
    "           '라고', '과','은', '는', '부터','.',',', '_']\n",
    "\n",
    "# Ture = nouns(명사)만 추출, False=형태소 추출\n",
    "nouns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1204a00d-63a6-4fab-a4a3-584f3b0d2ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a1c627de0f4ff687122430313559e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43783339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제임스', '얼', '지미', '카터', '주니어', '민주당', '출신', '미국', '번', '대통령', '지미', '카터', '조지아주', '섬터', '카운티', '플', '##레인스', '마을', '공과', '##대학교']\n",
      "총 단어 수: 40479100\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "total_morph=[]\n",
    "for sentence in tqdm(data):\n",
    "    temp = mecab.morphs(sentence)\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    total_morph.append(temp)\n",
    "'''\n",
    "# mecab으로 형태소 혹은 명사만 분할할때, subword 앞에는 prefix '##' 추가함\n",
    "total_words=[]\n",
    "\n",
    "# 명사만 추출하는 경우\n",
    "if nouns == True:\n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.nouns(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count > 0:\n",
    "                    tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "# 형태소도 포함하여 추출하는 경우\n",
    "else:\n",
    "    \n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.morphs(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count > 0:\n",
    "                    tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "                \n",
    "print(total_words[:20])\n",
    "print(f'총 단어 수: {len(total_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14abac86-e25f-46f1-a12e-f6afb27d6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 367039\n",
      "[('년', 785772), ('월', 399363), ('것', 371028), ('일', 361400), ('수', 241798), ('##년', 237837), ('그', 155094), ('등', 152632), ('때', 126433), ('중', 116365), ('사용', 114881), ('후', 109502), ('이후', 95227), ('개', 93090), ('말', 91918), ('번', 88053), ('때문', 87041), ('시작', 82608), ('미국', 81315), ('사람', 76305), ('세', 74809), ('경우', 72749), ('지역', 70527), ('일본', 67279), ('명', 67069), ('기록', 65720), ('위', 64338), ('전', 62406), ('경기', 61580), ('다음', 60499), ('이름', 60194), ('시', 59094), ('세계', 57161), ('하나', 54882), ('당시', 53746), ('뒤', 52992), ('국가', 52477), ('만', 51251), ('대한민국', 50991), ('차', 48779), ('활동', 48687), ('선수', 47477), ('사이', 46564), ('조선', 46177), ('팀', 43399), ('포함', 43278), ('위치', 43067), ('시즌', 42418), ('한국', 42147), ('현재', 40753), ('모두', 40648), ('가능', 40579), ('신의', 40451), ('영국', 40361), ('등의', 39763), ('내', 39171), ('정부', 38319), ('주장', 38290), ('처음', 38055), ('존재', 37586), ('중국', 37549), ('대', 37219), ('점', 36434), ('정도', 36423), ('리그', 36326), ('전쟁', 36248), ('독일', 36082), ('##군', 36064), ('호', 35859), ('데', 35690), ('##그', 35670), ('역', 35514), ('동안', 35283), ('자신', 34906), ('끝', 34843), ('일부', 34689), ('영화', 34662), ('문제', 34516), ('프랑스', 34087), ('발표', 33712), ('도시', 33472), ('연구', 33469), ('성', 33121), ('서울', 33077), ('아들', 32501), ('구성', 32455), ('군', 32444), ('시간', 32436), ('일반', 32289), ('곳', 32165), ('대표', 31890), ('주', 31817), ('작품', 31601), ('개발', 31527), ('약', 31400), ('명의', 31062), ('제', 31047), ('사건', 30796), ('세기', 30693), ('발견', 30690), ('등이', 30310), ('속', 30153), ('대부분', 29832), ('사회', 29698), ('대통령', 29682), ('역사', 29522), ('공격', 29445), ('가지', 29087), ('생각', 29035), ('감독', 28757), ('결과', 28034), ('##국의', 28001), ('나', 27481), ('전투', 27429), ('##대의', 27319), ('의미', 27299), ('발생', 27233), ('계속', 26997), ('대학', 26906), ('필요', 26903), ('정치', 26623), ('진행', 26365), ('관련', 26233), ('결정', 26202), ('왕', 26177), ('##군의', 26161), ('영향', 26136), ('이용', 26054), ('##와의', 25780), ('우승', 25703), ('게임', 25685), ('중요', 25564), ('##전', 25493), ('유럽', 25386), ('중심', 25381), ('상태', 25321), ('참여', 25227), ('관계', 25172), ('지원', 25071), ('설치', 25030), ('대회', 24808), ('##아의', 24787), ('강', 24661), ('##회의', 24527), ('##학교', 24508), ('부분', 24500), ('사망', 24491), ('이상', 24468), ('반', 24449), ('시대', 24317), ('기술', 23871), ('##팀', 23806), ('제작', 23718), ('지방', 23714), ('뜻', 23611), ('발매', 23399), ('##씨', 23307), ('성공', 23272), ('자리', 23024), ('국제', 23010), ('아버지', 22874), ('##월', 22861), ('회', 22779), ('등장', 22772), ('과정', 22589), ('##자의', 22474), ('교육', 22374), ('##시', 22360), ('이것', 22222), ('조직', 22145), ('조', 22064), ('수도', 22009), ('경제', 21990), ('계획', 21983), ('설립', 21952), ('운영', 21926), ('차지', 21907), ('집', 21894), ('음악', 21762), ('방송', 21652), ('분', 21651), ('출신', 21538), ('##이후', 21375), ('축구', 21250), ('형태', 21198), ('방법', 20878), ('상대', 20794), ('기', 20716), ('이전', 20547), ('소속', 20533), ('나라', 20518), ('역할', 20502), ('해당', 20429), ('##간', 20425), ('문화', 20401), ('결국', 20361), ('고려', 20360), ('독립', 20295), ('가운데', 20287), ('프로그램', 20129), ('##기의', 20092), ('모습', 19971), ('승리', 19943), ('상', 19890), ('제공', 19845), ('선', 19776), ('곡', 19753), ('해', 19694), ('임명', 19585), ('지정', 19544), ('구조', 19528), ('공식', 19488), ('섬', 19386), ('앨범', 19367), ('운동', 19363), ('내용', 19278), ('총', 19214), ('인간', 19182), ('표현', 19163), ('참가', 18887), ('로마', 18884), ('##성의', 18841), ('아래', 18733), ('여성', 18661), ('생산', 18654), ('전체', 18633), ('형', 18590), ('평가', 18485), ('사실', 18477), ('볼', 18451), ('데뷔', 18381), ('당', 18368), ('우리', 18313), ('유지', 18278), ('정의', 18268), ('주요', 18258), ('인', 18192), ('##중', 18187), ('대신', 18174), ('출전', 18152), ('동시', 18109), ('##일', 17901), ('장', 17874), ('천', 17874), ('군사', 17828), ('직접', 17817), ('연결', 17800), ('추가', 17792), ('시스템', 17783), ('##본의', 17735), ('마지막', 17703), ('##서의', 17656), ('기능', 17613), ('외', 17533), ('사', 17519), ('러시아', 17485), ('달', 17463), ('상황', 17446), ('자유', 17430), ('기준', 17427), ('뿐', 17405), ('프로', 17259), ('인구', 17253), ('시기', 17173), ('##대', 17093), ('##등', 17044), ('알', 17023), ('시리즈', 17006), ('활약', 16984), ('방식', 16955), ('진출', 16953), ('정보', 16815), ('후보', 16755), ('원', 16741), ('발전', 16684), ('그녀', 16639), ('이야기', 16540), ('규모', 16529), ('중앙', 16482), ('날', 16471), ('예', 16418), ('변화', 16395), ('공간', 16373), ('올림픽', 16362), ('##시의', 16356), ('##대학교', 16289), ('##위원회', 16289), ('배', 16268), ('대학교', 16260), ('형성', 16179), ('반대', 16174), ('요구', 16165), ('건물', 16164), ('산', 16151), ('그것', 16117), ('안', 16070), ('이론', 16003), ('앞', 16000), ('졸업', 15995), ('한편', 15958), ('개의', 15833), ('회사', 15796), ('##초의', 15771), ('생활', 15766), ('국민', 15752), ('초기', 15744), ('공개', 15714), ('##부의', 15713), ('목적', 15673), ('구', 15658), ('제국', 15607), ('건설', 15586), ('변경', 15581), ('물', 15542), ('개인', 15492), ('##리그', 15464), ('억', 15374), ('선거', 15362), ('언어', 15327), ('여기', 15322), ('초', 15285), ('비판', 15283), ('계약', 15258), ('##대학', 15239), ('공', 15131), ('최', 15059), ('결혼', 15007), ('출연', 14968), ('딸', 14901), ('대상', 14879), ('기간', 14852), ('편', 14834), ('##도의', 14816), ('그리스', 14792), ('관리', 14792), ('학교', 14766), ('도입', 14727), ('인정', 14719), ('이탈리아', 14665), ('책', 14641), ('최초', 14607), ('기본', 14560), ('세의', 14549), ('운행', 14538), ('학생', 14519), ('소련', 14482), ('노래', 14459), ('밖', 14418), ('인물', 14342), ('개최', 14299), ('##대표', 14054), ('수상', 14044), ('세력', 14042), ('남', 14017), ('골', 14012), ('설명', 14001), ('층', 13964), ('시절', 13960), ('실시', 13887), ('최고', 13808), ('##주의', 13785), ('그룹', 13778), ('제외', 13757), ('특징', 13737), ('정책', 13671), ('어머니', 13634), ('간', 13626), ('모델', 13622), ('##제의', 13555), ('##원', 13533), ('자기', 13489), ('대전', 13476), ('교수', 13451), ('선정', 13447), ('자연', 13442), ('열', 13376), ('조사', 13289), ('수행', 13252), ('지구', 13229), ('음반', 13201), ('손', 13200), ('민족', 13147), ('확인', 13146), ('법', 13087), ('면', 13078), ('분류', 13062), ('여자', 13019), ('적용', 12956), ('힘', 12882), ('마을', 12854), ('명칭', 12842), ('##차', 12828), ('증가', 12826), ('군대', 12789), ('의원', 12782), ('교황', 12696), ('비교', 12693), ('종', 12686), ('전통', 12685), ('인도', 12666), ('실패', 12584), ('크기', 12518), ('지지', 12493), ('거리', 12470), ('출시', 12457), ('사랑', 12452), ('사상', 12446), ('역임', 12427), ('철도', 12416), ('남쪽', 12414), ('과학', 12402), ('보통', 12394), ('행정', 12325), ('열차', 12315), ('제도', 12286), ('황제', 12271), ('백', 12227), ('노선', 12223), ('사업', 12098), ('월드컵', 12058), ('현대', 12029), ('종교', 12027), ('서비스', 12026), ('스페인', 12021), ('국의', 12014), ('기업', 11997), ('북쪽', 11986), ('개념', 11943), ('문', 11939), ('##시대', 11929), ('공연', 11927), ('대의', 11898), ('종류', 11897), ('눈', 11882), ('회의', 11881), ('작업', 11841), ('인기', 11814), ('이상의', 11805), ('마음', 11805), ('기반', 11797), ('컴퓨터', 11759), ('분야', 11754), ('추정', 11701), ('나이', 11617), ('이적', 11551), ('시장', 11535), ('우주', 11511), ('지배', 11476), ('몸', 11449), ('도쿄', 11434), ('길', 11419), ('차이', 11418), ('이유', 11404), ('적', 11371), ('미', 11368), ('##부', 11343), ('방향', 11342), ('연방', 11336), ('유일', 11274), ('정', 11265), ('효과', 11259), ('존', 11247), ('공동', 11238), ('담당', 11231), ('부족', 11204), ('환경', 11178), ('##단의', 11153), ('싱글', 11150), ('버전', 11145), ('전국', 11124), ('행동', 11112), ('수록', 11106), ('교회', 11055), ('선출', 11030), ('선택', 11024), ('능력', 10995), ('연속', 10974), ('길이', 10969), ('바', 10968), ('규정', 10968), ('형식', 10946), ('조건', 10919), ('지', 10860), ('동쪽', 10859), ('부', 10858), ('밴드', 10830), ('자체', 10823), ('국내', 10795), ('통합', 10777), ('차량', 10770), ('이때', 10761), ('발', 10757), ('단체', 10706), ('에너지', 10672), ('보호', 10659), ('신', 10649), ('도움', 10639), ('드라마', 10624), ('이유로', 10619), ('클럽', 10607)]\n"
     ]
    }
   ],
   "source": [
    "# FreqDist를 이용하여 빈도수 계산(*오래걸림)\n",
    "vocab = FreqDist(np.hstack(total_words))\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "# 최대 빈도수 가장높은 500개만 뽑아봄\n",
    "print(vocab.most_common(500))\n",
    "\n",
    "# 특정 단어의 빈도수를 뽑아봄\n",
    "#print(vocab['미국'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22c1ccf-d857-402c-bd74-294e01073d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*단어 집합의 크기 : 50000\n",
      "*마지막 단어 정보 : ('음악당', 50)\n"
     ]
    }
   ],
   "source": [
    "# 상위 10000 개만 보존\n",
    "vocab_size = 50000\n",
    "vocab1 = vocab.most_common(vocab_size)\n",
    "\n",
    "vocab_len = len(vocab1)\n",
    "print('*단어 집합의 크기 : {}'.format(vocab_len))\n",
    "print('*마지막 단어 정보 : {}'.format(vocab1[vocab_len-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b936eb-4201-4257-8961-b1bbcb47119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "vocab2 = vocab.most_common(10)\n",
    "\n",
    "all_fdist = pd.Series(dict(vocab2))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04324119-dadd-4f50-bd8c-a51fab023fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7701807b98248098f6aad3fac6d1a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모두', '가능', '신의', '영국', '등의', '내', '정부', '주장', '처음', '존재', '중국', '대', '점', '정도', '리그', '전쟁', '독일', '##군', '호', '데']\n"
     ]
    }
   ],
   "source": [
    "# vocab을 list로 만듬\n",
    "new_vocab = []\n",
    "for index, word in tqdm(enumerate(vocab1)):\n",
    "    new_vocab.append(word[0])  # fword[0] 하면 단어만 추출\n",
    "    \n",
    "# 리스트 출력해봄\n",
    "print(new_vocab[50:70])\n",
    "#print(vocab['음악당'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ff872f-9f1e-4364-b2be-0a43ef7a65a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655141eb1d9745eea68e380f06f960e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new_vocab을 파일로 저장함\n",
    "new_vocab_out = 'new_vocab.txt'\n",
    "with open(new_vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(new_vocab):\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4825424-1f11-4029-bde4-ce7a6eb04e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f437ccbbd7c4d9c92e6e32f7a0c6e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['년', '월', '것', '일', '수', '##년', '그', '등', '때', '중']\n"
     ]
    }
   ],
   "source": [
    "# new_vocab 파일을 불러옴.\n",
    "new_vocab_out = 'new_vocab.txt'\n",
    "\n",
    "# 2차 불용어 정의\n",
    "stopwords=['##다', '##하', '있', '##고', '##로', '##한', '##적', '##되', '##었', '##_']\n",
    "\n",
    "new_vocab = []\n",
    "with open(new_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "for word in tqdm(data):\n",
    "    if not word in stopwords:  # 2차 불용어는 제외\n",
    "        new_vocab.append(word)\n",
    "        \n",
    "print(new_vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be46a8a-f837-49e4-887e-5d43c41cf183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127fad5abe9e4c79afb4a1e60506c7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n",
      "['વ', 'શ', 'ષ', 'સ']\n"
     ]
    }
   ],
   "source": [
    "# 원래 vocab 리스트로 읽어옴\n",
    "org_vocab_out = '../model/bert/bert-multilingual-cased/vocab/vocab.txt'\n",
    "org_vocab = []\n",
    "\n",
    "with open(org_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "for vocab in tqdm(data):\n",
    "     org_vocab.append(vocab)\n",
    "\n",
    "print(len(org_vocab))\n",
    "print(org_vocab[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ff7adf-befa-4dd3-aeb1-2e4e1a655111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167538\n",
      "['વ', 'શ', 'ષ', 'સ']\n"
     ]
    }
   ],
   "source": [
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거\n",
    "#temp = list(set(new_vocab) - set(org_vocab)) #순서 보존이 안됨\n",
    "    \n",
    "#또는\n",
    "\n",
    "s = set(org_vocab)\n",
    "temp = [x for x in new_vocab if x not in s] #순서 보존됨    \n",
    "\n",
    "vocablist = org_vocab + temp  #원본 리스트 + 중복제가한 신규리스트 2개의 리스트를 합침\n",
    "\n",
    "print(len(vocablist))\n",
    "print(vocablist[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebdc047b-ad66-41e2-842e-17fce5dd26bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 파일\"new_vocab.txt\" 추가 성공!. 출력 vocab 파일 : \"add_new_vocab.txt\"\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 파일로 저장\n",
    "outfilepath = 'add_new_vocab.txt'\n",
    "with open(outfilepath, 'w') as f:\n",
    "    f.write('\\n'.join(vocablist))\n",
    "    \n",
    "print('vocab 파일\"{}\" 추가 성공!. 출력 vocab 파일 : \"{}\"'.format(new_vocab_out, outfilepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d40acc9-034d-4f3c-940b-38d657ce8839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]']\n"
     ]
    }
   ],
   "source": [
    "# 저장된 add_new_vocab.txt를 불러와서 special 토큰 추가함\n",
    "tokenizer = BertTokenizer(vocab_file=outfilepath, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=False)\n",
    "\n",
    "special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# special token 체크\n",
    "print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431f8a3e-aebd-4e09-9ae6-4bc0b92fddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('new_vocab_nouns/tokenizer_config.json',\n",
       " 'new_vocab_nouns/special_tokens_map.json',\n",
       " 'new_vocab_nouns/vocab.txt',\n",
       " 'new_vocab_nouns/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "import os\n",
    "OUT_PATH = 'new_vocab_nouns'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb0a60-e94e-49ac-9d06-49c5cf6ea487",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#=============================================================================================\n",
    "# add_tokens으로 추가하면, BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#=============================================================================================\n",
    "\n",
    "# 기존 bert tokenizer 로딩\n",
    "vocab_path = '../model/bert/bert-multilingual-cased/vocab'\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
    "print(f'*기존 vocab 수: {len(tokenizer)}')\n",
    "\n",
    "# 기존 bert tokenizer에 신규 vocab 추가함 \n",
    "new_token_num = tokenizer.add_tokens(new_vocab)\n",
    "print(f'*신규 vocab 수: {len(tokenizer)}')\n",
    "print(f'*추가된 vocab: {new_token_num}')\n",
    "\n",
    "# 추가된 tokenizer 저장\n",
    "# => 추가된 token들은 added_tokens.json에 저장된다.\n",
    "import os\n",
    "OUT_PATH = 'new_vocab_nouns'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
