{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df21fe-46aa-4db9-9a72-de8382c869c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece_model_pb2 사용을 위해서는 google-api-python-client 설치해야 함\n",
    "#!pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a81cd36-fd6a-4707-ae2f-2297df7d0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import sentencepiece.sentencepiece_model_pb2 as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb71c11a-19de-437b-84cc-4bc57f2ebf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_spmodel_fpath = \"Tokenizer/kobert/kobert_news_wiki_ko_cased-ae5711deb3.spiece\"  # 기존 sentencepiece 모델(Kobert 모델)\n",
    "new_spmodel_fpath = \"Tokenizer/kobert/kobert_new_0208_1.model\"                      # 새롭게 추가할 Kobert 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec2d61e-7b62-4283-a0da-7da602aabae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371427"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = model.ModelProto()\n",
    "m.ParseFromString(open(old_spmodel_fpath, 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069e71cd-7ae7-449d-befd-326f969091b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piece: \"[UNK]\"\n",
      "score: 0.0\n",
      "type: UNKNOWN\n",
      "\n",
      "<class 'sentencepiece_model_pb2.SentencePiece'>\n"
     ]
    }
   ],
   "source": [
    "print(m.pieces[0])\n",
    "print(type(m.pieces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a53322-9ac3-4158-a32d-a3c47e7c2cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[piece: \"\\342\\200\\231,\"\n",
      "score: -9.645369529724121\n",
      ", piece: \"\\342\\200\\234\"\n",
      "score: -9.81615161895752\n",
      ", piece: \"\\342\\200\\235\"\n",
      "score: -5.978498935699463\n",
      ", piece: \"\\342\\200\\235,\"\n",
      "score: -9.58655834197998\n",
      ", piece: \"\\342\\200\\262\"\n",
      "score: -10.174786567687988\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(m.pieces[500:505])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12496b1b-4730-4997-8063-e39deecf338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_piece = type(m.pieces[0])()\n",
    "new_piece.piece = \"문서중앙화\"  #추가할 piece\n",
    "new_piece.score = 0.0          #**score = 0.0 하면 무조건 해당 토큰으로 잘림\n",
    "new_piece.type = 1             # type은  1\n",
    "\n",
    "m.pieces.append(new_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "287f052d-f661-4b53-aa7a-a6ab301938b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로은 sentencepiece 모델에 추가하기 \n",
    "with open(new_spmodel_fpath, \"wb\") as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94f0db34-9b50-486b-9ba9-8c425ef70510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '문서중앙화', '▁생', '성', '▁하기']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=new_spmodel_fpath)\n",
    "print(sp.encode(\"문서중앙화 생성 하기\", out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d52d889-746f-4aeb-ba09-34db6638c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_model size: 8003\n",
      "PiecetoId:8002\n",
      "IdtoPiece: 문서중앙화\n"
     ]
    }
   ],
   "source": [
    "print('new_model size: {}'.format(sp.GetPieceSize()))\n",
    "print('PiecetoId:{}'.format(sp.PieceToId('문서중앙화')))\n",
    "print('IdtoPiece: {}'.format(sp.IdToPiece(8002)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42e9d26-6a2f-4807-a4b2-5142b2944b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델을 테스트 해봄 \n",
    "import torch\n",
    "import gluonnlp as nlp                  # GluonNLP는 버트를 간단하게 로딩하는 인터페이스를 제공하는 API 임\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c092e2c5-d8e3-49bd-89ba-41ae6a0dfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_file=\"Tokenizer/kobert/kobert_news_wiki_ko_cased-ae5711deb3.spiece\" # Kobert vocab\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(new_spmodel_fpath, padding_token=\"[PAD]\")\n",
    "tok = nlp.data.BERTSPTokenizer(new_spmodel_fpath, vocab, lower=False)\n",
    "transform = nlp.data.BERTSentenceTransform(\n",
    "            tok, max_seq_length = 128, pad=True, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f546baf7-0b14-450f-878a-d879e1e7c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"식당에 가서 밥을 배 부르게 먹고 문서중앙화 낙시배를 타고 고기 잡고 요트배를 타고 관광을 해야 겠다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c550b907-e8b2-4366-9b5d-b484effe324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentece:\n",
      "['[CLS]', '▁식당', '에', '▁', '가', '서', '▁밥', '을', '▁배', '▁부르', '게', '▁먹고', '▁', '문서중앙화', '▁낙', '시', '배', '를', '▁타고', '▁고', '기', '▁잡고', '▁요', '트', '배', '를', '▁타고', '▁관광', '을', '▁해야', '▁', '겠다', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token_ids:\n",
      "[   2 3007 6896  517 5330 6553 2266 7088 2287 2432 5400 2011  517 8002\n",
      " 1404 6705 6312 6116 4700  993 5561 3951 3480 7659 6312 6116 4700 1080\n",
      " 7088 5010  517 5406    3    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "valid_length:\n",
      "33\n",
      "segment_ids:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "transform_data = [transform([i[0]]) for i in [test_sentence]]\n",
    "token_ids = transform_data[0][0]\n",
    "valid_length = transform_data[0][1]\n",
    "segment_ids = transform_data[0][2]\n",
    "\n",
    "test_sentence_list = []\n",
    "for i, ids in enumerate(token_ids):\n",
    "    test_sentence_list.append(vocab.idx_to_token[ids])\n",
    "\n",
    "print(\"sentece:\\r\\n{}\".format(test_sentence_list))\n",
    "print(\"token_ids:\\r\\n{}\".format(token_ids))\n",
    "print(\"valid_length:\\r\\n{}\".format(valid_length))\n",
    "print(\"segment_ids:\\r\\n{}\".format(segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed8e25-d900-4b0c-a196-a6b5f3c10f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
