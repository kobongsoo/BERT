{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f609ad3e-79ef-474f-bc84-f3d74edb35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================================================\n",
    "# sentencepiece vocab에 기존 단어 중복 검사하면서 신규 단어 추가하는 예제\n",
    "# => 'albert-base-v2' 모델의 sentencepiece vocab(사이즈:30,000개)에 신규 단어 추가해 봄\n",
    "#\n",
    "#=================================================================================\n",
    "# sentencepiece_model_pb2 사용을 위해서는 google-api-python-client 설치해야 함\n",
    "#!pip install --upgrade google-api-python-client\n",
    "\n",
    "import sentencepiece as spm\n",
    "import sentencepiece.sentencepiece_model_pb2 as spmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55382cd-1fce-43fb-a038-23e8cae01a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760289"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 sentenctpiece 모델 불러옴\n",
    "# => albert-base-v2 모델이 vocab 경로(spiece.model) 지정.\n",
    "smodel_path = '../data11/model/bert/albert-base-v2/spiece.model'\n",
    "m = spmodel.ModelProto()\n",
    "m.ParseFromString(open(smodel_path, 'rb').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f2ccc5-5478-478e-84e4-58838342b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 단어 추가시, 중복 검사를 위해..\n",
    "# 기존 sentenctpiece vocab 목록을 리스트에 저장해 둠.\n",
    "vocab_list = []\n",
    "count = 0\n",
    "for i, piece in enumerate(m.pieces):\n",
    "    #print(piece.piece)\n",
    "    vocab_list.append(piece.piece)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da72699c-d7c3-4ec6-871d-a4d742a88837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁are',\n",
       " '▁my',\n",
       " '▁not',\n",
       " '▁one',\n",
       " '▁or',\n",
       " '▁me',\n",
       " '▁which',\n",
       " '▁have',\n",
       " 'a',\n",
       " '▁they',\n",
       " '?',\n",
       " '▁him',\n",
       " 'e',\n",
       " '▁has',\n",
       " '▁first',\n",
       " '▁all',\n",
       " '▁their',\n",
       " '▁also',\n",
       " 'ing',\n",
       " 'ed',\n",
       " '▁out',\n",
       " '▁up',\n",
       " '▁who',\n",
       " ';',\n",
       " '▁been',\n",
       " '▁after',\n",
       " '▁when',\n",
       " '▁into',\n",
       " '▁new',\n",
       " 'm',\n",
       " '▁there',\n",
       " '▁two',\n",
       " '▁its',\n",
       " '▁would',\n",
       " '▁over',\n",
       " '▁time',\n",
       " '▁so',\n",
       " '▁said',\n",
       " '▁about',\n",
       " '▁other',\n",
       " '▁no',\n",
       " '▁more',\n",
       " '▁can',\n",
       " 'y',\n",
       " '▁then',\n",
       " '▁we',\n",
       " 'th',\n",
       " '▁back',\n",
       " '▁what',\n",
       " 're']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d7f8b8-9ea9-4a2e-8dc1-4fc55b1e3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piece: \"[CLS]\"\n",
      "score: 0.0\n",
      "type: CONTROL\n",
      "\n",
      "<class 'sentencepiece_model_pb2.SentencePiece'>\n"
     ]
    }
   ],
   "source": [
    "print(m.pieces[2])\n",
    "print(type(m.pieces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1e9d9e-df41-438e-abaf-7b5bc025cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:0, vocab:문서중앙화\n",
      "idx:1, vocab:보안파일서버\n",
      "idx:2, vocab:엠파워\n",
      "idx:3, vocab:Mpower\n",
      "idx:4, vocab:EZis-C\n",
      "idx:5, vocab:모코엠시스\n",
      "idx:6, vocab:M드라이브\n"
     ]
    }
   ],
   "source": [
    "# 새로운 단어들 추가\n",
    "# => 기존 cat, w 단어들은 기존에 존재하는 단어이므로, 중복 검사 하여 추가하지 않음.\n",
    "new_vocab_list = ['문서중앙화', '보안파일서버', '엠파워', 'Mpower', 'EZis-C', '모코엠시스', 'M드라이브', 'cat', 'w']\n",
    "\n",
    "# for문을 돌면서, 기존 vocab에 있는 단어인지 검사 후,\n",
    "# 없는 단어들만 추가함.\n",
    "for idx, vocab in enumerate(new_vocab_list):\n",
    "    \n",
    "    # 기존 vocab에 없는 단어들만 추가 \n",
    "    if vocab not in vocab_list:\n",
    "        print(f'idx:{idx}, vocab:{vocab}')\n",
    "        new_piece = type(m.pieces[0])()\n",
    "        new_piece.piece = vocab\n",
    "        new_piece.score = 0.0\n",
    "        new_piece.type = 1\n",
    "\n",
    "        m.pieces.append(new_piece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "428269cf-4e13-4b71-8f7f-47e87e3ca24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델에 serialize 함.(저장함)\n",
    "new_smodel_path = '../data11/model/bert/albert-base-v2/spiece_new.model'\n",
    "with open(new_smodel_path, 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03423b65-5295-4856-8d42-a723e38554f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '모코엠시스에서는', '▁', '문서중앙화', '▁', '및', '▁', '보안파일서버', '▁', '솔루션인', '▁', '엠파워를', '▁', '출시하였다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 기존 spmodel 테스트\n",
    "text = \"모코엠시스에서는 문서중앙화 및 보안파일서버 솔루션인 엠파워를 출시하였다.\"\n",
    "sp_old = spm.SentencePieceProcessor(model_file=smodel_path)\n",
    "print(sp_old.encode(text, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b47f619f-2a2f-4d7f-be77-592f6732f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '모코엠시스', '에서는', '▁', '문서중앙화', '▁', '및', '▁', '보안파일서버', '▁', '솔루션인', '▁', '엠파워', '를', '▁', '출시하였다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 새로운 spmodel 테스트 \n",
    "# => 추가된 단어들 별루 분리가 잘 된다.\n",
    "sp_new = spm.SentencePieceProcessor(model_file=new_smodel_path)\n",
    "print(sp_new.encode(text, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6552e426-cbb1-404e-b538-3995bcab508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_Model_size:29956\n",
      "vocab:29951\n"
     ]
    }
   ],
   "source": [
    "# 새로운 모델 vocba 출력해 봄\n",
    "print('new_Model_size:{}'.format(sp_new.GetPieceSize()))\n",
    "print('vocab:{}'.format(sp_new.PieceToId('엠파워')))\n",
    "#print('idtoPiece:{}'.format(sp_new.IdToPiece(30002)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4f554cc-6641-4834-9bb4-48f4d6741f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5782\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "# 기존 모델에 단어들이 새로운 모델에 어떻게 적용되었는지 출력 해봄\n",
    "print(sp_new.PieceToId('cat'))\n",
    "print(sp_new.PieceToId('w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be30ff97-d35f-4e52-a368-a17255d094dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForMaskedLM were not initialized from the model checkpoint at ../data11/model/bert/albert-base-v2 and are newly initialized: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30007\n",
      "AlbertForMaskedLM(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30007, 128)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictions): AlbertMLMHead(\n",
      "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dense): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (decoder): Linear(in_features=128, out_features=30007, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 신규 모델에 파일을 불러옴\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForMaskedLM, AlbertModel\n",
    "\n",
    "vocab_path = '../data11/model/bert/albert-base-v2/newvocab' # 앞에서 만들어진 spiece_new.model 이 있는 경로 지정해줌\n",
    "model_path = '../data11/model/bert/albert-base-v2'\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(vocab_path)\n",
    "print(len(tokenizer))\n",
    "\n",
    "model = AlbertForMaskedLM.from_pretrained(model_path)\n",
    "\n",
    "# resize_token_embeddings 으로 신규 tokenizer 사이즈로 지정 해줌.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc9beef-e68e-4768-b255-91194d2f1836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11222583"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb381d56-ecb2-467a-b75f-107d1028db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,    13, 30005,     1,    13, 30000,    13,     1,    13, 30001,\n",
      "            13,     1,    13, 30002,     1,    13,     1,     9,     3,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# text tokenizer 해봄.\n",
    "token_ids = tokenizer.encode_plus(text, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "print(token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
