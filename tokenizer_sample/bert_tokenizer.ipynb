{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a88fe7f-983a-47ce-b0f0-4938213a6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# bertwordpiece 방식으로 토큰 사전 생성 \n",
    "#\n",
    "# 1) mecab 로 한번 말뭉치 분석\n",
    "# 2) 분석된 말뭉치를 가지고 wordpiece 토큰화(훈련)\n",
    "# 3) special token 추가\n",
    "####################################################################\n",
    "\n",
    "#변수들 지정 \n",
    "#\n",
    "# input : 말뭉치 경로\n",
    "# mecab_output : 입력된 말뭉치를 mecab 으로 형태소 분석한 output 파일 \n",
    "# mecab_generation = True 이면, 형태소 앞에 ## 붙임, False면 안붙임\n",
    "\n",
    "input = '../../data11/ai_hub/tl1/tl1-1줄.txt'\n",
    "#mecab_output = '../../data11/my_corpus/re-kowiki-202206-mecab.txt' # [\"wiki_20190620_small.txt\", \"my_data/nsmc_subword.txt\"], 여러개 파일도 추가할수 있음\n",
    "mecab_output = '../../data11/ai_hub/tl1/tl1-1줄-mecab.txt' # [\"wiki_20190620_small.txt\", \"my_data/nsmc_subword.txt\"], 여러개 파일도 추가할수 있음\n",
    "mecab_generation = False\n",
    "\n",
    "vocab_size = 30000   # 출력 vocab 크기\n",
    "min_frequency = 200  # 빈도수\n",
    "\n",
    "# 출력 경로 \n",
    "vocab_folder = '../../data11/ai_hub/vocab/' #출력 폴더(*반드시 우선 생성해야함)\n",
    "vocab_output = 'tl1-1줄-mecab-30000'  #출력 파일명\n",
    "vocab_special_output = 'tl1-1줄-mecab-30000-speical'   # tokenizer 생성할때 파일명\n",
    "\n",
    "# 출력폴더 없으면 생성\n",
    "import os\n",
    "os.makedirs(vocab_folder, exist_ok=True)\n",
    "\n",
    "#seed 설정 해도 vocab 은 다르게 생성됨\n",
    "#import sys\n",
    "#sys.path.append(\"..\")\n",
    "#from myutils import seed_everything\n",
    "#seed_everything(111)\n",
    "\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929d09e-2db8-4a48-971f-0a18ec2fc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "## yield 를 이용하여 generator 함수 정의  \n",
    "## => 10만 단위로 쪼갬\n",
    "def get_generator_corpus(data, max_len: int=100000):\n",
    "    \n",
    "    dataset = data\n",
    "    #dataset = list(set(data)) # ****중복 문장 제거(*순서 유지 안함)\n",
    "    \n",
    "    for start_idx in range(0, len(dataset), max_len):\n",
    "        samples = dataset[start_idx : start_idx + max_len]\n",
    "        yield samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97fad7-127c-4a5c-b771-10fae39ca9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Mecab 를 이용하여, 말뭉치 형태소 분석 \n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 말뭉치 불러오기\n",
    "with open(input, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "print(data[:3])\n",
    "print(len(data))\n",
    "\n",
    "# mecab 출력 파일 열기\n",
    "f1 = open(mecab_output, 'w', encoding='utf-8')\n",
    "\n",
    "# 10만 단위에 generator 정의\n",
    "gen_corpus = get_generator_corpus(data)\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "# True: '어릴때' -> '어릴', '##'때' 로 일반화 시킴\n",
    "#sentence_count = 0\n",
    "if mecab_generation == True:\n",
    "  total_morph=[]\n",
    "  for sentences in tqdm(gen_corpus):\n",
    "    for sentence in sentences:\n",
    "        morph_sentence=[]\n",
    "        count=0\n",
    "        for token_mecab in mecab.morphs(sentence):\n",
    "          token_mecab_save = token_mecab\n",
    "          if count > 0:\n",
    "            token_mecab_save = \"##\" + token_mecab_save\n",
    "            morph_sentence.append(token_mecab_save)\n",
    "          else:\n",
    "            morph_sentence.append(token_mecab_save)\n",
    "            count += 1\n",
    "\n",
    "        f1.write(' '.join(morph_sentence)+'\\n')\n",
    "    \n",
    "    #if sentence_count % 1000000 == 0:\n",
    "    #    print('sentence_count:{}'.format(sentence_count))\n",
    "    \n",
    "    #sentence_count += 1\n",
    "        \n",
    "# False: '어릴때' -> '어릴', '때' 로 일반화 시킴\n",
    "else:\n",
    "  total_morph=[]\n",
    "  for sentences in tqdm(gen_corpus):\n",
    "    for sentence in sentences:\n",
    "        morph_sentence = mecab.morphs(sentence)\n",
    "        f1.write(' '.join(morph_sentence)+'\\n')\n",
    "          \n",
    "        #total_morph.append(morph_sentence)\n",
    "   \n",
    "    #if sentence_count % 1000000 == 0:\n",
    "    #   print('sentence_count:{}'.format(sentence_count))\n",
    "    \n",
    "    #sentence_count += 1\n",
    "          \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35addfce-1479-42fc-9974-4486d175630a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertWordPieceTokenizer=>\n",
      "train=>\n",
      "\n",
      "\n",
      "\n",
      "save_model=>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-vocab.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Bert Tokenizer 훈련 시작\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "print(f'BertWordPieceTokenizer=>')\n",
    "WPT = BertWordPieceTokenizer(\n",
    "    #vocab='/content/wordPieceTokenizer/wiki_20190620-vocab.txt', # 기존 vocab 넣어도 사전 학습 안됨\n",
    "    clean_text=True,       # 토큰에서 공백은 제거 \n",
    "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼갬\n",
    "    strip_accents=False,  # True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "    lowercase=False       # 한국어에서는 반드시 False\n",
    ")\n",
    "\n",
    "print(f'train=>')\n",
    "WPT.train(\n",
    "    #files=[\"my_data/wiki_20190620_small.txt\", \"my_data/nsmc_subword.txt\"],  #여러개 파일도 추가할수 있음\n",
    "    files=[mecab_output],\n",
    "    vocab_size=vocab_size,       # 만들고자 하는 vocab의 size, 보통 '32000' 정도가 좋다고 알려져 있다\n",
    "    min_frequency=min_frequency,        # merge를 수행할 최소 빈도수, 2로 설정 시 2회 이상 등장한 pair만 수행한다\n",
    "    show_progress=True,     # 학습 진행과정 show\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],# Tokenizer에 추가하고 싶은 special token 지정\n",
    "    wordpieces_prefix=\"##\"\n",
    "    #limit_alphabet=6000  # merge 수행 전 initial tokens이 유지되는 숫자 제한\n",
    "    #initial_alphabet   # 꼭 포함됐으면 하는 initial alphabet, 이곳에 설정한 token은 학습되지 않고 그대로 포함되도록 설정된다.\n",
    ")\n",
    "print(f'save_model=>')\n",
    "# vcoab_folder/vocab_ouput-vocab.txt 파일로 저장\n",
    "# => 뒤에 vocab이 붙음\n",
    "WPT.save_model(vocab_folder, vocab_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf71d7-71df-45f9-9d0d-f74c70e84560",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이후 작업은 실제 BERT 토크너나이저 스페셜 토큰을 추가하여 만들때 사용하는 것이므로 pass해도 됨.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4265317e-410d-4069-870c-2c3e4cf6c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(WPT.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce74afd6-3a2a-4a0a-bd4a-c0db10309154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '##은', '경마', '##장', '##에', '##서', '조', '##랑', '##말', '탄다']\n",
      "[2477, 1261, 15785, 1207, 1039, 1107, 733, 1037, 1299, 27075]\n"
     ]
    }
   ],
   "source": [
    "text = \"오늘은 경마장에서 조랑말 탄다\"\n",
    "tokenized_text = WPT.encode(text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e2914b-4f23-4211-9a19-02f73f7a4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### speical 토큰 추가 및 json 파일 생성\n",
    "from transformers import BertTokenizer, BertTokenizerFast, DistilBertTokenizer, DistilBertTokenizerFast\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a89904-210b-40c1-bb55-2d99f4cab678",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = vocab_folder + '/' + vocab_output + '-vocab.txt'\n",
    "vocab_special_path =  vocab_folder + '/' + vocab_special_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d000ab3-e72c-4d1c-9da6-dffccf926918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/ai_hub/vocab//tl1-1줄-mecab-30000-vocab.txt\n",
      "../../data11/ai_hub/vocab//tl1-1줄-mecab-30000-speical\n"
     ]
    }
   ],
   "source": [
    "print(vocab_path)\n",
    "print(vocab_special_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e97667-523d-42ca-b58c-b4e899f7a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizerFast, BertTokenizer 둘중 하나를 사용하면 됨\n",
    "'''\n",
    "tokenizer = BertTokenizerFast(vocab_file=vocab_path, \n",
    "                              max_len=128, \n",
    "                              do_lower_case=False)\n",
    "'''                              \n",
    "\n",
    "#print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\"))\n",
    "#tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_path, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe400f9-6fef-44f3-b897-20f851201aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Special Token 지정\n",
    "\n",
    "-special_tokens : 언어모델의 범용성을 위해 Dummy token, 여러 개의 [unused]와 [UNK]를 꼭 설정해야 한다.\n",
    "\n",
    "-언어모델에 번역, 요약, 개체명 인식 모델을 fine-tuning시 Dummy token이 필요한 경우가 많다. 또 도메인 특화된 task를 수행할 땐 도메인 토큰을 따로 선언하는게 필수이다.\n",
    "\n",
    "-이를 고려하지 않으면 편법을 사용해야 한다. ETRI의 KorBERT는 Dummy token이 없어 빈도수가 작은 token을 Dummy로 대체해서 쓰기도 한다.\n",
    "\n",
    "-꼭 충분한 unused와 UNK를 설정하자. 그리고 BOS(문장 시작), EOS(문장 끝) 등등도 추가하자. 본 프로젝트에선 10개의 UNK와 200개의 unsed token을 선언하였다.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da15abbd-6d89-4329-a9f7-1b25b82b6667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2467a7a0-56e5-453f-93dd-2f60885f4302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[CLS]',\n",
       " '[MASK]',\n",
       " '[BOS]',\n",
       " '[EOS]',\n",
       " '[UNK0]',\n",
       " '[UNK1]',\n",
       " '[UNK2]',\n",
       " '[UNK3]',\n",
       " '[UNK4]',\n",
       " '[UNK5]',\n",
       " '[UNK6]',\n",
       " '[UNK7]',\n",
       " '[UNK8]',\n",
       " '[UNK9]',\n",
       " '[unused0]',\n",
       " '[unused1]',\n",
       " '[unused2]',\n",
       " '[unused3]',\n",
       " '[unused4]',\n",
       " '[unused5]',\n",
       " '[unused6]',\n",
       " '[unused7]',\n",
       " '[unused8]',\n",
       " '[unused9]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token 체크\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f88b41-33a7-4107-8833-d14fbdecd495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data11/ai_hub/vocab//tl1-1줄-mecab-30000-speical/tokenizer_config.json',\n",
       " '../../data11/ai_hub/vocab//tl1-1줄-mecab-30000-speical/special_tokens_map.json',\n",
       " '../../data11/ai_hub/vocab//tl1-1줄-mecab-30000-speical/vocab.txt',\n",
       " '../../data11/ai_hub/vocab//tl1-1줄-mecab-30000-speical/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "tokenizer.save_pretrained(vocab_special_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77167b7a-105d-49c8-87de-a025c6e55526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]']\n",
      "vocab size : 30000\n",
      "Tokens (str)      : ['[CLS]', '나', '##는', '오늘', '아침', '##밥', '##을', '먹', '##었', '##다', '.', '[SEP]']\n",
      "Tokens (int)      : [2, 194, 1283, 2477, 3348, 1482, 1303, 411, 1282, 1044, 17, 3]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# special vocab 확인\n",
    "\n",
    "tokenizer_check = BertTokenizerFast.from_pretrained(vocab_special_path)\n",
    "\n",
    "print('check special tokens : %s'%tokenizer_check.all_special_tokens[:20])\n",
    "\n",
    "print('vocab size : %d' % tokenizer_check.vocab_size)\n",
    "tokenized_input_for_pytorch = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
    "#tokenized_input_for_tensorflow = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
    "\n",
    "print(\"Tokens (str)      : {}\".format([tokenizer_check.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
    "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
    "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a046a4-8f00-42c0-a3d1-718d40e75086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
