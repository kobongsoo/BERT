{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88fe7f-983a-47ce-b0f0-4938213a6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# bertwordpiece 방식으로 토큰 사전 생성 \n",
    "#\n",
    "# 1) mecab 로 한번 말뭉치 분석\n",
    "# 2) 분석된 말뭉치를 가지고 wordpiece 토큰화 \n",
    "# 3) special token 추가\n",
    "####################################################################\n",
    "\n",
    "#변수들 지정 \n",
    "#\n",
    "# input : 말뭉치 경로\n",
    "# mecab_output : 입력된 말뭉치를 mecab 으로 형태소 분석한 output 파일 \n",
    "# mecab_generation = True 이면, 형태소 앞에 ## 붙임, False면 안붙임\n",
    "\n",
    "input = '../../korpora/mycorpus/bong_corpus.txt'\n",
    "mecab_output = '../../tokenizer/my_bong_vocab/bong_corpus_mecab.txt' # [\"wiki_20190620_small.txt\", \"my_data/nsmc_subword.txt\"], 여러개 파일도 추가할수 있음\n",
    "mecab_generation = False\n",
    "\n",
    "vocab_folder = '../tokenizer'\n",
    "vocab_output = 'wiki_20190620_false_0311'\n",
    "vocab_special_output = 'wiki_20190620_false_0311_speical'\n",
    "\n",
    "vocab_size = 32000\n",
    "min_frequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134acfa0-0e37-4349-87ba-380a44095d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "#from utils import seed_everything\n",
    "\n",
    "#seed 설정 해도 vocab 은 다르게 생성됨\n",
    "#seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6d3b5-4ada-499d-8c69-e697ec64bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Mecab 를 이용하여, 말뭉치 형태소 분석 \n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4cda2-9fc1-44c8-bec7-530be21df113",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dafd2c-68ad-4172-802e-e9a93084e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:3])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e05782-051c-42bc-a1bd-b1173179e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97fad7-127c-4a5c-b771-10fae39ca9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: '어릴때' -> '어릴', '##'때' 로 일반화 시킴\n",
    "#sentence_count = 0\n",
    "if mecab_generation == True:\n",
    "  total_morph=[]\n",
    "  for sentence in tqdm(data):\n",
    "    morph_sentence=[]\n",
    "    count=0\n",
    "    for token_mecab in mecab.morphs(sentence):\n",
    "      token_mecab_save = token_mecab\n",
    "      if count > 0:\n",
    "        token_mecab_save = \"##\" + token_mecab_save\n",
    "        morph_sentence.append(token_mecab_save)\n",
    "      else:\n",
    "        morph_sentence.append(token_mecab_save)\n",
    "        count += 1\n",
    "    total_morph.append(morph_sentence)  \n",
    "    \n",
    "    #if sentence_count % 1000000 == 0:\n",
    "    #    print('sentence_count:{}'.format(sentence_count))\n",
    "    \n",
    "    #sentence_count += 1\n",
    "        \n",
    "# False: '어릴때' -> '어릴', '때' 로 일반화 시킴\n",
    "else:\n",
    "  total_morph=[]\n",
    "  for sentence in tqdm(data):\n",
    "    morph_sentence = mecab.morphs(sentence)\n",
    "    total_morph.append(morph_sentence)\n",
    "   \n",
    "    #if sentence_count % 1000000 == 0:\n",
    "    #   print('sentence_count:{}'.format(sentence_count))\n",
    "    \n",
    "    #sentence_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da132c29-774e-4839-95bc-95b68107682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_morph[:3])\n",
    "print(len(total_morph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7bb55-903e-454f-b30f-16bae52793be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_count = 0\n",
    "with open(mecab_output, 'w', encoding='utf-8') as f:\n",
    "  for line in tqdm(total_morph):\n",
    "    # ** 앞에 ' ' 공백을 넣어서 형태로 별로 분리하여 저장함\n",
    "    f.write(' '.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35addfce-1479-42fc-9974-4486d175630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bert Tokenizer \n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff59481-7b46-4817-96be-b6e75d062621",
   "metadata": {},
   "outputs": [],
   "source": [
    "WPT = BertWordPieceTokenizer(\n",
    "    #vocab='/content/wordPieceTokenizer/wiki_20190620-vocab.txt', # 기존 vocab 넣어도 사전 학습 안됨\n",
    "    clean_text=True,       # 토큰에서 공백은 제거 \n",
    "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼갬\n",
    "    strip_accents=False,  # True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "    lowercase=False       # 한국어에서는 반드시 False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47043d-d7f6-463d-b7ed-9f4ce28bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "WPT.train(\n",
    "    #files=[\"my_data/wiki_20190620_small.txt\", \"my_data/nsmc_subword.txt\"],  #여러개 파일도 추가할수 있음\n",
    "    files=[mecab_output],\n",
    "    vocab_size=vocab_size,       # 만들고자 하는 vocab의 size, 보통 '32000' 정도가 좋다고 알려져 있다\n",
    "    min_frequency=min_frequency,        # merge를 수행할 최소 빈도수, 2로 설정 시 2회 이상 등장한 pair만 수행한다\n",
    "    show_progress=True,     # 학습 진행과정 show\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],# Tokenizer에 추가하고 싶은 special token 지정\n",
    "    wordpieces_prefix=\"##\"\n",
    "    #limit_alphabet=6000  # merge 수행 전 initial tokens이 유지되는 숫자 제한\n",
    "    #initial_alphabet   # 꼭 포함됐으면 하는 initial alphabet, 이곳에 설정한 token은 학습되지 않고 그대로 포함되도록 설정된다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4953640-1217-4f33-b3eb-4f6f37bd9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 파일 저장\n",
    "WPT.save_model(vocab_folder, vocab_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265317e-410d-4069-870c-2c3e4cf6c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WPT.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74afd6-3a2a-4a0a-bd4a-c0db10309154",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"오늘은 경마장에서 조랑말 탄다\"\n",
    "tokenized_text = WPT.encode(text)\n",
    "print(tokenized_text.tokens)\n",
    "print(tokenized_text.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2914b-4f23-4211-9a19-02f73f7a4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### speical 토큰 추가 및 json 파일 생성\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a89904-210b-40c1-bb55-2d99f4cab678",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = vocab_folder + '/' + vocab_output + '-vocab.txt'\n",
    "vocab_special_path =  vocab_folder + '/' + vocab_special_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d000ab3-e72c-4d1c-9da6-dffccf926918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_path)\n",
    "print(vocab_special_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e97667-523d-42ca-b58c-b4e899f7a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizerFast, BertTokenizer 둘중 하나를 사용하면 됨\n",
    "'''\n",
    "tokenizer = BertTokenizerFast(vocab_file=vocab_path, \n",
    "                              max_len=128, \n",
    "                              do_lower_case=False)\n",
    "'''                              \n",
    "\n",
    "#print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\"))\n",
    "#tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=vocab_path, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe400f9-6fef-44f3-b897-20f851201aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Special Token 지정\n",
    "\n",
    "-special_tokens : 언어모델의 범용성을 위해 Dummy token, 여러 개의 [unused]와 [UNK]를 꼭 설정해야 한다.\n",
    "\n",
    "-언어모델에 번역, 요약, 개체명 인식 모델을 fine-tuning시 Dummy token이 필요한 경우가 많다. 또 도메인 특화된 task를 수행할 땐 도메인 토큰을 따로 선언하는게 필수이다.\n",
    "\n",
    "-이를 고려하지 않으면 편법을 사용해야 한다. ETRI의 KorBERT는 Dummy token이 없어 빈도수가 작은 token을 Dummy로 대체해서 쓰기도 한다.\n",
    "\n",
    "-꼭 충분한 unused와 UNK를 설정하자. 그리고 BOS(문장 시작), EOS(문장 끝) 등등도 추가하자. 본 프로젝트에선 10개의 UNK와 200개의 unsed token을 선언하였다.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15abbd-6d89-4329-a9f7-1b25b82b6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467a7a0-56e5-453f-93dd-2f60885f4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special token 체크\n",
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f88b41-33a7-4107-8833-d14fbdecd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "tokenizer.save_pretrained(vocab_special_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77167b7a-105d-49c8-87de-a025c6e55526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special vocab 확인\n",
    "\n",
    "tokenizer_check = BertTokenizerFast.from_pretrained(vocab_special_path)\n",
    "\n",
    "print('check special tokens : %s'%tokenizer_check.all_special_tokens[:20])\n",
    "\n",
    "print('vocab size : %d' % tokenizer_check.vocab_size)\n",
    "tokenized_input_for_pytorch = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"pt\")\n",
    "#tokenized_input_for_tensorflow = tokenizer_check(\"나는 오늘 아침밥을 먹었다.\", return_tensors=\"tf\")\n",
    "\n",
    "print(\"Tokens (str)      : {}\".format([tokenizer_check.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
    "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
    "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a046a4-8f00-42c0-a3d1-718d40e75086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
