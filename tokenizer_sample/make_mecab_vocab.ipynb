{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6013926b-4331-43f9-9a93-13d0b3cb34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# 기존 bert vocab에 새로운 도메인 vocab만들어서 추가하기\n",
    "#\n",
    "# =>기존 pre-trained bert vocab에는 전문단어 domain이 없다(예:문서중앙화, COVID 등)  \n",
    "# 따라서 전문 domain을 기존 bert vocab에 추가해야 한다.\n",
    "# => 따라서 여기서는 mecab(한국어 형태로 분석기)를 이용하여, 추가 vocab을 만들고, \n",
    "# bert wordpiecetokenzer에 추가하는 방법에 대해 설명한다\n",
    "# \n",
    "# 참고 자료 \n",
    "# https://github.com/piegu/language-models/blob/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb\n",
    "# https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41\n",
    "# https://wikidocs.net/64517\n",
    "#\n",
    "# [과정]\n",
    "# 1) 도메인 말뭉치(예:kowiki.txt)에서 mecab을 이용하여 형태소 분석하여 단어들을 추출함.\n",
    "#   => mecab으로 형태소 혹은 명사만 분할하면서, subword 앞에는 '##' prefix 추가함(**ertwordpiece subword와 동일하게)\n",
    "# 2) NLTK 빈도수 계산하는 FreqDist()를 이용하여, 단어들이 빈도수 계산\n",
    "# 3) 상위 빈도수(예:30000)를 가지는 단어들만 add_vocab.txt 로 만듬\n",
    "# 4) 기존 bert vocab.txt 파일에 직접, add_vocab.txt 토큰들 추가(*이때 중복 제거함)\n",
    "# 5) 추가한 vocab을 가지고, tokenizer 생성하고, special 토큰 추가 후, 저장\n",
    "#\n",
    "# **원래 참고자료에는 tokenizer.add_tokens() 으로 추가하면, added_tokens.json 파일에 추가되는데, \n",
    "# 이때 BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름:엄청 느려지는것 같음)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#==============================================================================================\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db39d2f-f474-482b-95dd-8321d62d1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Bible', 'Coloring']\n",
      "49925504\n"
     ]
    }
   ],
   "source": [
    "# wiki_20190620.txt 말뭉치 불러옴.\n",
    "corpus = '../../korpora/mycorpus/bong_corpus.txt'\n",
    "#corpus = '../korpora/kowiki_20190620/wiki_20190620.txt'\n",
    "\n",
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split(' ') # 공백으로 구분해서 단어들을 추출함\n",
    "\n",
    "print(data[:3])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f16ffd7-2e89-4057-83d0-9db4a468ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 형태소 분석기를 이용하여, 읽어온 말뭉치를 단어,조사등으로 분리함\n",
    "# => 불용어는 제거함\n",
    "# => mecab으로 형태소 분할하면서, subword 앞에는 prefix '##' 추가함\n",
    "\n",
    "# Mecab 선언\n",
    "mecab = Mecab()\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords=['이','가','께서','에서','이다','의','을','를','에','에게','께','와','에서', \n",
    "           '라고', '과','은', '는', '부터','.',',', '_']\n",
    "\n",
    "# Ture = nouns(명사)만 추출, False=형태소 추출\n",
    "nouns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1204a00d-63a6-4fab-a4a3-584f3b0d2ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545690583eaa47f2a8d6731c97308250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49925504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['성경', '이야기', '체험', '수', '컬러', '앱', '씨티', '##은행', '일', '푸리', '##토의', '베스트셀러', '해외', '입소문', '차', '완', '##판', '기록', '장', '예수']\n",
      "총 단어 수: 38961199\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "total_morph=[]\n",
    "for sentence in tqdm(data):\n",
    "    temp = mecab.morphs(sentence)\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    total_morph.append(temp)\n",
    "'''\n",
    "# mecab으로 형태소 혹은 명사만 분할할때, subword 앞에는 prefix '##' 추가함\n",
    "total_words=[]\n",
    "\n",
    "# 명사만 추출하는 경우\n",
    "if nouns == True:\n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.nouns(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count > 0:\n",
    "                    tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "# 형태소도 포함하여 추출하는 경우\n",
    "else:\n",
    "    \n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.morphs(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count > 0:\n",
    "                    tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "                \n",
    "print(total_words[:20])\n",
    "print(f'총 단어 수: {len(total_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14abac86-e25f-46f1-a12e-f6afb27d6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 226518\n",
      "[('것', 544850), ('수', 429271), ('일', 321147), ('등', 256988), ('년', 233256), ('우리', 147050), ('말', 145846), ('월', 133348), ('중', 122797), ('경우', 118851), ('사람', 115206), ('때', 114075), ('명', 108120), ('내', 101348), ('나', 99595), ('전', 95569), ('그', 92545), ('때문', 89031), ('가능', 88571), ('한국', 88270), ('만', 87652), ('필요', 85644), ('제', 78248), ('개', 77744), ('사용', 76063), ('저', 76035), ('지역', 73514), ('생각', 69880), ('##원', 68291), ('시', 66986), ('서울', 65148), ('시간', 63029), ('후', 62192), ('관련', 61413), ('번', 59608), ('미국', 59559), ('이번', 58394), ('점', 57491), ('조', 56999), ('진행', 56981), ('결과', 55916), ('문제', 55601), ('위', 55540), ('발생', 54095), ('시작', 53904), ('운영', 53888), ('연구', 53854), ('지원', 52747), ('곳', 51900), ('경기', 51638), ('확인', 51352), ('당신', 50848), ('억', 49893), ('다음', 49692), ('환자', 49368), ('기업', 48028), ('정부', 47955), ('대표', 47608), ('국내', 47229), ('게', 46803), ('하나', 46718), ('원', 46457), ('시장', 46445), ('상황', 45697), ('이상', 45296), ('세계', 44357), ('이후', 44235), ('차', 42540), ('현재', 42364), ('사업', 42035), ('중국', 41739), ('금융', 41442), ('모두', 41410), ('정도', 41216), ('최근', 41008), ('기술', 40924), ('해당', 40612), ('학생', 40418), ('뒤', 40379), ('코로나', 40274), ('사건', 39838), ('선수', 39362), ('교육', 38815), ('등이', 38454), ('제공', 38250), ('서비스', 38206), ('이용', 38163), ('분석', 37810), ('사회', 37559), ('개발', 37535), ('분', 36836), ('참여', 36687), ('학교', 35900), ('거', 35854), ('가운데', 35636), ('정보', 35612), ('사실', 35196), ('방법', 35059), ('대상', 34825), ('앞', 34819), ('설명', 34478), ('중요', 34156), ('문화', 33756), ('친구', 33351), ('여부', 33342), ('조사', 33300), ('대', 33287), ('건강', 33258), ('관리', 33178), ('국가', 33080), ('대통령', 33047), ('평가', 32942), ('데', 32895), ('마을', 32554), ('올해', 32527), ('경제', 32023), ('포함', 32004), ('등의', 31956), ('증가', 31887), ('부분', 31466), ('지금', 31333), ('지난해', 31270), ('##자의', 31262), ('관심', 30929), ('김', 30898), ('씨', 30871), ('활용', 30760), ('예정', 30609), ('적용', 30170), ('확진', 30062), ('결정', 29809), ('투자', 29764), ('이날', 29720), ('영향', 29636), ('활동', 29240), ('약', 29186), ('회사', 29167), ('계획', 28986), ('호', 28959), ('직접', 28940), ('집', 28881), ('규모', 28826), ('건', 28758), ('기준', 28627), ('감독', 28425), ('회', 28282), ('추가', 28188), ('기록', 28167), ('주', 28061), ('과정', 27646), ('##기관', 27632), ('모습', 27626), ('내용', 27491), ('세', 27423), ('이유', 27089), ('처음', 27063), ('제품', 27038), ('행사', 26990), ('볼', 26862), ('관계자', 26771), ('아이', 26696), ('동안', 26657), ('자리', 26655), ('##팀', 26617), ('일본', 26524), ('이야기', 26391), ('일부', 26321), ('치료', 26258), ('여기', 26216), ('기대', 26101), ('병원', 26045), ('여성', 25918), ('이것', 25841), ('끝', 25826), ('개인', 25663), ('당시', 25612), ('판단', 25589), ('달', 25493), ('구청장', 25492), ('여러분', 25444), ('사이', 25390), ('##씨', 25244), ('팀', 25178), ('위치', 25155), ('의미', 25144), ('경찰', 25046), ('날', 25020), ('##위원회', 24962), ('명의', 24911), ('구성', 24808), ('상태', 24625), ('프로그램', 24568), ('효과', 24458), ('공개', 24428), ('가족', 24411), ('설치', 24339), ('준비', 24337), ('그것', 24255), ('원고', 24248), ('##센터', 24217), ('##간', 24179), ('검사', 24150), ('규정', 23904), ('가지', 23860), ('자신', 23844), ('발표', 23820), ('국민', 23805), ('변화', 23752), ('주장', 23730), ('마련', 23365), ('부산', 23219), ('적극', 23213), ('만큼', 23162), ('전망', 22941), ('계속', 22899), ('사진', 22841), ('고객', 22834), ('속', 22776), ('오늘', 22769), ('도시', 22718), ('##일', 22677), ('마음', 22662), ('방문', 22591), ('시행', 22459), ('환경', 22419), ('감사', 22297), ('총', 22284), ('##병원', 22236), ('추진', 22201), ('실시', 22184), ('##회의', 22157), ('수도', 22149), ('지급', 22014), ('전체', 21955), ('##시장', 21906), ('항', 21888), ('노력', 21684), ('의원', 21669), ('업무', 21664), ('확대', 21513), ('기간', 21431), ('의료', 21311), ('예상', 21243), ('시스템', 21238), ('기존', 21094), ('간', 20962), ('사례', 20891), ('상대', 20800), ('이름', 20564), ('오후', 20551), ('대부분', 20526), ('지방', 20460), ('산업', 20446), ('중심', 20440), ('정상', 20430), ('관계', 20392), ('배', 20388), ('성장', 20374), ('고려', 20366), ('지속', 20308), ('인정', 20252), ('북한', 20125), ('가격', 20111), ('생활', 20055), ('주민', 20024), ('사항', 19758), ('일반', 19748), ('미', 19747), ('안', 19683), ('손', 19683), ('##사업', 19664), ('정책', 19654), ('주요', 19652), ('방식', 19456), ('법', 19392), ('특별', 19391), ('회장', 19374), ('돈', 19356), ('신청', 19321), ('##법', 19305), ('부담', 19290), ('인', 19273), ('경험', 19265), ('요구', 19223), ('이하', 19158), ('개선', 19038), ('영화', 19032), ('게임', 18979), ('물', 18976), ('우려', 18810), ('전문', 18805), ('아래', 18654), ('유지', 18648), ('직원', 18579), ('분야', 18548), ('무엇', 18487), ('지적', 18471), ('시민', 18459), ('제출', 18442), ('판매', 18429), ('시즌', 18420), ('목적', 18376), ('영상', 18212), ('해외', 18154), ('전국', 18073), ('구조', 18053), ('공간', 18033), ('데이터', 18016), ('피해', 18011), ('위험', 18004), ('사고', 17948), ('선정', 17913), ('학부모', 17865), ('이해', 17825), ('공동', 17737), ('여행', 17714), ('미래', 17692), ('기', 17669), ('자연', 17664), ('도움', 17459), ('생산', 17433), ('감염', 17381), ('시기', 17362), ('기능', 17316), ('제안', 17312), ('국제', 17308), ('위원', 17281), ('상', 17235), ('상품', 17220), ('구', 17202), ('강화', 17201), ('사랑', 17153), ('##관리', 17112), ('##시간', 17094), ('##시설', 17067), ('시설', 17058), ('연결', 17049), ('개최', 16989), ('성공', 16986), ('역할', 16906), ('##정보', 16899), ('발견', 16882), ('자료', 16880), ('신의', 16877), ('대회', 16868), ('비교', 16866), ('의견', 16865), ('눈', 16841), ('의사', 16719), ('수준', 16663), ('기회', 16646), ('개월', 16566), ('건물', 16547), ('현장', 16510), ('교수', 16495), ('음식', 16489), ('뿐', 16478), ('줄', 16457), ('선택', 16338), ('조성', 16327), ('저희', 16266), ('오전', 16237), ('지정', 16229), ('##전', 16215), ('최대', 16166), ('문', 16094), ('참석', 16078), ('층', 16004), ('장', 15991), ('계약', 15989), ('위원장', 15977), ('사', 15945), ('은행', 15936), ('겁니다', 15918), ('자', 15902), ('강조', 15866), ('주변', 15861), ('감소', 15787), ('처리', 15756), ('몸', 15729), ('##지역', 15688), ('밖', 15650), ('여자', 15638), ('반', 15556), ('글로벌', 15523), ('##조사', 15415), ('수행', 15414), ('너', 15368), ('역사', 15353), ('대구', 15284), ('집중', 15277), ('걸', 15252), ('도', 15242), ('알', 15169), ('자유', 15167), ('##시', 15151), ('체험', 15131), ('해결', 15087), ('대비', 15063), ('현지', 15040), ('수사', 15023), ('##학교', 15006), ('확보', 14984), ('비용', 14962), ('인천', 14905), ('발전', 14852), ('입장', 14824), ('대학', 14805), ('남자', 14780), ('기반', 14761), ('안전', 14755), ('웃음', 14723), ('조치', 14663), ('운동', 14639), ('부족', 14618), ('기관', 14589), ('일정', 14550), ('조례', 14531), ('요청', 14487), ('자체', 14407), ('##교육', 14407), ('공연', 14371), ('난', 14325), ('예방', 14311), ('실제', 14268), ('경기도', 14240), ('##명', 14111), ('안정', 14097), ('방송', 14050), ('검찰', 13990), ('인터넷', 13960), ('지', 13948), ('보호', 13941), ('주택', 13928), ('소속', 13909), ('군', 13868), ('변경', 13851), ('##장', 13768), ('확산', 13743), ('업체', 13725), ('##은행', 13688), ('##기업', 13644), ('증상', 13591), ('도입', 13568), ('전문가', 13565), ('팬', 13549), ('##부', 13526), ('길', 13520), ('##중', 13518), ('도로', 13516), ('차지', 13499), ('대출', 13472), ('후보', 13459), ('이전', 13444), ('제작', 13424), ('행정', 13411), ('단체', 13408), ('가치', 13379), ('인기', 13358), ('혐의', 13353), ('거래', 13338), ('하루', 13335), ('얘기', 13322), ('평균', 13305), ('보도', 13287), ('국회', 13254), ('힘', 13249), ('##위원', 13245), ('단계', 13190), ('부모', 13168), ('##장의', 13123), ('형성', 13100), ('그녀', 13069), ('부동산', 13069), ('거리', 13055), ('이곳', 13050), ('##기술', 13039), ('지난달', 13011), ('수익', 12998), ('공급', 12993), ('신고', 12980), ('리그', 12979), ('온라인', 12960), ('제외', 12955), ('차량', 12939), ('외국인', 12937), ('외', 12932), ('제시', 12902), ('취소', 12850), ('차이', 12848), ('범위', 12822), ('모델', 12751), ('마지막', 12728), ('##지원', 12712), ('방', 12672), ('##나라', 12610), ('상승', 12597), ('전자', 12575), ('나라', 12550), ('선', 12545)]\n"
     ]
    }
   ],
   "source": [
    "# FreqDist를 이용하여 빈도수 계산(*오래걸림)\n",
    "vocab = FreqDist(np.hstack(total_words))\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "# 최대 빈도수 가장높은 500개만 뽑아봄\n",
    "print(vocab.most_common(500))\n",
    "\n",
    "# 특정 단어의 빈도수를 뽑아봄\n",
    "#print(vocab['미국'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22c1ccf-d857-402c-bd74-294e01073d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*단어 집합의 크기 : 32000\n",
      "*마지막 단어 정보 : ('지관', 58)\n"
     ]
    }
   ],
   "source": [
    "# 상위 30000 개만 보존\n",
    "vocab_size = 32000\n",
    "vocab1 = vocab.most_common(vocab_size)\n",
    "\n",
    "vocab_len = len(vocab1)\n",
    "print('*단어 집합의 크기 : {}'.format(vocab_len))\n",
    "print('*마지막 단어 정보 : {}'.format(vocab1[vocab_len-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b936eb-4201-4257-8961-b1bbcb47119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "vocab2 = vocab.most_common(10)\n",
    "\n",
    "all_fdist = pd.Series(dict(vocab2))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04324119-dadd-4f50-bd8c-a51fab023fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861da4c726fb425e8629c0d5cb7dda9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['확인', '당신', '억', '다음', '환자', '기업', '정부', '대표', '국내', '게', '하나', '원', '시장', '상황', '이상', '세계', '이후', '차', '현재', '사업']\n"
     ]
    }
   ],
   "source": [
    "# vocab을 list로 만듬\n",
    "new_vocab = []\n",
    "for index, word in tqdm(enumerate(vocab1)):\n",
    "    new_vocab.append(word[0])  # fword[0] 하면 단어만 추출\n",
    "    \n",
    "# 리스트 출력해봄\n",
    "print(new_vocab[50:70])\n",
    "#print(vocab['음악당'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5ff872f-9f1e-4364-b2be-0a43ef7a65a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfea3a760c5465a96cf985d32ac2f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new_vocab을 파일로 저장함\n",
    "new_vocab_out = 'new_vocab.txt'\n",
    "with open(new_vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(new_vocab):\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4825424-1f11-4029-bde4-ce7a6eb04e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eaf5b5a084428b83b2a459d7136ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['것', '수', '일', '등', '년', '우리', '말', '월', '중', '경우']\n"
     ]
    }
   ],
   "source": [
    "# new_vocab 파일을 불러옴.\n",
    "new_vocab_out = 'new_vocab.txt'\n",
    "\n",
    "# 2차 불용어 정의\n",
    "stopwords=['##다', '##하', '있', '##고', '##로', '##한', '##적', '##되', '##었', '##_']\n",
    "\n",
    "new_vocab = []\n",
    "with open(new_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "for word in tqdm(data):\n",
    "    if not word in stopwords:  # 2차 불용어는 제외\n",
    "        new_vocab.append(word)\n",
    "        \n",
    "print(new_vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be46a8a-f837-49e4-887e-5d43c41cf183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da099b84b3b449fa581d9169e8e41a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n",
      "['વ', 'શ', 'ષ', 'સ']\n"
     ]
    }
   ],
   "source": [
    "# 원래 vocab 리스트로 읽어옴\n",
    "org_vocab_out = '../../model/bert/bert-multilingual-cased/vocab.txt'\n",
    "org_vocab = []\n",
    "\n",
    "with open(org_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "for vocab in tqdm(data):\n",
    "     org_vocab.append(vocab)\n",
    "\n",
    "print(len(org_vocab))\n",
    "print(org_vocab[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ff7adf-befa-4dd3-aeb1-2e4e1a655111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149794\n",
      "['વ', 'શ', 'ષ', 'સ']\n"
     ]
    }
   ],
   "source": [
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거\n",
    "#temp = list(set(new_vocab) - set(org_vocab)) #순서 보존이 안됨\n",
    "    \n",
    "#또는\n",
    "\n",
    "s = set(org_vocab)\n",
    "temp = [x for x in new_vocab if x not in s] #순서 보존됨    \n",
    "\n",
    "vocablist = org_vocab + temp  #원본 리스트 + 중복제가한 신규리스트 2개의 리스트를 합침\n",
    "\n",
    "print(len(vocablist))\n",
    "print(vocablist[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebdc047b-ad66-41e2-842e-17fce5dd26bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 파일\"new_vocab.txt\" 추가 성공!. 출력 vocab 파일 : \"add_new_vocab.txt\"\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 파일로 저장\n",
    "outfilepath = 'add_new_vocab.txt'\n",
    "with open(outfilepath, 'w') as f:\n",
    "    f.write('\\n'.join(vocablist))\n",
    "    \n",
    "print('vocab 파일\"{}\" 추가 성공!. 출력 vocab 파일 : \"{}\"'.format(new_vocab_out, outfilepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bfb223a-273f-4c78-90ce-2675c7b64764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizer 로 새롭게 저장된 vocalfile 불러옴\n",
    "tokenizer = BertTokenizer(vocab_file=outfilepath, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d40acc9-034d-4f3c-940b-38d657ce8839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]']\n"
     ]
    }
   ],
   "source": [
    "add_special_token = False\n",
    "\n",
    "if add_special_token:\n",
    "    # 옵션 : 저장된 add_new_vocab.txt를 불러와서 special 토큰 추가함\n",
    "    special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n",
    "\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # special token 체크\n",
    "    print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f8a3e-aebd-4e09-9ae6-4bc0b92fddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('new_vocab/tokenizer_config.json',\n",
       " 'new_vocab/special_tokens_map.json',\n",
       " 'new_vocab/vocab.txt',\n",
       " 'new_vocab/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "import os\n",
    "OUT_PATH = 'new_vocab'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb0a60-e94e-49ac-9d06-49c5cf6ea487",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#=============================================================================================\n",
    "# add_tokens으로 추가하면, BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#=============================================================================================\n",
    "\n",
    "# 기존 bert tokenizer 로딩\n",
    "vocab_path = '../model/bert/bert-multilingual-cased/vocab'\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
    "print(f'*기존 vocab 수: {len(tokenizer)}')\n",
    "\n",
    "# 기존 bert tokenizer에 신규 vocab 추가함 \n",
    "new_token_num = tokenizer.add_tokens(new_vocab)\n",
    "print(f'*신규 vocab 수: {len(tokenizer)}')\n",
    "print(f'*추가된 vocab: {new_token_num}')\n",
    "\n",
    "# 추가된 tokenizer 저장\n",
    "# => 추가된 token들은 added_tokens.json에 저장된다.\n",
    "import os\n",
    "OUT_PATH = 'new_vocab_nouns'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
