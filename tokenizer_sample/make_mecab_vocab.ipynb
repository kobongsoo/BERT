{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6013926b-4331-43f9-9a93-13d0b3cb34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# 기존 bert vocab에 새로운 도메인 vocab만들어서 추가하기\n",
    "#\n",
    "# =>기존 pre-trained bert vocab에는 전문단어 domain이 없다(예:문서중앙화, COVID 등)  \n",
    "# 따라서 전문 domain을 기존 bert vocab에 추가해야 한다.\n",
    "# => 따라서 여기서는 mecab(한국어 형태로 분석기)를 이용하여, 추가 vocab을 만들고, \n",
    "# bert wordpiecetokenzer에 추가하는 방법에 대해 설명한다\n",
    "# \n",
    "# 참고 자료 \n",
    "# https://github.com/piegu/language-models/blob/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb\n",
    "# https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41\n",
    "# https://wikidocs.net/64517\n",
    "#\n",
    "# [과정]\n",
    "# 1) 도메인 말뭉치(예:kowiki.txt)에서 mecab을 이용하여 형태소 분석하여 단어들을 추출함.\n",
    "#   => mecab으로 형태소 혹은 명사만 분할하면서, subword 앞에는 '##' prefix 추가함(**ertwordpiece subword와 동일하게)\n",
    "# 2) NLTK 빈도수 계산하는 FreqDist()를 이용하여, 단어들이 빈도수 계산\n",
    "# 3) 상위 빈도수(예:30000)를 가지는 단어들만 add_vocab.txt 로 만듬\n",
    "# 4) 기존 bert vocab.txt 파일에 직접, add_vocab.txt 토큰들 추가(*이때 중복 제거함)\n",
    "# 5) 추가한 vocab을 가지고, tokenizer 생성하고, special 토큰 추가 후, 저장\n",
    "#\n",
    "# **원래 참고자료에는 tokenizer.add_tokens() 으로 추가하면, added_tokens.json 파일에 추가되는데, \n",
    "# 이때 BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름:엄청 느려지는것 같음)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#==============================================================================================\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db39d2f-f474-482b-95dd-8321d62d1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*len:54354207\n",
      "['스포츠', '팀', '문화']\n"
     ]
    }
   ],
   "source": [
    "# wiki_20190620.txt 말뭉치 불러옴.\n",
    "corpus = '../../data11/my_corpus/re-kowiki-202206.txt'\n",
    "\n",
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    corpus_data = f.read().split(' ') # 공백으로 구분해서 단어들을 추출함\n",
    "\n",
    "print(f'*len:{len(corpus_data)}')\n",
    "print(corpus_data[:3])\n",
    "\n",
    "\n",
    "## yield 를 이용하여 generator 함수 정의  \n",
    "## => 10만 단위로 쪼갬\n",
    "def get_generator_corpus(data, max_len: int=100000):\n",
    "    \n",
    "    dataset = data\n",
    "    #dataset = list(set(data)) # ****중복 문장 제거(*순서 유지 안함)\n",
    "    \n",
    "    for start_idx in range(0, len(dataset), max_len):\n",
    "        samples = dataset[start_idx : start_idx + max_len]\n",
    "        yield samples\n",
    "        \n",
    "# 10만 단위에 generator 정의\n",
    "vocab_corpus = get_generator_corpus(corpus_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f16ffd7-2e89-4057-83d0-9db4a468ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 형태소 분석기를 이용하여, 읽어온 말뭉치를 단어,조사등으로 분리함\n",
    "# => 불용어는 제거함\n",
    "# => mecab으로 형태소 분할하면서, subword 앞에는 prefix '##' 추가함\n",
    "\n",
    "# Mecab 선언\n",
    "mecab = Mecab()\n",
    "\n",
    "# 불용어 정의\n",
    "#stopwords=['이','가','께서','에서','이다','의','을','를','에','에게','께','와','에서','라고', '과','은', '는', '부터','.',',', '_']\n",
    "#stopwords=['.',',', '_', '..','\"',';','<', '>', '(', ')', '[', ']', '=', '?', '!']\n",
    "stopwords=[]\n",
    "\n",
    "# True = nouns(명사)만 추출, False=형태소 추출(영어 와 그이외의 것으로 구분함)\n",
    "nouns = False\n",
    "\n",
    "# True = sentencepiece 일때 word = _ 붙임, subword=그대로\n",
    "# False = bertwordpiece 일때 word = 그대로, subword=## 붙임\n",
    "SentencePieceTokenizer = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1204a00d-63a6-4fab-a4a3-584f3b0d2ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14297dffbfb42d5aea76212fdc2bf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['스포츠', '팀', '문화', '활동', '해외', '여행', '등의', '다양', '##한', '과외', '##활동', '##을', '제공', '##한다', '##남자', '자유형', '200', '##m', '종목', '##의']\n",
      "*총 단어 수: 110338146\n",
      "*영어 단어 수: 2223055\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "total_morph=[]\n",
    "for sentence in tqdm(data):\n",
    "    temp = mecab.morphs(sentence)\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    total_morph.append(temp)\n",
    "'''\n",
    "# mecab으로 형태소 혹은 명사만 분할할때, subword 앞에는 prefix '##' 추가함\n",
    "total_words=[]\n",
    "en_words=[]  ## 영어단어만 저장해둘 리스트\n",
    "\n",
    "# 명사만 추출하는 경우\n",
    "if nouns == True:\n",
    "    for data1 in tqdm(vocab_corpus):\n",
    "        for words in data1:\n",
    "            count=0\n",
    "\n",
    "            for word in mecab.nouns(words):\n",
    "                if not word in stopwords:\n",
    "                    tmp = word\n",
    "\n",
    "                    if count > 0:\n",
    "                        if SentencePieceTokenizer == False:\n",
    "                            tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                        total_words.append(tmp)  \n",
    "                    else:\n",
    "                        if SentencePieceTokenizer == True:\n",
    "                             tmp = \"▁\" + tmp\n",
    "                        total_words.append(tmp)  \n",
    "                        count += 1\n",
    "# 형태소도 포함하여 추출하는 경우\n",
    "else:\n",
    "    for data1 in tqdm(vocab_corpus):\n",
    "        for words in data1:\n",
    "            count=0\n",
    "\n",
    "            for word in mecab.morphs(words):\n",
    "                if not word in stopwords:\n",
    "                    isen=False     # True=영어단어, false=영어단어가 아님\n",
    "                    tmp = word\n",
    "                    if tmp.encode().isalpha(): #영어단어인지, string형.isalpha() 함수로 확인\n",
    "                        isen=True\n",
    "\n",
    "                    if count > 0:\n",
    "                        if SentencePieceTokenizer == False:\n",
    "                            tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                        total_words.append(tmp)  \n",
    "                        if isen==True:   ## 영어단어인경우에는 en_words 별도 리스트에 저장해둠\n",
    "                            en_words.append(tmp)\n",
    "                    else:\n",
    "                        if SentencePieceTokenizer == True:\n",
    "                            tmp = \"▁\" + tmp\n",
    "                        total_words.append(tmp)  \n",
    "                        if isen==True:  ## 영어단어인경우에는 en_words 별도 리스트에 저장해둠\n",
    "                            en_words.append(tmp)  \n",
    "                        count += 1\n",
    "                \n",
    "print(total_words[:20])\n",
    "print(f'*총 단어 수: {len(total_words)}')\n",
    "print(f'*영어 단어 수: {len(en_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14abac86-e25f-46f1-a12e-f6afb27d6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 739503\n",
      "[('##다', 3007196), ('##는', 2984784), ('##에', 2610560), ('##이', 2574693), ('##을', 2546702), ('##하', 2160669), ('##의', 1725650), ('##은', 1629663), ('##를', 1514200), ('##년', 1350221), ('##고', 1295296), ('##에서', 1111884), ('##으로', 1100532), ('있', 1065394), ('##가', 968509), ('##었', 938707), ('##로', 894750), ('##한', 828752), ('##되', 766299), ('##들', 668519), ('##과', 608169), ('##했', 553284), ('##월', 549845), ('##적', 545029), ('##였', 514083), ('##와', 502548), ('이', 463787), ('##도', 462434), ('##인', 454037), ('것', 430140), ('##일', 424797), ('##어', 419397), ('##기', 403350), ('##게', 374434), ('그', 365072), ('##여', 350998), ('##지', 309992), ('하', 299505), ('##며', 294010), ('수', 281559), ('1', 276575), ('##된', 273833), ('되', 259924), ('2', 246056), ('##던', 235586), ('\"', 229244), ('##\"', 223143), ('3', 210424), ('##지만', 205249), ('##한다', 199897), ('않', 197711), ('는', 191640), ('##할', 190869), ('##았', 189170), ('##으며', 182622), ('##까지', 178850), ('##에게', 174807), ('등', 170714), ('4', 158356), ('##해', 154797), ('##그', 152666), ('##만', 149552), ('##는데', 146047), ('##자', 144807), ('은', 144028), (\"'\", 142330), ('제', 141448), (\"##'\", 140985), ('##부터', 139014), ('한', 137413), ('때', 137047), ('5', 136398), ('사용', 134688), ('중', 134417), ('받', 133929), ('의', 133471), ('##면서', 133230), ('##아', 132860), ('##면', 130547), ('없', 126107), ('##라고', 120068), ('후', 118815), ('##라는', 116119), ('6', 113865), ('10', 111536), ('에', 110915), ('##나', 109900), ('##다고', 109394), ('대한', 108084), ('미국', 106445), ('말', 105068), ('##개', 104182), ('및', 103477), ('한다', 103214), ('두', 102621), ('때문', 101521), ('했', 100994), ('전', 100593), ('##대', 100289), ('같', 100190)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3defe4abc68d43a8947c77822c6fcee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/739503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.FreqDist를 이용하여 빈도수 계산(*오래걸림)\n",
    "vocab = FreqDist(np.hstack(total_words))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "\n",
    "# 최대 빈도수 가장높은 100개만 뽑아봄\n",
    "print(vocab.most_common(100))\n",
    "# 특정 단어의 빈도수를 뽑아봄\n",
    "#print(vocab['미국'])\n",
    "\n",
    "# 임시파일로 저장 해줌.\n",
    "FreqDistFilePath = './re-kowiki2022-morphs-FreqDist.txt'\n",
    "with open(FreqDistFilePath, 'w', encoding='utf-8') as f:\n",
    "    for tmpvocab in tqdm(vocab.most_common(len(vocab))):\n",
    "        tmp=list(tmpvocab)  # tuple=>list로 변환\n",
    "        f.write(str(tmp[1])+' '+tmp[0]+'\\n') # 빈도수 명사 순으로 표기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d5c0a79-4a85-485c-a085-72057c5cff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*단어 집합의 크기 : 62000\n",
      "*마지막 단어 정보 : ('로게', 61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89212187c174705893c674ba2c1371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['않', '는', '##할', '##았', '##으며', '##까지', '##에게', '등', '4', '##해', '##그', '##만', '##는데', '##자', '은', \"'\", '제', \"##'\", '##부터', '한']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7614eec8f9c54b678492a4b78676f3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. 빈도수가 가장 많은 상위 단어들만 뽑아냄\n",
    "# 저장할 파일 경로 \n",
    "new_vocab_out = '../../data11/my_corpus/vocab2/re-moco-corpus2-morphs-62000.txt'\n",
    "vocab_size = 62000     # 상위 xxxx 개만 보존\n",
    "\n",
    "vocab1 = vocab.most_common(vocab_size)\n",
    "vocab_len = len(vocab1)\n",
    "print('*단어 집합의 크기 : {}'.format(vocab_len))\n",
    "print('*마지막 단어 정보 : {}'.format(vocab1[vocab_len-1]))\n",
    "\n",
    "# 3. vocab을 list로 만듬\n",
    "new_vocab = []\n",
    "for index, word in tqdm(enumerate(vocab1)):\n",
    "    new_vocab.append(word[0])  # fword[0] 하면 단어만 추출\n",
    "    \n",
    "# 리스트 출력해봄\n",
    "print(new_vocab[50:70])\n",
    "#print(vocab['음악당'])\n",
    "\n",
    "# 4. vocab을 파일로 저장함\n",
    "with open(new_vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(new_vocab):\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b936eb-4201-4257-8961-b1bbcb47119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "vocab2 = vocab.most_common(10)\n",
    "\n",
    "all_fdist = pd.Series(dict(vocab2))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaed46b-9bd0-431b-91c0-d9d81e9ed284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# 형태소 분석인 경우 영어단어들만 별도로 파일로 저장해 둠.\n",
    "# 영문 word 리스트 빈도수 계산 후 vocab_size 만큼 vocab을 파일로 저장\n",
    "#######################################################################\n",
    "if nouns == False:\n",
    "    en_vocab_size = 12000  # 최대 저장할 단어수 \n",
    "    new_en_vocab_out = '../../data11/my_corpus/vocab/re-moco-corpus2-en-morphs-12000.txt' #저장할 파일 경로 \n",
    "    \n",
    "    # 1. 빈도수 계산\n",
    "    en_vocab = FreqDist(np.hstack(en_words))\n",
    "    # 최대 빈도수 가장높은 500개만 뽑아봄\n",
    "    print(en_vocab.most_common(500))\n",
    "\n",
    "    # 2. 빈도수가 가장 많은 상위 단어들만 뽑아냄\n",
    "    en_vocab1 = en_vocab.most_common(en_vocab_size)\n",
    "    \n",
    "    en_vocab_len = len(en_vocab1)\n",
    "    print('*EN 단어 집합의 크기 : {}'.format(en_vocab_len))\n",
    "    print('*EN 마지막 단어 정보 : {}'.format(en_vocab1[en_vocab_len-1]))\n",
    "    \n",
    "    # 3. vocab을 list로 만듬\n",
    "    new_en_vocab = []\n",
    "    for index, word in tqdm(enumerate(en_vocab1)):\n",
    "        new_en_vocab.append(word[0])  # fword[0] 하면 단어만 추출\n",
    "    \n",
    "    # 리스트 출력해봄\n",
    "    print(new_en_vocab[0:5])\n",
    "    #print(vocab['음악당'])\n",
    "        \n",
    "    # 4.vocab을 파일로 저장함\n",
    "    with open(new_en_vocab_out, 'w', encoding='utf-8') as f:\n",
    "        for word in tqdm(new_en_vocab):\n",
    "            f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4825424-1f11-4029-bde4-ce7a6eb04e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_vocab 파일을 불러옴.\n",
    "#new_vocab_out = 'new_vocab.txt'\n",
    "\n",
    "# 2차 불용어 정의\n",
    "#stopwords=['##다', '##하', '있', '##고', '##로', '##한', '##적', '##되', '##었', '##_']\n",
    "stopwords=['##_', '〈', '..']\n",
    "\n",
    "new_vocab = []\n",
    "with open(new_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "for word in tqdm(data):\n",
    "    if not word in stopwords:  # 2차 불용어는 제외\n",
    "        new_vocab.append(word)\n",
    "        \n",
    "print(new_vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be46a8a-f837-49e4-887e-5d43c41cf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 vocab 리스트로 읽어옴\n",
    "org_vocab_out = '../../data11/model/distilbert/distilbert-base-multilingual-cased/vocab.txt'\n",
    "org_vocab = []\n",
    "\n",
    "with open(org_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "for vocab in tqdm(data):\n",
    "     org_vocab.append(vocab)\n",
    "\n",
    "print(len(org_vocab))\n",
    "print(org_vocab[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff7adf-befa-4dd3-aeb1-2e4e1a655111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거\n",
    "#temp = list(set(new_vocab) - set(org_vocab)) #순서 보존이 안됨\n",
    "    \n",
    "#또는\n",
    "\n",
    "s = set(org_vocab)\n",
    "temp = [x for x in new_vocab if x not in s] #순서 보존됨    \n",
    "\n",
    "vocablist = org_vocab + temp  #원본 리스트 + 중복제가한 신규리스트 2개의 리스트를 합침\n",
    "\n",
    "print(len(vocablist))\n",
    "print(vocablist[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc047b-ad66-41e2-842e-17fce5dd26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 파일로 저장\n",
    "outfilepath = '../../data11/my_corpus/kowiki-202206-1line-vocab.txt'\n",
    "with open(outfilepath, 'w') as f:\n",
    "    f.write('\\n'.join(vocablist))\n",
    "    \n",
    "print('vocab 파일\"{}\" 추가 성공!. 출력 vocab 파일 : \"{}\"'.format(new_vocab_out, outfilepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016358e0-def2-453e-9988-891af3270509",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **** 저장된 파일을 열어서, 중간(mbert는 119548라인)에 공백이 없는지 반드시 확인.\n",
    "\n",
    "## **** 반드시 또 추가할 단어들(예:문서중앙화, 보안파일서버, 엠파워, 모코엠시스, MOCOMSYS, MPOWER, EZis-C 등) 파일 맨 끝에 추가 *단 기존에 추가되어있는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb223a-273f-4c78-90ce-2675c7b64764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfilepath = './moco-vocab/moco-corpus-kowiki2022-nouns-42000-Deduplication-add-en-159552.txt'\n",
    "# BertTokenizer 로 새롭게 저장된 vocalfile 불러옴\n",
    "tokenizer = BertTokenizer(vocab_file=outfilepath, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40acc9-034d-4f3c-940b-38d657ce8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_special_token = False\n",
    "\n",
    "if add_special_token:\n",
    "    # 옵션 : 저장된 add_new_vocab.txt를 불러와서 special 토큰 추가함\n",
    "    special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n",
    "\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # special token 체크\n",
    "    print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f8a3e-aebd-4e09-9ae6-4bc0b92fddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "import os\n",
    "OUT_PATH = '../../data11/model/distilbert/distilbert-base-multilingual-cased/kowiki-202206-1line-vocab'\n",
    "#OUT_PATH = './mdistilbertV2.2'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb0a60-e94e-49ac-9d06-49c5cf6ea487",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#=============================================================================================\n",
    "# add_tokens으로 추가하면, BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#=============================================================================================\n",
    "\n",
    "# 기존 bert tokenizer 로딩\n",
    "vocab_path = '../model/bert/bert-multilingual-cased/vocab'\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
    "print(f'*기존 vocab 수: {len(tokenizer)}')\n",
    "\n",
    "# 기존 bert tokenizer에 신규 vocab 추가함 \n",
    "new_token_num = tokenizer.add_tokens(new_vocab)\n",
    "print(f'*신규 vocab 수: {len(tokenizer)}')\n",
    "print(f'*추가된 vocab: {new_token_num}')\n",
    "\n",
    "# 추가된 tokenizer 저장\n",
    "# => 추가된 token들은 added_tokens.json에 저장된다.\n",
    "import os\n",
    "OUT_PATH = 'new_vocab_nouns'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
