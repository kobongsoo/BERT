{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cd181f-ea2a-444c-8bac-f224200858a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================\n",
    "# sentencepiece 사전 모델 처음부터 만드는 예제\n",
    "# => 말뭉치를 가지고, sp vocab을 만듬(학습)\n",
    "#\n",
    "# [인자]\n",
    "# -input_file : 입력할 말뭉치 데이터 경로 지정\n",
    "# -vocab_size : BPE의 단어수를 얼마로 할 것인가 이다. 너무 적으면 한 글자 단위로 쪼개지는 경향이 있고, 너무 많으면 쓸데없는 단어들이 만들어진다. \n",
    "#               주로 3,2000이 가장 좋다고 알려져 있다.\n",
    "# -model_name : 저장할 이름이다. 학습하고 나면 .model, .vocab 2개의 파일이 만들어진다.\n",
    "# -model_type : bpe, unigram 등이 있는데 두가지를 모두 사용해 보고 성능이 좋은 거로 고르도록 하자.\n",
    "# -character_coverage : 모든 단어를 커버할것인가, 너무 희귀한 단어는 뺄 것인가 이다. 학습 코퍼스가 대용량이라면 보통 0.9995로 사용하면 된다. 그런데 코퍼스가 작다면 1.0으로 지정하자. 그럼 [UNK]가 없다.\n",
    "# -user_defined_symbols : BPE로 생성된 단어 외 알고리즘에서 사용할 특수문자들을 지정한다. BERT 기반 요약, 번역모델 개발 시 BOS, EOS, SEP 등 추가 토큰들이 필요하다. \n",
    "#                         그러므로 user_defined_symbols을 이용해 Dummy token(UNK, unused, BOS, EOS 등등)을 추가해준다. \n",
    "#                         HTTP나 network traffic에 BPE를 적용해야 할 경우가 있다. \n",
    "#                         이때 user_defined_symbols 인자에 'http, GET, POST, User-Agent 등 HTTP 데이터에서 자주 나오는 Token들을 지정하기도 했다.\n",
    "# -vocab_size에 비례하여 학습시간이 결정된다. 필자는 32,000으로 지정하여 노트북에서 대략 43sec가 소요됐다.\n",
    "# -input_sentence_size : 입력문장 size가 너무 크면 OOM 발생함. 따라서 적당한 사이즈를 지정함(보통 7G=700000 지정함)\n",
    "# -shuffle_input_sentence : 입력문장을 섞는다.\n",
    "#\n",
    "# sentencepiecr 설치\n",
    "#!pip install sentencepiece\n",
    "#=======================================================================================\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import time\n",
    "from time import localtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02779d3b-f4bf-49fe-9a55-e34fba4d0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/spm_30000_token\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================\n",
    "# parameter 설정 \n",
    "#==================================================================================\n",
    "input_file = '../../data11/ai_hub/tl1/tl1-1줄-mecab.txt'\n",
    "\n",
    "# 모델 root 경로 지정\n",
    "model_root = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/'\n",
    "if not os.path.isdir(model_root):\n",
    "    os.mkdir(model_root)\n",
    "    \n",
    "vocab_size=30000\n",
    "\n",
    "# 모델명과, 풀경로 지정    \n",
    "model_name = 'spm_{}_token'.format(vocab_size)\n",
    "model_fullpath = os.path.join(model_root, model_name)\n",
    "print(model_fullpath)\n",
    "\n",
    "model_type = 'unigram' #'bpe' # model_type : 사용할 모델 (unigram(default), bpe, char, word)\n",
    "\n",
    "character_coverage = 0.9995#1.0 # 0.9995 # 모든 단어 커버할지(1.0 혹은 0.9995로 지정)\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS]'\n",
    "\n",
    "input_sentence_size=22000000 # 입력 문장 샘플링 크기 제한(*OOM 발생하면, size를 더 줄인다)\n",
    "shuffle_input_sentence=True # 문장을 섞는다.\n",
    "num_threads=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20072d63-5bc4-43a3-ad73-2ebbb5b01e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=../../data11/ai_hub/tl1/tl1-1줄-mecab.txt --model_prefix=../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/spm_30000_token --vocab_size=30000 --user_defined_symbols=[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS] --model_type=unigram --input_sentence_size=22000000 --shuffle_input_sentence=True --num_threads=35 --character_coverage=0.9995\n",
      "=== 시작시간: 2022-11-25 14:8:7 ===\n"
     ]
    }
   ],
   "source": [
    "# 훈련 파라메터 설정\n",
    "input_argument = '--input={0}\\\n",
    " --model_prefix={1}\\\n",
    " --vocab_size={2}\\\n",
    " --user_defined_symbols={3}\\\n",
    " --model_type={4}\\\n",
    " --input_sentence_size={5}\\\n",
    " --shuffle_input_sentence={6}\\\n",
    " --num_threads={7}\\\n",
    " --character_coverage={8}'\\\n",
    ".format(input_file, model_fullpath, vocab_size, user_defined_symbols, model_type, \n",
    "        input_sentence_size, shuffle_input_sentence, num_threads, character_coverage)\n",
    "\n",
    "print(input_argument)\n",
    "\n",
    "# 시작시간\n",
    "start = time.time()\n",
    "tm = localtime(start)\n",
    "print(f'=== 시작시간: {tm.tm_year}-{tm.tm_mon}-{tm.tm_mday} {tm.tm_hour}:{tm.tm_min}:{tm.tm_sec} ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f664d5-713c-4f77-b6fe-1fed3f76d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 처리시간: 2414.223 초 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=../../data11/ai_hub/tl1/tl1-1줄-mecab.txt --model_prefix=../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/spm_30000_token --vocab_size=30000 --user_defined_symbols=[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS] --model_type=unigram --input_sentence_size=22000000 --shuffle_input_sentence=True --num_threads=35 --character_coverage=0.9995\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../../data11/ai_hub/tl1/tl1-1줄-mecab.txt\n",
      "  input_format: \n",
      "  model_prefix: ../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/spm_30000_token\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 22000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 35\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [PAD]\n",
      "  user_defined_symbols: [UNK]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [MASK]\n",
      "  user_defined_symbols: [BOS]\n",
      "  user_defined_symbols: [EOS]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ../../data11/ai_hub/tl1/tl1-1줄-mecab.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 5000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 6000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 7000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 8000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 9000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 10000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 11000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 12000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 13000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 14000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 15000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 16000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 17000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 18000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 19000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 20000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 21000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 22000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 23000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 24000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 25000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 26000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 27000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 28000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 29000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 30000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 31000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 32000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 33000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 34000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 35000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 36000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 37000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 38000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 39000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 40000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 41000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 42000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 43000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 44000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 45000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 46000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 47000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 48000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 49000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 50000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 51000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 52000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 53000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 54000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (22000000), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(387) LOG(INFO) Sampled 22000000 sentences from 54944002 sentences.\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=2067746334\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1397\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 22000000 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 405940 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 22000000\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 453080\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 453080 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=254654 obj=8.15708 num_tokens=897456 num_tokens/piece=3.52422\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=214629 obj=7.54676 num_tokens=880754 num_tokens/piece=4.10361\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=160804 obj=7.42461 num_tokens=879653 num_tokens/piece=5.47034\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=159769 obj=7.41237 num_tokens=879552 num_tokens/piece=5.50515\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=119820 obj=7.41241 num_tokens=921826 num_tokens/piece=7.69342\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=119808 obj=7.41346 num_tokens=921839 num_tokens/piece=7.6943\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=89856 obj=7.41437 num_tokens=982545 num_tokens/piece=10.9347\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=89856 obj=7.41406 num_tokens=982535 num_tokens/piece=10.9346\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=67392 "
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "# => 훈련이 완료되고 나면 해당경로에 xxx.model, xxx.vocab 2개의 파일이 생성된다. (오래 걸림)\n",
    "spm.SentencePieceTrainer.Train(input_argument)\n",
    "\n",
    "print(f'=== 처리시간: {time.time() - start:.3f} 초 ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7c880e-8850-441f-af4f-e0f09fec118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 3164, 1610, 3518, 485, 2696, 980, 4898, 670]\n",
      "['▁나', '는', '▁아침', '에', '▁배', '를', '▁먹', '었', '다']\n",
      "나는 아침에 배를 먹었다\n",
      "나는 아침에 배를 먹었다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "obj=7.41986 num_tokens=1050521 num_tokens/piece=15.5882\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=67392 obj=7.41868 num_tokens=1050515 num_tokens/piece=15.5881\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=50544 obj=7.43028 num_tokens=1115248 num_tokens/piece=22.0649\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=50544 obj=7.42758 num_tokens=1115244 num_tokens/piece=22.0648\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=37908 obj=7.44841 num_tokens=1185556 num_tokens/piece=31.2746\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=37908 obj=7.44385 num_tokens=1185564 num_tokens/piece=31.2748\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33000 obj=7.45929 num_tokens=1218025 num_tokens/piece=36.9098\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=33000 obj=7.45623 num_tokens=1218026 num_tokens/piece=36.9099\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: ../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/spm_30000_token.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ../../data11/ai_hub/vocab/tl1-1줄-mecab-30000-sp-unigram-22000000/spm_30000_token.vocab\n"
     ]
    }
   ],
   "source": [
    "# 검증 시작\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(model_fullpath))\n",
    "\n",
    "tokens = sp.encode_as_pieces('나는 아침에 배를 먹었다')\n",
    "ids = sp.encode_as_ids('나는 아침에 배를 먹었다')\n",
    "\n",
    "print(ids)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sp.decode_pieces(tokens)\n",
    "ids = sp.decode_ids(ids)\n",
    "\n",
    "print(ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92060ca5-c9f1-4720-917b-ce8ea115f66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
