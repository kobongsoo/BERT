{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70cd181f-ea2a-444c-8bac-f224200858a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================\n",
    "# sentencepiece 사전 모델 처음부터 만드는 예제\n",
    "# => 말뭉치를 가지고, sp vocab을 만듬(학습)\n",
    "#\n",
    "# [인자]\n",
    "# -input_file : 입력할 말뭉치 데이터 경로 지정\n",
    "# -vocab_size : BPE의 단어수를 얼마로 할 것인가 이다. 너무 적으면 한 글자 단위로 쪼개지는 경향이 있고, 너무 많으면 쓸데없는 단어들이 만들어진다. \n",
    "#               주로 3,2000이 가장 좋다고 알려져 있다.\n",
    "# -model_name : 저장할 이름이다. 학습하고 나면 .model, .vocab 2개의 파일이 만들어진다.\n",
    "# -model_type : bpe, unigram 등이 있는데 두가지를 모두 사용해 보고 성능이 좋은 거로 고르도록 하자.\n",
    "# -character_coverage : 모든 단어를 커버할것인가, 너무 희귀한 단어는 뺄 것인가 이다. 학습 코퍼스가 대용량이라면 보통 0.9995로 사용하면 된다. 그런데 코퍼스가 작다면 1.0으로 지정하자. 그럼 [UNK]가 없다.\n",
    "# -user_defined_symbols : BPE로 생성된 단어 외 알고리즘에서 사용할 특수문자들을 지정한다. BERT 기반 요약, 번역모델 개발 시 BOS, EOS, SEP 등 추가 토큰들이 필요하다. \n",
    "#                         그러므로 user_defined_symbols을 이용해 Dummy token(UNK, unused, BOS, EOS 등등)을 추가해준다. \n",
    "#                         HTTP나 network traffic에 BPE를 적용해야 할 경우가 있다. \n",
    "#                         이때 user_defined_symbols 인자에 'http, GET, POST, User-Agent 등 HTTP 데이터에서 자주 나오는 Token들을 지정하기도 했다.\n",
    "# vocab_size에 비례하여 학습시간이 결정된다. 필자는 32,000으로 지정하여 노트북에서 대략 43sec가 소요됐다.\n",
    "#\n",
    "# sentencepiecr 설치\n",
    "#!pip install sentencepiece\n",
    "#=======================================================================================\n",
    "import sentencepiece as spm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02779d3b-f4bf-49fe-9a55-e34fba4d0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../sp/spm_32000_token\n"
     ]
    }
   ],
   "source": [
    "input_file = '../../../AI/kopora/kowiki_20190620_Full/wiki_20190620.txt'\n",
    "vocab_size=32000\n",
    "\n",
    "# 모델 root 경로 지정\n",
    "model_root = '../../sp/'\n",
    "if not os.path.isdir(model_root):\n",
    "    os.mkdir(model_root)\n",
    "    \n",
    "# 모델명과, 풀경로 지정    \n",
    "model_name = 'spm_{}_token'.format(vocab_size)\n",
    "model_fullpath = os.path.join(model_root, model_name)\n",
    "print(model_fullpath)\n",
    "\n",
    "model_type = 'bpe'\n",
    "character_coverage = 0.9995\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20072d63-5bc4-43a3-ad73-2ebbb5b01e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=../../../AI/kopora/kowiki_20190620_Full/wiki_20190620.txt --model_prefix=../../sp/spm_32000_token --vocab_size=32000 --user_defined_symbols=[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS] --model_type=bpe --character_coverage=0.9995\n"
     ]
    }
   ],
   "source": [
    "# 훈련 파라메터 설정\n",
    "input_argument = '--input={0}\\\n",
    " --model_prefix={1}\\\n",
    " --vocab_size={2}\\\n",
    " --user_defined_symbols={3}\\\n",
    " --model_type={4}\\\n",
    " --character_coverage={5}'\\\n",
    ".format(input_file, model_fullpath, vocab_size, user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "print(input_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f664d5-713c-4f77-b6fe-1fed3f76d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "# => 훈련이 완료되고 나면 해당경로에 xxx.model, xxx.vocab 2개의 파일이 생성된다. (오래 걸림)\n",
    "spm.SentencePieceTrainer.Train(input_argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7c880e-8850-441f-af4f-e0f09fec118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3906, 19469, 8434, 1046, 19]\n",
      "['▁나는', '▁아침에', '▁배를', '▁먹', '었다']\n",
      "나는 아침에 배를 먹었다\n",
      "나는 아침에 배를 먹었다\n"
     ]
    }
   ],
   "source": [
    "# 검증 시작\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(model_fullpath))\n",
    "\n",
    "tokens = sp.encode_as_pieces('나는 아침에 배를 먹었다')\n",
    "ids = sp.encode_as_ids('나는 아침에 배를 먹었다')\n",
    "\n",
    "print(ids)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sp.decode_pieces(tokens)\n",
    "ids = sp.decode_ids(ids)\n",
    "\n",
    "print(ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92060ca5-c9f1-4720-917b-ce8ea115f66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
