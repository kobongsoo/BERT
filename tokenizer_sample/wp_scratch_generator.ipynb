{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d917d8-7e21-4948-b647-d8c7f4090dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# 기존 bert vocab 형태의 새로운 vocab tokenizer 만들기\n",
    "#\n",
    "# => 기존 bert vocab을 모두 지우고, 새로운 vocab을 가지는 tokenizer 만들기\n",
    "# => AutoTokenizer.train_new_from_iterator() 이용\n",
    "# => 기존 bert-base-cased 모델에 있는 기존 영문 vocab들 대신에 한국어 말뭉치를 가지고 새롭게 한국어 vocab을 만든다.\n",
    "#\n",
    "# => 말뭉치데이터는 python 제너레이터 형태로 만든다.\n",
    "# => yield 문을 사용하여 for 루프 내에서 제너레이터(generator)를 정의한다.\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166821\n",
    "#\n",
    "# 허깅페이스 허브를 이용하여, 토크너나이즈 올리기\n",
    "# => git 설치되어 있어야 함.\n",
    "# => 출처 : https://zerolang.tistory.com/65\n",
    "#\n",
    "#==============================================================================================\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from os import sys\n",
    "sys.path.append('../')\n",
    "from myutils import seed_everything\n",
    "\n",
    "seed_everything(111)\n",
    "\n",
    "# 입력 말뭉치\n",
    "corpus = '../../data11/my_corpus/re-moco-corpus2.txt'\n",
    "# MECAB으로 전처리된 말뭉치\n",
    "out_corpus = '../../data11/my_corpus/re-moco-corpus2-mecab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b3eed1-4e8d-4fb6-83eb-57657e14f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example of an actor using the system It is not a use case and in most projects it does not survive', '37 서버명 업무내용 제조사 비고 형상관리 형상관리업무 HP 행정정보1 행정정보업무 HP 행정정보2 행정정보업무 HP 행정정보테스트 HP 파생상품1 파생상품업무 HP 파생상품2 파생상품업무 HP 파생상품3 파생상품업무 HP 파생상품테스트 파생상품테스트업무 HP EDDDB1 EDD DB업무 HP RAC EDDDB2 EDD DB업무 HP EDDWAS1 EDD WAS업무 HP EDDWAS2 EDD WAS업무 HP EDDWEB1 EDD WEB업무 HP EDDWEB2 EDD WEB업무 HP EDD테스트 EDD테스트업무 HP IFRS DB업무 HP IFRS테스트 IFRS DB테스트업무 HP', '변경에 따른 관련 법령 예 전자정부법 IT자원의 공동활용신설 공개SW의 이용 및 라이선스 등 하위규정 등']\n",
      "*len:3337234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be6f090727d4d8cb9d9ac1216ffe3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3337234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afa1af8b1994103b775498219be8712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3337234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#한국어 말뭉치인 경우에는 mecab(메케브)로 전처리 해줌\n",
    "##### Mecab 를 이용하여, 말뭉치 형태소 분석 \n",
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "print(data[:3])\n",
    "print(f'*len:{len(data)}')\n",
    "\n",
    "mecab = Mecab()\n",
    "# False: '어릴때' -> '어릴', '때' 로 일반화 시킴\n",
    "total_morph=[]\n",
    "\n",
    "for sentence in tqdm(data):\n",
    "    morph_sentence = mecab.morphs(sentence)\n",
    "    total_morph.append(morph_sentence)\n",
    "    \n",
    "# 파일로 저장\n",
    "with open(out_corpus, 'w', encoding='utf-8') as f:\n",
    "    for line in tqdm(total_morph):\n",
    "        # ** 앞에 ' ' 공백을 넣어서 형태로 별로 분리하여 저장함\n",
    "        f.write(' '.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100e08a3-39ef-4629-aba8-e0ae70893747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea020836063a4349be353e5758acef47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3337234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example of an actor using the system It is not a use case and in most projects it does not survive', '37 서버 명 업무 내용 제조사 비고 형상 관리 형상 관리 업무 HP 행정 정보 1 행정 정보 업무 HP 행정 정보 2 행정 정보 업무 HP 행정 정보 테스트 HP 파생 상품 1 파 생 상품 업무 HP 파생 상품 2 파 생 상품 업무 HP 파생 상품 3 파 생 상품 업무 HP 파생 상품 테스트 파생 상품 테스트 업무 HP EDDDB 1 EDD DB 업무 HP RAC EDDDB 2 EDD DB 업무 HP EDDWAS 1 EDD WAS 업무 HP EDDWAS 2 EDD WAS 업무 HP EDDWEB 1 EDD WEB 업무 HP EDDWEB 2 EDD WEB 업무 HP EDD 테스트 EDD 테스트 업무 HP IFRS DB 업무 HP IFRS 테스트 IFRS DB 테스트 업무 HP', '변경 에 따른 관련 법령 예 전자 정부 법 IT 자원 의 공동 활용 신설 공개 SW 의 이용 및 라이선스 등 하위 규정 등']\n",
      "3337234\n"
     ]
    }
   ],
   "source": [
    "# wiki_20190620.txt 말뭉치 불러옴.\n",
    "#corpus = '../../data11/my_corpus/re-moco-corpus2.txt'\n",
    "#corpus = '../../data11/my_corpus/re-kowiki-202206.txt'\n",
    "#corpus = '../../data11/my_corpus/test.txt'\n",
    "\n",
    "with open(out_corpus, 'r', encoding='utf-8') as f:\n",
    "    data = [line for line in tqdm(f.read().splitlines()) if (len(line) > 0 and not line.isspace())]\n",
    "\n",
    "print(data[:3])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10fc504-3a05-44e4-b205-e63b023ef1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 제너레이터생성\\n# Python 제너레이터(generator)를 사용하면 실제로 필요할 때까지 Python이 메모리에 아무 것도 로드하지 않도록 할 수 있다.\\n# 이러한 생성기를 만들려면 꺽쇠괄호(brackets)를 소괄호(parentheses)로 바꾸기만 하면 된다.\\ndef get_training_corpus():\\n    return (\\n        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\\n        for i in range(0, len(raw_datasets[\"train\"]), 1000)\\n    )\\n\\ntraining_corpus = get_training_corpus()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 제너레이터생성\n",
    "# Python 제너레이터(generator)를 사용하면 실제로 필요할 때까지 Python이 메모리에 아무 것도 로드하지 않도록 할 수 있다.\n",
    "# 이러한 생성기를 만들려면 꺽쇠괄호(brackets)를 소괄호(parentheses)로 바꾸기만 하면 된다.\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125c41d1-c4e9-484b-8fdd-1a964eea85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yield 문을 사용하여 for 루프 내에서 제너레이터(generator)를 정의할 수도 있다.\n",
    "def get_generator_corpus(max_len: int=100000):\n",
    "    print(f\"len:{max_len}\")\n",
    "    dataset = data\n",
    "    for start_idx in range(0, len(dataset), max_len):\n",
    "        samples = dataset[start_idx : start_idx + max_len]\n",
    "        #print(samples)\n",
    "        yield samples\n",
    "        \n",
    "training_corpus = get_generator_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c35680-f498-4358-a665-96b2f923590b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_generator_corpus at 0x7f15f5712ac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b7b771-46e2-4f98-9026-41eb88d7f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 bert-base-cased 모델 로딩\n",
    "from transformers import AutoTokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a862a6-ea1a-4446-a669-8e94f95b1064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'love',\n",
       " 'you',\n",
       " '오',\n",
       " '##늘',\n",
       " '##은',\n",
       " '날',\n",
       " '##씨',\n",
       " '##가',\n",
       " '참',\n",
       " '좋',\n",
       " '##다',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 토큰 테스트 \n",
    "example = '''i love you\n",
    "    오늘은 날씨가 참 좋다.\n",
    "    '''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407eacf4-640c-429b-aa1c-576495ece85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:100000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작 (**오래 걸림)\n",
    "# => 새로운 토큰을 만듬. vocab 수는 32000개\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 42000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01427411-f139-4a42-afb6-b1c2aa955785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'lo', '##ve', 'you', '오늘', '##은', '날씨', '##가', '참', '좋', '##다', '[UNK]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새롭게 만든 토큰 테스트 \n",
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6b51fc-4689-4a70-8e37-ff11a4c25ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data11/my_corpus/vocab/bert-re-moco-corpus2-mecab-42000/tokenizer_config.json',\n",
       " '../../data11/my_corpus/vocab/bert-re-moco-corpus2-mecab-42000/special_tokens_map.json',\n",
       " '../../data11/my_corpus/vocab/bert-re-moco-corpus2-mecab-42000/vocab.txt',\n",
       " '../../data11/my_corpus/vocab/bert-re-moco-corpus2-mecab-42000/added_tokens.json',\n",
       " '../../data11/my_corpus/vocab/bert-re-moco-corpus2-mecab-42000/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 tokenizer 저장\n",
    "tokenizer.save_pretrained(\"../../data11/my_corpus/vocab/bert-re-moco-corpus2-mecab-42000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b757e1-b0c8-41da-85b2-65b4d874b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Hugging face 허브에 저장.\\n# => 로그인 : 허깅페이스(https://huggingface.co)-settings 가면 access token이 있음\\nfrom huggingface_hub import notebook_login\\nnotebook_login()\\n\\n# 노트북 환경이 아닌 경우에는 터미널에서 아래 입력\\n# huggingface-cli login\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Hugging face 허브에 저장.\n",
    "# => 로그인 : 허깅페이스(https://huggingface.co)-settings 가면 access token이 있음\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# 노트북 환경이 아닌 경우에는 터미널에서 아래 입력\n",
    "# huggingface-cli login\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106dab5f-8a5f-45f0-afbb-1534d319ca46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# tokenizer 허깅 페이스 허브에 푸시함.\\n# 해당 이름으로 리포지토리 생성되고 추가됨\\ntokenizer.push_to_hub(\"bert-tokenizer-kowiki20220620-32000\", use_temp_dir=True)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# tokenizer 허깅 페이스 허브에 푸시함.\n",
    "# 해당 이름으로 리포지토리 생성되고 추가됨\n",
    "tokenizer.push_to_hub(\"bert-tokenizer-kowiki20220620-32000\", use_temp_dir=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83059716-450d-4092-b850-477fdcbeccbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 추가한 tokenizer 불러옴.\\nmytokenizer = AutoTokenizer.from_pretrained(\"kobongsoo/bert-tokenizer-kowiki20220620-32000\")\\ntokens = mytokenizer.tokenize(example)\\ntokens\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 추가한 tokenizer 불러옴.\n",
    "mytokenizer = AutoTokenizer.from_pretrained(\"kobongsoo/bert-tokenizer-kowiki20220620-32000\")\n",
    "tokens = mytokenizer.tokenize(example)\n",
    "tokens\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
