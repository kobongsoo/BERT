{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e2ccc3-0858-479d-ac48-a997dd15cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gluonnlp as nlp                  # GluonNLP는 버트를 간단하게 로딩하는 인터페이스를 제공하는 API 임\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ca0cd7-58ed-4fa5-bfe4-bfe050a80046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_file=\"Tokenizer/kobert/kobert_news_wiki_ko_cased-ae5711deb3.spiece\" # Kobert vocab\n",
    "vocab_file=\"Tokenizer/kobert/kobert_news_wiki_ko_cased-ae5711deb3.spiece\"\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file, padding_token=\"[PAD]\")\n",
    "tok = nlp.data.BERTSPTokenizer(vocab_file, vocab, lower=False)\n",
    "transform = nlp.data.BERTSentenceTransform(\n",
    "            tok, max_seq_length = 128, pad=True, pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772192c7-dc3b-4eee-b774-eeef6870afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"식당에 가서 밥을 배 부르게 먹고 문서 중앙화 낙시배를 타고 고기 잡고 요트배를 타고 관광을 해야 겠다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff7cc2e-d677-412b-a24f-f8369b3c7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentece:\n",
      "['[CLS]', '▁식당', '에', '▁', '가', '서', '▁밥', '을', '▁배', '▁부르', '게', '▁먹고', '▁문서', '▁중앙', '화', '▁낙', '시', '배', '를', '▁타고', '▁고', '기', '▁잡고', '▁요', '트', '배', '를', '▁타고', '▁관광', '을', '▁해야', '▁', '겠다', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token_ids:\n",
      "[   2 3007 6896  517 5330 6553 2266 7088 2287 2432 5400 2011 2121 4269\n",
      " 7941 1404 6705 6312 6116 4700  993 5561 3951 3480 7659 6312 6116 4700\n",
      " 1080 7088 5010  517 5406    3    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "valid_length:\n",
      "34\n",
      "segment_ids:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "transform_data = [transform([i[0]]) for i in [test_sentence]]\n",
    "token_ids = transform_data[0][0]\n",
    "valid_length = transform_data[0][1]\n",
    "segment_ids = transform_data[0][2]\n",
    "\n",
    "test_sentence_list = []\n",
    "for i, ids in enumerate(token_ids):\n",
    "    test_sentence_list.append(vocab.idx_to_token[ids])\n",
    "\n",
    "print(\"sentece:\\r\\n{}\".format(test_sentence_list))\n",
    "print(\"token_ids:\\r\\n{}\".format(token_ids))\n",
    "print(\"valid_length:\\r\\n{}\".format(valid_length))\n",
    "print(\"segment_ids:\\r\\n{}\".format(segment_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50cc7be9-b9ad-4107-812b-8d07e963ab07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'힙'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx_to_token[8001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6339a7a-618f-4358-95b2-4ed1b19d09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file='my_data1/special_0214/vocab.txt',\n",
    "    max_len=128,\n",
    "    do_lower_case=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d92c84f-3ac7-4a1a-993a-7d2f24c11e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom konlpy.tag import Mecab\\nmecab = Mecab()\\n\\ntest_mecab = mecab.morphs(test_sentence[0])\\nprint(test_mecab)\\nprint(test_mecab[1])\\nprint(type(test_mecab))\\n\\ndata = ' '.join(test_mecab)\\nprint(data)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "test_mecab = mecab.morphs(test_sentence[0])\n",
    "print(test_mecab)\n",
    "print(test_mecab[1])\n",
    "print(type(test_mecab))\n",
    "\n",
    "data = ' '.join(test_mecab)\n",
    "print(data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6716dce8-ef87-4512-a2c1-3932d2c6e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(test_sentence[0], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a36ce1-980f-4370-98be-a9d97264e6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##힙'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(8001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf934ad2-3c72-45ba-890d-5577d7a0c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', '식당', '##에', '[UNK]', '밥', '##을', '배', '부르', '##게', '먹고', '문서', '중앙', '##화', '낙', '##시', '##배', '##를', '타고', '고', '##기', '잡고', '요', '##트', '##배', '##를', '타고', '관광', '##을', '해야', '[UNK]', '[SEP]']]\n",
      "[[2, 3007, 6896, 0, 2266, 7088, 2287, 2432, 5400, 2011, 2121, 4269, 7941, 1404, 6705, 6312, 6116, 4700, 993, 5561, 3951, 3480, 7659, 6312, 6116, 4700, 1080, 7088, 5010, 0, 3]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "##문을\n"
     ]
    }
   ],
   "source": [
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "\n",
    "# token indexs ( 토큰을 index로 변한한 값)\n",
    "token_ids = [tokenized_input['input_ids'].tolist()[0]]\n",
    "# attention_mask (중요토큰 : 1)\n",
    "token_attention_mask = [tokenized_input['attention_mask'].tolist()[0]]\n",
    "# segment_id (첫번째문자:0, 다음 문장:1)\n",
    "token_type_ids = [tokenized_input['token_type_ids'].tolist()[0]]\n",
    "\n",
    "print(token_str)\n",
    "print(token_ids)\n",
    "print(token_attention_mask)\n",
    "print(token_type_ids)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(6235))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c127c9-b7a9-4039-ab1c-bac6077fa6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input1 = tokenizer(test_sentence[0], return_tensors=\"pt\")\n",
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input1['input_ids'].tolist()[0]]]\n",
    "\n",
    "# token indexs ( 토큰을 index로 변한한 값)\n",
    "token_ids = [tokenized_input['input_ids'].tolist()[0]]\n",
    "# attention_mask (중요토큰 : 1)\n",
    "token_attention_mask = [tokenized_input['attention_mask'].tolist()[0]]\n",
    "# segment_id (첫번째문자:0, 다음 문장:1)\n",
    "token_type_ids = [tokenized_input['token_type_ids'].tolist()[0]]\n",
    "\n",
    "print(token_str)\n",
    "print(token_ids)\n",
    "print(token_attention_mask)\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec7fce-b7ea-4c7b-8301-a75df26c57a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
