{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6013926b-4331-43f9-9a93-13d0b3cb34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# 원래 vocab 파일에 신규 vocab 파일에 대해 중복제거해서 추가하기\n",
    "#==============================================================================================\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3204a5-b608-43df-a21a-e8dca76f1b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9571310ebfc743e6a616a40972e100fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '\"', '#', \"'\", '0', '1']\n"
     ]
    }
   ],
   "source": [
    "# bert_tokenizer.ipynb에서 만든 new_vocab 파일을 불러옴.\n",
    "#new_vocab_out = '../../data11/ai_hub/vocab/tl1-1줄-mecab-30000/vocab.txt'\n",
    "new_vocab_out = '../../data11/my_corpus/re-moco-corpus2/re-moco-corpus2-20000-vocab-add.txt'\n",
    "\n",
    "# 2차 불용어 정의\n",
    "#stopwords=['##다', '##하', '있', '##고', '##로', '##한', '##적', '##되', '##었', '##_']\n",
    "stopwords=['']\n",
    "\n",
    "new_vocab = []\n",
    "with open(new_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "for word in tqdm(data):\n",
    "    if not word in stopwords:  # 2차 불용어는 제외\n",
    "        new_vocab.append(word)\n",
    "        \n",
    "print(new_vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fae4384-a4aa-47fc-ace6-1d75fd37629e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0e69d2c3584d26a3a3bb2de99b0ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36440\n"
     ]
    }
   ],
   "source": [
    "# 원본 모델의 vocab 리스트로 읽어옴\n",
    "#org_vocab_out = './moco-vocab/mdistilbertV1.1/vocab.txt'\n",
    "org_vocab_out = '../../data11/model/bert/kpfbert/vocab.txt'\n",
    "\n",
    "org_vocab = []\n",
    "\n",
    "with open(org_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "for vocab in tqdm(data):\n",
    "     org_vocab.append(vocab)\n",
    "\n",
    "print(len(org_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a02f91-473e-4a8b-abaf-7d1f1e40da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복제거 시작==>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca39ff928304afd97ce7006119d7dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43889\n",
      "['假', '偉', '偏', '停']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "start_vocab_id = 2059    # ***추가할 vocab.txt 를 열어서, 추가 시작할 번호 지정 \n",
    "vocab_len= 10000 # *** 새롭게 추가할 vocab 수 \n",
    "\n",
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거\n",
    "\n",
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거\n",
    "#temp = list(set(new_vocab) - set(org_vocab)) #순서 보존이 안됨\n",
    "    \n",
    "#또는\n",
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거print(org_vocab[1111:1115])\n",
    "print('중복제거 시작==>')\n",
    "s = set(org_vocab)\n",
    "temp = [x for x in tqdm(new_vocab[start_vocab_id:]) if x not in s] #순서 보존됨    \n",
    "\n",
    "vocablist = org_vocab + temp[:vocab_len]  #원본 리스트 + 중복제가한 신규리스트 2개의 리스트를 합침\n",
    "\n",
    "print(len(vocablist))\n",
    "print(vocablist[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f387b28-5834-4a5a-897d-efd55558d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 vocab에서 원본에 중복제거한 목록을 파일로 저장\n",
    "outfilepath2 = '../../data11/model/bert/kpfbert/re-moco-corpus2-20000-vocab-add-new.txt'\n",
    "with open(outfilepath2, 'w') as f:\n",
    "    f.write('\\n'.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7965e5eb-7474-4c9d-a92b-4363bd82062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 파일\"../../data11/my_corpus/re-moco-corpus2/re-moco-corpus2-20000-vocab-add.txt\" 추가 성공!. 출력 vocab 파일 : \"../../data11/model/bert/kpfbert/add-re-moco-corpus2-20000-vocab-add-new.txt\"\n"
     ]
    }
   ],
   "source": [
    "# 원본 vocab+중복제거 신규 vocab 을 리스트를 파일로 저장\n",
    "outfilepath = '../../data11/model/bert/kpfbert/add-re-moco-corpus2-20000-vocab-add-new.txt'\n",
    "with open(outfilepath, 'w') as f:\n",
    "    #f.write('\\n'.join(vocablist[:159547])) #159547개만 저장\n",
    "    f.write('\\n'.join(vocablist))  #모두 저장\n",
    "    \n",
    "print('vocab 파일\"{}\" 추가 성공!. 출력 vocab 파일 : \"{}\"'.format(new_vocab_out, outfilepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e863c9-0e66-40f9-9075-7c7969c9f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#outfilepath = './moco-vocab/moco-corpus-kowiki2022-nouns-42000-Deduplication-add-en-159552.txt'\n",
    "# BertTokenizer 로 새롭게 저장된 vocalfile 불러옴\n",
    "tokenizer = BertTokenizer(vocab_file=outfilepath, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ba66d5-87ff-4609-9023-7af250df54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_special_token = False\n",
    "\n",
    "if add_special_token:\n",
    "    # 옵션 : 저장된 add_new_vocab.txt를 불러와서 special 토큰 추가함\n",
    "    special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n",
    "\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # special token 체크\n",
    "    print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c9740f-2444-4a4b-8733-45f3cd3b25c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data11/model/bert/kpfbert/re-moco-corpus2-20000-vocab-add/tokenizer_config.json',\n",
       " '../../data11/model/bert/kpfbert/re-moco-corpus2-20000-vocab-add/special_tokens_map.json',\n",
       " '../../data11/model/bert/kpfbert/re-moco-corpus2-20000-vocab-add/vocab.txt',\n",
       " '../../data11/model/bert/kpfbert/re-moco-corpus2-20000-vocab-add/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "import os\n",
    "OUT_PATH = '../../data11/model/bert/kpfbert/re-moco-corpus2-20000-vocab-add/'\n",
    "#OUT_PATH = './mdistilbertV2.2'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730717e-938b-4eb8-ab03-c42713db2f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
