{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013926b-4331-43f9-9a93-13d0b3cb34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# 기존 bert vocab에 새로운 도메인 vocab만들어서 추가하기\n",
    "#\n",
    "# =>기존 pre-trained bert vocab에는 전문단어 domain이 없다(예:문서중앙화, COVID 등)  \n",
    "# 따라서 전문 domain을 기존 bert vocab에 추가해야 한다.\n",
    "# => 따라서 여기서는 mecab(한국어 형태로 분석기)를 이용하여, 추가 vocab을 만들고, \n",
    "# bert wordpiecetokenzer에 추가하는 방법에 대해 설명한다\n",
    "# \n",
    "# 참고 자료 \n",
    "# https://github.com/piegu/language-models/blob/master/nlp_how_to_add_a_domain_specific_vocabulary_new_tokens_to_a_subword_tokenizer_already_trained_like_BERT_WordPiece.ipynb\n",
    "# https://medium.com/@pierre_guillou/nlp-how-to-add-a-domain-specific-vocabulary-new-tokens-to-a-subword-tokenizer-already-trained-33ab15613a41\n",
    "# https://wikidocs.net/64517\n",
    "#\n",
    "# [과정]\n",
    "# 1) 도메인 말뭉치(예:kowiki.txt)에서 mecab을 이용하여 형태소 분석하여 단어들을 추출함.\n",
    "#   => mecab으로 형태소 혹은 명사만 분할하면서, subword 앞에는 '##' prefix 추가함(**ertwordpiece subword와 동일하게)\n",
    "# 2) NLTK 빈도수 계산하는 FreqDist()를 이용하여, 단어들이 빈도수 계산\n",
    "# 3) 상위 빈도수(예:30000)를 가지는 단어들만 add_vocab.txt 로 만듬\n",
    "# 4) 기존 bert vocab.txt 파일에 직접, add_vocab.txt 토큰들 추가(*이때 중복 제거함)\n",
    "# 5) 추가한 vocab을 가지고, tokenizer 생성하고, special 토큰 추가 후, 저장\n",
    "#\n",
    "# **원래 참고자료에는 tokenizer.add_tokens() 으로 추가하면, added_tokens.json 파일에 추가되는데, \n",
    "# 이때 BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름:엄청 느려지는것 같음)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#==============================================================================================\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae6691-ce4c-4cdd-b024-64f4ff130e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요에 따라 load_dataset 에서 corpus 불러와서 data 폴더에 cache 파일로 저장.\n",
    "# 이후 저장된 캐쉬경로 .data/downloads/36004098ce1bbc76bbe4c17d14c097bf0e0a785581a03723a78e11c65f731422  파일을 kowiki2022.txt 로 이름변경해서 사용하면 됨.\n",
    "\n",
    "#from datasets import load_dataset\n",
    "#dataset = load_dataset('bongsoo/kowiki20220620', cache_dir='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2890e-88b3-4d78-991c-7f8a11f261fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fe943-45fe-4a92-a8b9-c15d89f92cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['train']['text'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db39d2f-f474-482b-95dd-8321d62d1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_20190620.txt 말뭉치 불러옴.\n",
    "#corpus = '../../data11/my_corpus/my/pre-kowiki-20220620-1줄.txt'\n",
    "#corpus = '../korpora/kowiki_20190620/wiki_20190620.txt'\n",
    "corpus = '../../data11/vocab.txt'\n",
    "with open(corpus, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split(' ') # 공백으로 구분해서 단어들을 추출함\n",
    "\n",
    "print(data[:3])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16ffd7-2e89-4057-83d0-9db4a468ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 형태소 분석기를 이용하여, 읽어온 말뭉치를 단어,조사등으로 분리함\n",
    "# => 불용어는 제거함\n",
    "# => mecab으로 형태소 분할하면서, subword 앞에는 prefix '##' 추가함\n",
    "\n",
    "# Mecab 선언\n",
    "mecab = Mecab()\n",
    "\n",
    "# 불용어 정의\n",
    "#stopwords=['이','가','께서','에서','이다','의','을','를','에','에게','께','와','에서','라고', '과','은', '는', '부터','.',',', '_']\n",
    "stopwords=['.',',', '_', '..','\"',';','<', '>', '(', ')', '[', ']', '#', '!', '?', '~']\n",
    "# Ture = nouns(명사)만 추출, False=형태소 추출\n",
    "nouns = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204a00d-63a6-4fab-a4a3-584f3b0d2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "total_morph=[]\n",
    "for sentence in tqdm(data):\n",
    "    temp = mecab.morphs(sentence)\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    total_morph.append(temp)\n",
    "'''\n",
    "# mecab으로 형태소 혹은 명사만 분할할때, subword 앞에는 prefix '##' 추가함\n",
    "total_words=[]\n",
    "\n",
    "# 명사만 추출하는 경우\n",
    "if nouns == True:\n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.nouns(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count > 0:\n",
    "                    tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "# 형태소도 포함하여 추출하는 경우\n",
    "else:\n",
    "    \n",
    "    for words in tqdm(data):\n",
    "        count=0\n",
    "\n",
    "        for word in mecab.morphs(words):\n",
    "            if not word in stopwords:\n",
    "                tmp = word\n",
    "\n",
    "                if count > 0:\n",
    "                    tmp = \"##\" + tmp  ## subword 앞에는 prefix '##' 추가함\n",
    "                    total_words.append(tmp)  \n",
    "                else:\n",
    "                    total_words.append(tmp)  \n",
    "                    count += 1\n",
    "                \n",
    "print(total_words[:20])\n",
    "print(f'총 단어 수: {len(total_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abac86-e25f-46f1-a12e-f6afb27d6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreqDist를 이용하여 빈도수 계산(*오래걸림)\n",
    "vocab = FreqDist(np.hstack(total_words))\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "# 최대 빈도수 가장높은 1500개만 뽑아봄\n",
    "print(vocab.most_common(1500))\n",
    "\n",
    "# 특정 단어의 빈도수를 뽑아봄\n",
    "#print(vocab['미국'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c1ccf-d857-402c-bd74-294e01073d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 32000 개만 보존\n",
    "vocab_size = 32000\n",
    "vocab1 = vocab.most_common(vocab_size)\n",
    "\n",
    "vocab_len = len(vocab1)\n",
    "print('*단어 집합의 크기 : {}'.format(vocab_len))\n",
    "print('*마지막 단어 정보 : {}'.format(vocab1[vocab_len-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b936eb-4201-4257-8961-b1bbcb47119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "vocab2 = vocab.most_common(10)\n",
    "\n",
    "all_fdist = pd.Series(dict(vocab2))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=30);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04324119-dadd-4f50-bd8c-a51fab023fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab을 list로 만듬\n",
    "new_vocab = []\n",
    "for index, word in tqdm(enumerate(vocab1)):\n",
    "    new_vocab.append(word[0])  # fword[0] 하면 단어만 추출\n",
    "    \n",
    "# 리스트 출력해봄\n",
    "print(new_vocab[50:70])\n",
    "#print(vocab['음악당'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff872f-9f1e-4364-b2be-0a43ef7a65a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_vocab을 파일로 저장함\n",
    "new_vocab_out = './moco-vocab/kowiki20220620-32000-vocab.txt'\n",
    "with open(new_vocab_out, 'w', encoding='utf-8') as f:\n",
    "    for word in tqdm(new_vocab):\n",
    "        f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4825424-1f11-4029-bde4-ce7a6eb04e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_vocab 파일을 불러옴.\n",
    "new_vocab_out = './moco-vocab/moco-corpus-kowiki202206-42000-vocab.txt'\n",
    "\n",
    "# 2차 불용어 정의\n",
    "#stopwords=['##다', '##하', '있', '##고', '##로', '##한', '##적', '##되', '##었', '##_']\n",
    "stopwords=['##_', '〈', '..']\n",
    "\n",
    "new_vocab = []\n",
    "with open(new_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "for word in tqdm(data):\n",
    "    if not word in stopwords:  # 2차 불용어는 제외\n",
    "        new_vocab.append(word)\n",
    "        \n",
    "print(new_vocab[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c96a06-3154-48f9-a596-84b3a152c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# bongsoo/mdistilbertV1.1 모델 불러옴.\n",
    "vocab_path=\"bongsoo/mdistilbertV1.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(vocab_path, do_lower_case=False)\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'{vocab_path} is_fast:{tokenizer.is_fast}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a1c39-a8f2-4d3f-8b97-5920179980cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 저장\n",
    "SAVE_VOCAB_PATH = './moco-vocab/mdistilbertV1.1'\n",
    "tokenizer.save_pretrained(SAVE_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be46a8a-f837-49e4-887e-5d43c41cf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 vocab 리스트로 읽어옴\n",
    "#org_vocab_out = './moco-vocab/mdistilbertV1.1/vocab.txt'\n",
    "org_vocab_out = '../../data11/model/bert/bert-multilingual-cased/vocab.txt'\n",
    "\n",
    "org_vocab = []\n",
    "\n",
    "with open(org_vocab_out, 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    \n",
    "for vocab in tqdm(data):\n",
    "     org_vocab.append(vocab)\n",
    "\n",
    "print(len(org_vocab))\n",
    "print(org_vocab[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff7adf-befa-4dd3-aeb1-2e4e1a655111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신규 vocab 리스트에서 원본 vocab 리스트 중복값을 제거\n",
    "#temp = list(set(new_vocab) - set(org_vocab)) #순서 보존이 안됨\n",
    "    \n",
    "#또는\n",
    "\n",
    "s = set(org_vocab)\n",
    "temp = [x for x in new_vocab if x not in s] #순서 보존됨    \n",
    "\n",
    "vocablist = org_vocab + temp  #원본 리스트 + 중복제가한 신규리스트 2개의 리스트를 합침\n",
    "\n",
    "print(len(vocablist))\n",
    "print(vocablist[1111:1115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc047b-ad66-41e2-842e-17fce5dd26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 파일로 저장\n",
    "outfilepath = './moco-vocab/vocab1.txt'\n",
    "with open(outfilepath, 'w') as f:\n",
    "    f.write('\\n'.join(vocablist))\n",
    "    \n",
    "print('vocab 파일\"{}\" 추가 성공!. 출력 vocab 파일 : \"{}\"'.format(new_vocab_out, outfilepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016358e0-def2-453e-9988-891af3270509",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **** 저장된 파일을 열어서, 중간(mbert는 119548라인)에 공백이 없는지 반드시 확인.\n",
    "\n",
    "## **** 반드시 또 추가할 단어들(예:문서중앙화, 보안파일서버, 엠파워, 모코엠시스, MOCOMSYS, MPOWER, EZis-C 등) 파일 맨 끝에 추가 *단 기존에 추가되어있는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb223a-273f-4c78-90ce-2675c7b64764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertTokenizer 로 새롭게 저장된 vocalfile 불러옴\n",
    "outfilepath = '../../data11/model/sbert/sbert-mbertV2.0/vocab.txt'\n",
    "tokenizer = BertTokenizer(vocab_file=outfilepath, \n",
    "                          strip_accents=False, \n",
    "                          do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40acc9-034d-4f3c-940b-38d657ce8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_special_token = False\n",
    "\n",
    "if add_special_token:\n",
    "    # 옵션 : 저장된 add_new_vocab.txt를 불러와서 special 토큰 추가함\n",
    "    special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',]\n",
    "\n",
    "    special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    # special token 체크\n",
    "    print(tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f8a3e-aebd-4e09-9ae6-4bc0b92fddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special token 추가한 special vocab을 저장함\n",
    "import os\n",
    "OUT_PATH = './moco-vocab/mdistilbertV2.1/'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb0a60-e94e-49ac-9d06-49c5cf6ea487",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#=============================================================================================\n",
    "# add_tokens으로 추가하면, BertTokenizer.from_pretrained() 함수로 호출할때, 호출이 안됨(**이유 모름)\n",
    "# => 따라서 직접 vocab.txt에 추가하는 방법을 사용함\n",
    "#=============================================================================================\n",
    "\n",
    "# 기존 bert tokenizer 로딩\n",
    "vocab_path = '../model/bert/bert-multilingual-cased/vocab'\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
    "print(f'*기존 vocab 수: {len(tokenizer)}')\n",
    "\n",
    "# 기존 bert tokenizer에 신규 vocab 추가함 \n",
    "new_token_num = tokenizer.add_tokens(new_vocab)\n",
    "print(f'*신규 vocab 수: {len(tokenizer)}')\n",
    "print(f'*추가된 vocab: {new_token_num}')\n",
    "\n",
    "# 추가된 tokenizer 저장\n",
    "# => 추가된 token들은 added_tokens.json에 저장된다.\n",
    "import os\n",
    "OUT_PATH = 'new_vocab_nouns'\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(OUT_PATH)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
