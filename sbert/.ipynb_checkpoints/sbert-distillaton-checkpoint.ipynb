{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5e3edf-a94e-472d-a7b3-5f8c414b1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-04-12.log\n",
      "logfilepath:qnadataset_2022-04-12.log\n",
      "logfilepath:s-bert-ts_2022-04-12.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# sentence-bert 를 tearch-student 관계 모델로 구성하여, 영어 sbert 학습을 학국어 모델에 증류학습시키는 예시\n",
    "# -> 선생님모델은  영어 bert가 되고, 학생모델은 학국어 포함된다국어 bert로 설정\n",
    "# -> 영어 bert가 다국어 bert를 가리키는 방식으로 학습됨\n",
    "#\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# => 여기서는 교사모델을 distiluse-base-multilingual-cased-v1 로, 학생모델은 distilbert-base-multilingual 로 하여 학습시캄.\n",
    "# => distiluse-base-multilingual-cased-v1 는 Teacher: mUSE; Student: distilbert-base-multilingual 로 학습시킨 s-bert 모델임\n",
    "# => distiluse-base-multilingual-cased-v2 도 있는데, 성능평가에는 v1이 v2 보다 2% 정도 더 높은 성능을 보였음\n",
    "#     (https://www.sbert.net/docs/pretrained_models.html 참조)\n",
    "#\n",
    "# => ** 중요한 것은 교사와 학생모델간 word_embedding dimension은 서로 일치해야 함.(일치하지않으면 훈련시 아래와 같은 에러 발생함)\n",
    "#     에러 : The size of tensor a (768) must match the size of tensor b (384) at non-singleton dimension 1\n",
    "#\n",
    "# [참고 소스] \n",
    "# https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/multilingual/make_multilingual_sys.py\n",
    "#\n",
    "# pip install -U sentence-transformers\n",
    "#======================================================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert-ts\", logfilename=\"s-bert-ts\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bfcf70-f795-46e8-8833-6e43b1408b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name ../../model/sbert/teacher/paraphrase-multilingual-MiniLM-L12-v2. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load teacher model\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 선생님 모델 설정\n",
    "print(\"Load teacher model\")\n",
    "#teacher_model_name = 'bert-base-nli-stsb-mean-tokens'\n",
    "teacher_model_name = \"../../model/sbert/teacher/all-mpnet-base-v2\"\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8b428c-7028-44e7-a986-acec80f86359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load student model\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#==========================================================================================================\n",
    "# 학생 모델 설정\n",
    "# => * 학생모델이 이미 sentencebert일지라도, 아래처럼 sbert모델 아닌 것처럼 word_embedding_model, pooling_model 을 각각\n",
    "#    만들어서 처리하는것이 테스트 시 효율의 좋음\n",
    "#\n",
    "# [학생 모델 생성 방법]\n",
    "# 1) word_embedding 모델 생성\n",
    "# 2) pooling 모델 생성 : pooling 정책을 설정함 : CLS, 평균, MAX 정책중 택1(*평균 정책이 효율의 가장 좋다고 함)\n",
    "# 3) 1) + 2) 모델을 연결시켜서 하나의 sbert 모델 만듬\n",
    "#==========================================================================================================\n",
    "student_model_name = \"../../model/sbert/sbert-ts2022-04-01-distiluse-7\"\n",
    "\n",
    "print(\"Load student model\")\n",
    "\n",
    "\n",
    "# === *sbert 모델 아닌 경우 =====\n",
    "# word embedding 모델 설정(기존 다국어 모델 불러옴)\n",
    "word_embedding_model = models.Transformer(student_model_name)\n",
    "\n",
    "# pooling 정책 설정(mean 평균 정책으로 지정)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "# 학생 SBERT 생성\n",
    "# -> word_embedding model 과 pooling_model를 연결시켜줌\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "# === *sbert 모델 인 경우 =====\n",
    "# 기존 s-model 로딩 함\n",
    "#student_model = SentenceTransformer(student_model_name)\n",
    "\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8f7e8b-e46e-410a-b991-bcee2848b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 평가 데이터 불러오고, 손실함수(MSELoss) 설정함(*학생모델에 설정함)\n",
    "# 원본 소스코드 : \n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/datasets/ParallelSentencesDataset.py\n",
    "\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "\n",
    "max_seq_length = 128\n",
    "train_batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "###### Load train sets ######    \n",
    "#train_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-train.tsv'\n",
    "train_file = '../../korpora/pair/TED2020-en-ko/TED2020-en-ko-train.tsv'\n",
    "\n",
    "train_reader = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model)\n",
    "train_reader.load_data(train_file)\n",
    "train_dataloader = DataLoader(train_reader, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbcac7e-cc7d-496f-b16c-f3c80fca6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589814\n",
      "('There are many differences among the arts, but there are also universal, cross-cultural aesthetic pleasures and values.', {'예술들엔 많은 차이점이 있지만, 보편적이고 문화를 뛰어넘는 미적 기쁨과 가치가 있죠.', 'There are many differences among the arts, but there are also universal, cross-cultural aesthetic pleasures and values.'})\n"
     ]
    }
   ],
   "source": [
    "print(len(train_reader))\n",
    "train_reader.__getitem__(0)\n",
    "print(train_reader.next_entry(0)) # (source, {traget}) 첫번째 문장을 출력해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56c40a-a560-4302-876c-272894ef2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "###### Load dev sets ######\n",
    "# 평가 데이터 불러와서 유사도 측정 평가자 설정함\n",
    "#=>stst 파일 있는 경우에만 지정해줌.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentencesDataset, losses,readers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator\n",
    "\n",
    "evaluators = []\n",
    "dev_samples = []\n",
    "\n",
    "eval_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-dev-1.tsv'\n",
    "with open(eval_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        if s1[0] == \"\" or s1[1] == \"\":\n",
    "            continue\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        dev_samples.append(InputExample(texts= [s1,s2], label=score))\n",
    "\n",
    "# 영어 문장, 한국어 문장 유사도 측정을 위한 평가자(Evaluator) 설정\n",
    "evaluator_sts = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 batch_size=train_batch_size, \n",
    "                                                                 name='dev')\n",
    "# evaluators에 추가함(*아래 테스트 데이터 evaluators도 추가함)\n",
    "evaluators.append(evaluator_sts)\n",
    "print(len(evaluators))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55961955-a3c4-4224-95ae-a42c4a44d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Load test sets ######\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentencesDataset, losses,readers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator, TranslationEvaluator\n",
    "evaluators = []\n",
    "# 테스트 데이터 불러와서 MSE 평가자 설정함\n",
    "src_sentences = []\n",
    "trg_sentences = []\n",
    "\n",
    "#test_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-test.tsv'\n",
    "test_file = '../../korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv'\n",
    "\n",
    "# 참고소스: https://texasvaluesaction.org/Foysal87/Bangla-sentence-embedding-transformer/blob/master/Bangla_transformer.py\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    for line in fIn:\n",
    "        splits = line.strip().split('\\t')\n",
    "        if len(splits) != 2:\n",
    "            continue\n",
    "\n",
    "        if splits[0] != \"\" and splits[1] != \"\":\n",
    "            src_sentences.append(splits[0])\n",
    "            trg_sentences.append(splits[1])\n",
    "        \n",
    "test_mse = MSEEvaluator(src_sentences, trg_sentences, teacher_model=teacher_model, name='test')\n",
    "evaluators.append(test_mse)\n",
    "\n",
    "# TranslationEvaluator computes the embeddings for all parallel sentences. \n",
    "# It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "test_acc = TranslationEvaluator(src_sentences, trg_sentences, batch_size=train_batch_size)\n",
    "evaluators.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfddc50-571a-4fc8-aa9c-4a1bce2b79c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 09:57:36,130 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "INFO:s-bert-ts:----------------------------------------------------------------------\n",
      "2022-04-12 09:57:36,132 - s-bert-ts - INFO - *Warmup-steps:9216, ephocs:5, train_data_len:589814, train_batch_size: 32\n",
      "INFO:s-bert-ts:*Warmup-steps:9216, ephocs:5, train_data_len:589814, train_batch_size: 32\n",
      "2022-04-12 09:57:36,134 - s-bert-ts - INFO - *teacher_model: ../../model/sbert/teacher/paraphrase-multilingual-MiniLM-L12-v2\n",
      "INFO:s-bert-ts:*teacher_model: ../../model/sbert/teacher/paraphrase-multilingual-MiniLM-L12-v2\n",
      "2022-04-12 09:57:36,136 - s-bert-ts - INFO - *student_model_name: ../../model/sbert/sbert-ts2022-04-01-distiluse-7\n",
      "INFO:s-bert-ts:*student_model_name: ../../model/sbert/sbert-ts2022-04-01-distiluse-7\n",
      "2022-04-12 09:57:36,138 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "INFO:s-bert-ts:----------------------------------------------------------------------\n",
      "2022-04-12 09:57:36,139 - s-bert-ts - INFO - *train_file: ../../korpora/pair/TED2020-en-ko/TED2020-en-ko-train.tsv\n",
      "INFO:s-bert-ts:*train_file: ../../korpora/pair/TED2020-en-ko/TED2020-en-ko-train.tsv\n",
      "2022-04-12 09:57:36,141 - s-bert-ts - INFO - *test_file: ../../korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv\n",
      "INFO:s-bert-ts:*test_file: ../../korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv\n",
      "2022-04-12 09:57:36,142 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "INFO:s-bert-ts:----------------------------------------------------------------------\n",
      "2022-04-12 09:57:36,143 - s-bert-ts - INFO - *out_path: ../../model/sbert/sbert-ts2022-04-12\n",
      "INFO:s-bert-ts:*out_path: ../../model/sbert/sbert-ts2022-04-12\n",
      "2022-04-12 09:57:36,145 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "INFO:s-bert-ts:----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff9cf354ec043389fe8d8882592e3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba14f45924d34d6e8ccd1a94d20ec0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/18432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:537: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  labels = torch.tensor(labels).to(self._target_device)\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([32, 384])) that is different to the input size (torch.Size([32, 768])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (384) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_74240/1251498066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSequentialEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_score_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    710\u001b[0m                         \u001b[0mskip_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mscale_before_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m                         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/sentence_transformers/losses/MSELoss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence_features, labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (384) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "###### Train model ######\n",
    "# 훈련 시작\n",
    "# 훈련을 시작하면, output_path/eval/ 폴더에 mse 테스트, similarity 테스트 csv 파일에 기록됨\n",
    "# (mse_evaluation_test_results.csv , similarity_evaluation_dev_results.csv)\n",
    "import time\n",
    "\n",
    "#10% of train data for warm-up\n",
    "warmup_steps = math.ceil(len(train_reader) * num_epochs / train_batch_size * 0.1) \n",
    "evaluation_steps = warmup_steps\n",
    "output_path = \"../../model/sbert/sbert-ts\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*Warmup-steps:{}, ephocs:{}, train_data_len:{}, train_batch_size: {}\".format(warmup_steps, num_epochs, len(train_reader), train_batch_size))\n",
    "logger.info(\"*teacher_model: {}\".format(teacher_model_name))\n",
    "logger.info(\"*student_model_name: {}\".format(student_model_name))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*train_file: {}\".format(train_file))\n",
    "#logger.info(\"*eval_file: {}\".format(eval_file))\n",
    "logger.info(\"*test_file: {}\".format(test_file))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*out_path: {}\".format(output_path))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1]),\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,   # 옵티마이저 2e-5까지 처음 1000 번은 아주작게 스탭을 옮김\n",
    "          scheduler='warmupconstant',\n",
    "          output_path=output_path,\n",
    "          save_best_model=True,\n",
    "          optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "          )\n",
    "\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61b8a8-3cb8-41ac-bac7-ec1592e14c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
