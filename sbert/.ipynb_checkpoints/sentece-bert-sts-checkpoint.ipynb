{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2266a-06cc-41fc-8573-51fee4628282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================================\n",
    "# sentence-bert STS 데이터셋을 가지고, 훈련 및 평가 예시\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# 도큐먼트 : https://www.sbert.net/index.html\n",
    "# 소스참고 : https://github.com/BM-K/KoSentenceBERT-ETRI\n",
    "\n",
    "# pip install -U sentence-transformers\n",
    "#======================================================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert1\", logfilename=\"s-bert1\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f4d38-e45e-4c30-87c8-7a16a002a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# s-bert로 만들 원본 bert 경로\n",
    "model_path = \"../model/distilbert/distilbert-0331-TS-nli-0.1-10\"\n",
    "\n",
    "# 원본 bert를 sentencebert로 만든후 만들어진 s-bert 저장 경로\n",
    "#smodel_path = 'output/training_nli_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "smodel_path = \"../model/sbert/sbert-distilbert-0331-TS-nli-0.1-10\"\n",
    "\n",
    "# 평가시 cosine 유사도등 측정 결과값 파일 (similarity_evaluation_xxxx.xls) 저장될 경로\n",
    "output_path = smodel_path\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "train_file_type = 1 #0이면 korsts.tsv 파일, 1이면 klue-stst.json 파일\n",
    "\n",
    "if train_file_type == 0:\n",
    "    train_file = '../korpora/korsts/tune_train.tsv'\n",
    "    eval_file = '../korpora/korsts/tune_dev.tsv'\n",
    "elif train_file_type == 1:\n",
    "    train_file = '../korpora/klue-sts/klue-sts-v1.1_train.json'\n",
    "    eval_file = '../korpora/klue-sts/klue-sts-v1.1_dev.json'\n",
    "\n",
    "test_file = '../korpora/korsts/tune_test.tsv'\n",
    "\n",
    "train_batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# 모델과 tokenizer 를 불러옴\n",
    "# => **사전파일(vocab.txt, *.json) 와 model 경로(config.json, pytorch_model.bin)가 같은 경로에 있어야 함.\n",
    "word_embedding_model = models.Transformer(model_path, max_seq_length=128)\n",
    "\n",
    "# word embedding_model 출력 \n",
    "print(word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30816f29-8579-4b62-ab99-3f6436035657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 bert 모델의 임베딩 풀링 정책을 설정(cls 이용, 워드임베딩 평균이용, 워드임베딩 max 이용)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),  #모델이 dimension(768)\n",
    "                               pooling_mode_mean_tokens=True,  # 워드 임베딩 평균을 이용\n",
    "                               pooling_mode_cls_token=False,   # cls 를 이용\n",
    "                               pooling_mode_max_tokens=False)  # 워드 임베딩 값중 max 값을 이용\n",
    "# pooling model 출력 \n",
    "print(pooling_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609d844-3e00-42f9-ae0f-c12102b0925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT 모델 생성\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9a825-5d52-425f-aa0b-7afd2ce336f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# korsts 훈련 데이터 불러오기\n",
    "# => [sentence1, sentence2], labels 식으로 만듬\n",
    "logger.info(f\"Read STS train dataset=>{train_file}\")\n",
    "\n",
    "train_samples = []\n",
    "count = 0\n",
    "    \n",
    "if train_file_type == 0:\n",
    "    with open(train_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_a, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_a], label=score))\n",
    "            count += 1\n",
    "            \n",
    "# klue 훈련 데이터 불러오기\n",
    "elif train_file_type == 1:           \n",
    "    with open(train_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "        \n",
    "print(train_samples[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a2b16-b2f5-4f97-a751-248c9a7bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋, 데이터 로더, 손실함수 정의\n",
    "train_dataset = SentencesDataset(train_samples, model=model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ab735-a87e-4ca9-a3d7-03f8949b392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read STSbenchmark dataset and use it as development set\n",
    "# 평가데이터 불러오기\n",
    "#korsts 파일로 두 문장간 유사도를 수치로(5.0이 만점=매우 유사) 측정함.\n",
    "logger.info(f\"Read STS dev dataset=>{eval_file}\")\n",
    "dev_samples = []\n",
    "count = 0\n",
    "\n",
    "# korSTS.tsv 파일인 경우 \n",
    "if train_file_type == 0:\n",
    "    with open(eval_file, 'rt', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "            \n",
    "#KLUE-STS.json 파일인 경우            \n",
    "elif train_file_type == 1:\n",
    "     with open(eval_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "            \n",
    "print(dev_samples[0:3])\n",
    "\n",
    "# 2개의 bert 모델에서 구한 2개의 embedding 값들의 cosine 유사도를 구해서, 이를 실제 score와 비교해서 유사도 측정함\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 batch_size=train_batch_size, \n",
    "                                                                 name='sts-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea020cec-bd98-4385-a202-86c76adee505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warmup_step은 10% 로 설정\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) \n",
    "\n",
    "# evaluation_steps은 10%로 설정\n",
    "evaluation_steps = int(len(train_dataset) * num_epochs / train_batch_size * 0.2)\n",
    "\n",
    "logger.info(f\"model:{model_path}, smodel:{smodel_path}\")\n",
    "logger.info(\"*batch_size: {}, epoch:{}, train_dataset:{}, Warmup-steps: {}, evaluation_step: {}\".format(train_batch_size, num_peochs, len(train_dataset), warmup_steps, evaluation_steps))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=smodel_path\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ce8ee-8b39-4c09-9ce3-85ac036af96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# => 훈련되어서 저장된 s-bert 모델을 불러와서 성능 평가 해봄\n",
    "##############################################################################\n",
    "import time\n",
    "\n",
    "test_samples = []\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "        test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "\n",
    "logger.info(\"\\n\")\n",
    "logger.info(\"======================TEST===================\")\n",
    "logger.info(\"\\n\\n\")\n",
    "logger.info(f\"model save path > {smodel_path}\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer(smodel_path)\n",
    "\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='sts-test', show_progress_bar=True)\n",
    "result = test_evaluator(model, output_path=output_path)\n",
    "\n",
    "logger.info(f\"\\n\")\n",
    "logger.info(f\"model path: {smodel_path}\")\n",
    "logger.info(f'=== result: {result} ===')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79dcfa5-7474-4e72-ba49-b77dbdb2ee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
