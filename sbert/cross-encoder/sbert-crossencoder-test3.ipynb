{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c768bf-96d5-45cb-81f0-177738d2efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================\n",
    "# sbert 모델을 각 4가지 평가 데이터셋으로 한꺼번에 평가하는 예시임\n",
    "# => 평가 데이터 셋 : Korsts, Kluests, gluests, stsb_multi_mt \n",
    "# => 각 개별적 eval 출력 하는 예시는 sbert-test2.ipynb 참조\n",
    "#=> ** skt/kobert-base-V1  테스트시에는  tokenizer_config.json 에 tokenizer_class:\"XLNetTokenizer\" 인지 확인해야 함\n",
    "#==============================================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "sys.path.append('../../')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert-test\", logfilename=\"../../../log/s-bert-test\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af081cd9-c333-4d95-9168-f8e31d718d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 평가 설정 parameter 들 ########################################\n",
    "# 평가할 bert 모델 혹은 sbert모델 경로\n",
    "model_path = \"../../../data11/model/moco/cross/albert-small-kor-cross-sts-nli-sts-nli-sts-nli-sts\"\n",
    "#===============================================================================\n",
    "eval_batch_size = 64 #64 # 배치파일사이즈 = 크면 eval 시간이 단축됨.단 gpu 메모리 올라감.\n",
    "max_seq_length = 72 #128\n",
    "#===============================================================================\n",
    "\n",
    "# 평가시 cosine 유사도등 측정 결과값 파일 (similarity_evaluation_xxxx.xls) 저장될 경로\n",
    "output_path = 'eval'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "csvfilename = 'scratch'   # 유사도 저장할 파일명 (eval/similarity_evaluation_{csffilename}.csv 형식으로 저장)\n",
    "\n",
    "# 평가 sts 형태의 말뭉치 파일들\n",
    "use_korsts = 1     #  korsts 파일\n",
    "use_kluests = 1    # kluests_v1.1 파일\n",
    "use_glue_sts = 1   # true이면 영문 glue_sts 데이터셋 추가하여 평기 시킴\n",
    "use_stsb = 1     # true이면 영문 stsb_multi_mt데이터셋 추가하여 평가시킴.)\n",
    "   \n",
    "# korsts tsv 파일 경로\n",
    "korsts_file = '../../../data11/korpora/korsts/tune_test.tsv'\n",
    "\n",
    "# kluests json 파일 경로\n",
    "kluests_file = '../../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json'\n",
    "\n",
    "# glue test tsv 파일경로 => validataion 데이터셋으로 eval 구하므로 주석처리함\n",
    "#glue_test_file = '../../../data11/korpora/gluestsb/gluestsb-test.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b90080-0e93-423f-9cee-7a2f297aff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder(model_path, num_labels=1, device=device)\n",
    "logger.info(f\"*model path: {model_path}\")\n",
    "logger.info(f'{model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ef123-8c7a-41b4-a91e-8224b2a896d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "# 평가 함수 정의\n",
    "def evalsts(model,                      # 모델\n",
    "            output_path,                # eval 파일 출력 dir         \n",
    "            eval_datasets: List[int],   # eval 데이터셋(리스트)\n",
    "            csvfilename,                # eval 출력 파일 명 \n",
    "           ):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    test_evaluator = CECorrelationEvaluator.from_input_examples(eval_datasets, name=csvfilename)\n",
    "    result = test_evaluator(model, output_path=output_path)\n",
    "   \n",
    "    logger.info(f\"\\n\")\n",
    "    logger.info(f'=== result: {result} ===')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(\"=====================================================\")\n",
    "    logger.info(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4733aa8-92c0-4583-ade4-fda30e46ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korsts 평가 시작 \n",
    "from datasets import load_dataset\n",
    "\n",
    "if use_korsts == True:\n",
    "    test_samples = []\n",
    "    \n",
    "    logger.info(\"=====================================================\")\n",
    "    logger.info(f\"*korsts file({korsts_file})\")\n",
    "    \n",
    "    with open(korsts_file, 'rt', encoding='utf-8') as fIn1:\n",
    "        lines = fIn1.readlines()\n",
    "        for line in lines:\n",
    "            s1, s2, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0\n",
    "            test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "            \n",
    "    # 평가 시작\n",
    "    evalsts(model, output_path, test_samples, csvfilename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c942937-93c2-4d1d-b236-aa54a001d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kluests 평가 시작 \n",
    "from datasets import load_dataset\n",
    "import json\n",
    "    \n",
    "if use_kluests == True:\n",
    "    test_samples = []\n",
    "    \n",
    "    logger.info(\"=====================================================\")\n",
    "    logger.info(f\"*Kluests file({kluests_file})\")\n",
    "    \n",
    "    with open(kluests_file, \"r\", encoding='utf-8') as fIn2:\n",
    "        data = json.load(fIn2)\n",
    "        for el in data:\n",
    "            s1 = el[\"sentence1\"]\n",
    "            s2 = el[\"sentence2\"]\n",
    "            score = el[\"labels\"]['label']\n",
    "            test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "            \n",
    "    # 평가 시작\n",
    "    evalsts(model, output_path, test_samples, csvfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e85f2c-5db5-41ef-bade-dcc80d46b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gluests 평가 시작 \n",
    "from datasets import load_dataset\n",
    "    \n",
    "if use_glue_sts == True:\n",
    "    test_samples = []\n",
    "    \n",
    "    logger.info(\"=====================================================\")\n",
    "    logger.info(f\"*gluests\")\n",
    "    \n",
    "    #glue stsb valildation 데이터셋 불러옴(subset : \"stsb\" = 1,500)\n",
    "    glue_stsb_dataset = load_dataset(\"glue\",\"stsb\", split=\"validation\")\n",
    "    for data in glue_stsb_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        test_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "    '''\n",
    "    # glue-test는 허깅페이스 데이터셋에는 labe(score)가 -1로 등록되어 있어서, 실제 glue 사이트에서 org파일을 받아서 불러옴\n",
    "    # => 해당 sts-test.tsv 파일에서 몇가지 주석달린 문장들은 제거함(약 300개)\n",
    "    with open(glue_test_file, 'rt', encoding='utf-8') as fIn1:\n",
    "        lines = fIn1.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            _, _, _, _, score, text_a, text_b = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0\n",
    "            test_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "    '''\n",
    "            \n",
    "    # 평가 시작\n",
    "    evalsts(model, output_path, test_samples, csvfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ae096-69f5-46a8-a76b-c75e9b5d23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stsb_multi_mt 평가 시작 \n",
    "from datasets import load_dataset\n",
    "    \n",
    "if use_stsb == True:\n",
    "    test_samples = []\n",
    "    \n",
    "    logger.info(\"=====================================================\")\n",
    "    logger.info(f\"*stsb_multi_mt\")\n",
    "    \n",
    "    stsb_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"test\")\n",
    "    for data in stsb_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        test_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            \n",
    "    # 평가 시작\n",
    "    evalsts(model, output_path, test_samples, csvfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a6fc2-b5b6-4034-8734-27c271dbaa23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
