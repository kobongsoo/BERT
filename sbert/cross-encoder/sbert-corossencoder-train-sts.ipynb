{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d44e0a-ecff-417c-84f0-fb67a6d386a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================================================================\n",
    "# sentence-bert(sbert)에 CrossEncoder 방식 STS 훈련 예시임\n",
    "# => cross-encocoder 방식은 2개의 문장(문장1, 문장2)을 입력했을때 output으로 유사도(0~1값)을 출력해줌\n",
    "# => 반드시 num_label=1이어야 함.(*따라서 nli 모델은 num_labels=3이므로, 훈련시킬수 없음)\n",
    "# => bi-encoder와 차이점은 Evaluator가 다르다는 것임.\n",
    "# => bi-encoder: EmbeddingSimilarityEvaluator vs cross-encoder: CECorrelationEvaluator\n",
    "#\n",
    "# => 참고 : https://www.sbert.net/examples/training/cross-encoder/README.html\n",
    "#         https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/cross-encoder/training_stsbenchmark.py  \n",
    "#========================================================================================================================\n",
    "import torch \n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from os import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('../../')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "device = GPU_info()\n",
    "logger =  mlogging(loggername=\"sbertcross\", logfilename=\"../../../log/sbert-crossencocer-train-sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c60f6a-3271-4734-ac02-29f2cb453055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# parameter 설정\n",
    "#===============================================================================\n",
    "\n",
    "train_batch_size = 32  # base 이상 모델은=128, small 모델은=32 로 하는게 좋음\n",
    "eval_batch_size = 64\n",
    "num_epochs = 10      # 10 정도 해도 최상의 모델을 찾을수 있음 (*sbert는 eval이 최상인 모델이 out모델로 저장됨)\n",
    "max_seq_length = 128\n",
    "lr = 1e-4            # default=2e-5\n",
    "eps = 1e-6           #lr이 0으로 나뉘어져 계산이 엉키는 것을 방지하기 위해 epsilion\n",
    "seed=111\n",
    "\n",
    "# 말뭉치 사용.유무\n",
    "use_korsts = 1     # 한국어 korsts 파일 (tsv 5,749개)\n",
    "use_kluests = 1    # 한국어 kluests_v1.1 파일 (json 11,668개)\n",
    "use_sts17 = 1      # 한국어 sts17-crosslingual-sts (jsonl 2,846개)\n",
    "use_glue_sts = 1   # 영어 glue_sts (load_dataset 5,749개)\n",
    "use_en_sts = 1     # 영어 stsb_multi_mt(load_dataset 15,676개) = stsb_multi_mt(5,749개) + mteb/sickr-sts(9,927개)\n",
    "\n",
    "# sentence_transformers 2.2.2 부터는 'correct_bias' 인자가 없어졌음. => correct_bias : False 하면 sts 성능이 떨어짐(*원인 모름)\n",
    "use_correct_bias = 0\n",
    "\n",
    "if use_correct_bias == 0:\n",
    "    opt_params = {'lr': lr, 'eps': eps}  # defalut\n",
    "else:\n",
    "    opt_params = {'lr': lr, 'eps': eps, 'correct_bias': False}\n",
    "    print(f'**correct_bias:False')\n",
    "    \n",
    "# KorSTS 학습, 평가 파일들\n",
    "train_korsts_file = '../../../data11/korpora/korsts/tune_train.tsv'\n",
    "eval_korsts_file = '../../../data11/korpora/korsts/tune_dev.tsv'\n",
    "\n",
    "# KlueSTS 학습, 평가 파일들\n",
    "train_kluests_file = '../../../data11/korpora/klue-sts/klue-sts-v1.1_train.json'\n",
    "eval_kluests_file = '../../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json'\n",
    "\n",
    "# sts17-crosslingual-sts  학습 파일(*평가파일 없음)\n",
    "train_sts17_file = '../../../data11/korpora/sts17-crosslingual-sts/ko-ko.jsonl'\n",
    "\n",
    "# 모델 경로 \n",
    "model_path = \"../../../data11/model/moco/cross/albert-small-kor-cross-nli/bertmodel\"            # 불러올 모델 경로\n",
    "model_save_path = '../../../data11/model/moco/cross/albert-small-kor-cross-nli-sts'   # 저장할 모델 경로\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "'''\n",
    "# nli 모델인 경우 bert 모델로 저장\n",
    "# => nli 모델은 num_labels=3이므로, sts 훈련을 위해서는 bert모델로 다시 만들어야 함\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer, AlbertModel, AlbertTokenizer\n",
    "model_save_path = \"../../../data11/model/moco/cross/albert-small-kor-cross-nli\"\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_save_path, do_lower_case=True, keep_accent=False)\n",
    "bertmodel = AlbertModel.from_pretrained(model_save_path)\n",
    "OUTPATH = model_save_path + \"/bertmodel\"\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "bertmodel.save_pretrained(OUTPATH)\n",
    "tokenizer.save_pretrained(OUTPATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a067ba-c1b7-49d3-a07c-1198b6edd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩 \n",
    "# => nli 모델(num_labels=3)은 sts 훈련 못함 (반드시 num_labels=1 이어야함)\n",
    "model = CrossEncoder(model_path, max_length=max_seq_length, num_labels=1)     # CrossEncoder 모델 로딩\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaada5a-c732-4e63-9bdd-c0300eda7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorSTS 훈련 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_korsts == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_korsts_file}\")\n",
    "    with open(train_korsts_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{train_korsts_file} count: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "####################################################################################################\n",
    "# klue 훈련 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"sts\", split=\"train\")\n",
    "# for data in datas:\n",
    "#        text_a = data[\"sentence1\"]\n",
    "#        text_b = data[\"sentence2\"]\n",
    "#        score = data[\"labels\"][\"label\"]\n",
    "#        score = float(score) / 5.0  \n",
    "###################################################################################################           \n",
    "if use_kluests == True:  \n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_kluests_file}\")\n",
    "    with open(train_kluests_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{train_kluests_file} len: {count}')\n",
    "####################################################################################################\n",
    "# 한국어 sts17-crosslingual-sts 훈련 데이터셋 설정\n",
    "# => jsonl : 여러개의 json 형식 파일이 각 줄마다 기록되어 있는 형태 파일\n",
    "# => 패키지 설치 : !pip install jsonlines\n",
    "####################################################################################################\n",
    "if use_sts17 == True:\n",
    "    import jsonlines\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_sts17_file}\")\n",
    "    with jsonlines.open(train_sts17_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            text_a = line[\"sentence1\"]\n",
    "            text_a = line[\"sentence2\"]\n",
    "            score = line[\"score\"]\n",
    "            score = float(score) / 5.0\n",
    "            \n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "            \n",
    "    logger.info(f'*{train_kluests_file} len: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "#############################################################################################\n",
    "# 영문 sts 데이터셋 설정 (load_dataset)\n",
    "# => stsb_multi_mt , mteb/sickr-sts 영문 sts 훈련 데이터 셋 불러오기\n",
    "#############################################################################################\n",
    "if use_en_sts == True:\n",
    "    count = 0\n",
    "    en_sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"train\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*stsb_multi_mt_en len: {count}')\n",
    "    \n",
    "    # mteb/sickr-sts 훈련데이터 불러옴\n",
    "    count = 0    \n",
    "    en_sts_dataset = load_dataset(\"mteb/sickr-sts\", split=\"test\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*mteb/sickr-sts len: {count}')\n",
    "#############################################################################################           \n",
    " \n",
    "#############################################################################################\n",
    "# GLUE STS 훈련 데이터셋 설정 (load_dataset)\n",
    "#############################################################################################\n",
    "if use_glue_sts == True:\n",
    "    # glue stsb 훈련데이터 불러옴(5,749개)\n",
    "    count = 0    \n",
    "    en_sts_dataset = load_dataset(\"glue\",\"stsb\", split=\"train\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*glue_stsb len: {count}')\n",
    "#############################################################################################\n",
    "\n",
    "logger.info(f'------------------------------------------------------------------------')        \n",
    "logger.info(f'*train_samples_len:{len(train_samples)}')\n",
    "print(train_samples[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d206b0a-7e5a-4f55-bf64-4e28674d5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read STSbenchmark dataset and use it as development set\n",
    "# 평가데이터 불러오기\n",
    "#korsts 파일로 두 문장간 유사도를 수치로(5.0이 만점=매우 유사) 측정함.\n",
    "dev_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorSTS 평가 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_korsts == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS dev dataset=>{eval_korsts_file}\")\n",
    "    with open(eval_korsts_file, 'rt', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{eval_korsts_file} len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# KlueSTS 평가 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"sts\", split=\"test\")\n",
    "# for data in datas:\n",
    "#        text_a = data[\"sentence1\"]\n",
    "#        text_b = data[\"sentence2\"]\n",
    "#        score = data[\"labels\"][\"label\"]\n",
    "#        score = float(score) / 5.0  \n",
    "####################################################################################################           \n",
    "if use_kluests == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS dev dataset=>{eval_kluests_file}\")\n",
    "    with open(eval_kluests_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{eval_kluests_file} len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# 영문 stsb_multi_mt 데이터 셋 설정(load_dataset)\n",
    "####################################################################################################                \n",
    "# stsb_multi_mt 영문 sts dev 데이터 셋 불러오기\n",
    "if use_en_sts == True:\n",
    "    count = 0\n",
    "    en_sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"dev\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*stsb_multi_mt len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# 영문 GLUE 데이터 셋 설정(load_dataset)\n",
    "####################################################################################################  \n",
    "if use_glue_sts == True:\n",
    "    count = 0\n",
    "    glue_stsb_dataset = load_dataset(\"glue\",\"stsb\", split=\"validation\")\n",
    "    for data in glue_stsb_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        \n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "        dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "        \n",
    "        #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # **한국어 모델 훈련시에는 500개(총 1,500개 중) 정도만 작게 평가 데이터셋으로 설정\n",
    "        #if count > 499:\n",
    "        #    break\n",
    "        #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        \n",
    "    logger.info(f'*glue-stsb len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "logger.info(f'------------------------------------------------------------------------')        \n",
    "logger.info(f'*dev_samples_len:{len(dev_samples)}')\n",
    "print(dev_samples[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf1b85-061a-4453-89a5-ff192d4f2d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더, evaluator \n",
    "# We wrap train_samples (which is a List[InputExample]) into a pytorch DataLoader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "#** bi-encoder와 차이점.\n",
    "# We add an evaluator, which evaluates the performance during training\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(dev_samples, name='sts-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234fab20-46fb-47b0-8f4f-79a30e6c96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작 \n",
    "# => model_save_path에 모델과, 평가 CECorrelationEvaluator_sts-dev_results.csv 파일 생성됨\n",
    "\n",
    "warmup_steps = math.ceil(len(train_samples) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "# evaluation_steps은 20%로 설정\n",
    "evaluation_steps = int(len(train_samples) * num_epochs / train_batch_size * 0.2)\n",
    "\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "logger.info(f\"*IN-model:{model_path}\")\n",
    "logger.info(f\"*OUT-model:{model_save_path}\")\n",
    "logger.info(\"*seed:{}, train_batch:{}, eval_batch:{}, epoch:{}, lr:{}, eps:{}, max_seq_length:{}, train_samples:{}, Warmup-steps: {}, evaluation_step: {}\".format(seed, train_batch_size, eval_batch_size, num_epochs, lr, eps, max_seq_length, len(train_samples), warmup_steps, evaluation_steps))\n",
    "logger.info(f\"*use_correct_bias:{use_correct_bias}\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,\n",
    "          optimizer_params= opt_params, \n",
    "          save_best_model=True, # **기본 = True : eval 가장 best 모델을 output_Path에 저장함\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95a69e-13f4-4d2f-b587-11792c3e1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# => 훈련되어서 저장된 s-bert 모델을 불러와서 성능 평가 해봄\n",
    "##############################################################################\n",
    "import time \n",
    "\n",
    "#model_save_path = \"../../../data11/model/moco/cross/albert-small-kor-cross-sts\"\n",
    "#model_save_path = \"../../../model/classification/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327-ft-nli-0328/bertmodel\"\n",
    "\n",
    "\n",
    "test_file = '../../../data11/korpora/korsts/tune_test.tsv'\n",
    "\n",
    "# 훈련 데이터 불러옴 \n",
    "test_samples = []\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "        test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "\n",
    "start = time.time()       \n",
    "model = CrossEncoder(model_save_path)\n",
    "\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "result = evaluator(model)\n",
    "\n",
    "logger.info(f\"\\n\")\n",
    "logger.info(f\"model path: {model_save_path}\")\n",
    "logger.info(f'=== result: {result} ===')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(\"==============================================\")\n",
    "logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a623a-de83-44df-a039-7eaa93cdb632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
