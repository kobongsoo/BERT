{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cc833-f6a3-4eb9-9ec4-3de894616963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================================================================\n",
    "# sentence-bert(sbert)에 CrossEncoder 방식 NLI 훈련 예시임\n",
    "# => cross-encocoder 방식은 2개의 문장(문장1, 문장2)을 입력했을때 output으로 유사도(0~1값)을 출력해줌\n",
    "#\n",
    "# => 참고 : https://www.sbert.net/examples/training/cross-encoder/README.html\n",
    "#         https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/cross-encoder/training_stsbenchmark.py  \n",
    "#========================================================================================================================\n",
    "import torch \n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from os import sys\n",
    "from datetime import datetime\n",
    "sys.path.append('../../')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "device = GPU_info()\n",
    "logger =  mlogging(loggername=\"sbertcross\", logfilename=\"../../../log/sbert-crossencocer-train-sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db2e32-28e6-4676-a6c8-6810f8e95bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batch_size = 64\n",
    "num_epochs = 3\n",
    "lr = 3e-5 # default=2e-5 \n",
    "eps = 1e-8 #lr이 0으로 나뉘어져 계산이 엉키는 것을 방지하기 위해 epsilion\n",
    "max_seq_length = 128 \n",
    "seed = 111 \n",
    "\n",
    "use_kornli = 1 # kornli 파일 \n",
    "use_kluenli = 1 # kluests_v1.1 파일 \n",
    "use_gluenli = 1 # glue 파일\n",
    "\n",
    "#KorNLI, KorSTS 파일 경로\n",
    "train_kornli_file = '../../../data11/korpora/kornli/snli_1.0_train.ko.tsv' \n",
    "eval_kornli_file = '../../../data11/korpora/kornli/xnli.dev.ko-1.tsv'\n",
    "\n",
    "#KLUENIL, KlueSTS 파일 경로\n",
    "train_kluenli_file = '../../../data11/korpora/klue-nli/klue-nli-v1.1_train.json' \n",
    "eval_kluenli_file = '../../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json'\n",
    "\n",
    "#GLUENLI, GLUESTS 파일 경로\n",
    "train_gluenli_file = '../../../data11/korpora/gluemnli/glue-mnli-train.tsv' \n",
    "eval_gluenli_file = '../../../data11/korpora/gluemnli/glue-mnli-valid.tsv'\n",
    "\n",
    "label2int = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "model_path = \"../../../data11/model/moco/cross/klue-cross-sts-nli-sts/bertmodel\"\n",
    "model_save_path = '../../../data11/model/moco/cross/klue-cross-sts-nli-sts-nli' # +datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# sts->nli 모델로 만들때도 bert 모델로 만든 후 훈련시켜야 함\n",
    "'''\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer, AlbertModel, AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_path, do_lower_case=True, keep_accent=False)\n",
    "bertmodel = AlbertModel.from_pretrained(model_path)\n",
    "OUTPATH = model_path + \"/bertmodel\"\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "bertmodel.save_pretrained(OUTPATH)\n",
    "tokenizer.save_pretrained(OUTPATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb5457-be3c-4536-812c-a3c4f1af6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 불러오기\n",
    "# => [sentence1, sentence2], labels 식으로 만듬\n",
    "train_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorNLI 훈련 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_kornli == 1:\n",
    "    count = 0\n",
    "    logger.info(f\"\\r\\nRead NLI train dataset:{train_kornli_file}\")\n",
    "\n",
    "    with open(train_kornli_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            s1, s2, label = line.split('\\t')\n",
    "            label = label2int[label.strip()]\n",
    "            if count < 5:\n",
    "                print(f\"{s1}, {s2}, {label}\")\n",
    "            \n",
    "            train_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "            count += 1\n",
    "        \n",
    "    logger.info(f'*kornli len: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "####################################################################################################\n",
    "# KlueNLI 훈련 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"nli\", split=\"train\")\n",
    "# for data in datas:\n",
    "#        s1 = data[\"sentence1\"]\n",
    "#        s2 = data[\"sentence2\"]\n",
    "#        label = data[\"label\"][\"label\"]\n",
    "###################################################################################################    \n",
    "# kluenli 훈련인 경우 \n",
    "if use_kluenli == 1:\n",
    "    count = 0\n",
    "    import json\n",
    "    logger.info(f\"\\r\\nRead NLI train dataset:{train_kluenli_file}\")\n",
    "\n",
    "    with open(train_kluenli_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            #print(data)\n",
    "            s1 = data[\"premise\"].strip()\n",
    "            s2 = data[\"hypothesis\"].strip()\n",
    "            label = label2int[data[\"gold_label\"].strip()]\n",
    "            if count < 5:\n",
    "                print(f\"{s1}, {s2}, {label}\")\n",
    "\n",
    "            train_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "            count += 1\n",
    "            \n",
    "    logger.info(f'*kluenli len: {count}')\n",
    "    \n",
    "####################################################################################################\n",
    "# GLUENLI 훈련 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_gluenli == 1:\n",
    "    count = 0\n",
    "    logger.info(f\"\\r\\nRead NLI train dataset:{train_gluenli_file}\")\n",
    "\n",
    "    with open(train_gluenli_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            s1, s2, label = line.split('\\t')\n",
    "            label = label2int[label.strip()]\n",
    "            if count < 5:\n",
    "                print(f\"{s1}, {s2}, {label}\")\n",
    "            \n",
    "            train_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "            count += 1\n",
    "        \n",
    "    logger.info(f'*gluenli len: {count}')\n",
    "####################################################################################################\n",
    "        \n",
    "logger.info(f'*train_samples_len:{len(train_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e557d-6615-4649-9d3e-228f6866abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터 불러오기\n",
    "# => [sentence1, sentence2], labels 식으로 만듬\n",
    "dev_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorNLI 평가 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_kornli == 1:\n",
    "    count = 0\n",
    "    logger.info(f\"\\r\\nRead NLI dev dataset:{eval_kornli_file}\")\n",
    "\n",
    "    with open(eval_kornli_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            s1, s2, label = line.split('\\t')\n",
    "            label = label2int[label.strip()]\n",
    "            if count < 5:\n",
    "                print(f\"{s1}, {s2}, {label}\")\n",
    "            \n",
    "            dev_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "            count += 1\n",
    "        \n",
    "    logger.info(f'*kornli len: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "####################################################################################################\n",
    "# KlueNLI 평가 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"nli\", split=\"train\")\n",
    "# for data in datas:\n",
    "#        s1 = data[\"sentence1\"]\n",
    "#        s2 = data[\"sentence2\"]\n",
    "#        label = data[\"label\"][\"label\"]\n",
    "###################################################################################################    \n",
    "# kluenli 훈련인 경우 \n",
    "if use_kluenli == 1:\n",
    "    count = 0\n",
    "    import json\n",
    "    logger.info(f\"\\r\\nRead NLI dev dataset:{eval_kluenli_file}\")\n",
    "\n",
    "    with open(eval_kluenli_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            #print(data)\n",
    "            s1 = data[\"premise\"].strip()\n",
    "            s2 = data[\"hypothesis\"].strip()\n",
    "            label = label2int[data[\"gold_label\"].strip()]\n",
    "            if count < 5:\n",
    "                print(f\"{s1}, {s2}, {label}\")\n",
    "\n",
    "            dev_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "            count += 1\n",
    "            \n",
    "    logger.info(f'*kluenli len: {count}')\n",
    "    \n",
    "####################################################################################################\n",
    "# GLUENLI 훈련 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_gluenli == 1:\n",
    "    count = 0\n",
    "    logger.info(f\"\\r\\nRead NLI dev dataset:{eval_gluenli_file}\")\n",
    "\n",
    "    with open(eval_gluenli_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            s1, s2, label = line.split('\\t')\n",
    "            label = label2int[label.strip()]\n",
    "            if count < 5:\n",
    "                print(f\"{s1}, {s2}, {label}\")\n",
    "            \n",
    "            dev_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "            count += 1\n",
    "        \n",
    "    logger.info(f'*gluenli len: {count}')\n",
    "####################################################################################################\n",
    "        \n",
    "logger.info(f'*dev_samples_len:{len(dev_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214bc8a7-1b93-45c5-a47e-07c145be3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We wrap train_samples, which is a list ot InputExample, in a pytorch DataLoader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "#During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
    "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(dev_samples, name='xnli.dev.ko.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c35036-9caa-42ed-b352-42f63b7bd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 \n",
    "model = CrossEncoder(model_path, max_length=max_seq_length, num_labels=len(label2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ebf68-7b50-49a2-9e19-e31dfe9473b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시작\n",
    "# => model_save_path에 모델과, 평가 CESoftmaxAccuracyEvaluator-dev_results.csv 파일 생성됨\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "# evaluation_steps은 20%로 설정\n",
    "evaluation_steps = warmup_steps * 2\n",
    "\n",
    "logger.info(f\"model:{model_path}, save_model:{model_save_path}\")\n",
    "logger.info(\"*train_batch: {}, epoch:{}, lr:{}, eps:{}, train_dataset:{}, Warmup-steps: {}, evaluation_step: {}\".format(train_batch_size, num_epochs, lr, eps, len(train_dataloader), warmup_steps, evaluation_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,\n",
    "          optimizer_params= {'lr': lr, 'eps': eps, 'correct_bias': False},\n",
    "          save_best_model=True, # **기본 = True : eval 가장 best 모델을 output_Path에 저장함\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e447064-ba36-4bbd-b9bb-f6e33cc4e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# => 훈련되어서 저장된 s-bert 모델을 불러와서 성능 평가 해봄\n",
    "##############################################################################\n",
    "import time \n",
    "\n",
    "#model_save_path = \"bongsoo/albert-small-kor-v1\"\n",
    "#model_save_path = \"../../../data11/model/moco/cross/albert-small-kor-cross-nli\"\n",
    "\n",
    "test_file = '../../../data11/korpora/kornli/xnli.test.ko-1.tsv'\n",
    "\n",
    "# 테스트 데이터 불러옴 \n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "test_samples = []\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, label = line.split('\\t')\n",
    "        label = label2int[label.strip()]\n",
    "        test_samples.append(InputExample(texts=[s1, s2], label=label))\n",
    "\n",
    "start = time.time()       \n",
    "model = CrossEncoder(model_save_path, num_labels=len(label2int))\n",
    "\n",
    "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(test_samples, name='xnli.test.ko.tsv')\n",
    "result = evaluator(model)\n",
    "\n",
    "logger.info(f\"\\n\")\n",
    "logger.info(f\"model path: {model_save_path}\")\n",
    "logger.info(f'=== result: {result} ===')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(\"==============================================\")\n",
    "logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d668d2-2eb1-4682-afb2-205a1b3198f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert 모델로 저장\n",
    "# => 추후 sts 훈련을 위해서는 bert모델로 다시 만들어야 하므로.\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer, AlbertModel, AlbertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_save_path, do_lower_case=True, keep_accent=False)\n",
    "bertmodel = BertModel.from_pretrained(model_save_path)\n",
    "OUTPATH = model_save_path + \"/bertmodel\"\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "bertmodel.save_pretrained(OUTPATH)\n",
    "tokenizer.save_pretrained(OUTPATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c48764-e472-4a9c-9668-be50fccadb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
