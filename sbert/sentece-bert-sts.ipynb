{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d2266a-06cc-41fc-8573-51fee4628282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/s-bert-sts_2022-10-20.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# sentence-bert STS 데이터셋을 가지고, 훈련 및 평가 예시\n",
    "#\n",
    "# => 기존 (distil)bert 모델을 가지고, STS 훈련 및 평가 후, S-BERT로 만드는 예시임.\n",
    "\n",
    "#=> 필요에 따라 출력 dimension을 768보다 작게 줄이고 싶을때 dense 모델을 추가해서 줄일수 있음\n",
    "#=> reduce_out_dimension = True 로 하면, 출력 임베딩 dimension이 줄어들게 설정가능함\n",
    "\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# **learning rate는 기본이 2e-5임\n",
    "#\n",
    "# 도큐먼트 : https://www.sbert.net/index.html\n",
    "# 소스참고 : https://github.com/BM-K/KoSentenceBERT-ETRI\n",
    "#\n",
    "# pip install -U sentence-transformers\n",
    "#\n",
    "#======================================================================================================\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert-sts\", logfilename=\"../../log/s-bert-sts\")\n",
    "device = GPU_info()\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560f4d38-e45e-4c30-87c8-7a16a002a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer({'max_seq_length': 72, 'do_lower_case': False}) with Transformer model: BertModel \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# s-bert로 만들 원본 bert 경로\n",
    "#model_path = \"../../data11/model/distilbert/bert-re-kowiki/\"\n",
    "model_path = \"beomi/kcbert-base\"\n",
    "\n",
    "# 원본 bert를 sentencebert로 만든후 만들어진 s-bert 저장 경로\n",
    "# => **해당 경로\\eval 폴더에 similarity_evaluation_sts-dev_result.csv 파일로 각 epoch 마다 평가된 결과가 기록된다.\n",
    "#smodel_path = 'output/training_nli_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "smodel_path = \"../../data11/model/sbert/kcbert-base-korsts/\"\n",
    "\n",
    "\n",
    "use_korsts = 1     # 한국어 korsts 파일 (tsv 5,749개)\n",
    "use_kluests = 0    # 한국어 kluests_v1.1 파일 (json 11,668개)\n",
    "use_sts17 = 0      # 한국어 sts17-crosslingual-sts (jsonl 2,846개)\n",
    "use_glue_sts = 0   # 영어 glue_sts (load_dataset 5,749개)\n",
    "use_en_sts = 0     # 영어 stsb_multi_mt(load_dataset 15,676개) = stsb_multi_mt(5,749개) + mteb/sickr-sts(9,927개)\n",
    "\n",
    "# KorSTS 학습, 평가 파일들\n",
    "train_korsts_file = '../../data11/korpora/korsts/tune_train.tsv'\n",
    "eval_korsts_file = '../../data11/korpora/korsts/tune_dev.tsv'\n",
    "\n",
    "# KlueSTS 학습, 평가 파일들\n",
    "train_kluests_file = '../../data11/korpora/klue-sts/klue-sts-v1.1_train.json'\n",
    "eval_kluests_file = '../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json'\n",
    "\n",
    "# sts17-crosslingual-sts  학습 파일(*평가파일 없음)\n",
    "train_sts17_file = '../../data11/korpora/sts17-crosslingual-sts/ko-ko.jsonl'\n",
    "\n",
    "train_batch_size = 32\n",
    "num_epochs = 5      # 128 정도 해도 최상의 모델을 찾을수 있음 (*sbert는 eval이 최상인 모델이 out모델로 저장됨)\n",
    "#num_epochs = 800\n",
    "max_seq_length = 72\n",
    "lr = 5e-5             # default=2e-5\n",
    "eps = 1e-6\n",
    "#============================================================================\n",
    "# *출력 dimension을 줄일 경우에는 True로 하고, out_dimension에 줄일 값을 설정함\n",
    "reduce_out_dimension = False  # True이면 dimension을 줄임=>Dense 모델 추가됨\n",
    "out_dimension = 128\n",
    "#============================================================================\n",
    "\n",
    "# 모델과 tokenizer 를 불러옴\n",
    "# => **사전파일(vocab.txt, *.json) 와 model 경로(config.json, pytorch_model.bin)가 같은 경로에 있어야 함.\n",
    "word_embedding_model = models.Transformer(model_path, max_seq_length=max_seq_length)\n",
    "\n",
    "# word embedding_model 출력 \n",
    "print(word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30816f29-8579-4b62-ab99-3f6436035657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# 2 bert 모델의 임베딩 풀링 정책을 설정(cls 이용, 워드임베딩 평균이용, 워드임베딩 max 이용)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),  #모델이 dimension(768)\n",
    "                               pooling_mode_mean_tokens=True,  # 워드 임베딩 평균을 이용\n",
    "                               pooling_mode_cls_token=False,   # cls 를 이용\n",
    "                               pooling_mode_max_tokens=False)  # 워드 임베딩 값중 max 값을 이용\n",
    "# pooling model 출력 \n",
    "print(pooling_model)\n",
    "print(pooling_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f5a82c1-c5a5-467e-b758-362c7056918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. dense 모델 추가(옵션)\n",
    "#=> 필요에 따라 출력 dimension을 768보다 작게 줄이고 싶을때 dense 모델을 추가해서 줄임.\n",
    "#=> https://www.sbert.net/docs/training/overview.html?highlight=dense 참조\n",
    "if reduce_out_dimension:\n",
    "    dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), # 입력 dimension은 앞에 pooling모델 embedding dimension으로 지정\n",
    "                               out_features=out_dimension,  # 출력 dimension\n",
    "                               activation_function=nn.Tanh())  # activation function은 Tahn으로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6609d844-3e00-42f9-ae0f-c12102b0925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 72, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# SBERT 모델 생성\n",
    "if reduce_out_dimension:\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "else:\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de9a825-5d52-425f-aa0b-7afd2ce336f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 17:28:41,205 - s-bert-sts - INFO - Read STS train dataset=>../../data11/korpora/korsts/tune_train.tsv\n",
      "2022-10-20 17:28:41,225 - s-bert-sts - INFO - *../../data11/korpora/korsts/tune_train.tsv count: 5749\n",
      "2022-10-20 17:28:41,227 - s-bert-sts - INFO - ------------------------------------------------------------------------\n",
      "2022-10-20 17:28:41,228 - s-bert-sts - INFO - *train_samples_len:5749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비행기가 이륙하고 있다., 비행기가 이륙하고 있다., 1.0\n",
      "한 남자가 큰 플루트를 연주하고 있다., 남자가 플루트를 연주하고 있다., 0.76\n",
      "한 남자가 피자에 치즈를 뿌려놓고 있다., 한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다., 0.76\n",
      "[<sentence_transformers.readers.InputExample.InputExample object at 0x7fdd3e047cd0>, <sentence_transformers.readers.InputExample.InputExample object at 0x7fdd33bcd310>, <sentence_transformers.readers.InputExample.InputExample object at 0x7fdd33b50b20>]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorSTS 훈련 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_korsts == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_korsts_file}\")\n",
    "    with open(train_korsts_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{train_korsts_file} count: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "####################################################################################################\n",
    "# klue 훈련 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"sts\", split=\"train\")\n",
    "# for data in datas:\n",
    "#        text_a = data[\"sentence1\"]\n",
    "#        text_b = data[\"sentence2\"]\n",
    "#        score = data[\"labels\"][\"label\"]\n",
    "#        score = float(score) / 5.0  \n",
    "###################################################################################################           \n",
    "if use_kluests == True:  \n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_kluests_file}\")\n",
    "    with open(train_kluests_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{train_kluests_file} len: {count}')\n",
    "####################################################################################################\n",
    "# 한국어 sts17-crosslingual-sts 훈련 데이터셋 설정\n",
    "# => jsonl : 여러개의 json 형식 파일이 각 줄마다 기록되어 있는 형태 파일\n",
    "# => 패키지 설치 : !pip install jsonlines\n",
    "####################################################################################################\n",
    "if use_sts17 == True:\n",
    "    import jsonlines\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_sts17_file}\")\n",
    "    with jsonlines.open(train_sts17_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            text_a = line[\"sentence1\"]\n",
    "            text_a = line[\"sentence2\"]\n",
    "            score = line[\"score\"]\n",
    "            score = float(score) / 5.0\n",
    "            \n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "            \n",
    "    logger.info(f'*{train_kluests_file} len: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "#############################################################################################\n",
    "# 영문 sts 데이터셋 설정 (load_dataset)\n",
    "# => stsb_multi_mt , mteb/sickr-sts 영문 sts 훈련 데이터 셋 불러오기\n",
    "#############################################################################################\n",
    "if use_en_sts == True:\n",
    "    count = 0\n",
    "    en_sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"train\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*stsb_multi_mt_en len: {count}')\n",
    "    \n",
    "    # mteb/sickr-sts 훈련데이터 불러옴\n",
    "    count = 0    \n",
    "    en_sts_dataset = load_dataset(\"mteb/sickr-sts\", split=\"test\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*mteb/sickr-sts len: {count}')\n",
    "#############################################################################################           \n",
    " \n",
    "#############################################################################################\n",
    "# GLUE STS 훈련 데이터셋 설정 (load_dataset)\n",
    "#############################################################################################\n",
    "if use_glue_sts == True:\n",
    "    # glue stsb 훈련데이터 불러옴(5,749개)\n",
    "    count = 0    \n",
    "    en_sts_dataset = load_dataset(\"glue\",\"stsb\", split=\"train\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*glue_stsb len: {count}')\n",
    "#############################################################################################\n",
    "\n",
    "logger.info(f'------------------------------------------------------------------------')        \n",
    "logger.info(f'*train_samples_len:{len(train_samples)}')\n",
    "print(train_samples[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840a2b16-b2f5-4f97-a751-248c9a7bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋, 데이터 로더, 손실함수 정의\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5ab735-a87e-4ca9-a3d7-03f8949b392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 17:28:41,250 - s-bert-sts - INFO - Read STS dev dataset=>../../data11/korpora/korsts/tune_dev.tsv\n",
      "2022-10-20 17:28:41,265 - s-bert-sts - INFO - *../../data11/korpora/korsts/tune_dev.tsv len: 1500\n",
      "2022-10-20 17:28:41,267 - s-bert-sts - INFO - ------------------------------------------------------------------------\n",
      "2022-10-20 17:28:41,269 - s-bert-sts - INFO - *dev_samples_len:1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전모를 가진 한 남자가 춤을 추고 있다., 안전모를 쓴 한 남자가 춤을 추고 있다., 1.0\n",
      "어린아이가 말을 타고 있다., 아이가 말을 타고 있다., 0.95\n",
      "한 남자가 뱀에게 쥐를 먹이고 있다., 남자가 뱀에게 쥐를 먹이고 있다., 1.0\n",
      "한 여성이 기타를 연주하고 있다., 한 남자가 기타를 치고 있다., 0.48\n",
      "한 여성이 플루트를 연주하고 있다., 남자가 플루트를 연주하고 있다., 0.55\n",
      "[<sentence_transformers.readers.InputExample.InputExample object at 0x7fdd3da3b3a0>, <sentence_transformers.readers.InputExample.InputExample object at 0x7fdd3da3b610>, <sentence_transformers.readers.InputExample.InputExample object at 0x7fdd3e030700>]\n"
     ]
    }
   ],
   "source": [
    "#Read STSbenchmark dataset and use it as development set\n",
    "# 평가데이터 불러오기\n",
    "#korsts 파일로 두 문장간 유사도를 수치로(5.0이 만점=매우 유사) 측정함.\n",
    "dev_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorSTS 평가 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_korsts == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS dev dataset=>{eval_korsts_file}\")\n",
    "    with open(eval_korsts_file, 'rt', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{eval_korsts_file} len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# KlueSTS 평가 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"sts\", split=\"test\")\n",
    "# for data in datas:\n",
    "#        text_a = data[\"sentence1\"]\n",
    "#        text_b = data[\"sentence2\"]\n",
    "#        score = data[\"labels\"][\"label\"]\n",
    "#        score = float(score) / 5.0  \n",
    "####################################################################################################           \n",
    "if use_kluests == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS dev dataset=>{eval_kluests_file}\")\n",
    "    with open(eval_kluests_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{eval_kluests_file} len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# 영문 stsb_multi_mt 데이터 셋 설정(load_dataset)\n",
    "####################################################################################################                \n",
    "# stsb_multi_mt 영문 sts dev 데이터 셋 불러오기\n",
    "if use_en_sts == True:\n",
    "    count = 0\n",
    "    en_sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"dev\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*stsb_multi_mt len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# 영문 GLUE 데이터 셋 설정(load_dataset)\n",
    "####################################################################################################  \n",
    "if use_glue_sts == True:\n",
    "    count = 0\n",
    "    glue_stsb_dataset = load_dataset(\"glue\",\"stsb\", split=\"validation\")\n",
    "    for data in glue_stsb_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        \n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "        dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*glue-stsb len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "logger.info(f'------------------------------------------------------------------------')        \n",
    "logger.info(f'*dev_samples_len:{len(dev_samples)}')\n",
    "print(dev_samples[0:3])\n",
    "\n",
    "# 2개의 bert 모델에서 구한 2개의 embedding 값들의 cosine 유사도를 구해서, 이를 실제 score와 비교해서 유사도 측정함\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 batch_size=train_batch_size, \n",
    "                                                                 name='sts-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea020cec-bd98-4385-a202-86c76adee505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 17:28:41,283 - s-bert-sts - INFO - *IN-model:beomi/kcbert-base\n",
      "2022-10-20 17:28:41,285 - s-bert-sts - INFO - *OUT-model:../../data11/model/sbert/kcbert-base-korsts/\n",
      "2022-10-20 17:28:41,286 - s-bert-sts - INFO - *batch_size:32, epoch:5, lr:5e-05, train_dataset:5749, Warmup-steps: 90, evaluation_step: 179\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac4c3dd7a534d259f68e69356d0aebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1541a7f71c64ff3a33a1e78752de86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2e70e3d5f34841b8498c62f21eb5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2a7a822a244f419693c421f925caad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602608bef56c4068af44f8fd92787539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd94ae3839e498d8e3e2fb4ec82634c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#warmup_step은 10% 로 설정\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) \n",
    "#warmup_steps = 0 # kcbert-config 참조함 \n",
    "\n",
    "# evaluation_steps은 20%로 설정\n",
    "evaluation_steps = int(len(train_dataset) * num_epochs / train_batch_size * 0.2)\n",
    "\n",
    "logger.info(f\"*IN-model:{model_path}\")\n",
    "logger.info(f\"*OUT-model:{smodel_path}\")\n",
    "logger.info(\"*batch_size:{}, epoch:{}, lr:{}, train_dataset:{}, Warmup-steps: {}, evaluation_step: {}\".format(train_batch_size, num_epochs, lr, len(train_dataset), warmup_steps, evaluation_steps))\n",
    "\n",
    "# Train the model\n",
    "# => **learning rate는 기본이 2e-5임\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,\n",
    "          optimizer_params= {'lr': lr, 'eps': eps, 'correct_bias': False},\n",
    "          output_path=smodel_path\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39ce8ee-8b39-4c09-9ce3-85ac036af96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 17:31:36,022 - s-bert-sts - INFO - \n",
      "\n",
      "2022-10-20 17:31:36,024 - s-bert-sts - INFO - ======================TEST===================\n",
      "2022-10-20 17:31:36,025 - s-bert-sts - INFO - \n",
      "\n",
      "\n",
      "2022-10-20 17:31:36,026 - s-bert-sts - INFO - model save path > ../../data11/model/sbert/kcbert-base-korsts/\n",
      "2022-10-20 17:31:37,299 - s-bert-sts - INFO - main_similarity: SimilarityFunction.COSINE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea51eb415cbd4353b2e7fd0e7e9be890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a853fd676ef4fb7bddff444061decab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 17:31:38,845 - s-bert-sts - INFO - \n",
      "\n",
      "2022-10-20 17:31:38,846 - s-bert-sts - INFO - model path: ../../data11/model/sbert/kcbert-base-korsts/\n",
      "2022-10-20 17:31:38,846 - s-bert-sts - INFO - === result: 0.7215849681482819 ===\n",
      "2022-10-20 17:31:38,847 - s-bert-sts - INFO - === 처리시간: 2.821 초 ===\n",
      "2022-10-20 17:31:38,847 - s-bert-sts - INFO - ==============================================\n",
      "2022-10-20 17:31:38,848 - s-bert-sts - INFO - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# => 훈련되어서 저장된 s-bert 모델을 불러와서 성능 평가 해봄\n",
    "##############################################################################\n",
    "import time\n",
    "from sentence_transformers.evaluation import SimilarityFunction\n",
    "\n",
    "# 테스트 파일=KorSTS 테스트파일 경로 지정\n",
    "test_file = '../../data11/korpora/korsts/tune_test.tsv'\n",
    "\n",
    "# 평가시 cosine 유사도등 측정 결과값 파일 (similarity_evaluation_xxxx.xls) 저장될 경로\n",
    "output_path = './sts-test2'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "test_samples = []\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "        test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "\n",
    "logger.info(\"\\n\")\n",
    "logger.info(\"======================TEST===================\")\n",
    "logger.info(\"\\n\\n\")\n",
    "logger.info(f\"model save path > {smodel_path}\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer(smodel_path)\n",
    "\n",
    "# 유사도 측정방식(COSINE, EUCLIDEAN, MANHATTAN, DOT_PRODUCT 중 선택 , 모두 spearman 방식임)\n",
    "# => None 이면 아래 값들중 MAX 값 추력함\n",
    "#main_similarity = None\n",
    "main_similarity = SimilarityFunction.COSINE\n",
    "#main_similarity = SimilarityFunction.EUCLIDEAN\n",
    "#main_similarity = SimilarityFunction.MANHATTAN\n",
    "#main_similarity = SimilarityFunction.DOT_PRODUCT\n",
    "\n",
    "logger.info(f\"main_similarity: {main_similarity}\")\n",
    "\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, main_similarity=main_similarity, batch_size=train_batch_size, name='sts-test', show_progress_bar=True)\n",
    "result = test_evaluator(model, output_path=output_path)\n",
    "\n",
    "logger.info(f\"\\n\")\n",
    "logger.info(f\"model path: {smodel_path}\")\n",
    "logger.info(f'=== result: {result} ===')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(\"==============================================\")\n",
    "logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79dcfa5-7474-4e72-ba49-b77dbdb2ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 model 저장\n",
    "#output_path = \"../../data11/model/sbert/sbert-mdistilbertV3.1-last\"\n",
    "#model.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830509ba-37af-4e92-aea8-f212a93ee771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
