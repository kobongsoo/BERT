{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d2266a-06cc-41fc-8573-51fee4628282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../log/s-bert-sts_2022-11-26.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# sentence-bert STS 데이터셋을 가지고, 훈련 및 평가 예시\n",
    "#\n",
    "# => 기존 (distil)bert 모델을 가지고, STS 훈련 및 평가 후, S-BERT로 만드는 예시임.\n",
    "\n",
    "#=> 필요에 따라 출력 dimension을 768보다 작게 줄이고 싶을때 dense 모델을 추가해서 줄일수 있음\n",
    "#=> reduce_out_dimension = True 로 하면, 출력 임베딩 dimension이 줄어들게 설정가능함\n",
    "\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# **learning rate는 기본이 2e-5임\n",
    "#\n",
    "# 도큐먼트 : https://www.sbert.net/index.html\n",
    "# 소스참고 : https://github.com/BM-K/KoSentenceBERT-ETRI\n",
    "#\n",
    "# pip install -U sentence-transformers\n",
    "#\n",
    "# # ** skt/kobert-base-V1  sbert 만들고 나서는 tokenizer_config.json 에 tokenizer_class:\"KoBERTTokenizer\" 를 tokenizer_class:\"XLNetTokenizer\" 로 변경해야함.\n",
    "#======================================================================================================\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert-sts\", logfilename=\"../../log/s-bert-sts\")\n",
    "device = GPU_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560f4d38-e45e-4c30-87c8-7a16a002a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542/ were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_len:30022\n",
      "Transformer({'max_seq_length': 72, 'do_lower_case': 0}) with Transformer model: BertModel \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ** skt/kobert-base-V1  sbert 만들고 나서는 tokenizer_config.json 에 tokenizer_class:\"KoBERTTokenizer\" 를 tokenizer_class:\"XLNetTokenizer\" 로 변경해야함.\n",
    "bisSKKobertModel = 0  # skt/kobert-base-V1 허깅페이스 모델 사용시에는 1로 해줌\n",
    "# s-bert로 만들 원본 bert 경로\n",
    "model_path = '../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542/'#\"skt/kobert-base-v1\"\n",
    "#model_path = \"bongsoo/mdistilbertV3.1\"\n",
    "\n",
    "# 원본 bert를 sentencebert로 만든후 만들어진 s-bert 저장 경로\n",
    "# => **해당 경로\\eval 폴더에 similarity_evaluation_sts-dev_result.csv 파일로 각 epoch 마다 평가된 결과가 기록된다.\n",
    "#smodel_path = 'output/training_nli_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "smodel_path = '../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542-sts-b32'#'../../data11/model/sbert/mdistilbertV3.1-sts-b32-lower'\n",
    "\n",
    "#=======================================================================================================\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "num_epochs = 5      # 128 정도 해도 최상의 모델을 찾을수 있음 (*sbert는 eval이 최상인 모델이 out모델로 저장됨)\n",
    "#num_epochs = 800\n",
    "max_seq_length = 72\n",
    "lr = 1e-4            # default=2e-5\n",
    "eps = 1e-6           #lr이 0으로 나뉘어져 계산이 엉키는 것을 방지하기 위해 epsilion\n",
    "seed=111\n",
    "\n",
    "do_lower_case_param = 0 # true = 대.소문자 구분없이 모두 소문자로 변환(*한국어는 True해도 상관없음)\n",
    "\n",
    "# sentence_transformers 2.2.2 부터는 'correct_bias' 인자가 없어졌음. => correct_bias : False 하면 sts 성능이 떨어짐(*원인 모름)\n",
    "use_correct_bias = 0\n",
    "\n",
    "if use_correct_bias == 0:\n",
    "    opt_params = {'lr': lr, 'eps': eps}  # defalut\n",
    "else:\n",
    "    opt_params = {'lr': lr, 'eps': eps, 'correct_bias': False}\n",
    "    print(f'**correct_bias:False')\n",
    "#=======================================================================================================\n",
    "\n",
    "use_korsts = 1     # 한국어 korsts 파일 (tsv 5,749개)\n",
    "use_kluests = 1    # 한국어 kluests_v1.1 파일 (json 11,668개)\n",
    "use_sts17 = 1      # 한국어 sts17-crosslingual-sts (jsonl 2,846개)\n",
    "use_glue_sts = 1   # 영어 glue_sts (load_dataset 5,749개)\n",
    "use_en_sts = 1     # 영어 stsb_multi_mt(load_dataset 15,676개) = stsb_multi_mt(5,749개) + mteb/sickr-sts(9,927개)\n",
    "\n",
    "# KorSTS 학습, 평가 파일들\n",
    "train_korsts_file = '../../data11/korpora/korsts/tune_train.tsv'\n",
    "eval_korsts_file = '../../data11/korpora/korsts/tune_dev.tsv'\n",
    "\n",
    "# KlueSTS 학습, 평가 파일들\n",
    "train_kluests_file = '../../data11/korpora/klue-sts/klue-sts-v1.1_train.json'\n",
    "eval_kluests_file = '../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json'\n",
    "\n",
    "# sts17-crosslingual-sts  학습 파일(*평가파일 없음)\n",
    "train_sts17_file = '../../data11/korpora/sts17-crosslingual-sts/ko-ko.jsonl'\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# *출력 dimension을 줄일 경우에는 True로 하고, out_dimension에 줄일 값을 설정함\n",
    "reduce_out_dimension = False  # True이면 dimension을 줄임=>Dense 모델 추가됨\n",
    "out_dimension = 128\n",
    "#============================================================================\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "# 모델과 tokenizer 를 불러옴\n",
    "# => **사전파일(vocab.txt, *.json) 와 model 경로(config.json, pytorch_model.bin)가 같은 경로에 있어야 함.\n",
    "word_embedding_model = models.Transformer(model_path, max_seq_length=max_seq_length, do_lower_case=do_lower_case_param)\n",
    "\n",
    "#========================================================================================================\n",
    "# skt/kobert 모델은 tokenizer을 XLNET Tokenizer 이므로, 자체 KoBERTTOkenizer 를 불러와서 사용해야 함.\n",
    "# => 설치 : !pip install 'git+https://github.com/SKTBrain/KOBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "# => 출처 : https://velog.io/@m0oon0/KoBERT-%EC%82%AC%EC%9A%A9%EB%B2%95\n",
    "if bisSKKobertModel == 1:\n",
    "    from kobert_tokenizer import KoBERTTokenizer\n",
    "    word_embedding_model.tokenizer = KoBERTTokenizer.from_pretrained(model_path)\n",
    "    print(f'load koBertTokenizer:{word_embedding_model.tokenizer}')\n",
    "#========================================================================================================\n",
    "\n",
    "# embedding 길이를 재조정 필요할때 auto_model.resize_token_embeddings 해줌\n",
    "print(f'token_len:{len(word_embedding_model.tokenizer)}')\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n",
    "\n",
    "# word embedding_model 출력 \n",
    "print(word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30816f29-8579-4b62-ab99-3f6436035657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# 2 bert 모델의 임베딩 풀링 정책을 설정(cls 이용, 워드임베딩 평균이용, 워드임베딩 max 이용)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),  #모델이 dimension(768)\n",
    "                               pooling_mode_mean_tokens=True,  # 워드 임베딩 평균을 이용\n",
    "                               pooling_mode_cls_token=False,   # cls 를 이용\n",
    "                               pooling_mode_max_tokens=False)  # 워드 임베딩 값중 max 값을 이용\n",
    "# pooling model 출력 \n",
    "print(pooling_model)\n",
    "print(pooling_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f5a82c1-c5a5-467e-b758-362c7056918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. dense 모델 추가(옵션)\n",
    "#=> 필요에 따라 출력 dimension을 768보다 작게 줄이고 싶을때 dense 모델을 추가해서 줄임.\n",
    "#=> https://www.sbert.net/docs/training/overview.html?highlight=dense 참조\n",
    "if reduce_out_dimension:\n",
    "    dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), # 입력 dimension은 앞에 pooling모델 embedding dimension으로 지정\n",
    "                               out_features=out_dimension,  # 출력 dimension\n",
    "                               activation_function=nn.Tanh())  # activation function은 Tahn으로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6609d844-3e00-42f9-ae0f-c12102b0925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 72, 'do_lower_case': 0}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# SBERT 모델 생성\n",
    "if reduce_out_dimension:\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "else:\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de9a825-5d52-425f-aa0b-7afd2ce336f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:44:59,022 - s-bert-sts - INFO - Read STS train dataset=>../../data11/korpora/korsts/tune_train.tsv\n",
      "2022-11-26 20:44:59,040 - s-bert-sts - INFO - *../../data11/korpora/korsts/tune_train.tsv count: 5749\n",
      "2022-11-26 20:44:59,041 - s-bert-sts - INFO - Read STS train dataset=>../../data11/korpora/klue-sts/klue-sts-v1.1_train.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비행기가 이륙하고 있다., 비행기가 이륙하고 있다., 1.0\n",
      "한 남자가 큰 플루트를 연주하고 있다., 남자가 플루트를 연주하고 있다., 0.76\n",
      "한 남자가 피자에 치즈를 뿌려놓고 있다., 한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다., 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:44:59,329 - s-bert-sts - INFO - *../../data11/korpora/klue-sts/klue-sts-v1.1_train.json len: 11668\n",
      "2022-11-26 20:44:59,335 - s-bert-sts - INFO - Read STS train dataset=>../../data11/korpora/sts17-crosslingual-sts/ko-ko.jsonl\n",
      "2022-11-26 20:44:59,351 - s-bert-sts - INFO - *../../data11/korpora/klue-sts/klue-sts-v1.1_train.json len: 2846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다., 숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다., 0.74\n",
      "위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다., 시민들 스스로 자발적인 예방 노력을 한 것은 아산 뿐만이 아니었다., 0.0\n",
      "회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘., 사람들이 주로 네이버 메일을 쓰는 이유를 알려줘, 0.06\n",
      "안전모를 쓴 한 남자가 춤을 추고 있다., 학회 홍보 메일은 회신 하지마, 1.0\n",
      "아이가 말을 타고 있다., 학회 홍보 메일은 회신 하지마, 0.95\n",
      "남자가 뱀에게 쥐를 먹이고 있다., 학회 홍보 메일은 회신 하지마, 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset stsb_multi_mt (/MOCOMSYS/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n",
      "2022-11-26 20:45:01,221 - s-bert-sts - INFO - *stsb_multi_mt_en len: 5749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A plane is taking off., An air plane is taking off., 1.0\n",
      "A man is playing a large flute., A man is playing a flute., 0.7599999904632568\n",
      "A man is spreading shreded cheese on a pizza., A man is spreading shredded cheese on an uncooked pizza., 0.7599999904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mteb--sickr-sts-1e81327897d49df9\n",
      "Reusing dataset json (/MOCOMSYS/.cache/huggingface/datasets/json/mteb--sickr-sts-1e81327897d49df9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A group of kids is playing in a yard and an old man is standing in the background, A group of boys in a yard is playing and a man is standing in the background, 0.9\n",
      "A group of children is playing in the house and there is no man standing in the background, A group of kids is playing in a yard and an old man is standing in the background, 0.64\n",
      "The young boys are playing outdoors and the man is smiling nearby, The kids are playing outdoors near a man with a smile, 0.9400000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:45:05,452 - s-bert-sts - INFO - *mteb/sickr-sts len: 9927\n",
      "Reusing dataset glue (/MOCOMSYS/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A plane is taking off., An air plane is taking off., 1.0\n",
      "A man is playing a large flute., A man is playing a flute., 0.7599999904632568\n",
      "A man is spreading shreded cheese on a pizza., A man is spreading shredded cheese on an uncooked pizza., 0.7599999904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:45:07,569 - s-bert-sts - INFO - *glue_stsb len: 5749\n",
      "2022-11-26 20:45:07,570 - s-bert-sts - INFO - ------------------------------------------------------------------------\n",
      "2022-11-26 20:45:07,571 - s-bert-sts - INFO - *train_samples_len:41688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<sentence_transformers.readers.InputExample.InputExample object at 0x7f33632ee610>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f3361208be0>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f3361208a90>]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorSTS 훈련 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_korsts == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_korsts_file}\")\n",
    "    with open(train_korsts_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{train_korsts_file} count: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "####################################################################################################\n",
    "# klue 훈련 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"sts\", split=\"train\")\n",
    "# for data in datas:\n",
    "#        text_a = data[\"sentence1\"]\n",
    "#        text_b = data[\"sentence2\"]\n",
    "#        score = data[\"labels\"][\"label\"]\n",
    "#        score = float(score) / 5.0  \n",
    "###################################################################################################           \n",
    "if use_kluests == True:  \n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_kluests_file}\")\n",
    "    with open(train_kluests_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{train_kluests_file} len: {count}')\n",
    "####################################################################################################\n",
    "# 한국어 sts17-crosslingual-sts 훈련 데이터셋 설정\n",
    "# => jsonl : 여러개의 json 형식 파일이 각 줄마다 기록되어 있는 형태 파일\n",
    "# => 패키지 설치 : !pip install jsonlines\n",
    "####################################################################################################\n",
    "if use_sts17 == True:\n",
    "    import jsonlines\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS train dataset=>{train_sts17_file}\")\n",
    "    with jsonlines.open(train_sts17_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            text_a = line[\"sentence1\"]\n",
    "            text_a = line[\"sentence2\"]\n",
    "            score = line[\"score\"]\n",
    "            score = float(score) / 5.0\n",
    "            \n",
    "            if count < 3:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "                \n",
    "            train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "            \n",
    "    logger.info(f'*{train_kluests_file} len: {count}')\n",
    "####################################################################################################\n",
    "\n",
    "#############################################################################################\n",
    "# 영문 sts 데이터셋 설정 (load_dataset)\n",
    "# => stsb_multi_mt , mteb/sickr-sts 영문 sts 훈련 데이터 셋 불러오기\n",
    "#############################################################################################\n",
    "if use_en_sts == True:\n",
    "    count = 0\n",
    "    en_sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"train\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*stsb_multi_mt_en len: {count}')\n",
    "    \n",
    "    # mteb/sickr-sts 훈련데이터 불러옴\n",
    "    count = 0    \n",
    "    en_sts_dataset = load_dataset(\"mteb/sickr-sts\", split=\"test\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*mteb/sickr-sts len: {count}')\n",
    "#############################################################################################           \n",
    " \n",
    "#############################################################################################\n",
    "# GLUE STS 훈련 데이터셋 설정 (load_dataset)\n",
    "#############################################################################################\n",
    "if use_glue_sts == True:\n",
    "    # glue stsb 훈련데이터 불러옴(5,749개)\n",
    "    count = 0    \n",
    "    en_sts_dataset = load_dataset(\"glue\",\"stsb\", split=\"train\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        train_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*glue_stsb len: {count}')\n",
    "#############################################################################################\n",
    "\n",
    "logger.info(f'------------------------------------------------------------------------')        \n",
    "logger.info(f'*train_samples_len:{len(train_samples)}')\n",
    "print(train_samples[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840a2b16-b2f5-4f97-a751-248c9a7bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋, 데이터 로더, 손실함수 정의\n",
    "\n",
    "train_dataset = SentencesDataset(train_samples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5ab735-a87e-4ca9-a3d7-03f8949b392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:45:07,590 - s-bert-sts - INFO - Read STS dev dataset=>../../data11/korpora/korsts/tune_dev.tsv\n",
      "2022-11-26 20:45:07,596 - s-bert-sts - INFO - *../../data11/korpora/korsts/tune_dev.tsv len: 1500\n",
      "2022-11-26 20:45:07,597 - s-bert-sts - INFO - Read STS dev dataset=>../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json\n",
      "2022-11-26 20:45:07,611 - s-bert-sts - INFO - *../../data11/korpora/klue-sts/klue-sts-v1.1_dev.json len: 519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전모를 가진 한 남자가 춤을 추고 있다., 안전모를 쓴 한 남자가 춤을 추고 있다., 1.0\n",
      "어린아이가 말을 타고 있다., 아이가 말을 타고 있다., 0.95\n",
      "한 남자가 뱀에게 쥐를 먹이고 있다., 남자가 뱀에게 쥐를 먹이고 있다., 1.0\n",
      "한 여성이 기타를 연주하고 있다., 한 남자가 기타를 치고 있다., 0.48\n",
      "한 여성이 플루트를 연주하고 있다., 남자가 플루트를 연주하고 있다., 0.55\n",
      "무엇보다도 호스트분들이 너무 친절하셨습니다., 무엇보다도, 호스트들은 매우 친절했습니다., 0.9800000000000001\n",
      "주요 관광지 모두 걸어서 이동가능합니다., 위치는 피렌체 중심가까지 걸어서 이동 가능합니다., 0.27999999999999997\n",
      "학생들의 균형 있는 영어능력을 향상시킬 수 있는 학교 수업을 유도하기 위해 2018학년도 수능부터 도입된 영어 영역 절대평가는 올해도 유지한다., 영어 영역의 경우 학생들이 한글 해석본을 암기하는 문제를 해소하기 위해 2016학년도부터 적용했던 EBS 연계 방식을 올해도 유지한다., 0.26\n",
      "다만, 도로와 인접해서 거리의 소음이 들려요., 하지만, 길과 가깝기 때문에 거리의 소음을 들을 수 있습니다., 0.74\n",
      "형이 다시 캐나다 들어가야 하니 가족모임 일정은 바꾸지 마세요., 가족 모임 일정은 바꾸지 말도록 하십시오., 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset stsb_multi_mt (/MOCOMSYS/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n",
      "2022-11-26 20:45:08,715 - s-bert-sts - INFO - *stsb_multi_mt len: 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man with a hard hat is dancing., A man wearing a hard hat is dancing., 1.0\n",
      "A young child is riding a horse., A child is riding a horse., 0.95\n",
      "A man is feeding a mouse to a snake., The man is feeding a mouse to the snake., 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/MOCOMSYS/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "2022-11-26 20:45:09,829 - s-bert-sts - INFO - *glue-stsb len: 1500\n",
      "2022-11-26 20:45:09,830 - s-bert-sts - INFO - ------------------------------------------------------------------------\n",
      "2022-11-26 20:45:09,831 - s-bert-sts - INFO - *dev_samples_len:5019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man with a hard hat is dancing., A man wearing a hard hat is dancing., 1.0\n",
      "A young child is riding a horse., A child is riding a horse., 0.95\n",
      "A man is feeding a mouse to a snake., The man is feeding a mouse to the snake., 1.0\n",
      "[<sentence_transformers.readers.InputExample.InputExample object at 0x7f33ad53d9d0>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f336b10d460>, <sentence_transformers.readers.InputExample.InputExample object at 0x7f336b10d0a0>]\n"
     ]
    }
   ],
   "source": [
    "#Read STSbenchmark dataset and use it as development set\n",
    "# 평가데이터 불러오기\n",
    "#korsts 파일로 두 문장간 유사도를 수치로(5.0이 만점=매우 유사) 측정함.\n",
    "dev_samples = []\n",
    "\n",
    "####################################################################################################\n",
    "# KorSTS 평가 데이터 셋 설정(.tsv 파일)\n",
    "####################################################################################################\n",
    "if use_korsts == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS dev dataset=>{eval_korsts_file}\")\n",
    "    with open(eval_korsts_file, 'rt', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            text_a, text_b, score = line.split('\\t')\n",
    "            score = score.strip()\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "            \n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{eval_korsts_file} len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# KlueSTS 평가 데이터 셋 설정(.json 파일)\n",
    "# => 아래처럼 load_dataset으로 불러와서 사용할수도 있음.\n",
    "# datas = load_dataset(\"klue\", \"sts\", split=\"test\")\n",
    "# for data in datas:\n",
    "#        text_a = data[\"sentence1\"]\n",
    "#        text_b = data[\"sentence2\"]\n",
    "#        score = data[\"labels\"][\"label\"]\n",
    "#        score = float(score) / 5.0  \n",
    "####################################################################################################           \n",
    "if use_kluests == True:\n",
    "    count = 0\n",
    "    logger.info(f\"Read STS dev dataset=>{eval_kluests_file}\")\n",
    "    with open(eval_kluests_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "        datas = json.load(f)\n",
    "        for data in datas:\n",
    "            text_a = data[\"sentence1\"]\n",
    "            text_b = data[\"sentence2\"]\n",
    "            score = data[\"labels\"][\"label\"]\n",
    "            score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "            if count < 5:\n",
    "                print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "            dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "            count += 1\n",
    "    logger.info(f'*{eval_kluests_file} len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# 영문 stsb_multi_mt 데이터 셋 설정(load_dataset)\n",
    "####################################################################################################                \n",
    "# stsb_multi_mt 영문 sts dev 데이터 셋 불러오기\n",
    "if use_en_sts == True:\n",
    "    count = 0\n",
    "    en_sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"dev\")\n",
    "    for data in en_sts_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"similarity_score\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "\n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "\n",
    "        dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*stsb_multi_mt len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "####################################################################################################\n",
    "# 영문 GLUE 데이터 셋 설정(load_dataset)\n",
    "####################################################################################################  \n",
    "if use_glue_sts == True:\n",
    "    count = 0\n",
    "    glue_stsb_dataset = load_dataset(\"glue\",\"stsb\", split=\"validation\")\n",
    "    for data in glue_stsb_dataset:\n",
    "        text_a = data[\"sentence1\"]\n",
    "        text_b = data[\"sentence2\"]\n",
    "        score = data[\"label\"]\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        \n",
    "        if count < 3:\n",
    "            print(f\"{text_a}, {text_b}, {score}\")\n",
    "            \n",
    "        dev_samples.append(InputExample(texts= [text_a,text_b], label=score))\n",
    "        count += 1\n",
    "    logger.info(f'*glue-stsb len: {count}')\n",
    "####################################################################################################  \n",
    "\n",
    "logger.info(f'------------------------------------------------------------------------')        \n",
    "logger.info(f'*dev_samples_len:{len(dev_samples)}')\n",
    "print(dev_samples[0:3])\n",
    "\n",
    "# 2개의 bert 모델에서 구한 2개의 embedding 값들의 cosine 유사도를 구해서, 이를 실제 score와 비교해서 유사도 측정함\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 batch_size=eval_batch_size, \n",
    "                                                                 name='sts-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea020cec-bd98-4385-a202-86c76adee505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 20:45:09,849 - s-bert-sts - INFO - *IN-model:../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542/\n",
      "2022-11-26 20:45:09,851 - s-bert-sts - INFO - *OUT-model:../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542-sts-b32\n",
      "2022-11-26 20:45:09,853 - s-bert-sts - INFO - *seed:111, train_batch:32, eval_batch:32, epoch:5, lr:0.0001, eps:1e-06, max_seq_length:72, train_dataset:41688, Warmup-steps: 652, evaluation_step: 1302\n",
      "2022-11-26 20:45:09,854 - s-bert-sts - INFO - *do_lower_case:0, use_correct_bias:0\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf380fbdc6248beb23563342aa0771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1220059716764960a66b6ce09bb61246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274777790fcd4d9a85d8daaaec8ea954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3549514ed446d4a59de2ce91c402fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6f0c1fc1a4433ab54192f5ba1fa9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2ad69b51884e059cc906ce7d6841bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#warmup_step은 10% 로 설정\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) \n",
    "#warmup_steps = 0 # kcbert-config 참조함 \n",
    "\n",
    "# evaluation_steps은 20%로 설정\n",
    "evaluation_steps = int(len(train_dataset) * num_epochs / train_batch_size * 0.2)\n",
    "\n",
    "logger.info(f\"*IN-model:{model_path}\")\n",
    "logger.info(f\"*OUT-model:{smodel_path}\")\n",
    "logger.info(\"*seed:{}, train_batch:{}, eval_batch:{}, epoch:{}, lr:{}, eps:{}, max_seq_length:{}, train_dataset:{}, Warmup-steps: {}, evaluation_step: {}\".format(seed, train_batch_size, eval_batch_size, num_epochs, lr, eps, max_seq_length, len(train_dataset), warmup_steps, evaluation_steps))\n",
    "logger.info(f\"*do_lower_case:{do_lower_case_param}, use_correct_bias:{use_correct_bias}\")\n",
    "\n",
    "# Train the model\n",
    "# => **learning rate는 기본이 2e-5임\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,\n",
    "          optimizer_params= opt_params, \n",
    "          save_best_model=True, # **기본 = True : eval 가장 best 모델을 output_Path에 저장함\n",
    "          output_path=smodel_path\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39ce8ee-8b39-4c09-9ce3-85ac036af96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:10:43,915 - s-bert-sts - INFO - \n",
      "\n",
      "2022-11-26 21:10:43,916 - s-bert-sts - INFO - ======================TEST===================\n",
      "2022-11-26 21:10:43,917 - s-bert-sts - INFO - \n",
      "\n",
      "\n",
      "2022-11-26 21:10:43,918 - s-bert-sts - INFO - model save path > ../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542-sts-b32\n",
      "2022-11-26 21:10:45,090 - s-bert-sts - INFO - main_similarity: SimilarityFunction.COSINE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4184767ff274eac894a49582797766e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e7afe5ab894cefb335cb91d5906cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 21:10:47,661 - s-bert-sts - INFO - \n",
      "\n",
      "2022-11-26 21:10:47,662 - s-bert-sts - INFO - model path: ../../data11/model/bert/mbertV3.0-aihub-NSPMLM-checkout/checkpoint-4129542-sts-b32\n",
      "2022-11-26 21:10:47,663 - s-bert-sts - INFO - === result: -0.03318774421177471 ===\n",
      "2022-11-26 21:10:47,663 - s-bert-sts - INFO - === 처리시간: 3.745 초 ===\n",
      "2022-11-26 21:10:47,664 - s-bert-sts - INFO - ==============================================\n",
      "2022-11-26 21:10:47,665 - s-bert-sts - INFO - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# => 훈련되어서 저장된 s-bert 모델을 불러와서 성능 평가 해봄\n",
    "##############################################################################\n",
    "import time\n",
    "from sentence_transformers.evaluation import SimilarityFunction\n",
    "\n",
    "# 테스트 파일=KorSTS 테스트파일 경로 지정\n",
    "test_file = '../../data11/korpora/korsts/tune_test.tsv'\n",
    "\n",
    "# 평가시 cosine 유사도등 측정 결과값 파일 (similarity_evaluation_xxxx.xls) 저장될 경로\n",
    "output_path = '../../log'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "test_samples = []\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "        test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "\n",
    "logger.info(\"\\n\")\n",
    "logger.info(\"======================TEST===================\")\n",
    "logger.info(\"\\n\\n\")\n",
    "logger.info(f\"model save path > {smodel_path}\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer(smodel_path)\n",
    "\n",
    "# 유사도 측정방식(COSINE, EUCLIDEAN, MANHATTAN, DOT_PRODUCT 중 선택 , 모두 spearman 방식임)\n",
    "# => None 이면 아래 값들중 MAX 값 추력함\n",
    "#main_similarity = None\n",
    "main_similarity = SimilarityFunction.COSINE\n",
    "#main_similarity = SimilarityFunction.EUCLIDEAN\n",
    "#main_similarity = SimilarityFunction.MANHATTAN\n",
    "#main_similarity = SimilarityFunction.DOT_PRODUCT\n",
    "\n",
    "logger.info(f\"main_similarity: {main_similarity}\")\n",
    "\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, main_similarity=main_similarity, batch_size=eval_batch_size, name='sts-test', show_progress_bar=True)\n",
    "result = test_evaluator(model, output_path=output_path)\n",
    "\n",
    "logger.info(f\"\\n\")\n",
    "logger.info(f\"model path: {smodel_path}\")\n",
    "logger.info(f'=== result: {result} ===')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(\"==============================================\")\n",
    "logger.info(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79dcfa5-7474-4e72-ba49-b77dbdb2ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 model 저장\n",
    "#output_path = \"../../data11/model/sbert/sbert-mdistilbertV3.1-last\"\n",
    "#model.save(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
