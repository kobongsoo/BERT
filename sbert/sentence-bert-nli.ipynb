{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9bfffa-9708-49ce-815c-2c95531244af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-04-13.log\n",
      "logfilepath:qnadataset_2022-04-13.log\n",
      "logfilepath:../../log/s-bert_2022-04-13.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# by kobongsoo, updata: 2022-04-13\n",
    "#\n",
    "# sentence-bert NLI 훈련 및 평가 예시\n",
    "# => 기존 (distil)bert 모델을 가지고, NLI로 훈련 및 평가 후, S-BERT로 만드는 예시임.\n",
    "#\n",
    "#=> 필요에 따라 출력 dimension을 768보다 작게 줄이고 싶을때 dense 모델을 추가해서 줄일수 있음\n",
    "#=> reduce_out_dimension = True 로 하면, 출력 임베딩 dimension이 줄어들게 설정가능함\n",
    "#\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# 도큐먼트 : https://www.sbert.net/index.html\n",
    "# 소스참고 : https://github.com/BM-K/KoSentenceBERT-ETRI\n",
    "\n",
    "# pip install -U sentence-transformers\n",
    "#======================================================================================================\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert\", logfilename=\"../../log/s-bert\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad0402a-9791-4e56-804d-97e664179203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../model/distilbert/distilbert-0331-TS-nli-0.1-10 were not used when initializing DistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# s-bert로 만들 원본 bert 경로\n",
    "model_name = \"../../model/distilbert/distilbert-0331-TS-nli-0.1-10\"\n",
    "\n",
    "# 원본 bert를 sentencebert로 만든후 만들어진 s-bert 저장 경로\n",
    "#smodel_path = 'output/training_nli_'+model_name.replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "smodel_path = \"../../model/sbert/sbert-distilbert-0331-TS-nli-0.1-10-nli-04-13\"\n",
    "\n",
    "# 평가시 cosine 유사도등 측정 결과값 파일 (similarity_evaluation_xxxx.xls) 저장될 경로\n",
    "output_path = \"../../model/sbert/output\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "train_file = '../../korpora/kornli/snli_1.0_train.ko.tsv'\n",
    "eval_file = '../../korpora/korsts/tune_dev.tsv'\n",
    "test_file = '../../korpora/korsts/tune_test.tsv'\n",
    "\n",
    "train_batch_size = 32\n",
    "num_epochs = 2\n",
    "\n",
    "#============================================================================\n",
    "# *출력 dimension을 줄일 경우에는 True로 하고, out_dimension에 줄일 값을 설정함\n",
    "reduce_out_dimension = True  # True이면 dimension을 줄임=>Dense 모델 추가됨\n",
    "out_dimension = 128\n",
    "#============================================================================\n",
    "\n",
    "# 모델과 tokenizer 를 불러옴\n",
    "# => **사전파일(vocab.txt, *.json) 와 model 경로(config.json, pytorch_model.bin)가 같은 경로에 있어야 함.\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=128)\n",
    "\n",
    "# word embedding_model 출력 \n",
    "print(word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d62b1e-7da7-4f63-b969-44c0f3812aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# 2 bert 모델의 임베딩 풀링 정책을 설정(cls 이용, 워드임베딩 평균이용, 워드임베딩 max 이용)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),  #모델이 dimension(768)\n",
    "                               pooling_mode_mean_tokens=True,  # 워드 임베딩 평균을 이용\n",
    "                               pooling_mode_cls_token=False,   # cls 를 이용\n",
    "                               pooling_mode_max_tokens=False)  # 워드 임베딩 값중 max 값을 이용\n",
    "# pooling model 출력 \n",
    "print(pooling_model)\n",
    "print(pooling_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344136f0-7274-4a54-91f6-528e3662480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. dense 모델 추가(옵션)\n",
    "#=> 필요에 따라 출력 dimension을 768보다 작게 줄이고 싶을때 dense 모델을 추가해서 줄임.\n",
    "#=> https://www.sbert.net/docs/training/overview.html?highlight=dense 참조\n",
    "if reduce_out_dimension:\n",
    "    dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), # 입력 dimension은 앞에 pooling모델 embedding dimension으로 지정\n",
    "                               out_features=out_dimension,  # 출력 dimension\n",
    "                               activation_function=nn.Tanh())  # activation function은 Tahn으로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4b4be6-0fcc-46ab-9542-3bf81abed98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Dense({'in_features': 768, 'out_features': 128, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# SBERT 모델 생성\n",
    "if reduce_out_dimension:\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "else:\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6deb2a-3609-4358-b665-ca971a55841e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 13:33:15,869 - s-bert - INFO - Read AllNLI train dataset\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 불러오기\n",
    "# => [sentence1, sentence2], labels 식으로 만듬\n",
    "logger.info(\"Read AllNLI train dataset\")\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_samples = []\n",
    "\n",
    "with open(train_file, \"rt\", encoding=\"utf-8\") as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, label = line.split('\\t')\n",
    "        label = label2int[label.strip()]\n",
    "        train_samples.append(InputExample(texts=[s1, s2], label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80277681-2a65-4096-b601-c1e6763a98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋, 데이터 로더, 손실함수 정의\n",
    "train_dataset = SentencesDataset(train_samples, model=model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, \n",
    "                                sentence_embedding_dimension=model.get_sentence_embedding_dimension(), \n",
    "                                num_labels=len(label2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c1829e-edb9-4bb7-b150-d478c8c854c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 13:33:18,463 - s-bert - INFO - Read STSbenchmark dev dataset\n"
     ]
    }
   ],
   "source": [
    "#Read STSbenchmark dataset and use it as development set\n",
    "# 평가데이터 불러오기\n",
    "#korsts 파일로 두 문장간 유사도를 수치로(5.0이 만점=매우 유사) 측정함.\n",
    "logger.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "\n",
    "with open(eval_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        dev_samples.append(InputExample(texts= [s1,s2], label=score))\n",
    "\n",
    "# 2개의 bert 모델에서 구한 2개의 embedding 값들의 cosine 유사도를 구해서, 이를 실제 score와 비교해서 유사도 측정함\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 batch_size=train_batch_size, \n",
    "                                                                 name='sts-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eaf5a2e-df6a-424b-90fc-1b55ed09d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 13:33:18,503 - s-bert - INFO - Warmup-steps: 3439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53680f75eaa94974802cee2f27be2391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bbf16c3e0344c1ba8b9bf2a02ac7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/17193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73e20c28985427896c602cb595a483f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/17193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1) #10% of train data for warm-up\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=dev_evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=smodel_path\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bb6494-f8ee-4e73-97cf-e27f8936c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:28:33,224 - s-bert - INFO - \n",
      "\n",
      "2022-04-13 14:28:33,225 - s-bert - INFO - ======================TEST===================\n",
      "2022-04-13 14:28:33,225 - s-bert - INFO - \n",
      "\n",
      "\n",
      "2022-04-13 14:28:33,226 - s-bert - INFO - model save path > ../../model/sbert/sbert-distilbert-0331-TS-nli-0.1-10-nli-04-13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be58ae51864448578e7f86af7093275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16904195c04f4c66b74bb258d05623de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:28:36,026 - s-bert - INFO - 처리시간 > 2.7993\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "# => 훈련되어서 저장된 s-bert 모델을 불러와서 성능 평가 해봄\n",
    "##############################################################################\n",
    "import time\n",
    "\n",
    "test_samples = []\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "        test_samples.append(InputExample(texts=[s1,s2], label=score))\n",
    "\n",
    "logger.info(\"\\n\")\n",
    "logger.info(\"======================TEST===================\")\n",
    "logger.info(\"\\n\\n\")\n",
    "logger.info(f\"model save path > {smodel_path}\")\n",
    "start = time.time()\n",
    "model = SentenceTransformer(smodel_path)\n",
    "\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='sts-test', show_progress_bar=True)\n",
    "test_evaluator(model, output_path=output_path)\n",
    "logger.info(f\"처리시간 > {time.time() - start:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529dd225-d60f-426b-960a-8bb7df22f25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
