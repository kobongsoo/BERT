{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5e3edf-a94e-472d-a7b3-5f8c414b1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-04-01.log\n",
      "logfilepath:qnadataset_2022-04-01.log\n",
      "logfilepath:s-bert-ts_2022-04-01.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# sentence-bert 를 tearch-student 관계 모델로 구성하여, 영어 sbert 학습을 학국어 모델에 증류학습시키는 예시\n",
    "# -> 선생님모델은  영어 bert가 되고, 학생모델은 학국어 포함된다국어 bert로 설정\n",
    "# -> 영어 bert가 다국어 bert를 가리키는 방식으로 학습됨\n",
    "#\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# [참고 소스] \n",
    "# https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/multilingual/make_multilingual_sys.py\n",
    "#\n",
    "# pip install -U sentence-transformers\n",
    "#======================================================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert-ts\", logfilename=\"s-bert-ts\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bfcf70-f795-46e8-8833-6e43b1408b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load teacher model\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 선생님 모델 설정\n",
    "\n",
    "print(\"Load teacher model\")\n",
    "#teacher_model_name = 'bert-base-nli-stsb-mean-tokens'\n",
    "teacher_model_name = \"../model/sbert/distiluse-base-multilingual-cased-v2-not-Dense\"\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8b428c-7028-44e7-a986-acec80f86359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load student model\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#==========================================================================================================\n",
    "# 학생 모델 설정\n",
    "# => * 학생모델이 이미 sentencebert일지라도, 아래처럼 sbert모델 아닌 것처럼 word_embedding_model, pooling_model 을 각각\n",
    "#    만들어서 처리하는것이 테스트 시 효율의 좋음\n",
    "#\n",
    "# [학생 모델 생성 방법]\n",
    "# 1) word_embedding 모델 생성\n",
    "# 2) pooling 모델 생성 : pooling 정책을 설정함 : CLS, 평균, MAX 정책중 택1(*평균 정책이 효율의 가장 좋다고 함)\n",
    "# 3) 1) + 2) 모델을 연결시켜서 하나의 sbert 모델 만듬\n",
    "#==========================================================================================================\n",
    "student_model_name = \"../model/sbert/sbert-ts2022-04-01-distiluse-6\"\n",
    "\n",
    "print(\"Load student model\")\n",
    "\n",
    "\n",
    "# === *sbert 모델 아닌 경우 =====\n",
    "# word embedding 모델 설정(기존 다국어 모델 불러옴)\n",
    "word_embedding_model = models.Transformer(student_model_name)\n",
    "\n",
    "# pooling 정책 설정(mean 평균 정책으로 지정)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "# 학생 SBERT 생성\n",
    "# -> word_embedding model 과 pooling_model를 연결시켜줌\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "# === *sbert 모델 인 경우 =====\n",
    "# 기존 s-model 로딩 함\n",
    "#student_model = SentenceTransformer(student_model_name)\n",
    "\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8f7e8b-e46e-410a-b991-bcee2848b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 평가 데이터 불러오고, 손실함수(MSELoss) 설정함(*학생모델에 설정함)\n",
    "# 원본 소스코드 : \n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/datasets/ParallelSentencesDataset.py\n",
    "\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "\n",
    "max_seq_length = 128\n",
    "train_batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "###### Load train sets ######    \n",
    "#train_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-train.tsv'\n",
    "train_file = '../korpora/pair/TED2020-en-ko/TED2020-en-ko-train.tsv'\n",
    "\n",
    "train_reader = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model)\n",
    "train_reader.load_data(train_file)\n",
    "train_dataloader = DataLoader(train_reader, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbcac7e-cc7d-496f-b16c-f3c80fca6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589814\n",
      "('There are many differences among the arts, but there are also universal, cross-cultural aesthetic pleasures and values.', {'예술들엔 많은 차이점이 있지만, 보편적이고 문화를 뛰어넘는 미적 기쁨과 가치가 있죠.', 'There are many differences among the arts, but there are also universal, cross-cultural aesthetic pleasures and values.'})\n"
     ]
    }
   ],
   "source": [
    "print(len(train_reader))\n",
    "train_reader.__getitem__(0)\n",
    "print(train_reader.next_entry(0)) # (source, {traget}) 첫번째 문장을 출력해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b56c40a-a560-4302-876c-272894ef2aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n###### Load dev sets ######\\n# 평가 데이터 불러와서 유사도 측정 평가자 설정함\\n#=>stst 파일 있는 경우에만 지정해줌.\\n\\nfrom torch.utils.data import DataLoader\\nfrom sentence_transformers import SentencesDataset, losses,readers\\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator\\n\\nevaluators = []\\ndev_samples = []\\n\\neval_file = \\'../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-dev-1.tsv\\'\\nwith open(eval_file, \\'rt\\', encoding=\\'utf-8\\') as fIn:\\n    lines = fIn.readlines()\\n    for line in lines:\\n        s1, s2, score = line.split(\\'\\t\\')\\n        if s1[0] == \"\" or s1[1] == \"\":\\n            continue\\n        score = score.strip()\\n        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\\n        dev_samples.append(InputExample(texts= [s1,s2], label=score))\\n\\n# 영어 문장, 한국어 문장 유사도 측정을 위한 평가자(Evaluator) 설정\\nevaluator_sts = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \\n                                                                 batch_size=train_batch_size, \\n                                                                 name=\\'dev\\')\\n# evaluators에 추가함(*아래 테스트 데이터 evaluators도 추가함)\\nevaluators.append(evaluator_sts)\\nprint(len(evaluators))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "###### Load dev sets ######\n",
    "# 평가 데이터 불러와서 유사도 측정 평가자 설정함\n",
    "#=>stst 파일 있는 경우에만 지정해줌.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentencesDataset, losses,readers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator\n",
    "\n",
    "evaluators = []\n",
    "dev_samples = []\n",
    "\n",
    "eval_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-dev-1.tsv'\n",
    "with open(eval_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        if s1[0] == \"\" or s1[1] == \"\":\n",
    "            continue\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        dev_samples.append(InputExample(texts= [s1,s2], label=score))\n",
    "\n",
    "# 영어 문장, 한국어 문장 유사도 측정을 위한 평가자(Evaluator) 설정\n",
    "evaluator_sts = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 batch_size=train_batch_size, \n",
    "                                                                 name='dev')\n",
    "# evaluators에 추가함(*아래 테스트 데이터 evaluators도 추가함)\n",
    "evaluators.append(evaluator_sts)\n",
    "print(len(evaluators))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55961955-a3c4-4224-95ae-a42c4a44d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Load test sets ######\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentencesDataset, losses,readers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator, TranslationEvaluator\n",
    "evaluators = []\n",
    "# 테스트 데이터 불러와서 MSE 평가자 설정함\n",
    "src_sentences = []\n",
    "trg_sentences = []\n",
    "\n",
    "#test_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-test.tsv'\n",
    "test_file = '../korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv'\n",
    "\n",
    "# 참고소스: https://texasvaluesaction.org/Foysal87/Bangla-sentence-embedding-transformer/blob/master/Bangla_transformer.py\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    for line in fIn:\n",
    "        splits = line.strip().split('\\t')\n",
    "        if len(splits) != 2:\n",
    "            continue\n",
    "\n",
    "        if splits[0] != \"\" and splits[1] != \"\":\n",
    "            src_sentences.append(splits[0])\n",
    "            trg_sentences.append(splits[1])\n",
    "        \n",
    "test_mse = MSEEvaluator(src_sentences, trg_sentences, teacher_model=teacher_model, name='test')\n",
    "evaluators.append(test_mse)\n",
    "\n",
    "# TranslationEvaluator computes the embeddings for all parallel sentences. \n",
    "# It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "test_acc = TranslationEvaluator(src_sentences, trg_sentences, batch_size=train_batch_size)\n",
    "evaluators.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfddc50-571a-4fc8-aa9c-4a1bce2b79c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 15:17:17,081 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "2022-04-01 15:17:17,082 - s-bert-ts - INFO - *Warmup-steps:9216, ephocs:5, train_data_len:589814, train_batch_size: 32\n",
      "2022-04-01 15:17:17,082 - s-bert-ts - INFO - *teacher_model: ../model/sbert/distiluse-base-multilingual-cased-v2-not-Dense\n",
      "2022-04-01 15:17:17,083 - s-bert-ts - INFO - *student_model_name: ../model/sbert/sbert-ts2022-04-01-distiluse-6\n",
      "2022-04-01 15:17:17,083 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "2022-04-01 15:17:17,083 - s-bert-ts - INFO - *train_file: ../korpora/pair/TED2020-en-ko/TED2020-en-ko-train.tsv\n",
      "2022-04-01 15:17:17,084 - s-bert-ts - INFO - *test_file: ../korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv\n",
      "2022-04-01 15:17:17,084 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "2022-04-01 15:17:17,084 - s-bert-ts - INFO - *out_path: ../model/sbert/sbert-ts2022-04-01\n",
      "2022-04-01 15:17:17,085 - s-bert-ts - INFO - ----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25ed46d67094b358e672c7bda8c96b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf457dbf4a14ccb99bdf28fb27a3dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/18432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:537: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  labels = torch.tensor(labels).to(self._target_device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471514f227f4d34bcace3cdabb33ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/18432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2890825e22438493b64334b1eaa6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/18432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ea5929c4c84847a709e6b5e1787ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/18432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3173f6b7678d4eda81cee75942586aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/18432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 16:43:10,757 - s-bert-ts - INFO - === 처리시간: 5153.671 초 ===\n",
      "2022-04-01 16:43:10,758 - s-bert-ts - INFO - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Train model ######\n",
    "# 훈련 시작\n",
    "# 훈련을 시작하면, output_path/eval/ 폴더에 mse 테스트, similarity 테스트 csv 파일에 기록됨\n",
    "# (mse_evaluation_test_results.csv , similarity_evaluation_dev_results.csv)\n",
    "import time\n",
    "\n",
    "#10% of train data for warm-up\n",
    "warmup_steps = math.ceil(len(train_reader) * num_epochs / train_batch_size * 0.1) \n",
    "evaluation_steps = warmup_steps\n",
    "output_path = \"../model/sbert/sbert-ts\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*Warmup-steps:{}, ephocs:{}, train_data_len:{}, train_batch_size: {}\".format(warmup_steps, num_epochs, len(train_reader), train_batch_size))\n",
    "logger.info(\"*teacher_model: {}\".format(teacher_model_name))\n",
    "logger.info(\"*student_model_name: {}\".format(student_model_name))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*train_file: {}\".format(train_file))\n",
    "#logger.info(\"*eval_file: {}\".format(eval_file))\n",
    "logger.info(\"*test_file: {}\".format(test_file))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*out_path: {}\".format(output_path))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1]),\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,   # 옵티마이저 2e-5까지 처음 1000 번은 아주작게 스탭을 옮김\n",
    "          scheduler='warmupconstant',\n",
    "          output_path=output_path,\n",
    "          save_best_model=True,\n",
    "          optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "          )\n",
    "\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61b8a8-3cb8-41ac-bac7-ec1592e14c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
