{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5e3edf-a94e-472d-a7b3-5f8c414b1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/huggingface_hub/snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:s-bert-ts_2023-01-08.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#======================================================================================================\n",
    "# sentence-bert 를 tearch-student 관계 모델로 구성하여, 영어 sbert 학습을 학국어 모델에 증류학습시키는 예시\n",
    "# -> 선생님모델은  영어 bert가 되고, 학생모델은 학국어 포함된다국어 bert로 설정\n",
    "# -> 영어 bert가 다국어 bert를 가리키는 방식으로 학습됨\n",
    "#\n",
    "# => sentence-transformers 패키지를 이용하여 구현 함.(*pip install -U sentence-transformers 설치 필요)\n",
    "#\n",
    "# => 여기서는 교사모델을 sentence-transformers/paraphrase-multilingual-mpnet-base-v2 혹은 sentence-transformers/all-mpnet-base-v2 로, \n",
    "# 학생모델은 distilbert-base-multilingual 로 하여 학습시캄.\n",
    "# => distiluse-base-multilingual-cased-v2 는 Teacher: mUSE; Student: distilbert-base-multilingual 로 학습시킨 s-bert 모델임\n",
    "#     (https://www.sbert.net/docs/pretrained_models.html 참조)\n",
    "#\n",
    "# => ** 중요한 것은 교사와 학생모델간 word_embedding dimension은 서로 일치해야 함.(일치하지않으면 훈련시 아래와 같은 에러 발생함)\n",
    "#     에러 : The size of tensor a (768) must match the size of tensor b (384) at non-singleton dimension 1\n",
    "#\n",
    "# [참고 소스] \n",
    "# https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/multilingual/make_multilingual_sys.py\n",
    "#\n",
    "# pip install -U sentence-transformers\n",
    "#======================================================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import SimilarityFunction, EmbeddingSimilarityEvaluator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "logger = mlogging(loggername=\"s-bert-ts\", logfilename=\"s-bert-ts\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)\n",
    "\n",
    "#==========================================================\n",
    "# hyper parameter\n",
    "max_seq_length = 128\n",
    "train_batch_size = 128\n",
    "eval_batch_size = 64\n",
    "test_batch_size = 64\n",
    "num_epochs = 10    # 40번 했는데..굳이 10번만 해보자\n",
    "lr = 5e-5\n",
    "eps = 1e-8\n",
    "\n",
    "# 임베딩 벡터 폴링 모드 선택 (*아래값중 문자열로 입력함, 기본=mean)\n",
    "# mean=단어 평균, max=최대값, cls=문장, \n",
    "#['mean', 'max', 'cls', 'weightedmean', 'lasttoken']\n",
    "pooling_mode = 'cls'\n",
    "\n",
    "# 평가 유사도 측정방식(COSINE, EUCLIDEAN, MANHATTAN, DOT_PRODUCT 중 선택 , 모두 spearman 방식임)\n",
    "# => None 이면 아래 값들중 MAX 값 추력함\n",
    "#main_similarity = None\n",
    "main_similarity = SimilarityFunction.COSINE\n",
    "#main_similarity = SimilarityFunction.EUCLIDEAN\n",
    "#main_similarity = SimilarityFunction.MANHATTAN\n",
    "#main_similarity = SimilarityFunction.DOT_PRODUCT\n",
    "#======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bfcf70-f795-46e8-8833-6e43b1408b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load teacher model\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 선생님 모델 설정\n",
    "print(\"Load teacher model\")\n",
    "\n",
    "# 아래 2모델중 하나를 영-한 말뭉치에대해 증류 훈련 시킴\n",
    "# - sentence-transformers/all-mpnet-base-v2  # 영문 SBERT 모델 중에서 가장 성능 좋음\n",
    "# - sentence-transformers/paraphrase-multilingual-mpnet-base-v2 # 다국어 SBERT 모델중에서 가장 성능 좋음\n",
    "\n",
    "teacher_model_name = \"../../data11/model/sbert/teacher/paraphrase-multilingual-mpnet-base-v2\"\n",
    "#teacher_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8b428c-7028-44e7-a986-acec80f86359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load student model\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: AlbertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#==========================================================================================================\n",
    "# 학생 모델 설정\n",
    "# => * 학생모델이 이미 sentencebert일지라도, 아래처럼 sbert모델 아닌 것처럼 word_embedding_model, pooling_model 을 각각\n",
    "#    만들어서 처리하는것이 테스트 시 효율의 좋음\n",
    "#\n",
    "# [학생 모델 생성 방법]\n",
    "# 1) word_embedding 모델 생성\n",
    "# 2) pooling 모델 생성 : pooling 정책을 설정함 : CLS, 평균, MAX 정책중 택1(*평균 정책이 효율의 가장 좋다고 함)\n",
    "# 3) 1) + 2) 모델을 연결시켜서 하나의 sbert 모델 만듬\n",
    "#==========================================================================================================\n",
    "student_model_name = \"../../data11/model/moco/sbert-albert-small-sts\"\n",
    "print(\"Load student model\")\n",
    "\n",
    "\n",
    "# === *sbert 모델 아닌 경우 =====\n",
    "# word embedding 모델 설정(기존 다국어 모델 불러옴)\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "\n",
    "# pooling 정책 설정(mean 평균 정책으로 지정)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode=pooling_mode)\n",
    "\n",
    "# 학생 SBERT 생성\n",
    "# -> word_embedding model 과 pooling_model를 연결시켜줌\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "# === *sbert 모델 인 경우 =====\n",
    "# 기존 s-model 로딩 함\n",
    "#student_model = SentenceTransformer(student_model_name)\n",
    "\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8f7e8b-e46e-410a-b991-bcee2848b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 평가 데이터 불러오고, 손실함수(MSELoss) 설정함(*학생모델에 설정함)\n",
    "# 원본 소스코드 : \n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/datasets/ParallelSentencesDataset.py\n",
    "\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "\n",
    "###### Load train sets ######    \n",
    "#train_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-train.tsv'\n",
    "#train_file = '../../data11/korpora/pair/en-ko/en_ko_train.tsv'   # 영어-한국어 사회-과학 병렬 말뭉치 : 1.1M\n",
    "train_file = '../../data11/korpora/pair/en-ko/news_talk_en_ko_train.tsv' #영어-한국어 대화-뉴스 병렬 말뭉치 : 1.38M\n",
    "\n",
    "train_reader = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model)\n",
    "train_reader.load_data(train_file)\n",
    "train_dataloader = DataLoader(train_reader, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbcac7e-cc7d-496f-b16c-f3c80fca6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387586\n",
      "(\"Actress Im Soo-hyang's heartfelt confession touched viewers' hearts.\", {\"Actress Im Soo-hyang's heartfelt confession touched viewers' hearts.\", '배우 임수향의 진심 어린 고백이 시청자들의 마음을 심쿵하게 했다.'})\n"
     ]
    }
   ],
   "source": [
    "print(len(train_reader))\n",
    "train_reader.__getitem__(0)\n",
    "print(train_reader.next_entry(0)) # (source, {traget}) 첫번째 문장을 출력해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b56c40a-a560-4302-876c-272894ef2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###### Load dev sets ######\n",
    "# 평가 데이터 불러와서 유사도 측정 평가자 설정함\n",
    "#=>stst 파일 있는 경우에만 지정해줌.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentencesDataset, losses,readers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator\n",
    "\n",
    "evaluators = []\n",
    "dev_samples = []\n",
    "\n",
    "eval_file = '../../data11/korpora/korsts/tune_test.tsv'\n",
    "with open(eval_file, 'rt', encoding='utf-8') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        if s1[0] == \"\" or s1[1] == \"\":\n",
    "            continue\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0  #5로 나눠서 0~1 사이가 되도록 함\n",
    "        dev_samples.append(InputExample(texts= [s1,s2], label=score))\n",
    "\n",
    "# 영어 문장, 한국어 문장 유사도 측정을 위한 평가자(Evaluator) 설정\n",
    "evaluator_sts = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, \n",
    "                                                                 main_similarity=main_similarity,\n",
    "                                                                 batch_size=eval_batch_size, \n",
    "                                                                 name='dev')\n",
    "# evaluators에 추가함(*아래 테스트 데이터 evaluators도 추가함)\n",
    "evaluators.append(evaluator_sts)\n",
    "print(len(dev_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55961955-a3c4-4224-95ae-a42c4a44d706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "###### Load test sets ######\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentencesDataset, losses,readers\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,MSEEvaluator, SequentialEvaluator, TranslationEvaluator\n",
    "evaluators = []\n",
    "# 테스트 데이터 불러와서 MSE 평가자 설정함\n",
    "src_sentences = []\n",
    "trg_sentences = []\n",
    "\n",
    "#test_file = '../korpora/pair/Tatoeba-eng-kor/Tatoeba-eng-kor-test.tsv'\n",
    "test_file = '../../data11/korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv'\n",
    "\n",
    "# 참고소스: https://texasvaluesaction.org/Foysal87/Bangla-sentence-embedding-transformer/blob/master/Bangla_transformer.py\n",
    "with open(test_file, 'rt', encoding='utf-8') as fIn:\n",
    "    for line in fIn:\n",
    "        splits = line.strip().split('\\t')\n",
    "        if len(splits) != 2:\n",
    "            continue\n",
    "\n",
    "        if splits[0] != \"\" and splits[1] != \"\":\n",
    "            src_sentences.append(splits[0])\n",
    "            trg_sentences.append(splits[1])\n",
    "        \n",
    "test_mse = MSEEvaluator(src_sentences, trg_sentences, teacher_model=teacher_model, name='test')\n",
    "evaluators.append(test_mse)\n",
    "\n",
    "# TranslationEvaluator computes the embeddings for all parallel sentences. \n",
    "# It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "test_acc = TranslationEvaluator(src_sentences, trg_sentences, batch_size=test_batch_size)\n",
    "evaluators.append(test_acc)\n",
    "print(len(src_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfddc50-571a-4fc8-aa9c-4a1bce2b79c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 18:34:14,569 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "2023-01-08 18:34:14,570 - s-bert-ts - INFO - *lr:5e-05, eps:1e-08, Warmup-steps:10841, checkpoint_save_steps:10841, ephocs:10, train_data_len:1387586, train_batch: 128, eval_batch: 64, test_batch: 64\n",
      "2023-01-08 18:34:14,570 - s-bert-ts - INFO - *teacher_model: ../../data11/model/sbert/teacher/paraphrase-multilingual-mpnet-base-v2\n",
      "2023-01-08 18:34:14,571 - s-bert-ts - INFO - *student_model_name: ../../data11/model/moco/sbert-albert-small-sts\n",
      "2023-01-08 18:34:14,571 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "2023-01-08 18:34:14,572 - s-bert-ts - INFO - *train_file: ../../data11/korpora/pair/en-ko/news_talk_en_ko_train.tsv\n",
      "2023-01-08 18:34:14,572 - s-bert-ts - INFO - *test_file: ../../data11/korpora/pair/TED2020-en-ko/TED2020-en-ko-dev.tsv\n",
      "2023-01-08 18:34:14,573 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "2023-01-08 18:34:14,573 - s-bert-ts - INFO - *out_path: ../../data11/model/moco/sbert-albert-small-sts-distil\n",
      "2023-01-08 18:34:14,574 - s-bert-ts - INFO - *check_path: ../../data11/model/moco/sbert-albert-small-sts-distil-2023-01-08\n",
      "2023-01-08 18:34:14,574 - s-bert-ts - INFO - ----------------------------------------------------------------------\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069826d607f647499f8e8c8ff25aa1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107f4da8e37849e6840004c21e8b316c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:537: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  labels = torch.tensor(labels).to(self._target_device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c5d470e9984f00826f2e98f4c6ffd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118fd434002a431d881ec718be61bc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c22694e485440be9e698f2dd93046d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c6661e05fa4ce983314fc4045cbae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddbcaa78984461da8e31cf5618551be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50f3410b518494fb4e599184a4e745d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e71aa6f999c47b1b42fa593aeca8340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9232cd8a72684515aa7265c048280047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b4007b692943b380d978079a26f5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/10841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-08 22:47:34,114 - s-bert-ts - INFO - === 처리시간: 15199.540 초 ===\n",
      "2023-01-08 22:47:34,116 - s-bert-ts - INFO - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Train model ######\n",
    "# 훈련 시작\n",
    "# 훈련을 시작하면, output_path/eval/ 폴더에 mse 테스트, similarity 테스트 csv 파일에 기록됨\n",
    "# (mse_evaluation_test_results.csv , similarity_evaluation_dev_results.csv)\n",
    "import time\n",
    "\n",
    "#10% of train data for warm-up\n",
    "warmup_steps = math.ceil(len(train_reader) * num_epochs / train_batch_size * 0.1) \n",
    "evaluation_steps = warmup_steps\n",
    "checkpoint_save_steps = warmup_steps\n",
    "\n",
    "output_path = \"../../data11/model/moco/sbert-albert-small-sts-distil\"\n",
    "check_path = \"../../data11/model/moco/sbert-albert-small-sts-distil-\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*lr:{}, eps:{}, Warmup-steps:{}, checkpoint_save_steps:{}, ephocs:{}, train_data_len:{}, train_batch: {}, eval_batch: {}, test_batch: {}\".format(lr, eps, warmup_steps, checkpoint_save_steps, num_epochs, len(train_reader), train_batch_size, eval_batch_size, test_batch_size))\n",
    "logger.info(\"*teacher_model: {}\".format(teacher_model_name))\n",
    "logger.info(\"*student_model_name: {}\".format(student_model_name))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*train_file: {}\".format(train_file))\n",
    "#logger.info(\"*eval_file: {}\".format(eval_file))\n",
    "logger.info(\"*test_file: {}\".format(test_file))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "logger.info(\"*out_path: {}\".format(output_path))\n",
    "logger.info(\"*check_path: {}\".format(check_path))\n",
    "logger.info(f\"----------------------------------------------------------------------\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1]),\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "          warmup_steps=warmup_steps,   # 처음 10%는 아주작게 스탭을 옮김\n",
    "          scheduler='warmupconstant',\n",
    "          output_path=output_path,\n",
    "          save_best_model=True, # **기본 = True : eval 가장 best 모델을 output_Path에 저장함\n",
    "          optimizer_params= {'lr': lr, 'eps': eps, 'correct_bias': False},\n",
    "          checkpoint_path=check_path,\n",
    "          checkpoint_save_steps=checkpoint_save_steps,\n",
    "          checkpoint_save_total_limit=5 \n",
    "          )\n",
    "\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb61b8a8-3cb8-41ac-bac7-ec1592e14c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 model 저장\n",
    "#output_path = \"../../data11/model/sbert/sbert-mdistilbertV2.1-distil-\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#student_model.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fbf25-f922-4f67-b486-1460db8f137a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
