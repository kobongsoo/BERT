{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984a5d9-f4c8-41d2-8b1e-faf3ac726899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================================\n",
    "# Huggingface와 ONNX 런타임을 이용한 동적 양자화 예시\n",
    "# => 훈련된 sentencebert 모델을 가지고, ONNX 모데로 만든 후, 테스트 하는 예시\n",
    "# => 문장임베딩이므로, ORTModelForFeatureExtraction 이용\n",
    "#\n",
    "#\n",
    "## 참고 : https://huggingface.co/docs/optimum/index\n",
    "##        https://github.com/huggingface/optimum\n",
    "##        https://huggingface.co/blog/optimum-inference\n",
    "##\n",
    "## 아래 패키지들을 설치해야 함\n",
    "'''\n",
    "!pip install datasets\n",
    "!pip install optimum\n",
    "!pip install optimum[onnxruntime]\n",
    "!pip install optimum[onnxruntime-gpu]  #gpu 사용인 경우\n",
    "\n",
    "'''    \n",
    "#===============================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "logger = mlogging(loggername=\"distilbertnlitest\", logfilename=\"distilbertnlitest\")\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d29422-e398-4372-b67d-2b049f1cf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 기존 bert 모델을 동적 양자화 시킴\n",
    "#============================================================\n",
    "\n",
    "from optimum.onnxruntime import ORTConfig, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# 기존 bert 모델 경로 \n",
    "model_checkpoint = \"./sentencebert_v1.0\"\n",
    "\n",
    "# 동적 양자화인 경우 is_static = False로 해야 함.\n",
    "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
    "\n",
    "# 분류 모델인 경우에는 feature=\"sequence-classification\"\n",
    "quantizer = ORTQuantizer.from_pretrained(model_checkpoint, feature=\"default\")\n",
    "\n",
    "# ONNX 모델로 만들고 양자화 함\n",
    "quantizer.export(\n",
    "    onnx_model_path=\"Smodel.onnx\",   # ONNX 모델 출력 경로\n",
    "    # onnx 양자화 모델이 생성되는 경로(이름은 model.onnx로 해야함=>그래야 huggingface 함수 이용시 경로만 지정해도 자동으로 불어옴)\n",
    "    onnx_quantized_model_output_path=\"./sentencebert_v1.0/model.onnx\",  \n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a554d-0514-47a1-9995-4770c2b62c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx 모델 구조 로딩 해봄.\n",
    "# => 맨 뒤에 return %last_hidden_state 리턴되면 => ORTModelForFeatureExtraction 사용 가능\n",
    "import onnx\n",
    "model = onnx.load(\"./sentencebert_v1.0/model.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4428b-23b2-456d-a63b-e499410103ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# ONNX 양자화 모델 불러옴\n",
    "# => 문장임베딩이므로, ORTModelForFeatureExtraction 로 로딩함\n",
    "#============================================================\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTModelForSequenceClassification\n",
    "\n",
    "vocab_path = \"./sentencebert_v1.0\"\n",
    "model_path = \"./sentencebert_v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(vocab_path)\n",
    "\n",
    "# 문장임베딩이므로, ORTModelForFeatureExtraction 로 로딩함\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(model_path)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fb9a8-e98b-415c-b783-cdcdb7778a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 해봄\n",
    "text = '난 널 사랑해'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "output = outputs.last_hidden_state\n",
    "print(output.shape)\n",
    "#print(output[0][5])\n",
    "onnx_embedding = output[0][5]\n",
    "print(onnx_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13e255-6422-498b-b004-123b728da7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence bert 원래 모델 로딩 \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "embedder = SentenceTransformer(model_path, device='cpu')\n",
    "print(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38d9b13b-d4f4-4c2f-ab17-768f5ef4fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        '난 널 사랑해', \n",
    "        '난 매우 행복해', \n",
    "        '날씨가 좋다',\n",
    "        '안 그래도 되는데 뭐. 괜찮아.',\n",
    "        '내일 저녁까지 보수 공사가 끝날 것으로 예상합니다.',\n",
    "        '감사합니다. 후회 없는 결정이 될 겁니다.',\n",
    "        '그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다.',\n",
    "        '프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?',\n",
    "        '내일 비가 온다',\n",
    "        '오늘은 운동을 해야지',\n",
    "        '내일은 다른팀과 축구경기가 있다',\n",
    "        '주말에는 등산을 갈 예정이다'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b946757-300e-4f9b-9b7e-abdaae29cc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,   8984,   9006, 119730,  14523,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   8984,  42608, 121717,  14523,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 123665,  11287,   9685,  11903,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,   9521,   8924,  37388,  12092,  54780,  28911,   9303,    119,\n",
      "           8904, 119250,  16985,    119,    102,      0,      0,      0,      0],\n",
      "        [   101, 129345, 121492,  18382, 120426, 120053,  11287,   8977,  41919,\n",
      "          23925, 120364,  33188,  48345,    119,    102,      0,      0,      0],\n",
      "        [   101, 121467,  33188,  48345,    119, 127364,  40364, 119583,  10739,\n",
      "           9100, 136592,    119,    102,      0,      0,      0,      0,      0],\n",
      "        [   101,   8924,  30873,  14867, 120578,  10150, 131192, 119994,  31398,\n",
      "           9952, 119217,    119, 129345,    100,    119,    102,      0,      0],\n",
      "        [   101, 139896, 134170,  11287,   9056,   9141,  12965, 119210, 119081,\n",
      "          48345,    119,   9074, 121962,  14843, 118671,  48549,    136,    102],\n",
      "        [   101, 129345,   9379,  11287,   9582,  11903,    102,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 122278,  10892, 119622,  10622,   9960,  21711,  12508,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 129345,  10892,  19709,  74399,  11882,  37905, 121048,  11287,\n",
      "          11506,    102,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 123569,  15303, 128441,  10622,   8847, 119801,  11925,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "# ** 멀티로 한번에 tokenizer 할때는 반드시 padding=True 해야 함.(그래야 최대 길이 token에 맞춰서 padding 됨)\n",
    "corpus_inputs = tokenizer(corpus, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\")\n",
    "print(corpus_inputs)\n",
    "print(type(corpus_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91db35b4-5847-40aa-bcae-7e30e779f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 15:01:23,142 - distilbertnlitest - INFO - ---------------------------------------------------------\n",
      "INFO:distilbertnlitest:---------------------------------------------------------\n",
      "2022-05-26 15:01:23,144 - distilbertnlitest - INFO - 난 널 사랑해, 유사도:0.9999998807907104\n",
      "INFO:distilbertnlitest:난 널 사랑해, 유사도:0.9999998807907104\n",
      "2022-05-26 15:01:23,145 - distilbertnlitest - INFO - 난 매우 행복해, 유사도:0.5999007225036621\n",
      "INFO:distilbertnlitest:난 매우 행복해, 유사도:0.5999007225036621\n",
      "2022-05-26 15:01:23,147 - distilbertnlitest - INFO - 날씨가 좋다, 유사도:0.38652727007865906\n",
      "INFO:distilbertnlitest:날씨가 좋다, 유사도:0.38652727007865906\n",
      "2022-05-26 15:01:23,148 - distilbertnlitest - INFO - 감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.3548429012298584\n",
      "INFO:distilbertnlitest:감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.3548429012298584\n",
      "2022-05-26 15:01:23,149 - distilbertnlitest - INFO - 오늘은 운동을 해야지, 유사도:0.3499101996421814\n",
      "INFO:distilbertnlitest:오늘은 운동을 해야지, 유사도:0.3499101996421814\n",
      "2022-05-26 15:01:23,150 - distilbertnlitest - INFO - 안 그래도 되는데 뭐. 괜찮아., 유사도:0.3064459562301636\n",
      "INFO:distilbertnlitest:안 그래도 되는데 뭐. 괜찮아., 유사도:0.3064459562301636\n",
      "2022-05-26 15:01:23,152 - distilbertnlitest - INFO - 주말에는 등산을 갈 예정이다, 유사도:0.30284780263900757\n",
      "INFO:distilbertnlitest:주말에는 등산을 갈 예정이다, 유사도:0.30284780263900757\n",
      "2022-05-26 15:01:23,153 - distilbertnlitest - INFO - 내일 비가 온다, 유사도:0.29386693239212036\n",
      "INFO:distilbertnlitest:내일 비가 온다, 유사도:0.29386693239212036\n",
      "2022-05-26 15:01:23,154 - distilbertnlitest - INFO - 내일은 다른팀과 축구경기가 있다, 유사도:0.1932632327079773\n",
      "INFO:distilbertnlitest:내일은 다른팀과 축구경기가 있다, 유사도:0.1932632327079773\n",
      "2022-05-26 15:01:23,155 - distilbertnlitest - INFO - 내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:0.1672656536102295\n",
      "INFO:distilbertnlitest:내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:0.1672656536102295\n",
      "2022-05-26 15:01:23,156 - distilbertnlitest - INFO - 그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.15212060511112213\n",
      "INFO:distilbertnlitest:그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.15212060511112213\n",
      "2022-05-26 15:01:23,157 - distilbertnlitest - INFO - 프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:0.07981503009796143\n",
      "INFO:distilbertnlitest:프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:0.07981503009796143\n",
      "2022-05-26 15:01:23,159 - distilbertnlitest - INFO - ---------------------------------------------------------\n",
      "INFO:distilbertnlitest:---------------------------------------------------------\n",
      "2022-05-26 15:01:23,161 - distilbertnlitest - INFO - === 유사도 처리시간: 0.024 초 ===\n",
      "INFO:distilbertnlitest:=== 유사도 처리시간: 0.024 초 ===\n",
      "2022-05-26 15:01:23,162 - distilbertnlitest - INFO - -END-\n",
      "\n",
      "INFO:distilbertnlitest:-END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# onnx 양자화 모델 문장 유사도 출력 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# onnx 양자화 모델 문장 유사도 구함\n",
    "outputs = model(**corpus_inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = torch.mean(last_hidden_state[0], dim=0)\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(last_hidden_state):\n",
    "    out_mean_sequence = torch.mean(hidden, dim=0)\n",
    "    simul_score = util.pytorch_cos_sim(in_mean_sequence, out_mean_sequence)[0]\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'양자화/최적화된 모델 : {model_checkpoint}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    logger.info('{}, 유사도:{}'.format(corpus[index-1], value[0]))\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f552e707-71bf-44b8-88a2-7c68b6239667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 15:19:43,320 - distilbertnlitest - INFO - ---------------------------------------------------------\n",
      "INFO:distilbertnlitest:---------------------------------------------------------\n",
      "2022-05-26 15:19:43,323 - distilbertnlitest - INFO - 난 널 사랑해, 유사도:1.000000238418579\n",
      "INFO:distilbertnlitest:난 널 사랑해, 유사도:1.000000238418579\n",
      "2022-05-26 15:19:43,325 - distilbertnlitest - INFO - 난 매우 행복해, 유사도:0.4053022861480713\n",
      "INFO:distilbertnlitest:난 매우 행복해, 유사도:0.4053022861480713\n",
      "2022-05-26 15:19:43,326 - distilbertnlitest - INFO - 날씨가 좋다, 유사도:0.20894736051559448\n",
      "INFO:distilbertnlitest:날씨가 좋다, 유사도:0.20894736051559448\n",
      "2022-05-26 15:19:43,328 - distilbertnlitest - INFO - 내일 비가 온다, 유사도:0.1295628547668457\n",
      "INFO:distilbertnlitest:내일 비가 온다, 유사도:0.1295628547668457\n",
      "2022-05-26 15:19:43,331 - distilbertnlitest - INFO - 감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.11919154971837997\n",
      "INFO:distilbertnlitest:감사합니다. 후회 없는 결정이 될 겁니다., 유사도:0.11919154971837997\n",
      "2022-05-26 15:19:43,333 - distilbertnlitest - INFO - 오늘은 운동을 해야지, 유사도:0.0809156745672226\n",
      "INFO:distilbertnlitest:오늘은 운동을 해야지, 유사도:0.0809156745672226\n",
      "2022-05-26 15:19:43,334 - distilbertnlitest - INFO - 주말에는 등산을 갈 예정이다, 유사도:0.05298909544944763\n",
      "INFO:distilbertnlitest:주말에는 등산을 갈 예정이다, 유사도:0.05298909544944763\n",
      "2022-05-26 15:19:43,335 - distilbertnlitest - INFO - 안 그래도 되는데 뭐. 괜찮아., 유사도:0.047101184725761414\n",
      "INFO:distilbertnlitest:안 그래도 되는데 뭐. 괜찮아., 유사도:0.047101184725761414\n",
      "2022-05-26 15:19:43,337 - distilbertnlitest - INFO - 그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.03845204785466194\n",
      "INFO:distilbertnlitest:그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다., 유사도:0.03845204785466194\n",
      "2022-05-26 15:19:43,338 - distilbertnlitest - INFO - 내일은 다른팀과 축구경기가 있다, 유사도:0.035530854016542435\n",
      "INFO:distilbertnlitest:내일은 다른팀과 축구경기가 있다, 유사도:0.035530854016542435\n",
      "2022-05-26 15:19:43,340 - distilbertnlitest - INFO - 내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:-0.00024395249783992767\n",
      "INFO:distilbertnlitest:내일 저녁까지 보수 공사가 끝날 것으로 예상합니다., 유사도:-0.00024395249783992767\n",
      "2022-05-26 15:19:43,341 - distilbertnlitest - INFO - 프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:-0.06807301938533783\n",
      "INFO:distilbertnlitest:프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?, 유사도:-0.06807301938533783\n",
      "2022-05-26 15:19:43,342 - distilbertnlitest - INFO - ---------------------------------------------------------\n",
      "INFO:distilbertnlitest:---------------------------------------------------------\n",
      "2022-05-26 15:19:43,343 - distilbertnlitest - INFO - === 유사도 처리시간: 0.026 초 ===\n",
      "INFO:distilbertnlitest:=== 유사도 처리시간: 0.026 초 ===\n",
      "2022-05-26 15:19:43,344 - distilbertnlitest - INFO - -END-\n",
      "\n",
      "INFO:distilbertnlitest:-END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentence bert 문장 유사도 출력 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# sentence bert 문장 임베딩값 구함\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "print(corpus_embeddings.shape)\n",
    "#print(corpus_embeddings)\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = corpus_embeddings[0]\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(corpus_embeddings):\n",
    "    out_mean_sequence = hidden\n",
    "    simul_score = util.pytorch_cos_sim(in_mean_sequence, out_mean_sequence)[0]\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'sbert모델: {model_checkpoint}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    logger.info('{}, 유사도:{}'.format(corpus[index-1], value[0]))\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e12aa-6581-45cf-af35-f15fb5a88e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
