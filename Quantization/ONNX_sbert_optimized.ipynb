{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eca75c-41e3-4840-9e9c-b49815d41aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================================\n",
    "# Huggingface와 ONNX 런타임을 이용한 동적 양자화 예시 4\n",
    "# => 훈련된 sbert 모델을 가지고, 최적화->양자화 과정을 거처 ONNX 모델로 로 만든 후, 테스트 하는 예시\n",
    "# => sbert는 embedding 모델이므로 ORTModelForFeatureExtraction 이용\n",
    "#\n",
    "## 참고 : https://huggingface.co/blog/optimum-inference\n",
    "##\n",
    "## 아래 패키지들을 설치해야 함\n",
    "'''\n",
    "pip install datasets\n",
    "pip install optimum\n",
    "pip install optimum[onnxruntime]\n",
    "pip install optimum[onnxruntime-gpu]  #gpu 사용인 경우\n",
    "pip install transformers[onnx]\n",
    "'''    \n",
    "\n",
    "## \n",
    "# cannot import name 'FeaturesManager' from 'transformers.onnx' 에러나면 \n",
    "# 아래처럼 transformers와 optimum 등을 업데이트 해줘야함.\n",
    "# - 업데이트 하면 transformers 버전이 4.15에서 4.21.2 버전으로 업데이트 됨.\n",
    "#\n",
    "'''\n",
    "pip uninstall transformers\n",
    "pip install transformers\n",
    "'''\n",
    "# 혹은\n",
    "'''\n",
    "python -m pip install git+https://github.com/huggingface/optimum.git\n",
    "python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]\n",
    "'''\n",
    "#참고: https://github.com/huggingface/optimum\n",
    "#\n",
    "# 이때 trnasformers를 업데이트 하고 나면 아래와 같은 모듈들도 모두 uninstall 하고 나서 최신으로 install 해줘야 함.\n",
    "# - 아래 외에도 몇가지 더 있을수 있음. (*코드 수행하면서 필요한 경우 삭제후 재설치 해줘야 함)\n",
    "'''\n",
    "pip uninstall optimum[onnxruntime]\n",
    "pip uninstall optimum[onnxruntime-gpu]  #gpu 사용인 경우\n",
    "pip uninstall transformers[onnx]\n",
    "pip uninstall optimum\n",
    "pip uninstall huggingface-hub\n",
    "pip uninstall pyyaml\n",
    "pip uninstall tqdm\n",
    "pip uninstall tokenizers\n",
    "pip uninstall accelerate\n",
    "pip uninstall regex\n",
    "pip uninstall nltk\n",
    "pip uninstall filelock\n",
    "pip uninstall click\n",
    "'''\n",
    "#\n",
    "#===============================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "logger = mlogging(loggername=\"sbert-optimized\", logfilename=\"sbert-optimized\")\n",
    "seed_everything(111)\n",
    "\n",
    "\n",
    "model_checkpoint = \"bongsoo/klue-sbert-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0688e-33b1-449f-9029-cc1980224820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49593407-05be-44a3-b24a-609573f0a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ONNX 모델로 변환 \n",
    "# => huggingface ORTModelForSequenceClassification 를 이용하여, ONNX 모델로 쉽게 저장할수 있다.(*단 여기서 ONNX 모델은 양자화된 모델은 아니고, 형식만 변경한 것임)\n",
    "# => 양자화 하려면. 위에서 처럼 ORTQuantizer 이용해야 함.\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Load model from hub and export it through the ONNX format \n",
    "model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    from_transformers=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Save the exported model\n",
    "onnx_path = './onnxfolder'\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47474c38-e543-4498-a2e5-dcd2c28f6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 최적화 적용\n",
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# optimization 최적화 실행\n",
    "# optimization_config=99 enables all available graph optimisations(99이면 모든 것을 최적화 시킴)\n",
    "optimization_config = OptimizationConfig(optimization_level=99)\n",
    "\n",
    "optimizer = ORTOptimizer.from_pretrained(model_checkpoint, feature=\"default\")\n",
    "\n",
    "optimizer.export(\n",
    "    onnx_model_path='./onnxfolder/model.onnx',   # 앞에서 만든 ONNX 모델 경로\n",
    "    onnx_optimized_model_output_path='./onnxfolder/model-optimized.onnx',  # 새롭게 만들 optimized 모델 경로\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a63346-37f8-4b87-8900-c725686cacba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 양자화 \n",
    "from optimum.onnxruntime import ORTConfig, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "\n",
    "# 동적 양자화인 경우 is_static = False로 해야 함.\n",
    "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
    "\n",
    "# 분류 모델인 경우에는 feature=\"sequence-classification\"\n",
    "quantizer = ORTQuantizer.from_pretrained(model_checkpoint, feature=\"default\")\n",
    "\n",
    "# ONNX 모델로 만들고 양자화 함\n",
    "quantizer.export(\n",
    "    onnx_model_path='./onnxfolder/model-optimized.onnx',   # optimized 모델 경로\n",
    "    onnx_quantized_model_output_path=\"./onnxfolder/model-quantized.onnx\",  \n",
    "    quantization_config=qconfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35b3bc-5941-4e22-b8a5-f0a1f2335993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnxfoloder 에 양자화된 model-quantized.onnx 모델이름 변경\n",
    "# => model-quantized.onnx 모델명을 model.onnx 로 변경 (*기존 model.onnx는 model_org.onnx로 이름 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed1140-6e37-455c-8646-751e69dbc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# 양자화 모델 불러옴\n",
    "# => 양자화 모델은 model.eval() 하면 에러남.\n",
    "#============================================================\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTModelForSequenceClassification\n",
    "\n",
    "vocab_path = \"bongsoo/klue-sbert-v1-onnx\"\n",
    "#vocab_path = \"../../data11/model/onnx/klue-sbert-v1-onnx\"   # model.onnx와 vocab파일이 있는 폴더 지정\n",
    "model_path = vocab_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(vocab_path)\n",
    "\n",
    "# 분류모델이면, ORTModelForSequenceClassification\n",
    "# 문장임베딩이면, ORTModelForFeatureExtraction 호출\n",
    "model = ORTModelForFeatureExtraction.from_pretrained(model_path)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be199ef-5f51-45d1-a4f4-58bb48f86993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# korsts 로딩\n",
    "test_file1 = '../../data11/korpora/korsts/tune_test.tsv'\n",
    "\n",
    "sentence1 = []\n",
    "sentence2 = []\n",
    "scores = [] \n",
    "    \n",
    "with open(test_file1, 'rt', encoding='utf-8') as fIn1:\n",
    "    lines = fIn1.readlines()\n",
    "    for line in lines:\n",
    "        s1, s2, score = line.split('\\t')\n",
    "        score = score.strip()\n",
    "        score = float(score) / 5.0\n",
    "            \n",
    "        sentence1.append(s1)\n",
    "        sentence2.append(s2)\n",
    "        scores.append(score)\n",
    "        \n",
    "print(f'sentence1: {len(sentence1)}')\n",
    "print(f'sentence2: {len(sentence2)}')\n",
    "print(f'scores: {len(scores)}')\n",
    "\n",
    "print(f'sentence1: {sentence1[0]}')\n",
    "print(f'sentence2: {sentence2[0]}')\n",
    "print(f'scores: {scores[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcffbc5-6d00-4967-a5b0-6be0df952ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence1 + sentence2를 묶어서 tokenizer 처리함\n",
    "corpus = sentence1 + sentence2\n",
    "print(len(corpus))\n",
    "\n",
    "corpus_inputs = tokenizer(corpus, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\")\n",
    "print(corpus_inputs)\n",
    "print(corpus_inputs['input_ids'])\n",
    "print(f'type:{type(corpus_inputs)}')\n",
    "print(corpus_inputs['input_ids'].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482021-aea6-41b3-b7b5-42045270a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 값 구하기 \n",
    "outputs = model(**corpus_inputs)\n",
    "embedding = outputs.last_hidden_state\n",
    "print(f'embed_len:{embedding.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed0922-3794-48f7-bd05-3e40c0d82df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구한 embeding 값을 sentence1, sentence2 로 나눔.\n",
    "embed_len = len(embedding)//2\n",
    "print(embed_len)\n",
    "tembed1 = embedding[0:embed_len]\n",
    "tembed2 = embedding[embed_len:]\n",
    "\n",
    "print(tembed1.shape)\n",
    "print(tembed2.shape)\n",
    "\n",
    "# embed1,2는 3차원->1차원으로 만들어야 함.\n",
    "# => 평균값으로 함\n",
    "embedlist1 = []\n",
    "for idx,embedding in enumerate(tembed1): # enumerate는 index, value 값이 리턴됨\n",
    "    embed = torch.mean(embedding, dim=0).numpy()\n",
    "    embedlist1.append(embed)\n",
    "\n",
    "embedlist2 = []\n",
    "for idx,embedding in enumerate(tembed2): # enumerate는 index, value 값이 리턴됨\n",
    "    embed = torch.mean(embedding, dim=0).numpy()\n",
    "    embedlist2.append(embed)\n",
    "\n",
    "\n",
    "print(len(embedlist1))\n",
    "print(len(embedlist2))\n",
    "\n",
    "print(embedlist1[0].shape)\n",
    "print(embedlist2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a79ab-a52e-426d-9b28-6e750f1149eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 을 이용하여 cosine_scores를 구함\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "cosine_scores = 1 - (paired_cosine_distances(embedlist1, embedlist2))\n",
    "\n",
    "print(type(cosine_scores))\n",
    "print(len(cosine_scores))\n",
    "print(cosine_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86844f6e-f0e6-4a6b-bcd9-be3c355e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson 과 spearman 평균을 구함.\n",
    "# => 실제 sts문장들 scores와 모델에서 구한 cosine_scores를 비교하여 Acc 평균 값들을 구함\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "eval_pearson_cosine, _ = pearsonr(scores, cosine_scores)\n",
    "eval_spearman_cosine, _ = spearmanr(scores, cosine_scores)\n",
    "\n",
    "print(eval_pearson_cosine)\n",
    "print(eval_spearman_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413afa5a-b9ef-46c8-9eb7-92d10bc179c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 해봄\n",
    "text = '난 널 사랑해'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "out = tokenizer.tokenize(text)\n",
    "print(out)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "output = outputs.last_hidden_state\n",
    "print(output.shape)\n",
    "#print(output[0][5])\n",
    "onnx_embedding = output[0][5]\n",
    "print(onnx_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce367e-253e-43be-9ad5-0f92ed8c0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence bert 원래 모델 로딩 \n",
    "#model_checkpoint = \"bongsoo/sentencebert_v1.2\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "embedder = SentenceTransformer(model_checkpoint, device='cpu')\n",
    "print(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31925d80-bc8e-4c2b-9e8e-7902d825e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        '난 널 사랑해', \n",
    "        '난 매우 행복해', \n",
    "        '날씨가 좋다',\n",
    "        '안 그래도 되는데 뭐. 괜찮아.',\n",
    "        '내일 저녁까지 보수 공사가 끝날 것으로 예상합니다.',\n",
    "        '감사합니다. 후회 없는 결정이 될 겁니다.',\n",
    "        '그러면 오전 10시에 보도록 하죠. 내일 뵙겠습니다.',\n",
    "        '프린트 카트리지가 다 떨어졌습니다. 더 주문할까요?',\n",
    "        '내일 비가 온다',\n",
    "        '오늘은 운동을 해야지',\n",
    "        '내일은 다른팀과 축구경기가 있다',\n",
    "        '주말에는 등산을 갈 예정이다'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329f16b-629b-4edb-83b0-3933b73a9c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** 멀티로 한번에 tokenizer 할때는 반드시 padding=True 해야 함.(그래야 최대 길이 token에 맞춰서 padding 됨)\n",
    "corpus_inputs = tokenizer(corpus, \n",
    "                 add_special_tokens=True, \n",
    "                 truncation=True, \n",
    "                 padding=True,   \n",
    "                 max_length=128, \n",
    "                 return_tensors=\"pt\")\n",
    "print(corpus_inputs)\n",
    "print(type(corpus_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab19c09-e05c-41b6-9dab-e756b73b39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx 양자화 모델 문장 유사도 출력 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# onnx 양자화 모델 문장 유사도 구함\n",
    "outputs = model(**corpus_inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    "\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = torch.mean(last_hidden_state[0], dim=0)\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(last_hidden_state):\n",
    "    out_mean_sequence = torch.mean(hidden, dim=0)\n",
    "    simul_score = util.pytorch_cos_sim(in_mean_sequence, out_mean_sequence)[0]\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'양자화/최적화된 모델 : {model_checkpoint}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    logger.info('{}, 유사도:{}'.format(corpus[index-1], value[0]))\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fd38d-6953-4d21-92d0-c326771fbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence bert 문장 유사도 출력 \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# sentence bert 문장 임베딩값 구함\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "print(corpus_embeddings.shape)\n",
    "#print(corpus_embeddings)\n",
    "\n",
    "# 첫번째 문장을 query로 지정함\n",
    "in_mean_sequence = corpus_embeddings[0]\n",
    "\n",
    "out_dict = {}\n",
    "# for문을 돌면서 유사도 비교\n",
    "for idx, hidden in enumerate(corpus_embeddings):\n",
    "    out_mean_sequence = hidden\n",
    "    simul_score = util.pytorch_cos_sim(in_mean_sequence, out_mean_sequence)[0]\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx+1)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'sbert모델: {model_checkpoint}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    index = int(count[0])\n",
    "    logger.info('{}, 유사도:{}'.format(corpus[index-1], value[0]))\n",
    "\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 유사도 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638e352-af21-4cf0-8e0b-882ec3500b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 참고\n",
    "# => huggingface ORTModelForSequenceClassification 를 이용하여, ONNX 모델로 쉽게 저장할수 있다.(*단 여기서 ONNX 모델은 양자화된 모델은 아니고, 형식만 변경한 것임)\n",
    "# => 양자화 하려면. 위에서 처럼 ORTQuantizer 이용해야 함.\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "model_checkpoint = \"./distilbert-0331-TS-nli-0.1-10\"\n",
    "\n",
    "# Load model from hub and export it through the ONNX format \n",
    "model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=3,\n",
    "    from_transformers=True\n",
    ")\n",
    "\n",
    "# Save the exported model\n",
    "model.save_pretrained(\"./onnxfolder\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c14f1-32a4-404e-ae36-35a9a706538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# onnx 모델 구조 로딩 해봄.\n",
    "# => 맨 뒤에 return %last_hidden_state 리턴되면 => ORTModelForFeatureExtraction 사용 가능\n",
    "import onnx\n",
    "model = onnx.load(\"./distilbert-nli/model.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "print(onnx.helper.printable_graph(model.graph))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
