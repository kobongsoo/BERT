{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# Huggingface load_dataset 으로 MLM 훈련 하기\n",
    "#\n",
    "# => load_dataset 으로 wiki 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 bongsoo/moco-corpus-kowiki202206 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, DistilBertTokenizerFast, BertConfig, DistilBertForMaskedLM\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/distilbert-MLM-Trainer_2022-08-24.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "#input_corpus = \"../../data11/my_corpus/my/pre-kowiki-20220620-1줄.txt\"\n",
    "#input_corpus = \"bongsoo/moco-corpus\"  # huggingface에 등록된 말뭉치 이용\n",
    "input_corpus = \"../../data11/my_corpus/kowiki-202206-nlp-corpus.txt\"  \n",
    "\n",
    "# eval 말뭉치 \n",
    "#eval_corpus = \"../../data11/my_corpus/bong_small_eval.txt\"\n",
    "eval_corpus = \"bongsoo/bongevalsmall\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"distilbert-base-multilingual-cased\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_path=\"../tokenizer_sample/moco-vocab/mdistilbertV1.2\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../data11/model/distilbert/mdistilbertV1.2-temp/'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 32       # batch_size\n",
    "token_max_len = 128   # token_seq_len\n",
    "############################################################################\n",
    "\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(333)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"distilbert-MLM-Trainer\", logfilename=\"../../log/distilbert-MLM-Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tokenizer_sample/moco-vocab/mdistilbertV1.2 is_fast:True\n",
      "*special_token_size: 5, *tokenizer.vocab_size: 164314\n",
      "*vocab_size: 164315\n",
      "*tokenizer_len: 164314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(164314, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=164314, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('*special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('*vocab_size: {}'.format(vocab_size))\n",
    "print('*tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "#config = BertConfig.from_pretrained(model_path)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')    \n",
    "\n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c5083eed7ffe81c1\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/default-c5083eed7ffe81c1/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe5d4b50dea4accab33193822ddbbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bongsoo--bongevalsmall-cfa82c943ea1c946\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/bongsoo--bongevalsmall-cfa82c943ea1c946/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7707c1c3be4821bc3ee2f939d37462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7680008\n",
      "    })\n",
      "})\n",
      "['Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'To estimate the time waited for reads incurred by rereading data blocks that had tobe written to disk because of a request from another instance, multiply the statistic(for example, the time waited for db ﬁle sequential reads) by the percentage of readI/O caused by previous cache ﬂushes as shown in this formula:', 'Where \"lock buffers for read\" is the value for lock converts from N to S derived fromV$LOCK_ACTIVITY and \"physical reads\" is from the V$SYSSTAT view.']\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset========================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "['국토교통부 관계자는  실무기구에서는 각 업계별로 규제혁신형 플랫폼 택시를 하기 위해서는 어떤 규제를 풀어야 한다는 자기 안이 있어야 한다 고 말했다 ', '국책연구기관의 한 관계자는  위원회가 전문성과 대표성을 갖추고 본연의 장점을 최대한 살리기 위해서는 외부 감시와 통제가 보다 활성화돼야 한다 고 지적했다 ', '게임업계 관계자는  현장 수요보다 의료진 등 특정한 누군가의 이익을 위해 게임을 중독물질  질병으로 만들려 한다는 합리적 의심이 든다 고 꼬집었다 ']\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# 평가 말뭉치 로딩\n",
    "eval_dataset = load_dataset(eval_corpus)\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset========================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['test']['text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca75da5-1048-4522-a13f-60d306741a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /MOCOMSYS/.cache/huggingface/datasets/text/default-c5083eed7ffe81c1/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-2652c7ec115fac69.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 225 ms, sys: 49 ms, total: 274 ms\n",
      "Wall time: 271 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80b130477a94e0892b2b52394beecb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 258 ms, sys: 114 ms, total: 372 ms\n",
      "Wall time: 78.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\\nprint(tokenized_dataset_fast['train']['text'][0:2])\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 처리\n",
    "def tokenizer_function(examples):\n",
    "    result =  tokenizer(examples['text'], truncation=True, max_length=token_max_len, return_overflowing_tokens=True)\n",
    "    \n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "# batched=True 하면 빠른 tokenizer 이용(Rust)\n",
    "%time train_dataset_fast = train_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "'''\n",
    "%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\n",
    "print(tokenized_dataset_fast['train']['text'][0:2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7686169\n",
      "    })\n",
      "})\n",
      "*fast_len:7686169, len:7680008\n",
      "{'text': ['Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'To estimate the time waited for reads incurred by rereading data blocks that had tobe written to disk because of a request from another instance, multiply the statistic(for example, the time waited for db ﬁle sequential reads) by the percentage of readI/O caused by previous cache ﬂushes as shown in this formula:'], 'input_ids': [[101, 150640, 10114, 10105, 159, 109, 146671, 168, 152248, 17904, 10142, 10635, 159651, 10111, 13551, 147264, 10142, 146964, 22115, 131, 102], [101, 11469, 78059, 10105, 10635, 159651, 10142, 91160, 158425, 10155, 11639, 66058, 10230, 11165, 47352, 10189, 10374, 149221, 13398, 10114, 50169, 12373, 10108, 169, 37449, 10188, 12864, 34469, 117, 154198, 10105, 154658, 113, 10142, 14351, 117, 10105, 10635, 159651, 10142, 49625, 146460, 10284, 148859, 91160, 114, 10155, 10105, 46971, 10108, 24944, 11281, 120, 152, 19513, 10155, 16741, 62070, 147287, 37026, 10171, 10146, 19989, 10106, 10531, 29659, 131, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "*fast_len:200, len:200\n",
      "{'text': ['국토교통부 관계자는  실무기구에서는 각 업계별로 규제혁신형 플랫폼 택시를 하기 위해서는 어떤 규제를 풀어야 한다는 자기 안이 있어야 한다 고 말했다 ', '국책연구기관의 한 관계자는  위원회가 전문성과 대표성을 갖추고 본연의 장점을 최대한 살리기 위해서는 외부 감시와 통제가 보다 활성화돼야 한다 고 지적했다 '], 'input_ids': [[101, 122834, 133309, 122378, 11018, 126397, 121481, 23635, 8844, 125019, 159425, 121809, 128462, 27506, 123644, 125465, 11513, 9952, 12310, 119797, 11018, 55910, 121809, 11513, 9937, 119828, 120460, 119748, 9521, 10739, 45893, 21711, 16139, 8888, 102055, 102], [101, 140604, 120781, 119899, 10459, 9954, 122378, 11018, 120566, 11287, 120009, 120714, 119573, 36456, 120360, 11664, 9358, 122300, 122131, 10622, 123910, 125632, 12310, 119797, 11018, 120267, 121765, 12638, 120442, 11287, 106154, 120866, 18227, 139010, 16139, 8888, 120093, 12490, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(f'*fast_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(eval_dataset_fast)\n",
    "print(f'*fast_len:{len(eval_dataset_fast[\"test\"])}, len:{len(eval_dataset[\"test\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['test'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast(MLM)=======================================\n",
      "tensor([   101,    103,  10114,  10105,    159,    109, 146671,  45011, 152248,\n",
      "         17904,  10142,  10635, 159651,  10111,  13551, 147264,  10142, 146964,\n",
      "         22115,    131,    102,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0])\n",
      "{'text': 'Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'input_ids': [101, 150640, 10114, 10105, 159, 109, 146671, 168, 152248, 17904, 10142, 10635, 159651, 10111, 13551, 147264, 10142, 146964, 22115, 131, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast(MLM)=======================================\n",
      "tensor([   101, 122834, 133309, 122378,  11018, 126397, 121481,  23635,   8844,\n",
      "        125019, 159425, 121809, 128462,  27506, 123644, 125465,    103,    103,\n",
      "         12310, 119797,  11018,    103, 121809,  11513,   9937, 119828, 120460,\n",
      "        119748,    103,  10739,  45893,  21711,  45770,   8888,    103,    102,\n",
      "             0,      0,      0])\n",
      "{'text': '국토교통부 관계자는  실무기구에서는 각 업계별로 규제혁신형 플랫폼 택시를 하기 위해서는 어떤 규제를 풀어야 한다는 자기 안이 있어야 한다 고 말했다 ', 'input_ids': [101, 122834, 133309, 122378, 11018, 126397, 121481, 23635, 8844, 125019, 159425, 121809, 128462, 27506, 123644, 125465, 11513, 9952, 12310, 119797, 11018, 55910, 121809, 11513, 9937, 119828, 120460, 119748, 9521, 10739, 45893, 21711, 16139, 8888, 102055, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:2])\n",
    "mlm_eval_sample = data_collator(eval_dataset_fast['test']['input_ids'][0:2])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_eval_sample['input_ids'][0])\n",
    "print(eval_dataset_fast['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*total_optim_steps: 1921542, *eval_steps:96077, *logging_steps:96077, *save_steps:192154\n",
      "*no_cuda: False\n"
     ]
    }
   ],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "\n",
    "epochs = 8           # epochs\n",
    "#lr = 3e-5  # 학습률\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.05)           # 평가 스탭수\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "save_total_limit=2                                 # 마지막 2개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    #learning_rate=lr,                      # lr: 기본 5e-5\n",
    "    per_gpu_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    save_total_limit=save_total_limit,      # 마지막 두 모델 빼고 과거 모델은 삭제\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 : steps\n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "# trainer로 훈련할때는 [mask] 처리된 input_ids 만 dataset으로 넘겨주면 됨.\n",
    "train_dataset_fast_input_ids = train_dataset_fast['train']['input_ids']\n",
    "eval_dataset_fast_input_ids = eval_dataset_fast['test']['input_ids']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_fast_input_ids,   # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_fast_input_ids      # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 7686169\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1921544\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8344' max='1921544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8344/1921544 17:00 < 64:59:42, 8.18 it/s, Epoch 0.03/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/distilbert/mdistilbertV1.2/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8a66c-831a-44f4-a063-9932aeaf7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
