{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf4d294-0c8c-4e2f-87ad-0f20da3baec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# Huggingface load_dataset 으로 MLM 훈련 하기\n",
    "#\n",
    "# => load_dataset 으로 wiki 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 bongsoo/moco-corpus-kowiki202206 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, DistilBertTokenizerFast, BertConfig, DistilBertForMaskedLM\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#%autosave 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/distilbert-MLM-Trainer_2022-10-07.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "#input_corpus = \"../../data11/my_corpus/my/pre-kowiki-20220620-1줄.txt\"\n",
    "#input_corpus = \"bongsoo/moco-corpus\"  # huggingface에 등록된 말뭉치 이용\n",
    "input_corpus = \"../../data11/my_corpus/moco-corpus-kowiki2022.txt\"\n",
    "\n",
    "# eval 말뭉치 \n",
    "#eval_corpus = \"bongsoo/bongeval\"\n",
    "eval_corpus = \"bongsoo/moco_eval\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../data11/model/distilbert/mdistilbertV3\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_path = \"../../data11/model/distilbert/mdistilbertV3\"\n",
    "\n",
    "# 출력\n",
    "OUTPATH = '../../data11/model/distilbert/mdistilbertV3.0/'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 32       # batch_size (32 이상이면 CUDA MEMORY 부족 함)\n",
    "token_max_len = 128   # token_seq_len\n",
    "epoch = 4             # epoch\n",
    "lr = 5e-5             # learning rate(기본:5e-5)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(333)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"distilbert-MLM-Trainer\", logfilename=\"../../log/distilbert-MLM-Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/model/distilbert/mdistilbertV3 is_fast:True\n",
      "*special_token_size: 5, *tokenizer.vocab_size: 159552\n",
      "*vocab_size: 159553\n",
      "*tokenizer_len: 159552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(159552, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=159552, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('*special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('*vocab_size: {}'.format(vocab_size))\n",
    "print('*tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "#config = BertConfig.from_pretrained(model_path)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')    \n",
    "\n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ad754964b5cc2b3a\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/default-ad754964b5cc2b3a/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467c4af598014c2a80b57fc4aa2d6542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bongsoo--moco_eval-9741016d12933a55\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/bongsoo--moco_eval-9741016d12933a55/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdfdf24c6cb450891f66e9489952544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 7680008\n",
      "    })\n",
      "})\n",
      "['Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'To estimate the time waited for reads incurred by rereading data blocks that had tobe written to disk because of a request from another instance, multiply the statistic(for example, the time waited for db ﬁle sequential reads) by the percentage of readI/O caused by previous cache ﬂushes as shown in this formula:', 'Where \"lock buffers for read\" is the value for lock converts from N to S derived fromV$LOCK_ACTIVITY and \"physical reads\" is from the V$SYSSTAT view.']\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset========================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "['필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', '외부시스템과 데이터를 주고 받기 위해 사용되는 디렉토리는 다음과 같다.', 'SOA는 비즈니스 프로세스를 기본적인 표준 빌딩 블럭 단위로 분할하여, 이를 IT 프로세스와 유연하게 일치시키는 특징이 있다.']\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# 평가 말뭉치 로딩\n",
    "eval_dataset = load_dataset(eval_corpus)\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset========================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['test']['text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca75da5-1048-4522-a13f-60d306741a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6171db025ac94c60b733360998194009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7681 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 59s, sys: 42min 28s, total: 1h 2min 28s\n",
      "Wall time: 4min 29s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a76f96ee704126b2ba40820bfc78db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 1.34 s, total: 2.36 s\n",
      "Wall time: 196 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\\nprint(tokenized_dataset_fast['train']['text'][0:2])\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 처리\n",
    "def tokenizer_function(examples):\n",
    "    result =  tokenizer(examples['text'], truncation=True, max_length=token_max_len, return_overflowing_tokens=True)\n",
    "    \n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "# batched=True 하면 빠른 tokenizer 이용(Rust)\n",
    "%time train_dataset_fast = train_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "'''\n",
    "%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\n",
    "print(tokenized_dataset_fast['train']['text'][0:2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7686845\n",
      "    })\n",
      "})\n",
      "*fast_len:7686845, len:7680008\n",
      "{'text': ['Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'To estimate the time waited for reads incurred by rereading data blocks that had tobe written to disk because of a request from another instance, multiply the statistic(for example, the time waited for db ﬁle sequential reads) by the percentage of readI/O caused by previous cache ﬂushes as shown in this formula:'], 'input_ids': [[101, 153267, 10114, 10105, 159, 109, 149756, 168, 154568, 17904, 10142, 10635, 83279, 10336, 10111, 13551, 150281, 10142, 150009, 22115, 131, 102], [101, 11469, 78059, 10105, 10635, 83279, 10336, 10142, 91160, 158810, 15711, 10155, 11639, 66058, 10230, 11165, 47352, 10189, 10374, 152020, 13398, 10114, 50169, 12373, 10108, 169, 37449, 10188, 12864, 34469, 117, 156397, 10105, 156834, 113, 10142, 14351, 117, 10105, 10635, 83279, 10336, 10142, 49625, 100, 151676, 91160, 114, 10155, 10105, 46971, 10108, 24944, 11281, 120, 152, 19513, 10155, 16741, 62070, 100, 10146, 19989, 10106, 10531, 29659, 131, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "*fast_len:3000, len:3000\n",
      "{'text': ['필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', '외부시스템과 데이터를 주고 받기 위해 사용되는 디렉토리는 다음과 같다.'], 'input_ids': [[101, 119567, 9485, 119622, 119550, 11513, 119606, 12178, 119834, 119600, 10739, 119548, 10530, 119753, 14843, 9460, 13767, 149683, 120394, 11882, 120107, 119810, 119568, 22766, 19709, 120430, 120231, 10530, 119753, 10622, 9247, 40410, 119, 102], [101, 119834, 119600, 11882, 119550, 11513, 9689, 11664, 9322, 12310, 19905, 119547, 24683, 120430, 120231, 11018, 39402, 53354, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(f'*fast_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(eval_dataset_fast)\n",
    "print(f'*fast_len:{len(eval_dataset_fast[\"test\"])}, len:{len(eval_dataset[\"test\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['test'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast(MLM)=======================================\n",
      "tensor([   101, 153267,  10114,  10105,    159,    109, 149756,    168, 154568,\n",
      "         17904,  10142,  10635,  83279,  10336,  10111,  13551, 150281,  10142,\n",
      "        150009,    103,    103,    102,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0])\n",
      "{'text': 'Refer to the V$SYSTEM_EVENT view for time waited and average waits for thefollowing actions:', 'input_ids': [101, 153267, 10114, 10105, 159, 109, 149756, 168, 154568, 17904, 10142, 10635, 83279, 10336, 10111, 13551, 150281, 10142, 150009, 22115, 131, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast(MLM)=======================================\n",
      "tensor([   101, 119567,   9485, 119622, 119550,  11513, 119606,  12178, 119834,\n",
      "        119600,  10739, 119548,    103,    103,  14843,   9460,  13767,    103,\n",
      "        120394,  11882, 120107,    103, 119568,  22766,  19709, 120430, 120231,\n",
      "         10530, 119753,  10622,   9247,  40410,    119,    102])\n",
      "{'text': '필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', 'input_ids': [101, 119567, 9485, 119622, 119550, 11513, 119606, 12178, 119834, 119600, 10739, 119548, 10530, 119753, 14843, 9460, 13767, 149683, 120394, 11882, 120107, 119810, 119568, 22766, 19709, 120430, 120231, 10530, 119753, 10622, 9247, 40410, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:2])\n",
    "mlm_eval_sample = data_collator(eval_dataset_fast['test']['input_ids'][0:2])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_eval_sample['input_ids'][0])\n",
    "print(eval_dataset_fast['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*total_optim_steps: 960855, *eval_steps:19217, *logging_steps:19217, *save_steps:96085\n",
      "*no_cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "\n",
    "epochs = epoch          # epochs\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.02)           # 평가 스탭수\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "#save_total_limit=5                                # 마지막 5개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    learning_rate=lr,                       # lr: 기본 5e-5\n",
    "    per_gpu_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_strategy=\"steps\",                  # 저장 전략 (no, epoch, steps 기본=steps) \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    #save_total_limit=save_total_limit,     # 마지막 x개 모델 빼고 과거 모델은 삭제\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 (no, epoch, steps 기본=no)  \n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "# trainer로 훈련할때는 [mask] 처리된 input_ids 만 dataset으로 넘겨주면 됨.\n",
    "train_dataset_fast_input_ids = train_dataset_fast['train']['input_ids']\n",
    "eval_dataset_fast_input_ids = eval_dataset_fast['test']['input_ids']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_fast_input_ids,   # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_fast_input_ids      # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7686845\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 960856\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960856' max='960856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [960856/960856 29:03:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19217</td>\n",
       "      <td>2.303800</td>\n",
       "      <td>2.932389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38434</td>\n",
       "      <td>2.365000</td>\n",
       "      <td>2.882050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57651</td>\n",
       "      <td>2.411100</td>\n",
       "      <td>2.981693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76868</td>\n",
       "      <td>2.444400</td>\n",
       "      <td>2.911633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96085</td>\n",
       "      <td>2.445600</td>\n",
       "      <td>2.926668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115302</td>\n",
       "      <td>2.440600</td>\n",
       "      <td>2.949776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134519</td>\n",
       "      <td>2.429800</td>\n",
       "      <td>2.901642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153736</td>\n",
       "      <td>2.427100</td>\n",
       "      <td>2.896030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172953</td>\n",
       "      <td>2.418600</td>\n",
       "      <td>2.800641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192170</td>\n",
       "      <td>2.413200</td>\n",
       "      <td>2.869819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211387</td>\n",
       "      <td>2.406000</td>\n",
       "      <td>2.846016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230604</td>\n",
       "      <td>2.398300</td>\n",
       "      <td>2.832274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249821</td>\n",
       "      <td>2.381400</td>\n",
       "      <td>2.864578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269038</td>\n",
       "      <td>2.364500</td>\n",
       "      <td>2.845547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288255</td>\n",
       "      <td>2.357400</td>\n",
       "      <td>2.855180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307472</td>\n",
       "      <td>2.353900</td>\n",
       "      <td>2.799095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326689</td>\n",
       "      <td>2.347200</td>\n",
       "      <td>2.760062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345906</td>\n",
       "      <td>2.337100</td>\n",
       "      <td>2.766765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365123</td>\n",
       "      <td>2.333200</td>\n",
       "      <td>2.757298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384340</td>\n",
       "      <td>2.324600</td>\n",
       "      <td>2.779670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403557</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>2.731682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422774</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>2.731119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441991</td>\n",
       "      <td>2.354600</td>\n",
       "      <td>2.753061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461208</td>\n",
       "      <td>2.347400</td>\n",
       "      <td>2.745525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480425</td>\n",
       "      <td>2.335900</td>\n",
       "      <td>2.703745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499642</td>\n",
       "      <td>2.303200</td>\n",
       "      <td>2.726098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518859</td>\n",
       "      <td>2.299100</td>\n",
       "      <td>2.683181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538076</td>\n",
       "      <td>2.292400</td>\n",
       "      <td>2.697961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557293</td>\n",
       "      <td>2.282800</td>\n",
       "      <td>2.668173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576510</td>\n",
       "      <td>2.275100</td>\n",
       "      <td>2.681008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595727</td>\n",
       "      <td>2.266200</td>\n",
       "      <td>2.706725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614944</td>\n",
       "      <td>2.258900</td>\n",
       "      <td>2.687447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634161</td>\n",
       "      <td>2.253200</td>\n",
       "      <td>2.623322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653378</td>\n",
       "      <td>2.240200</td>\n",
       "      <td>2.610014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672595</td>\n",
       "      <td>2.232400</td>\n",
       "      <td>2.597824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691812</td>\n",
       "      <td>2.223500</td>\n",
       "      <td>2.659304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711029</td>\n",
       "      <td>2.221500</td>\n",
       "      <td>2.559449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730246</td>\n",
       "      <td>2.200200</td>\n",
       "      <td>2.533650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749463</td>\n",
       "      <td>2.181300</td>\n",
       "      <td>2.611120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768680</td>\n",
       "      <td>2.177300</td>\n",
       "      <td>2.605572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787897</td>\n",
       "      <td>2.168900</td>\n",
       "      <td>2.606974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807114</td>\n",
       "      <td>2.162500</td>\n",
       "      <td>2.591892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826331</td>\n",
       "      <td>2.153600</td>\n",
       "      <td>2.528175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845548</td>\n",
       "      <td>2.147400</td>\n",
       "      <td>2.546839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864765</td>\n",
       "      <td>2.142600</td>\n",
       "      <td>2.515652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883982</td>\n",
       "      <td>2.129500</td>\n",
       "      <td>2.508490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903199</td>\n",
       "      <td>2.121200</td>\n",
       "      <td>2.495809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922416</td>\n",
       "      <td>2.120600</td>\n",
       "      <td>2.539486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941633</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>2.535280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960850</td>\n",
       "      <td>2.115400</td>\n",
       "      <td>2.529332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-96085\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-96085/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-96085/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-192170\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-192170/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-192170/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-288255\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-288255/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-288255/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-384340\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-384340/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-384340/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-480425\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-480425/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-480425/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-576510\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-576510/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-576510/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-672595\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-672595/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-672595/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-768680\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-768680/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-768680/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-864765\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-864765/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-864765/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-960850\n",
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-960850/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0/checkpoint-960850/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=960856, training_loss=2.2910882034182416, metrics={'train_runtime': 104586.2015, 'train_samples_per_second': 293.991, 'train_steps_per_second': 9.187, 'total_flos': 5.6359012850018496e+17, 'train_loss': 2.2910882034182416, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../../data11/model/distilbert/mdistilbertV3.0-last/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/mdistilbertV3.0-last/pytorch_model.bin\n",
      "tokenizer config file saved in ../../data11/model/distilbert/mdistilbertV3.0-last/tokenizer_config.json\n",
      "Special tokens file saved in ../../data11/model/distilbert/mdistilbertV3.0-last/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> save_model : ../../data11/model/distilbert/mdistilbertV3.0-last/\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/distilbert/mdistilbertV3.0-last/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8a66c-831a-44f4-a063-9932aeaf7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
