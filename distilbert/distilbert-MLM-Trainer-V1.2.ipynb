{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf4d294-0c8c-4e2f-87ad-0f20da3baec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# Huggingface load_dataset 으로 MLM 훈련 하기\n",
    "#\n",
    "# => load_dataset 으로 wiki 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 bongsoo/moco-corpus-kowiki202206 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, DistilBertTokenizerFast, BertConfig, DistilBertForMaskedLM\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#%autosave 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/distilbert-MLM-Trainer_2022-10-24.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "#input_corpus = \"../../data11/my_corpus/my/pre-kowiki-20220620-1줄.txt\"\n",
    "#input_corpus = \"bongsoo/moco-corpus\"  # huggingface에 등록된 말뭉치 이용\n",
    "input_corpus = \"../../data11/my_corpus/re-kowiki202206-moco2-mecab.txt\"  #re-kowiki-202206.txt, re-moco-corpus2.txt\n",
    "\n",
    "# eval 말뭉치 \n",
    "#eval_corpus = \"bongsoo/bongeval\"\n",
    "eval_corpus = \"bongsoo/moco_eval\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../data11/model/distilbert/distilbert-base-multilingual-cased\"\n",
    "#model_path = \"../../data11/model/distilbert/bert-re-moco-corpus2-mecab\"\n",
    "#model_path = \"../../data11/model/distilbert/bert-re-kowiki-mecab-10000-2\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_path = \"../../data11/my_corpus/vocab/bert-re-kowiki202206-moco2-mecab-vocab\"\n",
    "\n",
    "# 중간 출력\n",
    "#OUTPATH = '../../data11/model/distilbert/bert-re-kowiki/'\n",
    "#OUTPATH = '../../data11/model/distilbert/bert-re-kowiki-nouns/'\n",
    "OUTPATH = '../../data11/model/distilbert/bert-re-kowiki-moco2-mecab-4-check/'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 32       # batch_size (32 이상이면 CUDA MEMORY 부족 함)\n",
    "token_max_len = 128   # token_seq_len\n",
    "epoch = 4             # epoch\n",
    "lr = 1e-4             # learning rate(기본:5e-5)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"distilbert-MLM-Trainer\", logfilename=\"../../log/distilbert-MLM-Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/my_corpus/vocab/bert-re-kowiki-10000-moco2-mecab-10000 is_fast:True\n",
      "*special_token_size: 5, *tokenizer.vocab_size: 139548\n",
      "*vocab_size: 139549\n",
      "*tokenizer_len: 139548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(139548, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=139548, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, strip_accents=False, do_lower_case=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('*special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('*vocab_size: {}'.format(vocab_size))\n",
    "print('*tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "#config = BertConfig.from_pretrained(model_path)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')    \n",
    "\n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a65b4f99408fe421\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/default-a65b4f99408fe421/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f2b73f6bb74741bc3141128119fef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bongsoo--moco_eval-9741016d12933a55\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/bongsoo--moco_eval-9741016d12933a55/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cb366b33f847879c2b4b91e65cf6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3337234\n",
      "    })\n",
      "})\n",
      "['example of an actor using the system It is not a use case and in most projects it does not survive', '37 서버명 업무내용 제조사 비고 형상관리 형상관리업무 HP 행정정보1 행정정보업무 HP 행정정보2 행정정보업무 HP 행정정보테스트 HP 파생상품1 파생상품업무 HP 파생상품2 파생상품업무 HP 파생상품3 파생상품업무 HP 파생상품테스트 파생상품테스트업무 HP EDDDB1 EDD DB업무 HP RAC EDDDB2 EDD DB업무 HP EDDWAS1 EDD WAS업무 HP EDDWAS2 EDD WAS업무 HP EDDWEB1 EDD WEB업무 HP EDDWEB2 EDD WEB업무 HP EDD테스트 EDD테스트업무 HP IFRS DB업무 HP IFRS테스트 IFRS DB테스트업무 HP', '변경에 따른 관련 법령 예 전자정부법 IT자원의 공동활용신설 공개SW의 이용 및 라이선스 등 하위규정 등']\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset========================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "['필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', '외부시스템과 데이터를 주고 받기 위해 사용되는 디렉토리는 다음과 같다.', 'SOA는 비즈니스 프로세스를 기본적인 표준 빌딩 블럭 단위로 분할하여, 이를 IT 프로세스와 유연하게 일치시키는 특징이 있다.']\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# 평가 말뭉치 로딩\n",
    "eval_dataset = load_dataset(eval_corpus)\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset========================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['test']['text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca75da5-1048-4522-a13f-60d306741a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a199a8a87e374863b6439400b957d702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3338 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 54s, sys: 30min 10s, total: 43min 4s\n",
      "Wall time: 2min 45s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b4e117ad4a4a98b9c209a8dd8de148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 824 ms, sys: 1.29 s, total: 2.11 s\n",
      "Wall time: 173 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\\nprint(tokenized_dataset_fast['train']['text'][0:2])\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 처리\n",
    "def tokenizer_function(examples):\n",
    "    result =  tokenizer(examples['text'], truncation=True, max_length=token_max_len, return_overflowing_tokens=True)\n",
    "    \n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "# batched=True 하면 빠른 tokenizer 이용(Rust)\n",
    "%time train_dataset_fast = train_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "'''\n",
    "%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\n",
    "print(tokenized_dataset_fast['train']['text'][0:2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3426147\n",
      "    })\n",
      "})\n",
      "*fast_len:3426147, len:3337234\n",
      "{'text': ['example of an actor using the system It is not a use case and in most projects it does not survive', '37 서버명 업무내용 제조사 비고 형상관리 형상관리업무 HP 행정정보1 행정정보업무 HP 행정정보2 행정정보업무 HP 행정정보테스트 HP 파생상품1 파생상품업무 HP 파생상품2 파생상품업무 HP 파생상품3 파생상품업무 HP 파생상품테스트 파생상품테스트업무 HP EDDDB1 EDD DB업무 HP RAC EDDDB2 EDD DB업무 HP EDDWAS1 EDD WAS업무 HP EDDWAS2 EDD WAS업무 HP EDDWEB1 EDD WEB업무 HP EDDWEB2 EDD WEB업무 HP EDD테스트 EDD테스트업무 HP IFRS DB업무 HP IFRS테스트 IFRS DB테스트업무 HP'], 'input_ids': [[101, 14351, 10108, 10151, 14066, 13382, 10105, 11787, 10377, 10124, 10472, 169, 11760, 13474, 10111, 10106, 10992, 22846, 10271, 15107, 10472, 55681, 102], [101, 11204, 121764, 16758, 120370, 31605, 24974, 130237, 9379, 11664, 122779, 129352, 122779, 129352, 26784, 32537, 37496, 119694, 132111, 10759, 119694, 132111, 26784, 32537, 37496, 119694, 132111, 10729, 119694, 132111, 26784, 32537, 37496, 119694, 132111, 123098, 15184, 37496, 123486, 14871, 52951, 10759, 123486, 14871, 52951, 26784, 32537, 37496, 123486, 14871, 52951, 10729, 123486, 14871, 52951, 26784, 32537, 37496, 123486, 14871, 52951, 10884, 123486, 14871, 52951, 26784, 32537, 37496, 123486, 14871, 52951, 123098, 15184, 123486, 14871, 52951, 123098, 15184, 26784, 32537, 37496, 74053, 135452, 11274, 10759, 74053, 11490, 37654, 26784, 32537, 37496, 64007, 10858, 74053, 135452, 11274, 10729, 74053, 11490, 37654, 26784, 32537, 37496, 74053, 11490, 136850, 10759, 74053, 11490, 138563, 10731, 26784, 32537, 37496, 74053, 11490, 136850, 10729, 74053, 11490, 138563, 10731, 26784, 32537, 37496, 74053, 11490, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3002\n",
      "    })\n",
      "})\n",
      "*fast_len:3002, len:3000\n",
      "{'text': ['필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', '외부시스템과 데이터를 주고 받기 위해 사용되는 디렉토리는 다음과 같다.'], 'input_ids': [[101, 119649, 9485, 122246, 119997, 11513, 120348, 12178, 120597, 130613, 10739, 119725, 10530, 120196, 14843, 9460, 13767, 133914, 11127, 125763, 11882, 120733, 138361, 120551, 22766, 19709, 126356, 120565, 10530, 120196, 10622, 9247, 40410, 119, 102], [101, 120597, 130613, 11882, 119997, 11513, 129414, 9322, 12310, 19905, 119552, 24683, 126356, 120565, 11018, 39402, 53354, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(f'*fast_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(eval_dataset_fast)\n",
    "print(f'*fast_len:{len(eval_dataset_fast[\"test\"])}, len:{len(eval_dataset[\"test\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['test'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast(MLM)=======================================\n",
      "tensor([  101, 14351, 10108, 10151, 14066, 13382, 10105, 11787, 10377,   103,\n",
      "        10472,   169, 11760, 13474, 10111, 10106, 10992, 22846, 29630, 15107,\n",
      "        10472, 55681,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "{'text': 'example of an actor using the system It is not a use case and in most projects it does not survive', 'input_ids': [101, 14351, 10108, 10151, 14066, 13382, 10105, 11787, 10377, 10124, 10472, 169, 11760, 13474, 10111, 10106, 10992, 22846, 10271, 15107, 10472, 55681, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast(MLM)=======================================\n",
      "tensor([   101,    103,   9485, 122246, 119997,  11513, 120348,  12178, 120597,\n",
      "        130613,  10739, 119725,  10530, 120196,  14843,   9460,    103,    103,\n",
      "         11127, 125763,  11882, 120733, 138361, 120551,  22766,  19709, 126356,\n",
      "        120565,    103,    103,  10622,   9247,  40410,    119,    102])\n",
      "{'text': '필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', 'input_ids': [101, 119649, 9485, 122246, 119997, 11513, 120348, 12178, 120597, 130613, 10739, 119725, 10530, 120196, 14843, 9460, 13767, 133914, 11127, 125763, 11882, 120733, 138361, 120551, 22766, 19709, 126356, 120565, 10530, 120196, 10622, 9247, 40410, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:2])\n",
    "mlm_eval_sample = data_collator(eval_dataset_fast['test']['input_ids'][0:2])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_eval_sample['input_ids'][0])\n",
    "print(eval_dataset_fast['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*total_optim_steps: 428268, *eval_steps:8565, *logging_steps:8565, *save_steps:42826\n",
      "*no_cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "\n",
    "epochs = epoch          # epochs\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.02)           # 평가 스탭수\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "#save_total_limit=5                                # 마지막 5개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    learning_rate=lr,                       # lr: 기본 5e-5\n",
    "    per_gpu_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_strategy=\"steps\",                  # 저장 전략 (no, epoch, steps 기본=steps) \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    #save_total_limit=save_total_limit,     # 마지막 x개 모델 빼고 과거 모델은 삭제\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 (no, epoch, steps 기본=no)  \n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "# trainer로 훈련할때는 [mask] 처리된 input_ids 만 dataset으로 넘겨주면 됨.\n",
    "train_dataset_fast_input_ids = train_dataset_fast['train']['input_ids']\n",
    "eval_dataset_fast_input_ids = eval_dataset_fast['test']['input_ids']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_fast_input_ids,   # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_fast_input_ids      # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3426147\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 428272\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='428272' max='428272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [428272/428272 19:20:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8565</td>\n",
       "      <td>3.711200</td>\n",
       "      <td>4.343966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17130</td>\n",
       "      <td>3.175400</td>\n",
       "      <td>4.542943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25695</td>\n",
       "      <td>2.992300</td>\n",
       "      <td>4.525280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34260</td>\n",
       "      <td>2.870400</td>\n",
       "      <td>4.459577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42825</td>\n",
       "      <td>2.793900</td>\n",
       "      <td>4.499161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51390</td>\n",
       "      <td>2.730600</td>\n",
       "      <td>4.504906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59955</td>\n",
       "      <td>2.673600</td>\n",
       "      <td>4.586864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68520</td>\n",
       "      <td>2.636600</td>\n",
       "      <td>4.522192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77085</td>\n",
       "      <td>2.598500</td>\n",
       "      <td>4.449953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85650</td>\n",
       "      <td>2.566200</td>\n",
       "      <td>4.479713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94215</td>\n",
       "      <td>2.532600</td>\n",
       "      <td>4.496081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102780</td>\n",
       "      <td>2.506900</td>\n",
       "      <td>4.455515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111345</td>\n",
       "      <td>2.470300</td>\n",
       "      <td>4.515741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119910</td>\n",
       "      <td>2.442900</td>\n",
       "      <td>4.454218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128475</td>\n",
       "      <td>2.419200</td>\n",
       "      <td>4.462016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137040</td>\n",
       "      <td>2.399900</td>\n",
       "      <td>4.461525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145605</td>\n",
       "      <td>2.388600</td>\n",
       "      <td>4.421377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154170</td>\n",
       "      <td>2.360400</td>\n",
       "      <td>4.313177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162735</td>\n",
       "      <td>2.341700</td>\n",
       "      <td>4.351007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171300</td>\n",
       "      <td>2.330500</td>\n",
       "      <td>4.342056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179865</td>\n",
       "      <td>2.309900</td>\n",
       "      <td>4.294816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188430</td>\n",
       "      <td>2.292900</td>\n",
       "      <td>4.459564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196995</td>\n",
       "      <td>2.280900</td>\n",
       "      <td>4.359707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205560</td>\n",
       "      <td>2.264800</td>\n",
       "      <td>4.350204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214125</td>\n",
       "      <td>2.251300</td>\n",
       "      <td>4.349574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222690</td>\n",
       "      <td>2.220300</td>\n",
       "      <td>4.332328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231255</td>\n",
       "      <td>2.204700</td>\n",
       "      <td>4.331468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239820</td>\n",
       "      <td>2.188600</td>\n",
       "      <td>4.205513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248385</td>\n",
       "      <td>2.174300</td>\n",
       "      <td>4.273723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256950</td>\n",
       "      <td>2.168700</td>\n",
       "      <td>4.339061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265515</td>\n",
       "      <td>2.157400</td>\n",
       "      <td>4.258049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274080</td>\n",
       "      <td>2.138300</td>\n",
       "      <td>4.237289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282645</td>\n",
       "      <td>2.127400</td>\n",
       "      <td>4.175029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291210</td>\n",
       "      <td>2.121300</td>\n",
       "      <td>4.181995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299775</td>\n",
       "      <td>2.104900</td>\n",
       "      <td>4.244266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308340</td>\n",
       "      <td>2.097400</td>\n",
       "      <td>4.161159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316905</td>\n",
       "      <td>2.081900</td>\n",
       "      <td>4.086670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325470</td>\n",
       "      <td>2.064000</td>\n",
       "      <td>4.129324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334035</td>\n",
       "      <td>2.044400</td>\n",
       "      <td>4.228876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342600</td>\n",
       "      <td>2.040200</td>\n",
       "      <td>4.170143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351165</td>\n",
       "      <td>2.030500</td>\n",
       "      <td>4.154249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359730</td>\n",
       "      <td>2.020500</td>\n",
       "      <td>4.226148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368295</td>\n",
       "      <td>2.004100</td>\n",
       "      <td>4.083989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376860</td>\n",
       "      <td>1.994800</td>\n",
       "      <td>4.121178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385425</td>\n",
       "      <td>1.989500</td>\n",
       "      <td>4.087258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393990</td>\n",
       "      <td>1.979500</td>\n",
       "      <td>4.112712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402555</td>\n",
       "      <td>1.978100</td>\n",
       "      <td>4.059093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411120</td>\n",
       "      <td>1.967700</td>\n",
       "      <td>4.126269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419685</td>\n",
       "      <td>1.962500</td>\n",
       "      <td>4.145468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428250</td>\n",
       "      <td>1.956600</td>\n",
       "      <td>4.076495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-42826\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-42826/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-42826/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-85652\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-85652/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-85652/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-128478\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-128478/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-128478/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-171304\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-171304/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-171304/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-214130\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-214130/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-214130/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-256956\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-256956/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-256956/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-299782\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-299782/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-299782/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-342608\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-342608/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-342608/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-385434\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-385434/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-385434/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-428260\n",
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-428260/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab-check/checkpoint-428260/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=428272, training_loss=2.3231557280712587, metrics={'train_runtime': 69649.9576, 'train_samples_per_second': 196.764, 'train_steps_per_second': 6.149, 'total_flos': 4.218780309065447e+17, 'train_loss': 2.3231557280712587, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab/pytorch_model.bin\n",
      "tokenizer config file saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab/tokenizer_config.json\n",
      "Special tokens file saved in ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> save_model : ../../data11/model/distilbert/bert-re-kowiki-2-moco2-4-mecab/\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/distilbert/bert-re-kowiki-moco2-mecab-4/'\n",
    "\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8a66c-831a-44f4-a063-9932aeaf7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
