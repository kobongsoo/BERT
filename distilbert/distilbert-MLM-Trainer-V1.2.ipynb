{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf4d294-0c8c-4e2f-87ad-0f20da3baec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59d8373-51fc-4e45-bf92-fbf670af3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================================================================\n",
    "# Huggingface load_dataset 으로 MLM 훈련 하기\n",
    "#\n",
    "# => load_dataset 으로 wiki 말뭉치를 로딩하고, 이를 토크화 시키고, \n",
    "# input_ids 에 대해 15% 확률로 [MASK]를 씌워서, 실제 모델을 훈련시키는 예제 \n",
    "#\n",
    "# => MLM 훈련 말뭉치는 bongsoo/moco-corpus-kowiki202206 사용, 평가 말뭉치는 bongsoo/bongevalsmall 사용\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166817\n",
    "#=======================================================================================================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, DistilBertTokenizerFast, BertConfig, DistilBertForMaskedLM\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#%autosave 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4848b4-5068-43d4-8737-9419ec13fb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/distilbert-MLM-Trainer_2022-10-29.log\n"
     ]
    }
   ],
   "source": [
    "# 훈련시킬 말뭉치(사전 만들때 동일한 말뭉치 이용)\n",
    "#input_corpus = \"../../data11/my_corpus/my/pre-kowiki-20220620-1줄.txt\"\n",
    "#input_corpus = \"bongsoo/moco-corpus\"  # huggingface에 등록된 말뭉치 이용\n",
    "input_corpus = \"../../data11/my_corpus/re-kowiki-202206.txt\"  #re-kowiki-202206.txt, re-moco-corpus2.txt\n",
    "\n",
    "# eval 말뭉치 \n",
    "#eval_corpus = \"bongsoo/bongeval\"\n",
    "eval_corpus = \"bongsoo/moco_eval\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../data11/model/distilbert/distilbert-base-multilingual-cased\"\n",
    "#model_path = \"../../data11/model/distilbert/bert-re-moco-corpus2-mecab\"\n",
    "#model_path = \"../../data11/model/distilbert/bert-re-kowiki-moco2-mecab-4\"\n",
    "\n",
    "# 기존 사전 + 추가된 사전 파일\n",
    "vocab_path = \"../../data11/my_corpus/vocab2/bert-re-kowiki202206-mecab-vocab-20000\"\n",
    "\n",
    "# 중간 출력\n",
    "#OUTPATH = '../../data11/model/distilbert/bert-re-kowiki/'\n",
    "#OUTPATH = '../../data11/model/distilbert/bert-re-kowiki-nouns/'\n",
    "OUTPATH = '../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/'\n",
    "\n",
    "############################################################################\n",
    "# tokenizer 관련 hyper parameter 설정\n",
    "############################################################################\n",
    "batch_size = 32       # batch_size (32 이상이면 CUDA MEMORY 부족 함)\n",
    "token_max_len = 128   # token_seq_len\n",
    "epoch = 8             # epoch\n",
    "lr = 5e-5             # learning rate(기본:5e-5)\n",
    "seed = 111\n",
    "############################################################################\n",
    "\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"distilbert-MLM-Trainer\", logfilename=\"../../log/distilbert-MLM-Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d3d1638-7c83-46bd-bd34-c6d1382e8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data11/my_corpus/vocab2/bert-re-kowiki202206-mecab-vocab-20000 is_fast:True\n",
      "*special_token_size: 5, *tokenizer.vocab_size: 139547\n",
      "*vocab_size: 139548\n",
      "*tokenizer_len: 139547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(139547, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=139547, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "# tokenizer 생성\n",
    "# => BertTokenizer, BertTokenizerFast 둘중 사용하면됨\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(vocab_path, max_len=token_max_len, strip_accents=False, do_lower_case=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(vocab_path, max_len=token_max_len, do_lower_case=False)\n",
    "# fast 토크너나이즈인지 확인\n",
    "print(f'{vocab_path} is_fast:{tokenizer.is_fast}')\n",
    "\n",
    "# speical 토큰 계수 + vocab 계수 - 이미 vocab에 포함된 speical 토큰 계수(5)\n",
    "vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5 + 1\n",
    "#vocab_size = len(tokenizer.all_special_tokens) + tokenizer.vocab_size - 5\n",
    "print('*special_token_size: {}, *tokenizer.vocab_size: {}'.format(len(tokenizer.all_special_tokens), tokenizer.vocab_size))\n",
    "print('*vocab_size: {}'.format(vocab_size))\n",
    "print('*tokenizer_len: {}'.format(len(tokenizer)))\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "#config = BertConfig.from_pretrained(model_path)\n",
    "model = DistilBertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')    \n",
    "\n",
    "#################################################################################\n",
    "# 모델 embedding 사이즈를 tokenizer 크기 만큼 재 설정함.\n",
    "# 재설정하지 않으면, 다음과 같은 에러 발생함\n",
    "# CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)` CUDA 에러가 발생함\n",
    "#  indexSelectLargeIndex: block: [306,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
    "#\n",
    "#     해당 오류는 기존 Embedding(8002, 768, padding_idx=1) 처럼 입력 vocab 사이즈가 8002인데,\n",
    "#     0~8001 사이를 초과하는 word idx 값이 들어가면 에러 발생함.\n",
    "#################################################################################\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c678ff-b876-4432-91af-6ca2e11f70d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2734e23cb122ea6a\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/default-2734e23cb122ea6a/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d14065d6f54d1cac990ce2a9914182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bongsoo--moco_eval-9741016d12933a55\n",
      "Reusing dataset text (/MOCOMSYS/.cache/huggingface/datasets/text/bongsoo--moco_eval-9741016d12933a55/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85095c7050044447926c686437d4a411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3714432\n",
      "    })\n",
      "})\n",
      "['스포츠 팀 문화 활동 해외 여행 등의 다양한 과외활동을 제공한다', '남자 자유형 200m 종목의 예선기준 기록은 1분 47초 02이다', '1785년 정조 9 당시 성균관 대사성 민종현 이 왕명에 의해 편찬한 태학지 건치 에 반궁도 가 목판화로 있는데 본 계첩과 30여 년의 시간적 차이가 있어서 그간에 변화된 성균관의 건물을 비교해 볼 수 있다']\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset========================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n",
      "['필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', '외부시스템과 데이터를 주고 받기 위해 사용되는 디렉토리는 다음과 같다.', 'SOA는 비즈니스 프로세스를 기본적인 표준 빌딩 블럭 단위로 분할하여, 이를 IT 프로세스와 유연하게 일치시키는 특징이 있다.']\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================================\n",
    "# load_dataset을 이용하여, 훈련/평가 dataset 로딩.\n",
    "#\n",
    "# [로컬 데이터 파일 로딩]\n",
    "# => dataset = load_dataset(\"text\", data_files='로컬.txt')       # text 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.csv')        # csv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"csv\", data_files='로컬.tsv', delimiter=\"\\t\")  # tsv 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"json\", data_files='로컬.json')      # json 로컬 파일 로딩\n",
    "# => dataset = load_dataset(\"pandas\", data_files='로컬.pkl')     # pickled dataframe 로컬 파일 로딩\n",
    "#\n",
    "# [원격 데이터 파일 로딩]\n",
    "# url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "# data_files = {\n",
    "#    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "#    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "# }\n",
    "# squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "#\n",
    "# 출처 : https://wikidocs.net/166816\n",
    "#==================================================================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 훈련 말뭉치 로딩\n",
    "#train_dataset = load_dataset(input_corpus)\n",
    "train_dataset = load_dataset(\"text\", data_files=input_corpus) # text 로컬 파일 로딩\n",
    "\n",
    "# 평가 말뭉치 로딩\n",
    "eval_dataset = load_dataset(eval_corpus)\n",
    "\n",
    "# train_dataset 출력해봄\n",
    "print(f\"train_dataset=======================================\")\n",
    "print(train_dataset)\n",
    "print(train_dataset['train']['text'][0:3])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "# eval_dataset 출력해봄\n",
    "print(f\"eval_dataset========================================\")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset['test']['text'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca75da5-1048-4522-a13f-60d306741a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b7d06b4a0b45aebb33b6c8f9be3a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3715 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /MOCOMSYS/.cache/huggingface/datasets/text/bongsoo--moco_eval-9741016d12933a55/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4/cache-87caaf4e765ab88a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 18s, sys: 21min 59s, total: 33min 17s\n",
      "Wall time: 2min 21s\n",
      "CPU times: user 33.8 ms, sys: 5.22 ms, total: 39 ms\n",
      "Wall time: 38.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\\nprint(tokenized_dataset_fast['train']['text'][0:2])\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 처리\n",
    "def tokenizer_function(examples):\n",
    "    result =  tokenizer(examples['text'], truncation=True, max_length=token_max_len, return_overflowing_tokens=True)\n",
    "    \n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "# batched=True 하면 빠른 tokenizer 이용(Rust)\n",
    "%time train_dataset_fast = train_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "%time eval_dataset_fast = eval_dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "'''\n",
    "%time tokenized_dataset = text_dataset.map(tokenizer_function, batched=False)\n",
    "print(tokenized_dataset_fast['train']['text'][0:2])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa658e47-c0d5-47c6-a355-1097244ac3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3717139\n",
      "    })\n",
      "})\n",
      "*fast_len:3717139, len:3714432\n",
      "{'text': ['스포츠 팀 문화 활동 해외 여행 등의 다양한 과외활동을 제공한다', '남자 자유형 200m 종목의 예선기준 기록은 1분 47초 02이다'], 'input_ids': [[101, 120257, 9899, 119594, 119570, 120648, 120412, 28697, 53645, 8898, 78705, 119446, 69448, 119692, 14102, 102], [101, 76854, 132874, 10777, 10147, 120740, 10459, 120472, 12310, 54867, 119566, 10892, 122, 37712, 11413, 57030, 10983, 11925, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast=======================================\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3002\n",
      "    })\n",
      "})\n",
      "*fast_len:3002, len:3000\n",
      "{'text': ['필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', '외부시스템과 데이터를 주고 받기 위해 사용되는 디렉토리는 다음과 같다.'], 'input_ids': [[101, 119649, 9485, 122248, 119997, 11513, 120350, 12178, 120597, 121779, 108366, 10739, 119725, 10530, 120196, 14843, 9460, 13767, 143, 36966, 125736, 11882, 120733, 35465, 20309, 120551, 22766, 19709, 126329, 120565, 10530, 120196, 10622, 9247, 40410, 119, 102], [101, 120597, 121779, 108366, 11882, 119997, 11513, 129342, 9322, 12310, 19905, 119552, 24683, 126329, 120565, 11018, 39402, 53354, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_dataset_fast=======================================\")\n",
    "print(train_dataset_fast)\n",
    "print(f'*fast_len:{len(train_dataset_fast[\"train\"])}, len:{len(train_dataset[\"train\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(train_dataset_fast['train'][0:2])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast=======================================\")\n",
    "print(eval_dataset_fast)\n",
    "print(f'*fast_len:{len(eval_dataset_fast[\"test\"])}, len:{len(eval_dataset[\"test\"])}')  # fast_dataset과 dataset 길이를 비교함\n",
    "print(eval_dataset_fast['test'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651e30a6-0601-4272-bd40-0fdb9ce7be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_fast(MLM)=======================================\n",
      "tensor([   101,    103,   9899,    103, 119570, 120648, 120412,  28697,    103,\n",
      "          8898,  78705, 119446,  69448, 119692,  14102,    102,      0,      0,\n",
      "             0])\n",
      "{'text': '스포츠 팀 문화 활동 해외 여행 등의 다양한 과외활동을 제공한다', 'input_ids': [101, 120257, 9899, 119594, 119570, 120648, 120412, 28697, 53645, 8898, 78705, 119446, 69448, 119692, 14102, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "\n",
      "eval_dataset_fast(MLM)=======================================\n",
      "tensor([   101, 119649,   9485, 122248, 119997,  11513, 120350,    103, 120597,\n",
      "        121779, 108366,  10739, 119725,  10530, 120196,  14843,   9460,  13767,\n",
      "           143,    103, 125736,  11882, 120733,  35465,  20309, 120551,  22766,\n",
      "         19709, 126329, 120565,  10530, 120196,  10622,   9247,  40410,    119,\n",
      "           102])\n",
      "{'text': '필요 시 입력 데이터를 생성하는 외부시스템이 시스템에 접근할 수 있는 FTP 계정과 비밀번호 설정하며 다른 디렉토리에 접근을 막는다.', 'input_ids': [101, 119649, 9485, 122248, 119997, 11513, 120350, 12178, 120597, 121779, 108366, 10739, 119725, 10530, 120196, 14843, 9460, 13767, 143, 36966, 125736, 11882, 120733, 35465, 20309, 120551, 22766, 19709, 126329, 120565, 10530, 120196, 10622, 9247, 40410, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# MLM을 위한 DataCollatorForLangunageModeling 호출\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# input_ids에 대해 MLM 만들기\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# input_ids MLM 만들고 출력 해봄\n",
    "mlm_train_sample = data_collator(train_dataset_fast['train']['input_ids'][0:2])\n",
    "mlm_eval_sample = data_collator(eval_dataset_fast['test']['input_ids'][0:2])\n",
    "\n",
    "print(f\"train_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_train_sample['input_ids'][0])\n",
    "print(train_dataset_fast['train'][0])\n",
    "\n",
    "print(f'\\r\\n\\r\\n')\n",
    "\n",
    "print(f\"eval_dataset_fast(MLM)=======================================\")\n",
    "print(mlm_eval_sample['input_ids'][0])\n",
    "print(eval_dataset_fast['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1b230b-62af-4ee2-81a9-aaea54697a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*total_optim_steps: 929284, *eval_steps:18585, *logging_steps:18585, *save_steps:92928\n",
      "*no_cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "# 훈련 trainer 설정 \n",
    "# trainer \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "#########################################################################################\n",
    "# hyper parameter 설정\n",
    "#########################################################################################\n",
    "\n",
    "epochs = epoch          # epochs\n",
    "\n",
    "total_optim_steps = len(train_dataset_fast[\"train\"]) * epochs // batch_size   # 총 optimize(역전파) 스탭수 = 훈련dataset 계수 * epochs // 배치 크기\n",
    "eval_steps=int(total_optim_steps * 0.02)           # 평가 스탭수\n",
    "logging_steps=eval_steps                           # 로깅 스탭수(*평가스탭수 출력할때는 평가스탭수와 동일하게)\n",
    "save_steps=int(total_optim_steps * 0.1)            # 저장 스탭수 \n",
    "#save_total_limit=5                                # 마지막 5개 남기고 삭제 \n",
    "\n",
    "print(f'*total_optim_steps: {total_optim_steps}, *eval_steps:{eval_steps}, *logging_steps:{logging_steps}, *save_steps:{save_steps}')\n",
    "#########################################################################################\n",
    "\n",
    "# cpu 사용이면 'no_cuda = True' 설정함.\n",
    "no_cuda = False\n",
    "if device == 'cpu':\n",
    "    no_cuda = True\n",
    "print(f'*no_cuda: {no_cuda}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    no_cuda = no_cuda,                      # GPU 사용  안함\n",
    "    output_dir = OUTPATH,                   # 출력 모델 저장 경로 \n",
    "    overwrite_output_dir=True,         \n",
    "    num_train_epochs=epochs,                # 에폭\n",
    "    learning_rate=lr,                       # lr: 기본 5e-5\n",
    "    per_gpu_train_batch_size=batch_size,    # 배치 사이즈 \n",
    "    save_strategy=\"steps\",                  # 저장 전략 (no, epoch, steps 기본=steps) \n",
    "    save_steps=save_steps,                  # step 수마다 모델을 저장\n",
    "    #save_total_limit=save_total_limit,     # 마지막 x개 모델 빼고 과거 모델은 삭제\n",
    "    evaluation_strategy=\"steps\",            # 평가 전략 (no, epoch, steps 기본=no)  \n",
    "    eval_steps=eval_steps,                  # 평가할 스텝수\n",
    "    logging_steps=logging_steps             # 로깅할 스탭수\n",
    ")\n",
    "\n",
    "# trainer로 훈련할때는 [mask] 처리된 input_ids 만 dataset으로 넘겨주면 됨.\n",
    "train_dataset_fast_input_ids = train_dataset_fast['train']['input_ids']\n",
    "eval_dataset_fast_input_ids = eval_dataset_fast['test']['input_ids']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  #MLM(Masked Language Model)\n",
    "    train_dataset=train_dataset_fast_input_ids,   # 훈련 데이터셋\n",
    "    eval_dataset=eval_dataset_fast_input_ids      # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927efe3d-6472-4c37-825e-c14ef5ce2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3717139\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 929288\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='928925' max='929288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [928925/929288 29:27:22 < 00:41, 8.76 it/s, Epoch 8.00/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18585</td>\n",
       "      <td>4.354000</td>\n",
       "      <td>4.799882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37170</td>\n",
       "      <td>3.619100</td>\n",
       "      <td>4.605822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55755</td>\n",
       "      <td>3.392800</td>\n",
       "      <td>4.649975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74340</td>\n",
       "      <td>3.264800</td>\n",
       "      <td>4.578352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92925</td>\n",
       "      <td>3.178100</td>\n",
       "      <td>4.514098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111510</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>4.460934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130095</td>\n",
       "      <td>3.044800</td>\n",
       "      <td>4.420611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148680</td>\n",
       "      <td>3.004200</td>\n",
       "      <td>4.456695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167265</td>\n",
       "      <td>2.965700</td>\n",
       "      <td>4.421189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185850</td>\n",
       "      <td>2.931900</td>\n",
       "      <td>4.426220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204435</td>\n",
       "      <td>2.911300</td>\n",
       "      <td>4.427107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223020</td>\n",
       "      <td>2.882200</td>\n",
       "      <td>4.448413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241605</td>\n",
       "      <td>2.849700</td>\n",
       "      <td>4.377285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260190</td>\n",
       "      <td>2.822100</td>\n",
       "      <td>4.386744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278775</td>\n",
       "      <td>2.803600</td>\n",
       "      <td>4.378301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297360</td>\n",
       "      <td>2.787100</td>\n",
       "      <td>4.343366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315945</td>\n",
       "      <td>2.773500</td>\n",
       "      <td>4.294602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334530</td>\n",
       "      <td>2.752900</td>\n",
       "      <td>4.349833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353115</td>\n",
       "      <td>2.733100</td>\n",
       "      <td>4.290830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371700</td>\n",
       "      <td>2.710500</td>\n",
       "      <td>4.321643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390285</td>\n",
       "      <td>2.695000</td>\n",
       "      <td>4.285112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408870</td>\n",
       "      <td>2.680500</td>\n",
       "      <td>4.250584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427455</td>\n",
       "      <td>2.668300</td>\n",
       "      <td>4.288186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446040</td>\n",
       "      <td>2.655800</td>\n",
       "      <td>4.240317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464625</td>\n",
       "      <td>2.647300</td>\n",
       "      <td>4.263170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483210</td>\n",
       "      <td>2.610800</td>\n",
       "      <td>4.283512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501795</td>\n",
       "      <td>2.606200</td>\n",
       "      <td>4.221842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520380</td>\n",
       "      <td>2.597100</td>\n",
       "      <td>4.253551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538965</td>\n",
       "      <td>2.586500</td>\n",
       "      <td>4.145563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557550</td>\n",
       "      <td>2.573800</td>\n",
       "      <td>4.173849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576135</td>\n",
       "      <td>2.563600</td>\n",
       "      <td>4.189995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594720</td>\n",
       "      <td>2.540400</td>\n",
       "      <td>4.183137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613305</td>\n",
       "      <td>2.527600</td>\n",
       "      <td>4.185491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631890</td>\n",
       "      <td>2.521900</td>\n",
       "      <td>4.166626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650475</td>\n",
       "      <td>2.509700</td>\n",
       "      <td>4.161472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669060</td>\n",
       "      <td>2.501100</td>\n",
       "      <td>4.135896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687645</td>\n",
       "      <td>2.496700</td>\n",
       "      <td>4.248337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706230</td>\n",
       "      <td>2.474400</td>\n",
       "      <td>4.173281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724815</td>\n",
       "      <td>2.457900</td>\n",
       "      <td>4.133821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743400</td>\n",
       "      <td>2.450800</td>\n",
       "      <td>4.218086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761985</td>\n",
       "      <td>2.447300</td>\n",
       "      <td>4.119160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780570</td>\n",
       "      <td>2.435700</td>\n",
       "      <td>4.111846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799155</td>\n",
       "      <td>2.430300</td>\n",
       "      <td>4.065576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817740</td>\n",
       "      <td>2.424000</td>\n",
       "      <td>4.042101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836325</td>\n",
       "      <td>2.404300</td>\n",
       "      <td>4.103697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854910</td>\n",
       "      <td>2.402100</td>\n",
       "      <td>4.068314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873495</td>\n",
       "      <td>2.393400</td>\n",
       "      <td>4.000319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892080</td>\n",
       "      <td>2.387400</td>\n",
       "      <td>4.022682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910665</td>\n",
       "      <td>2.384400</td>\n",
       "      <td>4.016369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-92928\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-92928/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-92928/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-185856\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-185856/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-185856/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-278784\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-278784/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-278784/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-371712\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-371712/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-371712/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-464640\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-464640/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-464640/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-557568\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-557568/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-557568/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-650496\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-650496/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-650496/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-743424\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-743424/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-743424/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-836352\n",
      "Configuration saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-836352/config.json\n",
      "Model weights saved in ../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab-check/checkpoint-836352/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3002\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7243c-1c77-403c-bb71-281612b8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../../data11/model/distilbert/vocab2/bert-re-kowiki-bert-mecab/'\n",
    "\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8a66c-831a-44f4-a063-9932aeaf7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
