{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e57fb5-7735-44e1-a200-0caa3fa6c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-31.log\n",
      "logfilepath:qnadataset_2022-03-31.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:state_dict_2022-03-31.log\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================\n",
    "# Distillation 예제(증류)1\n",
    "#\n",
    "#: 교사 모델(BertModel) -> 학생모델(DistilBertModel) 로 distillation 하는 예시임\n",
    "# 여기서는 교사모델과 학생모델 Fine-tuning 하여 증류하는 예시임.\n",
    "# * 중요:교사모델이 더 잘 학습되어 있어야 하며, 교사/학생 모델이 tokenizer는 동일해야 한다.\n",
    "#\n",
    "# 자료 참고 \n",
    "# https://re-code-cord.tistory.com/entry/Knowledge-Distillation-1\n",
    "# https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f\n",
    "# https://www.philschmid.de/knowledge-distillation-bert-transformers\n",
    "# 소스 참고 \n",
    "#https://github.com/boostcampaitech2/model-optimization-level3-cv-04/blob/main/src/trainer.py\n",
    "#\n",
    "# [증류 과정]\n",
    "# 1. 교사모델 구조->학생 모델로 복사\n",
    "# => 교사모델이 bert-base 이고, 학생 모델이 distilbert 라면, 교사 bert 모델이 12개 hiddenlayer에\n",
    "# wegiht와 bias 값들을 학생모델 distilbert 6개 hiddenlayer로 복사함.\n",
    "# (* 이때 교사모델이 어떤 hiddenlayer를 학샘모델로 복사할때는 [0, 2, 4, 7, 9, 11] 식으로 레이어를 복사 하는데 좋다고 함)\n",
    "#\n",
    "# 2. 교사모델, 학생모델 fine-tuning 사전준비\n",
    "# => 각 교사, 학생모델을 classifcation이나 maksedlm 모델중 1나로 파인튜닝함\n",
    "# (*Huggingface transformers 모델이용하면 쉬움\n",
    "#\n",
    "# 3. loss 함수 정의\n",
    "# => loss 함수는 학생모델이 loss(1), 교사와 학생모델간 cross-entropy loss(2), 교사와 학생모델간 cosine-loss(3) \n",
    "# 3가지 인데, 이때 (2)와 (3) loss는 torch.nn.KLDivLoss 함수로 보통 대체 된다.\n",
    "# 즉 증류 손실함수 = alpha*학생모델이 loss + (1-alpah)*교사/학생모델간 torch.nn.KLDivLoss 함수\n",
    "#\n",
    "# 이때 KLDivLoss 함수는 교사와 학생간 Dark Knowledge(어둠지식)도 학습되도록 교사loss/Temperture와 학생loss/Temperture 식으로,\n",
    "# Temperture를 지정하는데, 보통 학습할때는 2~10으로 하고, 평가시에는 반드시 1로 해야 한다.\n",
    "# (Temperture==1 이면, softmax와 동일, 1보다 크면 확률이 평활화 되어서, 어둠 지식 습득이 많이됨)\n",
    "# 그리고 학생모델loss는 전체 loss에 0.1이 되도록 alpha값은 0.1이 좋다고 한다.\n",
    "#\n",
    "# 4. 학습\n",
    "# => 교사모델은 평가(eval)만 하고, 학생모델만 학습(train)한다.\n",
    "#\n",
    "#==================================================================================\n",
    "MaskedLM_FT = False         # MASKEDLM 파인 튜닝 적용하는 경우 \n",
    "Classification_FT = True  # Classifcation 파인 튜닝 적용하는 경우\n",
    "\n",
    "if MaskedLM_FT == True:\n",
    "    # maskedLM 으로 파인튜닝할 경우\n",
    "    from transformers import BertForMaskedLM, DistilBertForMaskedLM\n",
    "elif Classification_FT == True:\n",
    "    # sequenceclassificaiton으로 파인튜닝할 경우\n",
    "    from transformers import BertForSequenceClassification, DistilBertForSequenceClassification\n",
    "else:\n",
    "    assert MaskedLM_FT == True or Classification_FT == True, \"select fine-tuning model!\"\n",
    "\n",
    "#huggingface transformers 사용하다 보면 경고(warning)에러가 뜰때 안뜨도록 logging.set_verbosity_error() 추가함\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()  \n",
    "    \n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from myutils import seed_everything, mlogging, GPU_info\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "seed_everything(111)\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"state_dict\", logfilename=\"state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907a8b4a-a02c-4d2f-8738-796139a42e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias'])\n",
      "tensor([[ 0.0269, -0.0009, -0.0439,  ...,  0.0020, -0.0098,  0.0094],\n",
      "        [-0.0190, -0.0209, -0.0088,  ..., -0.0216, -0.0193, -0.0079],\n",
      "        [-0.0118, -0.0189, -0.0004,  ..., -0.0270, -0.0167, -0.0258],\n",
      "        ...,\n",
      "        [-0.0197, -0.0128, -0.0483,  ..., -0.0389, -0.0192, -0.0250],\n",
      "        [-0.0253, -0.0202, -0.0244,  ..., -0.0371, -0.0323, -0.0193],\n",
      "        [ 0.0077, -0.0302, -0.0408,  ..., -0.0224, -0.0575, -0.0220]])\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(167550, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 1. 교사모델 구조->학생 모델로 복사\n",
    "#\n",
    "# => bert 교사모델에서 state_dict 값을 distilbert 학생모델 state_dict 맞게 복사하여, student_state_dict을 만들고 나서,\n",
    "# 이를 학생 distilbertforMaskedLM에 적용함\n",
    "# ====================================================================\n",
    "# 2. 교사모델, 학생모델 fine-tuning 서전준비\n",
    "# => 여기서는 교사는 BertforMaskedLM 모델이고 학생은 DistilBertForMaskedLM모델로 지정해서 Fine-tuning 사전준비하\n",
    "# ====================================================================\n",
    "\n",
    "#출력 폴더\n",
    "OUTPATH = '../../model/distilbert/distilbert-0331-TS-nli-0.1-10/'\n",
    "\n",
    "# 교사 모델에서 state_dict 만 뽑아냄\n",
    "tearch_model_path='../../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-nouns-0327'\n",
    "\n",
    "if MaskedLM_FT == True:\n",
    "    tearch_model = BertForMaskedLM.from_pretrained(tearch_model_path, output_hidden_states=True)\n",
    "elif Classification_FT == True:\n",
    "    num_labels = 3\n",
    "    tearch_model = BertForSequenceClassification.from_pretrained(tearch_model_path, output_hidden_states=True, num_labels=num_labels)\n",
    "\n",
    "#state_dict 뽑아냄\n",
    "tearch_state_dict = tearch_model.state_dict()\n",
    "print(tearch_state_dict.keys())\n",
    "print(tearch_state_dict['bert.embeddings.word_embeddings.weight'])\n",
    "print(tearch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5719c22-a55c-480d-9d74-165d3bad8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172192514\n",
      "odict_keys(['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias'])\n",
      "tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  4.6951,   7.9118,  -2.2909,  ...,   1.2087,   5.5990,  -6.2665],\n",
      "        [  5.9853,  14.1727,  -0.7380,  ...,  11.4669,  11.1134,  -7.8186],\n",
      "        ...,\n",
      "        [  0.0751,  -9.8991,  -9.5650,  ...,   3.8927,   2.9925,  -6.4232],\n",
      "        [ 11.6854,   6.4417,  10.3435,  ...,  -1.5523,  15.6353,   1.3810],\n",
      "        [ -5.5161,  -4.8009,  -9.1509,  ...,  -2.9011,  -1.4914, -15.1198]])\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(167550, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 학생모델에 dict 표기해 봄\n",
    "student_model_path='../../model/distilbert/distilbert-0327-empty'\n",
    "\n",
    "if MaskedLM_FT == True:\n",
    "    student_model = DistilBertForMaskedLM.from_pretrained(student_model_path, output_hidden_states=True)\n",
    "elif Classification_FT == True:\n",
    "    student_model = DistilBertForSequenceClassification.from_pretrained(student_model_path, output_hidden_states=True, num_labels=2)\n",
    "\n",
    "print(student_model.num_parameters())\n",
    "#state_dict 뽑아냄\n",
    "student_state_dict = student_model.state_dict()\n",
    "print(student_state_dict.keys())\n",
    "print(student_state_dict['distilbert.embeddings.word_embeddings.weight'])\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51c6671-ab9d-4506-b873-fcde9d3a0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias'])\n"
     ]
    }
   ],
   "source": [
    "# 학생 state_dict 생성\n",
    "# ** 교사 state_dict를 학생 모델에 알맞게 복사하여 학생모델 state_dict 생성\n",
    "# => 교사모델에서 0, 2, 4, 7, 9, 11 값만 뽑아내서 distil_state_dict 만듬\n",
    "distil_sd = None\n",
    "#layers = [0, 2, 4, 7, 9, 11]\n",
    "sys.path.append('..')\n",
    "\n",
    "if MaskedLM_FT == True:\n",
    "    from myutils import make_sate_dict_bertMaskedLM_to_distillbertMaskedLM\n",
    "    distil_sd = make_sate_dict_bertMaskedLM_to_distillbertMaskedLM(tearch_model)\n",
    "    \n",
    "elif Classification_FT == True:\n",
    "    from myutils import make_sate_dict_bertSequenceClass_to_distillbertSequenceClass\n",
    "    distil_sd = make_sate_dict_bertSequenceClass_to_distillbertSequenceClass(tearch_model)\n",
    "\n",
    "print(distil_sd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9070c3-0072-47ef-b47e-6b382e332e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias'])\n",
      "tensor([[ 0.0269, -0.0009, -0.0439,  ...,  0.0020, -0.0098,  0.0094],\n",
      "        [-0.0190, -0.0209, -0.0088,  ..., -0.0216, -0.0193, -0.0079],\n",
      "        [-0.0118, -0.0189, -0.0004,  ..., -0.0270, -0.0167, -0.0258],\n",
      "        ...,\n",
      "        [-0.0197, -0.0128, -0.0483,  ..., -0.0389, -0.0192, -0.0250],\n",
      "        [-0.0253, -0.0202, -0.0244,  ..., -0.0371, -0.0323, -0.0193],\n",
      "        [ 0.0077, -0.0302, -0.0408,  ..., -0.0224, -0.0575, -0.0220]])\n"
     ]
    }
   ],
   "source": [
    "# 학생모델에 교사모델에서 뽑아낸 state_dict를 적용\n",
    "student_model_path='../../model/distilbert/distilbert-0327-empty'\n",
    "\n",
    "if MaskedLM_FT == True:\n",
    "    student_model = DistilBertForMaskedLM.from_pretrained(student_model_path, state_dict=distil_sd, output_hidden_states=True)\n",
    "elif Classification_FT == True:\n",
    "    student_model = DistilBertForSequenceClassification.from_pretrained(student_model_path, state_dict=distil_sd, output_hidden_states=True, num_labels=num_labels)\n",
    "\n",
    "#state_dict 뽑아냄\n",
    "student_state_dict = student_model.state_dict()\n",
    "print(student_state_dict.keys())\n",
    "print(student_state_dict['distilbert.embeddings.word_embeddings.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0baa055-f938-4dae-a0f6-106820e37591",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MaskedLM_FT == True:\n",
    "    # 학습할 mskedlm 데이터 로더 생성.\n",
    "    #\n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    from transformers import BertTokenizer\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "    from myutils import MLMDataset\n",
    "\n",
    "    batch_size = 8           # batch=32로 하면 CUDA MEMORY 오류 발생함\n",
    "    token_max_len = 128\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(student_model_path, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "    input_corpus = '../../korpora/kowiki_20190620/wiki_20190620_small.txt'\n",
    "\n",
    "    # 각 스페셜 tokenid를 구함\n",
    "    CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "    SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "    PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "    MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "    print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "\n",
    "    train_dataset = MLMDataset(corpus_path = input_corpus,\n",
    "                               tokenizer = tokenizer, \n",
    "                               CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                               SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                               UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                               PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                               Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                               max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                               mlm_probability=0.15,\n",
    "                               overwrite_cache=True\n",
    "                              )\n",
    "\n",
    "\n",
    "    # 학습 dataloader 생성\n",
    "    # => tenosor로 만듬\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              #shuffle=True, # dataset을 섞음\n",
    "                              sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                              num_workers=3\n",
    "                             )\n",
    "\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c72f58-cf03-410e-a715-a7e0aa646485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 18:37:19,125 - bwpdataset - INFO - Loading features from cached file ../../korpora/kornli/cached_BertTokenizer_128_multinli.train.ko.tsv [took 16.097 s]\n",
      "2022-03-31 18:37:19,279 - bwpdataset - INFO - Loading features from cached file ../../korpora/kornli/cached_BertTokenizer_128_xnli.test.ko.tsv [took 0.151 s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if Classification_FT == True:\n",
    "    # SequenceCalssification 데이터로더 생성\n",
    "\n",
    "    # 학습 data loader 생성\n",
    "    sys.path.append('..')\n",
    "    from myutils import ClassificationCSVCorpus, ClassificationDataset, data_collator, KlueNLICorpus,KorNLICorpus\n",
    "    from torch.utils.data import DataLoader, RandomSampler\n",
    "    from transformers import BertTokenizer\n",
    "\n",
    "    \n",
    "    #############################################################################\n",
    "    # 변수 설정\n",
    "    #############################################################################\n",
    "    max_seq_len = 128   # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "    batch_size = 32        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "\n",
    "    # 훈련할 csv 파일\n",
    "    #file_fpath = 'korpora/감성대화말뭉치/감성대화말뭉치(최종데이터)_renew_labelenc_Training.csv'\n",
    "    #file_fpath = '../../korpora/nsmc/ratings_test.txt'\n",
    "    #file_fpath = '../../korpora/klue-nli/klue-nli-v1.1_train.json'\n",
    "    file_fpath = '../../korpora/kornli/multinli.train.ko.tsv'\n",
    "    column_num = 3           # .csv 파일에 컬럼수(예: text, label만 있으면 =2)\n",
    "    csvfile = 0              # 0:tsv 파일, 1: csv 파일\n",
    "    label_list = [\"0\", \"1\"]  # .csv 파일에 레벨 목록( list로 입력해야 함)\n",
    "    #label_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]  # .csv 파일에 레벨 목록( list로 입력해야 함)\n",
    "    cache = False   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "    #############################################################################   \n",
    "\n",
    "    # 분류 corpus 파일 설정\n",
    "    #corpus = ClassificationCSVCorpus(column_num=column_num, iscsvfile=csvfile, label_list=label_list)\n",
    "    \n",
    "    # KlueNLI 혹은 KorNLI corpus 파일 생성\n",
    "    #corpus = KlueNLICorpus()\n",
    "    corpus = KorNLICorpus()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(tearch_model_path, max_len=max_seq_len, do_lower_case=False)\n",
    "\n",
    "    # 학습 dataset 생성\n",
    "    dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "    # 학습 dataloader 생성\n",
    "    train_loader = DataLoader(dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              #shuffle=True, # dataset을 섞음\n",
    "                              sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                              collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                              num_workers=4)\n",
    "    \n",
    "    # 평가 dataset 생성\n",
    "    #file_fpath = '../../korpora/klue-nli/klue-nli-v1.1_dev.json'\n",
    "    file_fpath = '../../korpora/kornli/xnli.test.ko.tsv'\n",
    "    dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "    # 평가 dataloader 생성\n",
    "    eval_loader = DataLoader(dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              #shuffle=True, # dataset을 섞음\n",
    "                              sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                              collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                              num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2aff12-9ea4-43f7-97fc-7c137935db3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 18:37:19,305 - state_dict - INFO - *total_steps: 61360, save_steps: 12272.0, p_itr: 3068.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4fe6a5d73a469fb4ae6030423543c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e16d8fb31f24a08a022d9cb562ea44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 18:46:17,853 - state_dict - INFO - [Epoch 1/5] Iteration 3068 -> Train Loss: 0.1098, student_loss: 1.0909, tearch_loss: 1.1001\n",
      "2022-03-31 18:55:16,293 - state_dict - INFO - [Epoch 1/5] Iteration 6136 -> Train Loss: 0.1082, student_loss: 1.0623, tearch_loss: 1.1002\n",
      "2022-03-31 19:04:07,766 - state_dict - INFO - [Epoch 1/5] Iteration 9204 -> Train Loss: 0.1075, student_loss: 1.0492, tearch_loss: 1.0999\n",
      "2022-03-31 19:12:59,615 - state_dict - INFO - [Epoch 1/5] Iteration 12272 -> Train Loss: 0.1070, student_loss: 1.0412, tearch_loss: 1.1000\n",
      "2022-03-31 19:13:01,189 - state_dict - INFO - Iteration 12272 -> save model:../../model/distilbert/distilbert-0331-TS-nli-0.1-10/12272\n",
      "2022-03-31 19:13:01,461 - state_dict - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e3a27a1e884df7972a6aa1f812f549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133995/930023384.py:185: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-03-31 19:13:05,429 - state_dict - INFO - [Epoch 1/5] Validatation Accuracy:0.680439121756487\n",
      "2022-03-31 19:13:05,431 - state_dict - INFO - ---------------------------------------------------------\n",
      "2022-03-31 19:13:05,432 - state_dict - INFO - === 처리시간: 3.970 초 ===\n",
      "2022-03-31 19:13:05,433 - state_dict - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ff047237854314a95137d474ee390d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 19:22:02,248 - state_dict - INFO - [Epoch 2/5] Iteration 15340 -> Train Loss: 0.1064, student_loss: 1.0295, tearch_loss: 1.1000\n",
      "2022-03-31 19:30:39,494 - state_dict - INFO - [Epoch 2/5] Iteration 18408 -> Train Loss: 0.1063, student_loss: 1.0272, tearch_loss: 1.1000\n",
      "2022-03-31 19:39:09,601 - state_dict - INFO - [Epoch 2/5] Iteration 21476 -> Train Loss: 0.1061, student_loss: 1.0251, tearch_loss: 1.0999\n",
      "2022-03-31 19:47:36,818 - state_dict - INFO - [Epoch 2/5] Iteration 24544 -> Train Loss: 0.1061, student_loss: 1.0236, tearch_loss: 1.1002\n",
      "2022-03-31 19:47:38,306 - state_dict - INFO - Iteration 24544 -> save model:../../model/distilbert/distilbert-0331-TS-nli-0.1-10/24544\n",
      "2022-03-31 19:47:38,616 - state_dict - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2129bdb608dd445aa7ccfdc975273518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 19:47:42,596 - state_dict - INFO - [Epoch 2/5] Validatation Accuracy:0.7163672654690619\n",
      "2022-03-31 19:47:42,599 - state_dict - INFO - ---------------------------------------------------------\n",
      "2022-03-31 19:47:42,600 - state_dict - INFO - === 처리시간: 3.984 초 ===\n",
      "2022-03-31 19:47:42,601 - state_dict - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a456e76b7d4e48be809f0543aff70ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 19:56:33,571 - state_dict - INFO - [Epoch 3/5] Iteration 27612 -> Train Loss: 0.1052, student_loss: 1.0085, tearch_loss: 1.0999\n",
      "2022-03-31 20:05:26,688 - state_dict - INFO - [Epoch 3/5] Iteration 30680 -> Train Loss: 0.1052, student_loss: 1.0086, tearch_loss: 1.0999\n",
      "2022-03-31 20:14:17,318 - state_dict - INFO - [Epoch 3/5] Iteration 33748 -> Train Loss: 0.1052, student_loss: 1.0078, tearch_loss: 1.1001\n",
      "2022-03-31 20:23:04,171 - state_dict - INFO - [Epoch 3/5] Iteration 36816 -> Train Loss: 0.1052, student_loss: 1.0079, tearch_loss: 1.1002\n",
      "2022-03-31 20:23:05,723 - state_dict - INFO - Iteration 36816 -> save model:../../model/distilbert/distilbert-0331-TS-nli-0.1-10/36816\n",
      "2022-03-31 20:23:06,019 - state_dict - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed606fc588e24b6a93998e5db6c56ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:23:09,951 - state_dict - INFO - [Epoch 3/5] Validatation Accuracy:0.7285429141716567\n",
      "2022-03-31 20:23:09,952 - state_dict - INFO - ---------------------------------------------------------\n",
      "2022-03-31 20:23:09,953 - state_dict - INFO - === 처리시간: 3.934 초 ===\n",
      "2022-03-31 20:23:09,954 - state_dict - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff9a9da25004eb1b5205aa9f68b5d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:31:57,190 - state_dict - INFO - [Epoch 4/5] Iteration 39884 -> Train Loss: 0.1045, student_loss: 0.9939, tearch_loss: 1.1001\n",
      "2022-03-31 20:40:46,149 - state_dict - INFO - [Epoch 4/5] Iteration 42952 -> Train Loss: 0.1045, student_loss: 0.9939, tearch_loss: 1.1001\n",
      "2022-03-31 20:49:44,215 - state_dict - INFO - [Epoch 4/5] Iteration 46020 -> Train Loss: 0.1044, student_loss: 0.9926, tearch_loss: 1.0996\n",
      "2022-03-31 20:58:32,879 - state_dict - INFO - [Epoch 4/5] Iteration 49088 -> Train Loss: 0.1045, student_loss: 0.9939, tearch_loss: 1.1003\n",
      "2022-03-31 20:58:34,441 - state_dict - INFO - Iteration 49088 -> save model:../../model/distilbert/distilbert-0331-TS-nli-0.1-10/49088\n",
      "2022-03-31 20:58:34,769 - state_dict - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6105783ffd0f4fbcbbee7b7655d0b1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:58:38,708 - state_dict - INFO - [Epoch 4/5] Validatation Accuracy:0.7335329341317365\n",
      "2022-03-31 20:58:38,710 - state_dict - INFO - ---------------------------------------------------------\n",
      "2022-03-31 20:58:38,710 - state_dict - INFO - === 처리시간: 3.942 초 ===\n",
      "2022-03-31 20:58:38,711 - state_dict - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d60280ba6274655bd91c2d65e97697f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 21:07:33,812 - state_dict - INFO - [Epoch 5/5] Iteration 52156 -> Train Loss: 0.1039, student_loss: 0.9827, tearch_loss: 1.1001\n",
      "2022-03-31 21:16:22,097 - state_dict - INFO - [Epoch 5/5] Iteration 55224 -> Train Loss: 0.1039, student_loss: 0.9830, tearch_loss: 1.0999\n",
      "2022-03-31 21:25:08,563 - state_dict - INFO - [Epoch 5/5] Iteration 58292 -> Train Loss: 0.1039, student_loss: 0.9829, tearch_loss: 1.1001\n",
      "2022-03-31 21:33:55,229 - state_dict - INFO - [Epoch 5/5] Iteration 61360 -> Train Loss: 0.1038, student_loss: 0.9825, tearch_loss: 1.0999\n",
      "2022-03-31 21:33:56,750 - state_dict - INFO - Iteration 61360 -> save model:../../model/distilbert/distilbert-0331-TS-nli-0.1-10/61360\n",
      "2022-03-31 21:33:57,036 - state_dict - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c944b5e6ee4dd6a4dd82a7142f1b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 21:34:01,016 - state_dict - INFO - [Epoch 5/5] Validatation Accuracy:0.7321357285429142\n",
      "2022-03-31 21:34:01,017 - state_dict - INFO - ---------------------------------------------------------\n",
      "2022-03-31 21:34:01,018 - state_dict - INFO - === 처리시간: 3.983 초 ===\n",
      "2022-03-31 21:34:01,019 - state_dict - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 4. 학습\n",
    "# => 교사모델은 평가(eval)만 하고, 학생모델만 학습(train)한다.\n",
    "# ====================================================================\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('..')\n",
    "from myutils import knowledge_distillation_loss1\n",
    "\n",
    "# 훈련 시작\n",
    "# 학생만 훈련시킴. 교사는 eval()\n",
    "##################################################\n",
    "epochs = 5           # epochs\n",
    "learning_rate = 2e-5  # 학습률\n",
    "#p_itr = 2000           # 손실률 보여줄 step 수\n",
    "#save_steps = 50000     # 50000 step마다 모델 저장\n",
    "\n",
    "# ==증류(distillation)과 연관된 변수 ==\n",
    "\n",
    "# 0.1이면 학생손실은 10%반영하고 Kld 손실(distillationloss)은 90% 반영하겠다는 의미\n",
    "alpha = 0.1   \n",
    "#alpha = 0.5   \n",
    "# 1이면 softmax 확률이고, 1보다 크면 softmax 확률이 평할화 되면서, ghkr어둠지식(Dark Knowledge)을 보다 많이 습득하게됨\n",
    "Temperture = 10   \n",
    "#Temperture = 5\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용=> 학생모델\n",
    "optimizer = AdamW(student_model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "save_steps = total_steps * 0.2       # 모델 저장할 step\n",
    "p_itr = total_steps * 0.05           # 손실률 보여줄 step 수\n",
    "\n",
    "logger.info('*total_steps: {}, save_steps: {}, p_itr: {}'.format(total_steps, save_steps, p_itr))\n",
    "    \n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "student_model.to(device)\n",
    "tearch_model.to(device)\n",
    "\n",
    "student_model.zero_grad()# 학생모델 초기화\n",
    "tearch_model.eval() # 교사모델은 평가모델로 설정.\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_student_loss = 0\n",
    "total_tearch_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    student_model.train() # 학생모델은 훈련모드로 변환\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        student_model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "     \n",
    "        # 교사모델 실행\n",
    "        tearch_outputs = tearch_model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    token_type_ids=token_type_ids,\n",
    "                                    labels=labels)\n",
    "        tearch_loss = tearch_outputs.loss\n",
    "        tearch_logits = tearch_outputs.logits\n",
    "        \n",
    "        # 학생모델 실행\n",
    "        student_outputs = student_model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        student_loss = student_outputs.loss\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # 총 손실 구함.\n",
    "        loss = knowledge_distillation_loss1(student_loss = student_loss, \n",
    "                                 student_logits = student_logits, \n",
    "                                 teacher_logits = tearch_logits, \n",
    "                                 alpha = alpha, \n",
    "                                 Temperture = Temperture)\n",
    "\n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)   \n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "          # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도 계산 \n",
    "            total_loss += loss.item()\n",
    "            total_student_loss += student_loss.item()\n",
    "            total_tearch_loss += tearch_loss.item()\n",
    "            \n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, student_loss: {:.4f}, tearch_loss: {:.4f}'\n",
    "                            .format(epoch+1, epochs, itr, total_loss/p_itr, total_student_loss/p_itr, total_tearch_loss/p_itr))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_student_loss = 0\n",
    "                total_tearch_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "            \n",
    "            if itr % save_steps == 0:\n",
    "                #전체모델 저장\n",
    "                TMP_OUT_PATH = OUTPATH + str(itr)\n",
    "                os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "                # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "                student_model.save_pretrained(TMP_OUT_PATH)\n",
    "                #torch.save(model, TMP_OUT_PATH + 'pytorch_model.bin') \n",
    "\n",
    "                # tokeinizer 파일 저장(vocab)\n",
    "                VOCAB_PATH = TMP_OUT_PATH\n",
    "                #os.makedirs(VOCAB_PATH)\n",
    "                tokenizer.save_pretrained(VOCAB_PATH)\n",
    "                \n",
    "                logger.info('Iteration {} -> save model:{}'.format(itr, TMP_OUT_PATH))\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "    # 평가 시작\n",
    "    student_model.eval()\n",
    "\n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "\n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        #token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            outputs = student_model(input_ids=input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                #token_type_ids=token_type_ids,\n",
    "                                labels=labels)\n",
    "\n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            #loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # 총 손실류 구함\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_test_correct += correct.sum().item()\n",
    "            total_test_len += len(labels)\n",
    "\n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(f'-END-\\n')\n",
    "    ####################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effa82e3-16a8-42e1-9e74-b255a0babe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../model/distilbert/distilbert-0331-TS-nli-0.1-10/tokenizer_config.json',\n",
       " '../../model/distilbert/distilbert-0331-TS-nli-0.1-10/special_tokens_map.json',\n",
       " '../../model/distilbert/distilbert-0331-TS-nli-0.1-10/vocab.txt',\n",
       " '../../model/distilbert/distilbert-0331-TS-nli-0.1-10/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student 모델 저장\n",
    "#OUTPATH = '../../model/distilbert/distilbert-0327-TS-nli'\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "student_model.save_pretrained(OUTPATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f7e2c-574c-4a0f-9e9c-95ab107e39c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
