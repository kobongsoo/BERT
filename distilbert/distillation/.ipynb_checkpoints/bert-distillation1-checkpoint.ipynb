{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e57fb5-7735-44e1-a200-0caa3fa6c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-18.log\n",
      "logfilepath:qnadataset_2022-03-18.log\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================\n",
    "# Distillation 예제(증류)\n",
    "#\n",
    "#: 교사 모델(BertModel) -> 학생모델(DistilBertModel) 로 distillation 하는 예시임\n",
    "# 여기서는 훈련은 하지 않고, 교사모델 레이어들의 weigth와 bias들을 불러와서, \n",
    "# 학생모델 레이어에 적용하는 예시\n",
    "#\n",
    "# - state_dict를 이용하여, 필요한 교사 레이어들의 weigth, bias을 학생 레이어로 복사함\n",
    "# - *make_sate_dict_bertmodel_to_distillbertmodel 함수 참고, \n",
    "# 해당 함수는 교사가 bertmodel이고 학생이 dstilbertmodel인 경우에만 사용 가능.\n",
    "# 교사/학생 모델 이 bertformaskedlm 식으로 변경되는 경우에는 state_dict 에 keys를 확인해서 수정해야함.\n",
    "#\n",
    "#==================================================================================\n",
    "\n",
    "from transformers import BertModel, DistilBertModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything\n",
    "\n",
    "seed_everything(111)\n",
    "#logging 설정\n",
    "#logger =  mlogging(loggername=\"state_dict\", logfilname=\"state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907a8b4a-a02c-4d2f-8738-796139a42e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/bert/bmc_fpt_kowiki20200920.train_model_0225 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/bert/bmc_fpt_kowiki20200920.train_model_0225 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embeddings.position_ids', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n",
      "tensor([[-0.0365, -0.0847, -0.0117,  ..., -0.0047, -0.0148, -0.0068],\n",
      "        [-0.0457, -0.0858,  0.0144,  ..., -0.0060, -0.0010,  0.0038],\n",
      "        [ 0.0020, -0.0409, -0.0004,  ..., -0.0087, -0.0033, -0.0310],\n",
      "        ...,\n",
      "        [-0.0030, -0.0507,  0.0018,  ..., -0.0119, -0.0406, -0.0044],\n",
      "        [-0.0710, -0.0217,  0.0012,  ..., -0.0613, -0.0435, -0.0156],\n",
      "        [-0.0259, -0.0526, -0.0454,  ..., -0.0221, -0.0768, -0.0295]])\n"
     ]
    }
   ],
   "source": [
    "# 교사 모델에서 state_dict 만 뽑아냄\n",
    "tearch_model_path='../model/bert/bmc_fpt_kowiki20200920.train_model_0225'\n",
    "tearch_model = BertModel.from_pretrained(tearch_model_path, output_hidden_states=True)\n",
    "\n",
    "#state_dict 뽑아냄\n",
    "tearch_state_dict = tearch_model.state_dict()\n",
    "print(tearch_state_dict.keys())\n",
    "print(tearch_state_dict['embeddings.word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fea3ae-2aa1-4649-bb6c-847337f87599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel 교사 state_dict 을 -> DistilBertMode 학생 state_dict로 복사\n",
    "def make_sate_dict_bertmodel_to_distillbertmodel(\n",
    "        teacher_model,\n",
    "        layers: list,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extracts state dict from teacher model\n",
    "\n",
    "        Args:\n",
    "            teacher_model: model to extract\n",
    "            layers: layers indexes to initialize\n",
    "            prefix_teacher: name of the teacher model\n",
    "            prefix_student: name of the student model\n",
    "        \"\"\"\n",
    "        state_dict = teacher_model.state_dict()\n",
    "        compressed_sd = {}\n",
    "\n",
    "        # extract embeddings\n",
    "        # 입베딩 파라메터 복사\n",
    "        for w in [\"word_embeddings\", \"position_embeddings\"]:\n",
    "            compressed_sd[\n",
    "                f\"embeddings.{w}.weight\"\n",
    "            ] = state_dict[f\"embeddings.{w}.weight\"]\n",
    "        for w in [\"weight\", \"bias\"]:\n",
    "            compressed_sd[\n",
    "                f\"embeddings.LayerNorm.{w}\"\n",
    "            ] = state_dict[f\"embeddings.LayerNorm.{w}\"]\n",
    "            \n",
    "        # 레이어 파라메터 복사\n",
    "        # extract encoder\n",
    "        std_idx = 0\n",
    "        for teacher_idx in layers:\n",
    "            for w in [\"weight\", \"bias\"]:\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.q_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.self.query.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.k_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.self.key.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.v_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.self.value.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.attention.out_lin.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.output.dense.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.sa_layer_norm.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.attention.output.LayerNorm.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.ffn.lin1.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.intermediate.dense.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.ffn.lin2.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.output.dense.{w}\"  # noqa: E501\n",
    "                ]\n",
    "                compressed_sd[\n",
    "                    f\"transformer.layer.{std_idx}.output_layer_norm.{w}\"  # noqa: E501\n",
    "                ] = state_dict[\n",
    "                    f\"encoder.layer.{teacher_idx}.output.LayerNorm.{w}\"  # noqa: E501\n",
    "                ]\n",
    "\n",
    "            std_idx += 1\n",
    "\n",
    "        return compressed_sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51c6671-ab9d-4506-b873-fcde9d3a0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_sd = None\n",
    "layers = [0, 2, 4, 7, 9, 11]\n",
    "distil_sd = make_sate_dict_bertmodel_to_distillbertmodel(tearch_model, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366c9ea5-f8ab-4241-aee6-aaf687f7f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(distil_sd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5719c22-a55c-480d-9d74-165d3bad8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias'])\n",
      "tensor([[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [ 11.5033,  17.0256,  -0.4811,  ..., -17.6260,  13.6267,  12.7463],\n",
      "        [ -0.6367,  -8.8645,  12.1133,  ...,  -3.1399,  -4.9409,  11.3151],\n",
      "        ...,\n",
      "        [ -8.5191,  -6.5417,   8.7165,  ...,   0.3337, -22.0482, -10.7737],\n",
      "        [  6.7987, -13.2880,   1.9351,  ...,  -7.0658, -20.5492,  14.0916],\n",
      "        [  0.5617,   0.4938,   9.6365,  ...,   5.2563,  -2.4311,  10.0079]])\n"
     ]
    }
   ],
   "source": [
    "# 학생모델\n",
    "student_model_path='../model/distilbert/distilbert-0318-10'\n",
    "student_model = DistilBertModel.from_pretrained(student_model_path, output_hidden_states=True)\n",
    "\n",
    "#state_dict 뽑아냄\n",
    "student_state_dict = student_model.state_dict()\n",
    "print(student_state_dict.keys())\n",
    "print(student_state_dict['embeddings.word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9070c3-0072-47ef-b47e-6b382e332e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias'])\n",
      "tensor([[-0.0365, -0.0847, -0.0117,  ..., -0.0047, -0.0148, -0.0068],\n",
      "        [-0.0457, -0.0858,  0.0144,  ..., -0.0060, -0.0010,  0.0038],\n",
      "        [ 0.0020, -0.0409, -0.0004,  ..., -0.0087, -0.0033, -0.0310],\n",
      "        ...,\n",
      "        [-0.0030, -0.0507,  0.0018,  ..., -0.0119, -0.0406, -0.0044],\n",
      "        [-0.0710, -0.0217,  0.0012,  ..., -0.0613, -0.0435, -0.0156],\n",
      "        [-0.0259, -0.0526, -0.0454,  ..., -0.0221, -0.0768, -0.0295]])\n"
     ]
    }
   ],
   "source": [
    "# 학생모델\n",
    "student_model_path='../model/distilbert/distilbert-0318-10'\n",
    "student_model = DistilBertModel.from_pretrained(student_model_path, state_dict=distil_sd, output_hidden_states=True)\n",
    "\n",
    "#state_dict 뽑아냄\n",
    "student_state_dict = student_model.state_dict()\n",
    "print(student_state_dict.keys())\n",
    "print(student_state_dict['embeddings.word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b310b63-321d-4c38-b73d-27996aa763de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "import os\n",
    "OUTPATH = '../model/distilbert/distilbert-0318-ts2'\n",
    "\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "student_model.save_pretrained(OUTPATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
