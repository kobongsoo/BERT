{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7f205-d547-4333-b598-55821e45ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================\n",
    "# Distillation 예제(증류)2\n",
    "#\n",
    "#: 교사 모델(BertModel:12개 hiddenlayer) -> 학생모델(BertModel:6개 hiddenlayer) 로 distillation 하는 예시임\n",
    "# 여기서는 교사모델의 12개 hiddenlayer를 6개의 hiddenlayer로 추출한 후, 학생모델을 만들어, 증류하는 예시이다.\n",
    "# * 중요:교사모델이 더 잘 학습되어 있어야 하며, 교사/학생 모델이 tokenizer는 동일해야 한다.\n",
    "#\n",
    "# 자료 참고 \n",
    "# https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f\n",
    "# https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a\n",
    "#\n",
    "# [증류 과정]\n",
    "# 1. 교사모델 구조->학생모델 생성\n",
    "# => 교사모델이 bert-base 이면, 12개 hiddenlayer에서 6개 hiddenlayer를 추출하여, 학생모델을 만듬\n",
    "#\n",
    "# 2. 교사모델, 학생모델 fine-tuning 사전준비\n",
    "# => 각 교사, 학생모델을 classifcation이나 maksedlm 모델로 파인튜닝함\n",
    "# (*Huggingface transformers 모델이용하면 쉬움\n",
    "#\n",
    "# 3. loss 함수 정의\n",
    "# => loss 함수는 학생모델이 loss(1), 교사와 학생모델간 cross-entropy loss(2), 교사와 학생모델간 cosine-loss(3) \n",
    "# 3가지 인데, 이때 (2)와 (3) loss는 torch.nn.KLDivLoss 함수로 보통 대체 된다.\n",
    "# 즉 증류 손실함수 = alpha*학생모델이 loss + (1-alpah)*교사/학생모델간 torch.nn.KLDivLoss 함수\n",
    "#\n",
    "# 이때 KLDivLoss 함수는 교사와 학생간 Dark Knowledge(어둠지식)도 학습되도록 교사loss/Temperture와 학생loss/Temperture 식으로,\n",
    "# Temperture를 지정하는데, 보통 학습할때는 2~10으로 하고, 평가시에는 반드시 1로 해야 한다.\n",
    "# (Temperture==1 이면, softmax와 동일, 1보다 크면 확률이 평활화 되어서, 어둠 지식 습득이 많이됨)\n",
    "# 그리고 학생모델loss는 전체 loss에 0.1이 되도록 alpha값은 0.1이 좋다고 한다.\n",
    "#\n",
    "# 4. 학습\n",
    "# => 교사모델은 평가(eval)만 하고, 학생모델만 학습(train)한다.\n",
    "#\n",
    "#==================================================================================\n",
    "'''\n",
    "# 교사모델의 구조를 트리형태로 한번 출력해봄\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import visualize_bertmodel_tree\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "bert = AutoModelForMaskedLM.from_pretrained('../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321')\n",
    "\n",
    "visualize_bertmodel_tree(bert)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8092e8-ca88-43ad-8023-075fcd85806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#========================================================================================================\n",
    "# 1.교사 모델 설정\n",
    "#========================================================================================================\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "tearch_model_path='../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321'\n",
    "#tearch_model = BertForMaskedLM.from_pretrained(tearch_model_path, output_hidden_states=True)\n",
    "tearch_model = BertForSequenceClassification.from_pretrained(tearch_model_path, output_hidden_states=True, num_labels=2)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(tearch_model_path, do_lower_case=False, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0804d8e-fa5c-4caa-814a-20c3b5be1e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return_dict': True, 'output_hidden_states': True, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'architectures': ['BertForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321', 'transformers_version': '4.15.0', 'directionality': 'bidi', 'model_type': 'bert', 'pooler_fc_size': 768, 'pooler_num_attention_heads': 12, 'pooler_num_fc_layers': 3, 'pooler_size_per_head': 128, 'pooler_type': 'first_token_transform', 'vocab_size': 143773, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None}\n"
     ]
    }
   ],
   "source": [
    "# 교사모델의 configuration을 출력해 봄\n",
    "configuration = tearch_model.config.to_dict()\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5393ad-63e7-45dc-ad3d-75d6020fc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-22.log\n",
      "logfilepath:qnadataset_2022-03-22.log\n"
     ]
    }
   ],
   "source": [
    "#========================================================================================================\n",
    "# 2.distil 학생 모델 생성\n",
    "#========================================================================================================\n",
    "\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import bertdistillation\n",
    "\n",
    "# 교사모델에서 12개의 hiddenlayer를 추출하여 6개의 hideenlayer를 가지는 학생모델 만듬\n",
    "distilbert = bertdistillation(tearch_model)\n",
    "student_model = distilbert.make_studentbert()\n",
    "\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e874b1-3193-4465-a0a1-cdd23202f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tearch_model: ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321\n",
      "\n",
      "odict_keys(['bert.embeddings.position_ids', 'bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias'])\n",
      "-------------------------------------------------------------------\n",
      "tensor([[ 0.0008, -0.0201, -0.0530,  ...,  0.0295, -0.0012, -0.0110],\n",
      "        [-0.0121, -0.0388, -0.0302,  ..., -0.0108, -0.0142, -0.0313],\n",
      "        [-0.0097, -0.0347, -0.0290,  ..., -0.0196, -0.0122, -0.0362],\n",
      "        ...,\n",
      "        [ 0.0272,  0.0183,  0.0005,  ..., -0.0021, -0.0250, -0.0019],\n",
      "        [-0.0040,  0.0264,  0.0205,  ..., -0.0673, -0.0099,  0.0068],\n",
      "        [-0.0162, -0.0039, -0.0164,  ..., -0.0095, -0.0043, -0.0273]])\n",
      "-------------------------------------------------------------------\n",
      "student_model\n",
      "tensor([[ 0.0008, -0.0201, -0.0530,  ...,  0.0295, -0.0012, -0.0110],\n",
      "        [-0.0121, -0.0388, -0.0302,  ..., -0.0108, -0.0142, -0.0313],\n",
      "        [-0.0097, -0.0347, -0.0290,  ..., -0.0196, -0.0122, -0.0362],\n",
      "        ...,\n",
      "        [ 0.0272,  0.0183,  0.0005,  ..., -0.0021, -0.0250, -0.0019],\n",
      "        [-0.0040,  0.0264,  0.0205,  ..., -0.0673, -0.0099,  0.0068],\n",
      "        [-0.0162, -0.0039, -0.0164,  ..., -0.0095, -0.0043, -0.0273]])\n",
      "\n",
      "==유사도 : tensor([[1.0000]])==\n"
     ]
    }
   ],
   "source": [
    "# 만들어진 학생모델과 교사모델이 동일한 state_dict을 가지는지 한번 확인해 봄\n",
    "from os import sys\n",
    "sys.path.append('..')\n",
    "from myutils import pytorch_cos_sim\n",
    "\n",
    "# 학생모델이 교사 모델과 임베딩 값 복사가 제대로 되었는지 확인 \n",
    "print(f'tearch_model: {tearch_model_path}\\n')\n",
    "\n",
    "tearch_state_dict = tearch_model.state_dict()\n",
    "print(tearch_state_dict.keys())\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "tearch_embedding = tearch_state_dict['bert.embeddings.word_embeddings.weight']\n",
    "print(tearch_embedding)\n",
    "\n",
    "print('-------------------------------------------------------------------')\n",
    "print(f'student_model')\n",
    "student_state_dict = student_model.state_dict()\n",
    "#print(student_state_dict.keys())\n",
    "student_embedding = student_state_dict['bert.embeddings.word_embeddings.weight']\n",
    "print(student_embedding)\n",
    "\n",
    "# 코사인 유사도로 확인 \n",
    "simility = pytorch_cos_sim(tearch_embedding[100], student_embedding[100])\n",
    "print(f'\\n==유사도 : {simility}==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939be8a5-2d4b-4130-848a-abdabc55952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습할 mskedlm 데이터 로더 생성.\n",
    "#\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import MLMDataset\n",
    "\n",
    "batch_size = 16           # batch=32로 하면 CUDA MEMORY 오류 발생함\n",
    "token_max_len = 128\n",
    "\n",
    "input_corpus = '../korpora/kowiki_20190620/wiki_20190620_small.txt'\n",
    "\n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "\n",
    "train_dataset = MLMDataset(corpus_path = input_corpus,\n",
    "                           tokenizer = tokenizer, \n",
    "                           CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                           SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                           UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                           PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                           Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                           max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                           mlm_probability=0.15,\n",
    "                           overwrite_cache=True\n",
    "                          )\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          num_workers=3\n",
    "                         )\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf9621-641d-4645-94ee-5f0a94ecfb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 4. 학습\n",
    "# => 교사모델은 평가(eval)만 하고, 학생모델만 학습(train)한다.\n",
    "# ====================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('..')\n",
    "from myutils import knowledge_distillation_loss1\n",
    "\n",
    "# 훈련 시작\n",
    "# 학생만 훈련시킴. 교사는 eval()\n",
    "##################################################\n",
    "epochs = 5            # epochs\n",
    "learning_rate = 2e-5  # 학습률\n",
    "p_itr = 300           # 손실률 보여줄 step 수\n",
    "#save_steps = 10000     # 50000 step마다 모델 저장\n",
    "\n",
    "# ==증류(distillation)과 연관된 변수 ==\n",
    "\n",
    "# 0.1이면 학생손실은 10%반영하고 Kld 손실(distillationloss)은 90% 반영하겠다는 의미\n",
    "alpha = 0.1           \n",
    "# 1이면 softmax 확률이고, 1보다 크면 softmax 확률이 평할화 되면서, ghkr어둠지식(Dark Knowledge)을 보다 많이 습득하게됨\n",
    "Temperture = 10   \n",
    "##################################################\n",
    "\n",
    "# optimizer 적용=> 학생모델\n",
    "optimizer = AdamW(student_model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "student_model.to(device)\n",
    "tearch_model.to(device)\n",
    "\n",
    "student_model.zero_grad()# 학생모델 초기화\n",
    "tearch_model.eval() # 교사모델은 평가모델로 설정.\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    student_model.train() # 학생모델은 훈련모드로 변환\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        student_model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        '''\n",
    "        input_ids = data['input_ids']\n",
    "        attention_mask = data['attention_mask']\n",
    "        token_type_ids = data['token_type_ids']      \n",
    "        labels = data['labels']\n",
    "        '''\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device)       \n",
    "        labels = data['labels'].to(device)\n",
    "     \n",
    "        # 교사모델 실행\n",
    "        tearch_outputs = tearch_model(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    token_type_ids=token_type_ids,\n",
    "                                    labels=labels)\n",
    "        tearch_loss = tearch_outputs.loss\n",
    "        tearch_logits = tearch_outputs.logits\n",
    "        \n",
    "        # 학생모델 실행\n",
    "        student_outputs = student_model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        student_loss = student_outputs.loss\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # 총 손실 구함.\n",
    "        loss = knowledge_distillation_loss1(student_loss = student_loss, \n",
    "                                 student_logits = student_logits, \n",
    "                                 teacher_logits = tearch_logits, \n",
    "                                 alpha = alpha, \n",
    "                                 Temperture = Temperture)\n",
    "\n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)   \n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "          # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도 계산 \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}'.format(epoch+1, epochs, itr, total_loss/p_itr))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bfdf6-e14c-4947-9348-fb76c2fc0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student 모델 저장\n",
    "import os\n",
    "OUTPATH = '../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321-student'\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "student_model.save_pretrained(OUTPATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
