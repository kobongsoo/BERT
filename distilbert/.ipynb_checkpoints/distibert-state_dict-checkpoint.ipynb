{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994ba0f4-92d5-4a47-bee5-925e9109270b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-21.log\n",
      "logfilepath:qnadataset_2022-03-21.log\n",
      "logfilepath:state_dict_2022-03-21.log\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, BertModel, DistilBertForMaskedLM, BertForMaskedLM\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, pytorch_cos_sim, mlogging\n",
    "seed_everything(111)\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"state_dict\", logfilname=\"state_dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601a5f11-46e7-40dd-a568-970da3a5fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/distilbert/distilbert-0321-10-ts were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "odict_keys(['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias'])\n",
      "143773\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([143773, 768])\n"
     ]
    }
   ],
   "source": [
    "# 0 번 모델 word_embeding 구함\n",
    "\n",
    "model_path='../model/distilbert/distilbert-0321-10-ts'\n",
    "#model_path='../model/bert/bmc_fpt_kowiki20200920.train_model_0225'\n",
    "model = DistilBertModel.from_pretrained(model_path)\n",
    "#model = BertModel.from_pretrained(model_path)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "print(type(state_dict))\n",
    "print(state_dict.keys())\n",
    "\n",
    "word_embedding = state_dict['embeddings.word_embeddings.weight']            # BertModel / DistilbertModel일때 word_embedding\n",
    "#word_embedding = state_dict['distilbert.embeddings.word_embeddings.weight']  # DistilBertforMaskedLM 일때 word_embedding\n",
    "#word_embedding = state_dict['bert.embeddings.word_embeddings.weight']      # BertforMaskedLM 일때 word_embedding\n",
    "\n",
    "print(len(word_embedding))\n",
    "print(type(word_embedding))\n",
    "print(word_embedding.shape)\n",
    "#print(word_embedding)\n",
    "\n",
    "#for key, val in state_dict.items():\n",
    "#    print(key)\n",
    "#    print(val)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84ae51f-970e-4cb0-b3aa-7f39ebcdadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "odict_keys(['embeddings.position_ids', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])\n",
      "143773\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([143773, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1 번 모델 word_embeding 구함\n",
    "\n",
    "#model_path1='../model/bert/bmc_fpt_kowiki20200920.train_model_0225'\n",
    "model_path1='../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321'\n",
    "#model1 = BertForMaskedLM.from_pretrained(model_path1)\n",
    "#model1 = DistilBertModel.from_pretrained(model_path1, state_dict=torch.load(state_dict_fpath))\n",
    "model1 = BertModel.from_pretrained(model_path1)\n",
    "\n",
    "state_dict1 = model1.state_dict()\n",
    "print(type(state_dict1))\n",
    "print(state_dict1.keys())\n",
    "\n",
    "word_embedding1 = state_dict1['embeddings.word_embeddings.weight']            # BertModel / DistilbertModel일때 word_embedding\n",
    "\n",
    "#word_embedding1 = state_dict1['distilbert.embeddings.word_embeddings.weight']  # DistilBertforMaskedLM 일때 word_embedding\n",
    "#word_embedding1 = state_dict1['bert.embeddings.word_embeddings.weight']      # BertforMaskedLM 일때 word_embedding\n",
    "\n",
    "print(len(word_embedding1))\n",
    "print(type(word_embedding1))\n",
    "print(word_embedding1.shape)\n",
    "#print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0166b36-742e-4287-bd0b-d9e978701843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport torch\\nmodel_path2='../model/distilbert/distilbert-0318-10-ts-1/state_dict.pt'\\nstate_dict2 = torch.load(model_path2)\\nprint(type(state_dict2))\\nprint(state_dict2.keys())\\n\\n#print(state_dict2.keys())\\nword_embedding2 = state_dict2['student.distilbert.embeddings.word_embeddings.weight']\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 번 은 state_dict 에서 word_embeding 구함\n",
    "'''\n",
    "import torch\n",
    "model_path2='../model/distilbert/distilbert-0318-10-ts-1/state_dict.pt'\n",
    "state_dict2 = torch.load(model_path2)\n",
    "print(type(state_dict2))\n",
    "print(state_dict2.keys())\n",
    "\n",
    "#print(state_dict2.keys())\n",
    "word_embedding2 = state_dict2['student.distilbert.embeddings.word_embeddings.weight']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f746ba-dd30-409b-8c4e-4840a37ddae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 14:30:52,223 - state_dict - INFO - ===model:../model/distilbert/distilbert-0321-10-ts, idx:10000===\n",
      "2022-03-21 14:30:52,223 - state_dict - INFO - tensor([ 1.4698e-01, -6.9600e-02,  1.6880e-02,  3.2489e-02, -4.6434e-02,\n",
      "        -4.8402e-02, -5.6247e-02,  5.9874e-03, -4.3824e-02, -8.9942e-02,\n",
      "        -3.6621e-02, -1.3278e-02, -3.2203e-02, -3.7723e-02, -3.4226e-02,\n",
      "        -2.2230e-02, -5.7720e-02,  8.5562e-03, -6.3336e-02,  3.1977e-03,\n",
      "        -5.0260e-02, -6.6554e-02, -5.3665e-02, -1.4357e-02, -3.1855e-03,\n",
      "         3.8261e-02, -4.7674e-03, -1.5141e-02,  1.0285e-02,  3.4711e-02,\n",
      "         7.1557e-03, -4.9233e-02, -2.2528e-02, -5.5319e-02,  3.3549e-02,\n",
      "        -3.5695e-02,  4.2715e-04, -3.8878e-02, -1.1668e-02,  6.0990e-03,\n",
      "        -2.3456e-02, -5.4632e-02, -3.3506e-04,  1.0225e-01,  4.6256e-02,\n",
      "         1.7223e-03, -6.3815e-02, -2.6233e-02, -1.0798e-02, -2.8362e-02,\n",
      "        -5.6302e-02,  2.8684e-02, -1.7909e-02, -2.0526e-03, -6.0615e-02,\n",
      "        -1.1435e-02, -1.2069e-02, -8.3399e-02,  9.6786e-03, -4.3746e-03,\n",
      "        -6.8902e-03, -8.6681e-02,  4.3081e-02, -3.3492e-02, -1.9828e-02,\n",
      "        -3.3032e-02,  5.5227e-03,  6.5406e-02,  1.3006e-02, -4.0902e-02,\n",
      "         1.6297e-02, -7.4619e-02,  8.9756e-02,  1.1967e-02, -5.8816e-02,\n",
      "        -6.7000e-02, -2.4782e-02, -4.3039e-03, -3.9671e-02, -4.5324e-02,\n",
      "        -4.0389e-02, -4.8476e-03,  1.3738e-02, -3.0496e-02, -4.4190e-02,\n",
      "        -6.2466e-02,  7.2267e-02, -4.1547e-02, -4.5468e-02, -5.2229e-02,\n",
      "        -3.3337e-02, -6.5459e-02, -7.6427e-04, -2.5397e-02, -3.8260e-02,\n",
      "        -4.7334e-02, -6.7094e-03, -6.1858e-02, -5.7774e-02,  2.3433e-02,\n",
      "        -2.2777e-02, -3.4703e-02,  1.9618e-02,  6.2387e-02, -6.8716e-02,\n",
      "        -5.1165e-02, -8.9209e-02, -5.7397e-02, -2.8013e-02, -1.0110e-02,\n",
      "        -2.4515e-02, -1.5797e-02,  4.7561e-02, -6.3878e-02,  1.1131e-02,\n",
      "         2.2078e-02,  2.4049e-02, -6.1173e-02, -1.3791e-02, -1.1327e-02,\n",
      "         2.7119e-02, -2.4841e-02, -5.4038e-02, -4.1076e-02, -4.8061e-02,\n",
      "         4.3882e-02, -1.1161e-01, -3.8172e-02, -8.6171e-02,  2.2872e-03,\n",
      "         4.9471e-03, -5.6644e-02,  2.0561e-02, -8.2528e-02,  2.8931e-02,\n",
      "        -6.7059e-03,  1.4833e-02, -2.2239e-02, -1.2175e-02,  4.7354e-02,\n",
      "        -3.8095e-02, -2.4574e-02, -5.6459e-02, -6.3753e-02,  9.5260e-03,\n",
      "         6.5425e-03, -3.5080e-02,  1.0668e-02, -2.8601e-02, -7.3975e-02,\n",
      "         1.8832e-03, -7.2064e-03,  4.5209e-02, -1.5130e-02,  2.1295e-02,\n",
      "        -5.3984e-02,  1.8402e-02,  3.1361e-03, -9.7819e-03, -3.4773e-02,\n",
      "        -8.9683e-02, -1.1466e-02, -1.0679e-02, -1.5376e-02, -1.2055e-02,\n",
      "        -4.0912e-02, -1.4011e-02,  5.8287e-02,  1.0374e-02, -2.5957e-02,\n",
      "        -5.0437e-02,  2.9846e-02, -1.2470e-02, -9.3437e-03,  1.3875e-02,\n",
      "        -5.9109e-02,  3.4100e-02, -9.2852e-02, -6.1062e-02, -9.3334e-03,\n",
      "        -1.9817e-02,  7.0665e-02, -4.7771e-03, -8.1042e-02,  7.4723e-03,\n",
      "        -2.2022e-02, -2.7240e-02, -5.4235e-03,  2.6625e-02, -7.6866e-02,\n",
      "         4.4381e-02,  4.6700e-02, -5.3878e-02,  1.9045e-02,  1.7597e-02,\n",
      "        -5.4118e-02, -6.3823e-02, -5.5643e-02, -4.1471e-02, -1.2056e-02,\n",
      "        -3.7522e-02, -9.5087e-03, -2.6431e-02,  2.2137e-02, -2.9740e-02,\n",
      "        -2.4978e-02, -4.5414e-02, -1.1685e-01,  4.7456e-02, -4.3872e-02,\n",
      "        -1.5632e-02, -5.1722e-02,  5.0493e-03,  7.2424e-02, -3.8265e-02,\n",
      "        -6.5103e-02, -5.3444e-02,  1.2109e-02,  8.0564e-03, -2.2771e-02,\n",
      "        -1.7232e-02,  2.4901e-02,  5.5069e-02, -5.1354e-02, -2.8127e-02,\n",
      "         1.8796e-02, -1.0719e-02, -3.4093e-02, -4.1679e-02, -3.7508e-02,\n",
      "         1.1820e-03, -2.3604e-02, -2.1287e-02, -4.0028e-02,  4.1650e-02,\n",
      "         4.1101e-03,  1.4903e-02,  2.8595e-02, -1.1166e-02, -1.5121e-02,\n",
      "        -1.3495e-02, -4.7103e-02,  1.7767e-02,  1.0717e-02, -8.6181e-02,\n",
      "        -2.6289e-02, -2.8374e-02, -3.3984e-02, -1.3199e-02, -1.3677e-02,\n",
      "        -5.5263e-02, -2.4215e-02, -7.4454e-03, -1.1575e-02, -7.0818e-02,\n",
      "        -7.7547e-02, -3.1214e-02,  7.9350e-03, -1.4868e-03, -5.7819e-04,\n",
      "        -1.8355e-03, -4.7534e-02, -2.9101e-02,  1.9521e-02, -9.1161e-02,\n",
      "        -4.4666e-02, -7.6270e-02, -2.9261e-02, -1.8161e-02, -8.0665e-02,\n",
      "         2.9184e-03, -3.4314e-02, -3.6736e-02, -4.2721e-02,  3.0793e-02,\n",
      "         5.6000e-04, -1.3235e-02,  3.4686e-03, -9.6727e-02, -1.4328e-02,\n",
      "        -1.9487e-02, -6.2670e-02, -1.2884e-01, -2.8962e-02,  1.0947e-02,\n",
      "        -5.5343e-02, -6.0888e-02, -6.4859e-03, -3.1394e-02, -6.5766e-03,\n",
      "        -8.4956e-02, -2.4717e-02, -5.9420e-02,  1.2464e-02, -4.4984e-02,\n",
      "        -2.7345e-02, -3.5010e-02, -3.3212e-02, -2.2027e-03,  1.3657e-03,\n",
      "        -2.7399e-02,  3.3640e-03, -4.8303e-02, -9.8973e-02, -8.2290e-02,\n",
      "         1.4196e-02, -8.9932e-02,  2.5146e-02,  7.0461e-03, -7.5067e-02,\n",
      "        -4.7152e-02, -1.2818e-02,  6.7683e-02, -3.0276e-02, -5.3946e-02,\n",
      "        -1.1692e-02,  5.9528e-03,  8.6503e-03, -7.6715e-02, -5.6580e-02,\n",
      "        -1.3298e-01, -8.2797e-02, -2.0471e-02,  2.6901e-02,  1.3266e-02,\n",
      "        -5.5437e-02, -9.6273e-03,  5.1075e-02,  3.1206e-02, -2.8857e-02,\n",
      "        -3.9170e-02, -7.9913e-02,  4.5645e-02, -5.8696e-02, -7.0561e-03,\n",
      "        -3.0847e-02, -2.9487e-02,  6.0536e-02, -2.0344e-02, -6.1550e-02,\n",
      "         6.4371e-02, -3.2041e-03, -2.6343e-02,  3.8861e-02, -7.7567e-03,\n",
      "         9.0515e-03, -8.4596e-02, -8.6768e-03,  1.2427e-02,  2.8099e-02,\n",
      "         8.7289e-03, -3.7306e-02,  1.8811e-02, -7.8136e-02, -3.3853e-02,\n",
      "         1.0320e-02, -2.7607e-02,  5.3447e-02,  1.7142e-02, -5.5054e-02,\n",
      "        -4.0333e-02, -8.1321e-02,  5.8710e-02, -2.5640e-03, -9.5526e-02,\n",
      "        -1.2490e-02, -3.6817e-02, -4.4600e-02, -2.2535e-02, -1.2228e-02,\n",
      "        -5.2897e-02, -8.5973e-02, -5.5804e-02,  3.4293e-02, -4.2327e-02,\n",
      "         3.3390e-02, -3.7305e-02, -2.2153e-02, -2.3606e-03, -1.7687e-02,\n",
      "        -5.6360e-02,  7.4348e-03, -3.4223e-03, -5.7566e-02, -2.5550e-02,\n",
      "        -4.5567e-03,  3.2917e-02, -6.6518e-02,  4.2220e-03, -1.5261e-02,\n",
      "        -4.6098e-02, -5.5540e-02, -3.2761e-02, -5.1991e-02, -3.6859e-02,\n",
      "         2.5533e-02, -5.4397e-02, -1.3688e-03, -2.0763e-02,  8.2514e-03,\n",
      "        -2.3943e-03, -1.5606e-02, -3.5785e-02,  3.0369e-02, -8.8897e-03,\n",
      "        -6.6045e-02, -3.7776e-02, -3.9992e-02,  6.9861e-02, -5.2515e-02,\n",
      "        -8.0739e-02,  2.3609e-02, -1.8335e-03, -1.1620e-02, -6.6037e-02,\n",
      "        -7.2982e-03,  5.1383e-02, -3.1708e-02, -7.5707e-02,  2.5953e-02,\n",
      "        -2.4801e-02, -5.5956e-02, -5.5666e-02,  7.2359e-02, -3.7811e-02,\n",
      "         5.2775e-03, -6.2589e-02, -2.4429e-02, -1.6678e-02, -9.6714e-02,\n",
      "        -2.7821e-03,  1.6679e-02,  3.0381e-02, -4.3351e-02,  1.7555e-02,\n",
      "        -2.4420e-02, -2.1194e-01,  2.3890e-02, -4.5642e-02, -2.9645e-02,\n",
      "        -2.8870e-02, -4.6886e-02, -9.9826e-02,  1.6251e-02,  9.8725e-03,\n",
      "         4.9476e-02, -1.1786e-01, -5.7776e-02, -1.8259e-02, -4.7681e-02,\n",
      "        -2.7573e-02, -3.5128e-02, -5.3665e-03,  1.6861e-02, -1.7270e-02,\n",
      "        -1.9215e-02,  1.1645e-02, -1.6280e-02, -7.3825e-02, -2.7545e-02,\n",
      "         1.9605e-02,  2.3738e-02,  5.0365e-03,  6.3120e-02, -6.5405e-02,\n",
      "        -2.9978e-02, -4.8002e-03,  4.0796e-02, -2.5537e-02, -4.2317e-02,\n",
      "        -2.4484e-02, -2.7520e-02, -7.9339e-03, -2.8928e-02,  1.8087e-02,\n",
      "        -1.4570e-02, -5.8861e-02, -1.0149e-01, -9.7639e-02,  1.3393e-02,\n",
      "        -5.4461e-02,  2.5504e-03,  8.4554e-02, -5.1891e-02, -7.2778e-02,\n",
      "         7.8661e-02, -1.0895e-02,  4.1323e-02, -1.4311e-02, -9.5114e-02,\n",
      "        -7.2426e-02, -2.7953e-02, -1.1856e-01, -4.2124e-02, -6.9447e-02,\n",
      "         1.8012e-02, -6.0215e-02, -4.9199e-02, -4.1382e-02, -5.5186e-02,\n",
      "         6.2800e-02, -1.8705e-02,  3.4474e-03, -2.1494e-02, -3.0486e-02,\n",
      "        -7.3049e-02,  8.6661e-03, -4.1899e-02, -5.5777e-02,  2.7371e-02,\n",
      "        -8.4782e-03,  6.8620e-03, -4.5880e-02, -1.1762e-03,  4.2425e-02,\n",
      "        -2.5602e-02,  5.0061e-03, -6.5610e-02,  4.6073e-02, -1.7551e-02,\n",
      "        -9.3180e-02, -4.0353e-02, -5.5917e-02, -2.2817e-02, -4.5994e-02,\n",
      "        -1.5434e-02, -3.2928e-02, -6.5642e-03, -5.5931e-02,  3.4894e-02,\n",
      "        -4.5132e-03, -5.7019e-02,  2.7224e-04, -7.0170e-02, -2.9311e-02,\n",
      "        -8.2066e-02, -3.2459e-02,  2.1586e-03,  3.3815e-02, -2.2834e-02,\n",
      "        -3.6843e-02, -1.8803e-03, -5.0611e-02, -4.2860e-02,  3.5892e-02,\n",
      "        -1.7674e-02,  3.6343e-02, -6.9890e-02, -5.3310e-02, -5.0202e-02,\n",
      "        -3.0809e-02,  1.6495e-02, -2.3792e-02,  7.1559e-03,  6.3059e-02,\n",
      "        -1.0921e-01,  1.4287e-02,  8.6297e-03, -4.6421e-02, -6.5848e-02,\n",
      "        -2.0309e-02, -8.0499e-03, -2.6285e-03, -1.2755e-02, -2.2821e-02,\n",
      "        -5.5144e-02, -3.3585e-02,  7.1950e-03,  2.5476e-02, -1.6130e-02,\n",
      "         1.7090e-02,  6.4826e-02, -7.2352e-03,  4.2169e-03, -8.4444e-02,\n",
      "        -5.1494e-02,  2.7987e-02, -3.0684e-02, -8.3603e-02, -1.6220e-02,\n",
      "        -8.1456e-02,  1.9132e-03,  3.8715e-02, -7.3332e-02, -5.6423e-02,\n",
      "        -1.8241e-02, -8.4694e-03, -7.0016e-02, -5.0948e-02, -2.4903e-02,\n",
      "        -4.0513e-02, -2.3448e-02, -4.5076e-02,  2.4490e-02,  2.4456e-02,\n",
      "        -1.0787e-02, -1.7323e-02,  6.0478e-03, -7.7074e-02, -4.8632e-02,\n",
      "        -1.6535e-03, -2.0769e-02, -5.5926e-02, -4.3712e-02,  2.8001e-02,\n",
      "         2.5869e-03, -5.7118e-02, -1.1563e-02, -2.5527e-03, -5.9713e-02,\n",
      "         3.1521e-02, -1.2962e-01, -3.3266e-03, -4.6119e-02, -7.1362e-02,\n",
      "        -6.4986e-02, -2.0690e-02, -6.2613e-02,  6.7847e-02, -5.2406e-02,\n",
      "         2.1635e-02, -3.6702e-02,  1.3104e-02, -8.5288e-03, -3.4324e-02,\n",
      "         3.1553e-02, -4.0566e-02,  7.1205e-03, -4.8756e-02, -6.4173e-02,\n",
      "        -2.9487e-02, -1.4120e-02,  8.3846e-04,  3.2195e-02,  5.9710e-02,\n",
      "        -2.7079e-02,  4.7351e-02,  4.2103e-02,  5.5610e-03, -8.4010e-03,\n",
      "        -1.6095e-02, -2.4201e-02,  2.6736e-02, -3.9717e-03, -3.0467e-02,\n",
      "         2.6158e-02, -3.2186e-02,  3.1977e-02, -4.2115e-02, -6.5152e-02,\n",
      "         1.6922e-02, -2.9675e-02, -2.0200e-02, -8.2504e-02,  9.0447e-04,\n",
      "        -3.6846e-02, -2.7779e-02, -1.8830e-02,  3.0412e-02, -6.9081e-02,\n",
      "        -1.0350e-02, -9.9071e-02,  1.0304e-02,  4.0264e-02, -4.2378e-03,\n",
      "        -4.5575e-03, -3.5582e-02, -1.4135e-02,  2.0461e-02, -2.8541e-02,\n",
      "        -5.7418e-02, -2.7866e-02,  3.0324e-02, -7.9881e-02, -4.9025e-02,\n",
      "        -3.2308e-02, -2.1597e-02,  7.4719e-03,  2.8743e-03,  1.1222e-02,\n",
      "        -2.1318e-02, -1.5792e-02, -9.3614e-03, -1.3988e-02,  6.7754e-03,\n",
      "        -8.2764e-02,  1.7589e-03, -2.1692e-02, -1.0099e-01, -3.9625e-02,\n",
      "        -4.0611e-02, -5.5588e-02, -1.8535e-02,  1.8101e-02, -6.1583e-02,\n",
      "         4.6969e-02, -1.8490e-02, -3.7204e-02, -6.0962e-02, -7.6219e-03,\n",
      "         5.8990e-03, -1.0154e-02,  1.5181e-02,  4.5857e-02, -4.6118e-02,\n",
      "        -1.9931e-02,  3.8081e-02, -1.4219e-02, -7.7783e-02, -5.6240e-02,\n",
      "        -1.7285e-02, -2.6859e-02, -2.6337e-02, -2.3507e-03, -1.7585e-02,\n",
      "        -5.1062e-02,  2.6134e-02, -1.7478e-02, -3.9896e-02, -1.1559e-02,\n",
      "        -4.8684e-02, -7.0071e-02,  5.6997e-03, -4.0438e-02, -1.1321e-02,\n",
      "        -6.4313e-02, -5.0177e-02,  3.7705e-03, -9.7453e-03,  1.4504e-02,\n",
      "        -2.6324e-02, -3.1827e-03,  2.5199e-02,  2.7336e-02,  3.0656e-02,\n",
      "        -6.8731e-02,  1.4611e-03, -7.2023e-02, -6.7202e-02, -1.4663e-04,\n",
      "        -3.4158e-02,  4.6928e-02, -4.2786e-02, -3.4665e-02,  1.7802e-02,\n",
      "        -1.3838e-02,  1.9453e-02, -5.3116e-02,  4.8540e-02,  3.3316e-02,\n",
      "         3.1492e-02,  4.7759e-02,  8.2052e-03, -1.5129e-03, -5.4378e-02,\n",
      "        -3.4342e-02, -3.9718e-02,  3.5387e-02,  2.5861e-02, -3.1917e-02,\n",
      "        -3.7138e-02, -5.0437e-02, -1.1093e-02,  1.8035e-02,  4.3221e-02,\n",
      "        -2.0135e-02,  1.1537e-02, -5.4960e-02])\n",
      "2022-03-21 14:30:52,352 - state_dict - INFO - ===model:../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321, idx:10000===\n",
      "2022-03-21 14:30:52,353 - state_dict - INFO - tensor([ 0.1482, -0.0707,  0.0158,  0.0338, -0.0454, -0.0477, -0.0552,  0.0057,\n",
      "        -0.0436, -0.0909, -0.0387, -0.0140, -0.0334, -0.0382, -0.0338, -0.0226,\n",
      "        -0.0590,  0.0083, -0.0628,  0.0032, -0.0481, -0.0685, -0.0531, -0.0143,\n",
      "        -0.0045,  0.0377, -0.0074, -0.0139,  0.0087,  0.0369,  0.0070, -0.0488,\n",
      "        -0.0235, -0.0550,  0.0335, -0.0362, -0.0007, -0.0374, -0.0129,  0.0063,\n",
      "        -0.0227, -0.0533,  0.0017,  0.1030,  0.0480,  0.0020, -0.0641, -0.0256,\n",
      "        -0.0094, -0.0299, -0.0575,  0.0278, -0.0188, -0.0030, -0.0612, -0.0131,\n",
      "        -0.0115, -0.0844,  0.0095, -0.0041, -0.0082, -0.0873,  0.0426, -0.0343,\n",
      "        -0.0213, -0.0326,  0.0053,  0.0638,  0.0137, -0.0395,  0.0164, -0.0761,\n",
      "         0.0893,  0.0130, -0.0580, -0.0691, -0.0249, -0.0043, -0.0413, -0.0438,\n",
      "        -0.0407, -0.0041,  0.0127, -0.0293, -0.0414, -0.0632,  0.0717, -0.0405,\n",
      "        -0.0482, -0.0527, -0.0332, -0.0658, -0.0003, -0.0242, -0.0383, -0.0487,\n",
      "        -0.0071, -0.0640, -0.0571,  0.0225, -0.0221, -0.0342,  0.0186,  0.0634,\n",
      "        -0.0698, -0.0504, -0.0885, -0.0570, -0.0268, -0.0096, -0.0261, -0.0158,\n",
      "         0.0481, -0.0648,  0.0106,  0.0226,  0.0257, -0.0594, -0.0125, -0.0106,\n",
      "         0.0248, -0.0264, -0.0530, -0.0385, -0.0487,  0.0422, -0.1128, -0.0374,\n",
      "        -0.0866,  0.0015,  0.0049, -0.0558,  0.0221, -0.0843,  0.0289, -0.0059,\n",
      "         0.0160, -0.0206, -0.0125,  0.0454, -0.0376, -0.0233, -0.0579, -0.0626,\n",
      "         0.0090,  0.0054, -0.0337,  0.0100, -0.0314, -0.0752,  0.0014, -0.0074,\n",
      "         0.0465, -0.0136,  0.0221, -0.0531,  0.0175,  0.0034, -0.0090, -0.0342,\n",
      "        -0.0892, -0.0091, -0.0089, -0.0153, -0.0156, -0.0424, -0.0133,  0.0570,\n",
      "         0.0098, -0.0248, -0.0491,  0.0297, -0.0135, -0.0115,  0.0135, -0.0613,\n",
      "         0.0331, -0.0926, -0.0585, -0.0111, -0.0188,  0.0721, -0.0037, -0.0808,\n",
      "         0.0083, -0.0239, -0.0261, -0.0058,  0.0271, -0.0770,  0.0430,  0.0481,\n",
      "        -0.0540,  0.0190,  0.0181, -0.0523, -0.0641, -0.0551, -0.0426, -0.0104,\n",
      "        -0.0385, -0.0125, -0.0264,  0.0222, -0.0274, -0.0251, -0.0460, -0.1169,\n",
      "         0.0476, -0.0455, -0.0165, -0.0535,  0.0037,  0.0722, -0.0383, -0.0662,\n",
      "        -0.0543,  0.0109,  0.0065, -0.0223, -0.0183,  0.0262,  0.0549, -0.0524,\n",
      "        -0.0298,  0.0194, -0.0112, -0.0339, -0.0405, -0.0370,  0.0020, -0.0239,\n",
      "        -0.0224, -0.0392,  0.0403,  0.0057,  0.0143,  0.0289, -0.0099, -0.0150,\n",
      "        -0.0127, -0.0474,  0.0173,  0.0114, -0.0877, -0.0248, -0.0276, -0.0343,\n",
      "        -0.0138, -0.0148, -0.0561, -0.0251, -0.0082, -0.0109, -0.0687, -0.0785,\n",
      "        -0.0313,  0.0065, -0.0004, -0.0004, -0.0018, -0.0487, -0.0301,  0.0201,\n",
      "        -0.0913, -0.0451, -0.0761, -0.0297, -0.0171, -0.0817,  0.0038, -0.0353,\n",
      "        -0.0352, -0.0432,  0.0318, -0.0004, -0.0115,  0.0021, -0.0950, -0.0158,\n",
      "        -0.0204, -0.0631, -0.1299, -0.0295,  0.0120, -0.0542, -0.0620, -0.0083,\n",
      "        -0.0326, -0.0086, -0.0841, -0.0252, -0.0614,  0.0129, -0.0467, -0.0284,\n",
      "        -0.0344, -0.0324,  0.0002,  0.0012, -0.0277,  0.0029, -0.0472, -0.0980,\n",
      "        -0.0836,  0.0135, -0.0908,  0.0245,  0.0060, -0.0747, -0.0479, -0.0105,\n",
      "         0.0686, -0.0300, -0.0545, -0.0113,  0.0049,  0.0095, -0.0756, -0.0569,\n",
      "        -0.1338, -0.0837, -0.0210,  0.0278,  0.0133, -0.0558, -0.0094,  0.0508,\n",
      "         0.0318, -0.0318, -0.0405, -0.0787,  0.0459, -0.0579, -0.0063, -0.0285,\n",
      "        -0.0289,  0.0607, -0.0190, -0.0633,  0.0655, -0.0015, -0.0267,  0.0408,\n",
      "        -0.0071,  0.0109, -0.0853, -0.0083,  0.0119,  0.0267,  0.0098, -0.0357,\n",
      "         0.0192, -0.0769, -0.0337,  0.0089, -0.0255,  0.0545,  0.0186, -0.0567,\n",
      "        -0.0394, -0.0810,  0.0598, -0.0015, -0.0967, -0.0116, -0.0363, -0.0450,\n",
      "        -0.0215, -0.0116, -0.0549, -0.0852, -0.0553,  0.0333, -0.0438,  0.0322,\n",
      "        -0.0350, -0.0213, -0.0043, -0.0195, -0.0553,  0.0054, -0.0019, -0.0577,\n",
      "        -0.0254, -0.0041,  0.0336, -0.0671,  0.0038, -0.0152, -0.0471, -0.0549,\n",
      "        -0.0334, -0.0507, -0.0367,  0.0261, -0.0545, -0.0020, -0.0220,  0.0079,\n",
      "        -0.0023, -0.0165, -0.0370,  0.0314, -0.0085, -0.0661, -0.0381, -0.0413,\n",
      "         0.0707, -0.0516, -0.0822,  0.0229, -0.0002, -0.0138, -0.0653, -0.0053,\n",
      "         0.0503, -0.0314, -0.0747,  0.0247, -0.0247, -0.0543, -0.0549,  0.0721,\n",
      "        -0.0386,  0.0059, -0.0610, -0.0247, -0.0180, -0.0981, -0.0027,  0.0164,\n",
      "         0.0292, -0.0428,  0.0168, -0.0247, -0.2135,  0.0226, -0.0452, -0.0314,\n",
      "        -0.0304, -0.0467, -0.1006,  0.0172,  0.0102,  0.0510, -0.1168, -0.0585,\n",
      "        -0.0166, -0.0494, -0.0255, -0.0362, -0.0055,  0.0165, -0.0186, -0.0186,\n",
      "         0.0131, -0.0156, -0.0728, -0.0282,  0.0181,  0.0249,  0.0051,  0.0647,\n",
      "        -0.0651, -0.0314, -0.0069,  0.0393, -0.0248, -0.0432, -0.0240, -0.0287,\n",
      "        -0.0069, -0.0296,  0.0188, -0.0152, -0.0605, -0.1028, -0.0982,  0.0135,\n",
      "        -0.0563,  0.0029,  0.0873, -0.0512, -0.0742,  0.0780, -0.0126,  0.0410,\n",
      "        -0.0140, -0.0944, -0.0736, -0.0260, -0.1194, -0.0426, -0.0699,  0.0168,\n",
      "        -0.0603, -0.0478, -0.0417, -0.0527,  0.0609, -0.0185,  0.0020, -0.0211,\n",
      "        -0.0310, -0.0741,  0.0085, -0.0429, -0.0577,  0.0286, -0.0101,  0.0083,\n",
      "        -0.0474,  0.0015,  0.0423, -0.0247,  0.0036, -0.0648,  0.0466, -0.0187,\n",
      "        -0.0928, -0.0404, -0.0553, -0.0254, -0.0486, -0.0140, -0.0315, -0.0059,\n",
      "        -0.0571,  0.0360, -0.0038, -0.0574, -0.0008, -0.0692, -0.0297, -0.0832,\n",
      "        -0.0315,  0.0033,  0.0346, -0.0243, -0.0376, -0.0033, -0.0511, -0.0439,\n",
      "         0.0354, -0.0178,  0.0360, -0.0708, -0.0519, -0.0499, -0.0316,  0.0152,\n",
      "        -0.0221,  0.0073,  0.0649, -0.1087,  0.0135,  0.0096, -0.0470, -0.0667,\n",
      "        -0.0190, -0.0097, -0.0023, -0.0132, -0.0240, -0.0553, -0.0330,  0.0054,\n",
      "         0.0258, -0.0173,  0.0170,  0.0634, -0.0055,  0.0039, -0.0839, -0.0512,\n",
      "         0.0285, -0.0324, -0.0795, -0.0182, -0.0827,  0.0020,  0.0368, -0.0716,\n",
      "        -0.0576, -0.0195, -0.0068, -0.0686, -0.0505, -0.0262, -0.0408, -0.0230,\n",
      "        -0.0460,  0.0235,  0.0272, -0.0114, -0.0156,  0.0045, -0.0775, -0.0494,\n",
      "        -0.0030, -0.0172, -0.0554, -0.0435,  0.0285,  0.0031, -0.0574, -0.0132,\n",
      "        -0.0026, -0.0604,  0.0327, -0.1299, -0.0027, -0.0474, -0.0712, -0.0642,\n",
      "        -0.0211, -0.0637,  0.0669, -0.0542,  0.0217, -0.0381,  0.0148, -0.0098,\n",
      "        -0.0326,  0.0308, -0.0393,  0.0058, -0.0485, -0.0651, -0.0273, -0.0156,\n",
      "        -0.0008,  0.0318,  0.0595, -0.0261,  0.0463,  0.0435,  0.0065, -0.0099,\n",
      "        -0.0160, -0.0233,  0.0278, -0.0048, -0.0312,  0.0257, -0.0325,  0.0307,\n",
      "        -0.0435, -0.0658,  0.0180, -0.0303, -0.0171, -0.0819,  0.0009, -0.0378,\n",
      "        -0.0291, -0.0200,  0.0300, -0.0685, -0.0092, -0.1003,  0.0104,  0.0422,\n",
      "        -0.0039, -0.0057, -0.0367, -0.0134,  0.0222, -0.0284, -0.0571, -0.0283,\n",
      "         0.0311, -0.0801, -0.0498, -0.0309, -0.0223,  0.0073,  0.0023,  0.0131,\n",
      "        -0.0216, -0.0167, -0.0089, -0.0126,  0.0062, -0.0818,  0.0028, -0.0219,\n",
      "        -0.1025, -0.0392, -0.0392, -0.0554, -0.0171,  0.0187, -0.0605,  0.0454,\n",
      "        -0.0205, -0.0381, -0.0626, -0.0078,  0.0065, -0.0121,  0.0155,  0.0436,\n",
      "        -0.0463, -0.0209,  0.0361, -0.0143, -0.0786, -0.0549, -0.0162, -0.0264,\n",
      "        -0.0261, -0.0016, -0.0178, -0.0524,  0.0242, -0.0180, -0.0407, -0.0132,\n",
      "        -0.0458, -0.0719,  0.0053, -0.0392, -0.0093, -0.0647, -0.0499,  0.0062,\n",
      "        -0.0077,  0.0142, -0.0273, -0.0018,  0.0267,  0.0285,  0.0291, -0.0683,\n",
      "         0.0014, -0.0741, -0.0674, -0.0005, -0.0335,  0.0472, -0.0417, -0.0356,\n",
      "         0.0165, -0.0140,  0.0195, -0.0521,  0.0479,  0.0345,  0.0323,  0.0486,\n",
      "         0.0095, -0.0021, -0.0545, -0.0357, -0.0392,  0.0354,  0.0263, -0.0329,\n",
      "        -0.0413, -0.0526, -0.0106,  0.0176,  0.0433, -0.0182,  0.0136, -0.0557])\n",
      "2022-03-21 14:30:52,360 - state_dict - INFO - ===유사도: idx:10000 ==\n",
      "2022-03-21 14:30:52,361 - state_dict - INFO - ../model/distilbert/distilbert-0321-10-ts vs ../model/bert/bmc-fpt-wiki_20190620_mecab_false_0311-0321\n",
      "2022-03-21 14:30:52,363 - state_dict - INFO - tensor([[0.9997]])\n"
     ]
    }
   ],
   "source": [
    "# 2개의 모델의 유사도를 구함 \n",
    "idx = 10000\n",
    "word_em1 = word_embedding[idx]\n",
    "word_em2 = word_embedding1[idx]\n",
    "model1 = model_path\n",
    "model2 = model_path1\n",
    "\n",
    "logger.info(f'===model:{model1}, idx:{idx}===')\n",
    "logger.info(word_em1)\n",
    "\n",
    "logger.info(f'===model:{model2}, idx:{idx}===')\n",
    "logger.info(word_em2)\n",
    "\n",
    "logger.info(f'===유사도: idx:{idx} ==')\n",
    "logger.info(f'{model1} vs {model2}')\n",
    "\n",
    "cos_sim = pytorch_cos_sim(word_em1, word_em2)\n",
    "\n",
    "logger.info(f'{cos_sim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74ae6d-03ea-41fa-be23-c322a7ff0891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
