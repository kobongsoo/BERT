{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789c56c1-04fa-4f6d-9042-db88df640d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM 테스트 예제\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import DistilBertTokenizer, BertConfig, DistilBertForMaskedLM\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging, MLMDatasetbyDistilBert, AccuracyForMaskedToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed53b75-93c6-476f-b523-8c1dcb42cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/distilbert-MLM-Test_2022-10-09.log\n"
     ]
    }
   ],
   "source": [
    "# eval 말뭉치 \n",
    "eval_corpus = \"bongsoo/moco_eval\"\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../data11/model/distilbert/mdistilbertV3.0/\"\n",
    "\n",
    "batch_size = 32\n",
    "token_max_len = 128\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(333)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"distilbert-MLM-Test\", logfilename=\"../../log/distilbert-MLM-Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79554037-bd6f-4678-afa8-fcb2de1f6dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(159552, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=159552, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "model = DistilBertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5273dff-5603-4522-84ae-8d3c49f6d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "*corpus:bongsoo/moco_eval\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bongsoo/cached_lm_DistilBertTokenizer_128_moco_eval.lock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLSid:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, SEPid:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, UNKid:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, PADid:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, MASKid:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#===============================================================================\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# eval dataloader 생성\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMLMDatasetbyDistilBert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_corpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mCLStokeinid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCLStokenid\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# [CLS] 토큰 id\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mSEPtokenid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSEPtokenid\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [SEP] 토큰 id\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mUNKtokenid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mUNKtokenid\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [UNK] 토큰 id\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mPADtokenid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPADtokenid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [PAD] 토큰 id\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mMasktokenid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMASKtokenid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# [MASK] 토큰 id\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_sequence_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_max_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# max_sequence_len)\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmlm_probability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moverwrite_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# eval dataloader 생성\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# => tenosor로 만듬\u001b[39;00m\n\u001b[1;32m     32\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m DataLoader(eval_dataset, \n\u001b[1;32m     33\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     34\u001b[0m                          \u001b[38;5;66;03m#shuffle=True, # dataset을 섞음\u001b[39;00m\n\u001b[1;32m     35\u001b[0m                          sampler\u001b[38;5;241m=\u001b[39mRandomSampler(eval_dataset, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;66;03m#dataset을 랜덤하게 샘플링함\u001b[39;00m\n\u001b[1;32m     36\u001b[0m                          num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     37\u001b[0m                         )\n",
      "File \u001b[0;32m~/dev/bong/BERT/distilbert/../myutils/bwpdataset.py:925\u001b[0m, in \u001b[0;36mMLMDatasetbyDistilBert.__init__\u001b[0;34m(self, corpus_path, tokenizer, CLStokeinid, SEPtokenid, UNKtokenid, PADtokenid, Masktokenid, max_sequence_len, mlm_probability, cache_dir, overwrite_cache, Maskvocab_list)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# Make sure only the first process in distributed training processes the dataset,\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# and the others will use the cache.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m lock_path \u001b[38;5;241m=\u001b[39m cached_features_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FileLock(lock_path):\n\u001b[1;32m    926\u001b[0m     \n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m####################################################################\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# 캐쉬파일이 있고  overwrite_cache = False인 경우에는 캐쉬 파일을 읽어옴\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m####################################################################\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cached_features_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite_cache:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==>[Start] cached file read: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcached_features_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/filelock/_api.py:220\u001b[0m, in \u001b[0;36mBaseFileLock.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseFileLock:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Acquire the lock.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    :return: the lock object\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/filelock/_api.py:173\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[0;34m(self, timeout, poll_interval, poll_intervall, blocking)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[1;32m    172\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to acquire lock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n\u001b[0;32m--> 173\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_locked:\n\u001b[1;32m    176\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m acquired on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lock_id, lock_filename)\n",
      "File \u001b[0;32m~/anaconda3/envs/bong/lib/python3.9/site-packages/filelock/_unix.py:35\u001b[0m, in \u001b[0;36mUnixFileLock._acquire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     open_mode \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mO_RDWR \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mO_CREAT \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mO_TRUNC\n\u001b[0;32m---> 35\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopen_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         fcntl\u001b[38;5;241m.\u001b[39mflock(fd, fcntl\u001b[38;5;241m.\u001b[39mLOCK_EX \u001b[38;5;241m|\u001b[39m fcntl\u001b[38;5;241m.\u001b[39mLOCK_NB)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bongsoo/cached_lm_DistilBertTokenizer_128_moco_eval.lock'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "# true이면, 무조건 원본파일 읽고, cache 파일을 만듬.\n",
    "# False로 하면 cache파일이 있으면 cache파일 이용함. cache파일 없으면 원본파일 일고, cache파일은 만들지 않음\n",
    "overwrite_cache = False\n",
    "\n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "#===============================================================================\n",
    "# eval dataloader 생성\n",
    "eval_dataset = MLMDatasetbyDistilBert(corpus_path = eval_corpus,\n",
    "                        tokenizer = tokenizer, \n",
    "                        CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                        SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                        UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                        PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                        Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                        max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                        mlm_probability=0.15,\n",
    "                        overwrite_cache=False\n",
    "                        )\n",
    "\n",
    "\n",
    "# eval dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         #shuffle=True, # dataset을 섞음\n",
    "                         sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                         num_workers=4\n",
    "                        )\n",
    "#===============================================================================\n",
    "\n",
    "\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16aec7-0609-44f9-9b2b-a2ee0e73bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "total_test_loss = 0\n",
    "total_test_correct = 0\n",
    "total_test_len = 0           \n",
    "list_validation_acc = []\n",
    "count = 0\n",
    "\n",
    "start = time.time()\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for eval_data in eval_loader:\n",
    "    # 입력 값 설정\n",
    "    input_ids = eval_data['input_ids'].to(device)\n",
    "    attention_mask = eval_data['attention_mask'].to(device)\n",
    "    #token_type_ids = eval_data['token_type_ids'].to(device)      #distilbert일때는 token_type_ids 없으므로 주석처리함\n",
    "    labels = eval_data['labels'].to(device)\n",
    "\n",
    "    # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "    # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "    # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "    with torch.no_grad():\n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        #token_type_ids=token_type_ids,             #distilbert일때는 token_type_ids 없으므로 주석처리함\n",
    "                        labels=labels)\n",
    "\n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        total_test_loss += loss\n",
    "        #===========================================\n",
    "        # 정확도(Accurarcy) 계산\n",
    "        correct, masked_len = AccuracyForMaskedToken(logits, labels, input_ids, MASKtokenid)           \n",
    "        total_test_correct += correct.sum().item() \n",
    "        total_test_len += masked_len \n",
    "        #=========================================\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "val_acc = total_test_correct/total_test_len\n",
    "val_loss = total_test_loss/count\n",
    "    \n",
    "logger.info(f'*model: {model_path}')\n",
    "logger.info(f'*evalcorpus: {eval_corpus}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info('*Val loss: {:.5f}, *Val Acc:{:.5f}'.format(val_loss, val_acc, total_test_correct, total_test_len))\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38c07f-1b9d-4165-9edb-72013d18ec26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
