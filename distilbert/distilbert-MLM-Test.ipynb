{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789c56c1-04fa-4f6d-9042-db88df640d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM 테스트 예제\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer, BertConfig, DistilBertForMaskedLM, BertForMaskedLM, RobertaForMaskedLM\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import GPU_info, seed_everything, mlogging, MLMDatasetbyDistilBert, MLMDataset, AccuracyForMaskedToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed53b75-93c6-476f-b523-8c1dcb42cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../../log/distilbert-MLM-Test_2022-10-19.log\n"
     ]
    }
   ],
   "source": [
    "# eval 말뭉치 \n",
    "#eval_corpus = \"../../data11/my_data/moco_eval.txt\"\n",
    "eval_corpus = \"../../data11/my_data/bong_eval.txt\"\n",
    "\n",
    "# model 타입 : 0=distilbert, 1=bert, 2=Roberta\n",
    "#=>Roberta 모델에는 distilbert처럼 token_type_id 입력 없음.\n",
    "model_type = 0\n",
    "\n",
    "# 기존 사전훈련된 모델\n",
    "model_path = \"../../data11/model/distilbert/mdistilbertV3.1/\"\n",
    "#model_path = \"xlm-roberta-base\"\n",
    "\n",
    "batch_size = 16\n",
    "token_max_len = 128\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(333)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"distilbert-MLM-Test\", logfilename=\"../../log/distilbert-MLM-Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79554037-bd6f-4678-afa8-fcb2de1f6dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(159552, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=159552, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeinzier 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, max_len=token_max_len, do_lower_case=False)\n",
    "\n",
    "# 모델 로딩 further pre-training \n",
    "if model_type == 0:\n",
    "    model = DistilBertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "elif model_type == 1:\n",
    "    model = BertForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "elif model_type == 2:\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_path, from_tf=bool(\".ckpt\" in model_path)) \n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5273dff-5603-4522-84ae-8d3c49f6d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSid:101, SEPid:102, UNKid:100, PADid:0, MASKid:103\n",
      "*corpus:../../data11/my_data/bong_eval.txt\n",
      "*max_sequence_len:128\n",
      "*mlm_probability:0.15\n",
      "*CLStokenid:101, SEPtokenid:102, UNKtokenid:100, PADtokeinid:0, Masktokeid:103\n",
      "*total_line: 1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde00989f3f5482e952282715bd77ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54eb2ac81dda4d47a198f48826d23078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([   101, 120422, 120078, 120832, 148959, 124211,  10892, 120005, 131640,\n",
      "         10459,  11978, 121062,    103,   9663, 118878,  11102, 135894, 123674,\n",
      "          9597,    103,    103, 119214,  10892, 121530,  11489,    103,  10954,\n",
      "         18784,    103,  12453,   9408,   9460,  13767, 121346,  10739,  11506,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   101, 120422, 120078, 120832, 148959, 124211,  10892, 120005, 131640,\n",
      "         10459,  11978, 121062,  10459,   9663, 118878,  11102, 135894, 123674,\n",
      "          9597,  35979, 120190, 119214,  10892, 121530,  11489,    129,  10954,\n",
      "         18784, 128595,  12453,   9408,   9460,  13767, 121346,  10739,  11506,\n",
      "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "# true이면, 무조건 원본파일 읽고, cache 파일을 만듬.\n",
    "# False로 하면 cache파일이 있으면 cache파일 이용함. cache파일 없으면 원본파일 일고, cache파일은 만들지 않음\n",
    "overwrite_cache = False\n",
    "\n",
    "# 각 스페셜 tokenid를 구함\n",
    "CLStokenid = tokenizer.convert_tokens_to_ids('[CLS]')\n",
    "SEPtokenid = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "UNKtokenid = tokenizer.convert_tokens_to_ids('[UNK]')\n",
    "PADtokenid = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "MASKtokenid = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "print('CLSid:{}, SEPid:{}, UNKid:{}, PADid:{}, MASKid:{}'.format(CLStokenid, SEPtokenid, UNKtokenid, PADtokenid, MASKtokenid))\n",
    "\n",
    "#===============================================================================\n",
    "# eval dataloader 생성\n",
    "if model_type == 0 or model_type == 2:\n",
    "    eval_dataset = MLMDatasetbyDistilBert(corpus_path = eval_corpus,\n",
    "                            tokenizer = tokenizer, \n",
    "                            CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                            SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                            UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                            PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                            Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                            max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                            mlm_probability=0.15,\n",
    "                            overwrite_cache=False\n",
    "                            )\n",
    "elif model_type == 1:\n",
    "    eval_dataset = MLMDataset(corpus_path = eval_corpus,\n",
    "                            tokenizer = tokenizer, \n",
    "                            CLStokeinid = CLStokenid ,   # [CLS] 토큰 id\n",
    "                            SEPtokenid = SEPtokenid ,    # [SEP] 토큰 id\n",
    "                            UNKtokenid = UNKtokenid ,    # [UNK] 토큰 id\n",
    "                            PADtokenid = PADtokenid,    # [PAD] 토큰 id\n",
    "                            Masktokenid = MASKtokenid,   # [MASK] 토큰 id\n",
    "                            max_sequence_len=token_max_len,  # max_sequence_len)\n",
    "                            mlm_probability=0.15,\n",
    "                            overwrite_cache=False\n",
    "                            )\n",
    "\n",
    "\n",
    "# eval dataloader 생성\n",
    "# => tenosor로 만듬\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         #shuffle=True, # dataset을 섞음\n",
    "                         sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                         num_workers=4\n",
    "                        )\n",
    "#===============================================================================\n",
    "\n",
    "\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c16aec7-0609-44f9-9b2b-a2ee0e73bdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 09:02:06,716 - distilbert-MLM-Test - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d308bd425d4f95a73939d80ce3104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 09:02:11,215 - distilbert-MLM-Test - INFO - *model: ../../data11/model/distilbert/mdistilbertV3.1/\n",
      "2022-10-19 09:02:11,218 - distilbert-MLM-Test - INFO - *evalcorpus: ../../data11/my_data/bong_eval.txt\n",
      "2022-10-19 09:02:11,219 - distilbert-MLM-Test - INFO - ---------------------------------------------------------\n",
      "2022-10-19 09:02:11,221 - distilbert-MLM-Test - INFO - *Val loss: 15.70804, *Val Acc:0.44161, total_test_correct:3770.0, total_test_len:8537.0\n",
      "2022-10-19 09:02:11,223 - distilbert-MLM-Test - INFO - ---------------------------------------------------------\n",
      "2022-10-19 09:02:11,224 - distilbert-MLM-Test - INFO - === 처리시간: 4.508 초 ===\n",
      "2022-10-19 09:02:11,225 - distilbert-MLM-Test - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_test_loss = 0\n",
    "total_test_correct = 0\n",
    "total_test_len = 0           \n",
    "list_validation_acc = []\n",
    "count = 0\n",
    "\n",
    "start = time.time()\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for eval_data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = eval_data['input_ids'].to(device)\n",
    "        attention_mask = eval_data['attention_mask'].to(device)\n",
    "        if model_type == 1:\n",
    "            token_type_ids = eval_data['token_type_ids'].to(device)      #distilbert일때는 token_type_ids 없으므로 주석처리함\n",
    "        labels = eval_data['labels'].to(device)\n",
    "\n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            if model_type == 0 or model_type == 2:\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                #token_type_ids=token_type_ids,             #distilbert일때는 token_type_ids 없으므로 주석처리함\n",
    "                                labels=labels)\n",
    "            elif model_type == 1:\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids,             #distilbert일때는 token_type_ids 없으므로 주석처리함\n",
    "                                labels=labels)\n",
    "\n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_test_loss += loss\n",
    "            #===========================================\n",
    "            # 정확도(Accurarcy) 계산\n",
    "            correct, masked_len = AccuracyForMaskedToken(logits, labels, input_ids, MASKtokenid)           \n",
    "            total_test_correct += correct.sum().item() \n",
    "            total_test_len += masked_len \n",
    "            #=========================================\n",
    "\n",
    "            count += 1\n",
    "\n",
    "val_acc = total_test_correct/total_test_len\n",
    "val_loss = total_test_loss/count\n",
    "    \n",
    "logger.info(f'*model: {model_path}')\n",
    "logger.info(f'*evalcorpus: {eval_corpus}')\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info('*Val loss: {:.5f}, *Val Acc:{:.5f}, total_test_correct:{:.1f}, total_test_len:{:.1f}'.format(val_loss, val_acc, total_test_correct, total_test_len))\n",
    "logger.info(f'---------------------------------------------------------')\n",
    "logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "logger.info(f'-END-\\n')\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38c07f-1b9d-4165-9edb-72013d18ec26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
