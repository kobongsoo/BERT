{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee1bd61-1480-4322-a670-d64bd88ff866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-18.log\n",
      "logfilepath:qnadataset_2022-03-18.log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gluonnlp as nlp     # GluonNLP는 버트를 간단하게 로딩하는 인터페이스를 제공하는 API 임\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from myutils import seed_everything, GPU_info, pytorch_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b4d96e-7383-4801-a859-547ddfcac820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = '../model/distilbert/distilbert-0318-1/'\n",
    "vocab_path = '../model/distilbert/distilbert-0318-1/'\n",
    "\n",
    "# True로 해야, hidden_states 가 출력됨\n",
    "output_hidden_states = True\n",
    "\n",
    "#False로 지정하는 경우 일반적인 tuple을 리턴, True인 경우는 transformers.file_utils.ModelOutput 으로 리턴\n",
    "return_dict = False\n",
    "\n",
    "seed = 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3bd1a2-b925-4e69-a22c-d2e339d0112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = GPU_info()\n",
    "print(cuda)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a405d84a-462a-4724-a7b7-2dd830f3594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# tokenize 설정\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6a9c77-2854-4bbf-8fb4-81c587c9736e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(143772, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 불러옴\n",
    "model = DistilBertModel.from_pretrained(model_path, \n",
    "                                  output_hidden_states=output_hidden_states,\n",
    "                                  return_dict=return_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0903aac4-df6b-431b-a67a-ac8ed988f6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prameters:153338880\n"
     ]
    }
   ],
   "source": [
    "print('prameters:{}'.format(model.num_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966abd8c-127e-4ee7-be39-1b8934e67c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = ['오늘은 오후 부터 춥고 비가 올것 같다.']\n",
    "\n",
    "\n",
    "classification_sentence = [['오늘은 가끔 흐리고, 눈이 올수 있다'],\n",
    "                 ['여기 식당은 파스타가 맛있다'],\n",
    "                 ['오늘 증시는 내림으로 마감 하였다'],\n",
    "                 ['내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7cecf1-2e5e-4158-8a57-97d6922c3583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', '오늘', '##은', '오후', '부터', '[UNK]', '비', '##가', '올', '##것', '같다', '.', '[SEP]']]\n",
      "{'input_ids': tensor([[   101, 120059,  10892, 120647, 119569,    100,   9379,  11287,   9583,\n",
      "         118627,  53354,    119,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "===0====\n",
      "<class 'tuple'>\n",
      "2\n",
      "torch.Size([1, 13, 768])\n",
      "[['[CLS]', '오늘', '##은', '가', '##끔', '흐리', '##고', ',', '눈', '##이', '올', '##수', '있다', '[SEP]']]\n",
      "{'input_ids': tensor([[   101, 120059,  10892,   8843, 118707, 141713,  11664,    117,   9034,\n",
      "          10739,   9583,  15891,  11506,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '여기', '식당', '##은', '파스타', '##가', '맛있', '##다', '[SEP]']]\n",
      "{'input_ids': tensor([[   101, 119777, 123966,  10892, 140366,  11287, 130646,  11903,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '오늘', '증', '##시', '##는', '내림', '##으로', '마감', '하였다', '[SEP]']]\n",
      "{'input_ids': tensor([[   101, 120059,   9705,  14040,  11018, 132190,  11467, 124162,  28750,\n",
      "            102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[['[CLS]', '내일', '##은', '오전', '##에는', '흐리', '##지만', ',', '오후', '##에는', '날씨', '##가', '좋', '##겠', '##다', '[SEP]']]\n",
      "{'input_ids': tensor([[   101, 123490,  10892, 121408,  15303, 141713,  28578,    117, 120647,\n",
      "          15303, 126746,  11287,   9685, 118632,  11903,    102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장을 불러와서 출력값들을 구함\n",
    "tokenized_input = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "print(token_str)\n",
    "print(tokenized_input)\n",
    "in_outputs = model(**tokenized_input)\n",
    "\n",
    "print('===0====')\n",
    "print(type(in_outputs))\n",
    "print(len(in_outputs))\n",
    "print(in_outputs[0].shape)\n",
    "\n",
    "# return_dict=True 면 0번째 배열을 아래처럼 가져올수 있음\n",
    "#last_hidden_states = in_outputs.last_hidden_state\n",
    "#print(type(last_hidden_states))\n",
    "#print(last_hidden_states.shape)\n",
    "\n",
    "# out_hidden_states = True인 경우에만 hidden_states가 출력됨\n",
    "if len(in_outputs) > 2:\n",
    "    print('===1====')\n",
    "    hidden_states = in_outputs[1]\n",
    "    print(type(hidden_states))\n",
    "    print(f'layer:{len(hidden_states)}')\n",
    "    print(f'batch:{len(hidden_states[0])}')\n",
    "    print(f'token:{len(hidden_states[0][0])}')\n",
    "    print(f'hidden:{len(hidden_states[0][0][0])}')\n",
    "\n",
    "\n",
    "\n",
    "# 분류 문장을 불러와서, tokenize 한 다음, 모델에 넣고 출력값을 얻어옴\n",
    "sequence_out_list = []\n",
    "#pooled_out_list = []  # distilbert에는 pooled_out 없음\n",
    "hidden_states_list = []\n",
    "\n",
    "for idx in range(len(classification_sentence)):\n",
    "    sentence = classification_sentence[idx][0]\n",
    "    tokenized_input = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "    print(token_str)\n",
    "    print(tokenized_input)\n",
    "    outputs = model(**tokenized_input)\n",
    "\n",
    "    sequence_out_list.append(outputs[0])\n",
    "    #pooled_out_list.append(outputs[1])   # distilbert에는 pooled_out 없음\n",
    "    hidden_states_list.append(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73e68ca-18fb-481b-939d-abaf92e9fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "# case 1) sequence_output 평균값으로 유사도 측정\n",
    "\n",
    "print(len(sequence_out_list))\n",
    "print(sequence_out_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9c29e6-675c-4a26-b272-076dface05a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:['오늘은 오후 부터 춥고 비가 올것 같다.']\n",
      "\n",
      "['오늘은 가끔 흐리고, 눈이 올수 있다'], 유사도:0.8336136341094971\n",
      "['오늘 증시는 내림으로 마감 하였다'], 유사도:0.8296330571174622\n",
      "['내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다'], 유사도:0.7992400527000427\n",
      "['여기 식당은 파스타가 맛있다'], 유사도:0.7883613109588623\n"
     ]
    }
   ],
   "source": [
    "in_mean_sequence = torch.mean(in_outputs[0], dim=1)\n",
    "#print(in_mean_sequence.shape)\n",
    "out_dict = {}\n",
    "\n",
    "for idx in range(len(sequence_out_list)):\n",
    "    out_mean_sequence = torch.mean(sequence_out_list[idx], dim=1)\n",
    "    simul_score = pytorch_cos_sim(in_mean_sequence, out_mean_sequence)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "print(f'query:{input_sentence}\\n')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    idx = int(count[0])\n",
    "    print(f'{classification_sentence[idx]}, 유사도:{value[0][0]}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b47e17-e1c2-4f82-9079-be39f8d70936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:['오늘은 오후 부터 춥고 비가 올것 같다.']\n",
      "\n",
      "['오늘은 가끔 흐리고, 눈이 올수 있다'], 유사도:0.8078755140304565\n",
      "['내일은 오전에는 흐리지만, 오후에는 날씨가 좋겠다'], 유사도:0.7914364337921143\n",
      "['오늘 증시는 내림으로 마감 하였다'], 유사도:0.7912730574607849\n",
      "['여기 식당은 파스타가 맛있다'], 유사도:0.7589171528816223\n"
     ]
    }
   ],
   "source": [
    "# case 2) hidden_state 마지막 2번째 레이어의 평균값으로 유사도 측정\n",
    "out_dict = {}\n",
    "ebd = torch.stack(list(in_outputs[1]), dim=0)\n",
    "# 마지막 2번째(-2) 레이어에 평균을 구한다. \n",
    "in_hidden = torch.mean(ebd[-2][0], dim=0)\n",
    "  \n",
    "for idx,embedding in enumerate(hidden_states_list): # enumerate는 index, value 값이 리턴됨\n",
    "    #print('index: {:d}, type: {}'.format(idx, type(embedding)))\n",
    "    \n",
    "    # hidden_state_list[0]은 tuple 이므로 이를 tensor로 변환 하면서, dim=0 하여 레이어를 결합해서 큰 텐서[13,1,128,768] 를 만든다.\n",
    "    ebd = torch.stack(list(embedding), dim=0)\n",
    "    \n",
    "    # 마지막 2번째(-2) 레이어에 평균을 구한다. \n",
    "    out_hidden = torch.mean(ebd[-2][0], dim=0)\n",
    "    simul_score = pytorch_cos_sim(in_hidden, out_hidden)\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "     # 사전 key로 순번으로 하고, 유사도를 저장함\n",
    "    key = str(idx)\n",
    "    out_dict[key] = simul_score\n",
    "    #print(\"input vs {:d} 유사도:{}\".format(idx, simul_score))\n",
    "    \n",
    "#print(out_dict)\n",
    "\n",
    "# 사전 정렬(value(유사도)로 reverse=True 하여 내림차순으로 정렬함)\n",
    "sorted_dict = sorted(out_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "#print(sorted_dict)\n",
    "\n",
    "# 내립차순으로 정렬된 사전출력 \n",
    "print(f'query:{input_sentence}\\n')\n",
    "for count in (sorted_dict):\n",
    "    value = count[1].tolist() # count[1]은 2차원 tensor이므로 이를 list로 변환\n",
    "    idx = int(count[0])\n",
    "    print(f'{classification_sentence[idx]}, 유사도:{value[0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08060b11-3009-42b4-b556-12dc10eb430f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9925b71e-82b3-4ddf-b1a9-e551317dc4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
