{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159bc54e-0c70-4cc7-b3ff-cb2b3683cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:../../../log/distilbertftmultitrain_2022-11-02.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "# NLI(Natural Language Interference:자연어 추론) 훈련 예제\n",
    "#\n",
    "# => input_ids : [CLS]senetence1(전제)[SEP]sentence2(가설)\n",
    "# => attention_mask : 1111111111(전체,가설)0000000(그외)\n",
    "# => token_type_ids : 0000000(전제)1111111(가설)00000000(그외)\n",
    "# => laels : 참(수반:entailment), 거짓(모순:contradiction), 모름(중립:neutral)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "logger = mlogging(loggername=\"distilbertfttrain\", logfilename=\"../../../log/distilbertftmultitrain\")\n",
    "device = GPU_info()\n",
    "# Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731f6745-a2f4-4b2d-9a19-1fa2efa6f3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bongsoo/mdistilbertV3.1 were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bongsoo/mdistilbertV3.1 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(159552, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################################################\n",
    "# 변수들 설정\n",
    "# - model_path : from_pretrained() 로 호출하는 경우에는 모델파일이 있는 폴더 경로나 \n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')\n",
    "#          torch.load(model)로 로딩하는 경우에는 모델 파일 풀 경로\n",
    "#\n",
    "# - vocab_path : from_pretrained() 호출하는 경우에는 모델파일이 있는 폴더 경로나\n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')   \n",
    "#          BertTokenizer() 로 호출하는 경우에는 vocab.txt 파일 풀 경로,\n",
    "#\n",
    "# - OUTPATH : 출력 모델, vocab 저장할 폴더 경로\n",
    "#############################################################################################\n",
    "\n",
    "##################################################\n",
    "# 변수 설정\n",
    "##################################################\n",
    "seed = 111\n",
    "epochs = 3            # epochs\n",
    "lr = 3e-5  # 학습률\n",
    "eps = 1e-8\n",
    "max_seq_len = 72     # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "train_batch_size = 32      # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "eval_batch_size = 64\n",
    "##################################################\n",
    "\n",
    "seed_everything(seed) # seed 설정\n",
    "\n",
    "cache = True   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "\n",
    "use_kornli = 1     #  kornli 파일\n",
    "use_kluenli = 1    # kluests_v1.1 파일\n",
    "use_gluenli = 1    # glue 파일\n",
    "\n",
    "kor_train_file_fpath = '../../../data11/korpora/kornli/snli_1.0_train.ko.tsv'\n",
    "kor_eval_file_fpath = '../../../data11/korpora/kornli/xnli.dev.ko.tsv'\n",
    "\n",
    "klue_train_file_fpath = '../../../data11/korpora/klue-nli/klue-nli-v1.1_train.json'\n",
    "klue_eval_file_fpath = '../../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json'\n",
    "\n",
    "glue_train_file_fpath = '../../../data11/korpora/gluemnli/glue-mnli-train.tsv'\n",
    "glue_eval_file_fpath = '../../../data11/korpora/gluemnli/glue-mnli-valid.tsv'\n",
    "\n",
    "\n",
    "# model 타입 : 0=distilbert, 1=bert, 2=Roberta, 3=Kobert\n",
    "#=>Roberta 모델에는 distilbert처럼 token_type_id 입력 없음.\n",
    "model_type = 0\n",
    "model_path = 'bongsoo/mdistilbertV3.1' #  #distilbert-base-multilingual-cased #bert-re-kowiki-bert-mecab \n",
    "vocab_path = 'bongsoo/mdistilbertV3.1'\n",
    "OUTPATH = '../../../data11/model/NLI/mdistilbertV3.1-nli'\n",
    "\n",
    "# tokeniaer 및 model 설정\n",
    "# strip_accents=False : True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "# do_lower_case=False : # 소문자 입력 사용 안함(한국어에서는 반드시 False)\n",
    "if model_type == 3:\n",
    "    from kobert_tokenizer import KoBERTTokenizer\n",
    "    tokenizer = KoBERTTokenizer.from_pretrained(model_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(vocab_path, strip_accents=False, do_lower_case=False) \n",
    "                        \n",
    "# NLI 모델에서 레벨은 3개지(참,거짓,모름) 이므로, num_labels=3을 입력함\n",
    "\n",
    "if model_type == 0:\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "elif model_type == 1 or model_type == 3:\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "\n",
    "# 레벨을 멀티로 선택해야 하는 경우\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, problem_type=\"multi_label_classification\",num_labels=6)\n",
    "                   \n",
    "#기존 모델 파일을 로딩하는 경우    \n",
    "#model = torch.load(model_path) \n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918a05d8-aee5-42df-9a1c-1dc2443f655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166050819"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d611fe-b906-45ee-afc2-dcbfcd760751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at ../../../data11/korpora/kornli/snli_1.0_train.ko.tsv\n",
      "loading data... LOOKING AT ../../../data11/korpora/kornli/snli_1.0_train.ko.tsv\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 45.168227672576904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a27619d9a3d4ef493b22d4095e40625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 말을 탄 사람이 고장난 비행기 위로 뛰어오른다. + 한 사람이 식당에서 오믈렛을 주문하고 있다.\n",
      "tokens: [CLS] 말 ##을 탄 사람이 고장 ##난 비행기 위로 뛰 ##어 ##오 ##른다 . [SEP] 한 사람이 식당 ##에서 [UNK] 주문 ##하고 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 9251, 10622, 9847, 97802, 123248, 33305, 122116, 121623, 9150, 12965, 28188, 66346, 119, 102, 9954, 97802, 123875, 11489, 100, 121399, 12453, 11506, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "sentence A, B: 말을 탄 사람이 고장난 비행기 위로 뛰어오른다. + 사람은 야외에서 말을 타고 있다.\n",
      "tokens: [CLS] 말 ##을 탄 사람이 고장 ##난 비행기 위로 뛰 ##어 ##오 ##른다 . [SEP] 사람 ##은 야외 ##에서 말 ##을 타 ##고 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: entailment\n",
      "features: ClassificationFeatures(input_ids=[101, 9251, 10622, 9847, 97802, 123248, 33305, 122116, 121623, 9150, 12965, 28188, 66346, 119, 102, 119555, 10892, 126814, 11489, 9251, 10622, 9845, 11664, 11506, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../../data11/korpora/kornli/cached_DistilBertTokenizerFast_72_snli_1.0_train.ko.tsv 17.661457777023315\n",
      "Creating features from dataset file at ../../../data11/korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "loading data... LOOKING AT ../../../data11/korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 2.450220823287964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ec84200d7e4b7cbb12bdca6126a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 힛걸 진심 최고다 그 어떤 히어로보다 멋지다 + 힛걸 진심 최고로 멋지다.\n",
      "tokens: [CLS] [UNK] 진심 최고 ##다 그 어떤 히어로 ##보다 멋 ##지 ##다 [SEP] [UNK] 진심 최고 ##로 멋 ##지 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: entailment\n",
      "features: ClassificationFeatures(input_ids=[101, 100, 128736, 83491, 11903, 8924, 55910, 128156, 80001, 9270, 12508, 11903, 102, 100, 128736, 83491, 11261, 9270, 12508, 11903, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n",
      "*** Example ***\n",
      "sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n",
      "tokens: [CLS] 100 ##분 ##간 [UNK] 그 ##래 ##도 [UNK] 2 ##점 ##준 ##다 [SEP] 100 ##분 ##간 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10407, 37712, 18784, 100, 8924, 37388, 12092, 100, 123, 34907, 54867, 11903, 102, 10407, 37712, 18784, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../../data11/korpora/klue-nli/cached_DistilBertTokenizerFast_72_klue-nli-v1.1_train.json 0.9283018112182617\n",
      "Creating features from dataset file at ../../../data11/korpora/gluemnli/glue-mnli-train.tsv\n",
      "loading data... LOOKING AT ../../../data11/korpora/gluemnli/glue-mnli-train.tsv\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 49.40560007095337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41356fdd50e243199015a61a960d3571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: Conceptually cream skimming has two basic dimensions - product and geography. + Product and geography are what make cream skimming work. \n",
      "tokens: [CLS] Concept ##ually cream ski ##mming has two basic dimensions - product and ge ##ography . [SEP] Product and ge ##ography are what make cream ski ##mming work . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: neutral\n",
      "features: ClassificationFeatures(input_ids=[101, 77961, 79090, 93461, 40122, 97469, 10393, 10551, 25090, 38590, 118, 21535, 10111, 46503, 34850, 119, 102, 93218, 10111, 46503, 34850, 10301, 12976, 13086, 93461, 40122, 97469, 11424, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "sentence A, B: you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him + You lose the things to the following level if the people recall.\n",
      "tokens: [CLS] you know during the season and i guess at at your level u ##h you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a gu ##y from triple A then a double A gu ##y goes up to replace him and [SEP] You lose the things to the following level if the people recall . [SEP]\n",
      "label: entailment\n",
      "features: ClassificationFeatures(input_ids=[101, 13028, 21852, 10939, 10105, 11226, 10111, 177, 156573, 10160, 10160, 20442, 13277, 189, 10237, 13028, 48742, 11345, 10114, 10105, 13451, 13277, 12277, 12277, 10689, 19068, 10114, 158148, 10105, 10105, 43045, 11121, 10105, 97242, 19068, 10114, 20575, 10114, 158148, 169, 75980, 10157, 10188, 40159, 138, 11059, 169, 15790, 138, 75980, 10157, 25441, 10741, 10114, 37156, 10957, 10111, 102, 11065, 48742, 10105, 24682, 10114, 10105, 11901, 13277, 12277, 10105, 11426, 158148, 119, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../../data11/korpora/gluemnli/cached_DistilBertTokenizerFast_72_glue-mnli-train.tsv 12.282769441604614\n",
      "Creating features from dataset file at ../../../data11/korpora/kornli/xnli.dev.ko.tsv\n",
      "loading data... LOOKING AT ../../../data11/korpora/kornli/xnli.dev.ko.tsv\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 0.2463853359222412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca8688f489b40c68ead02a93dde70ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 그리고 그가 말했다, \"엄마, 저 왔어요.\" + 그는 학교 버스가 그를 내려주자마자 엄마에게 전화를 걸었다.\n",
      "tokens: [CLS] 그리고 그가 말했다 , \" 엄마 , 저 왔 ##어 ##요 . \" [SEP] 그는 학교 버스 ##가 그를 내 ##려 ##주자 ##마 ##자 엄마 ##에게 전화 ##를 걸 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: neutral\n",
      "features: ClassificationFeatures(input_ids=[101, 23289, 57790, 102055, 117, 107, 123845, 117, 9663, 9594, 12965, 48549, 119, 107, 102, 17889, 119811, 120214, 11287, 76203, 8996, 26737, 133626, 23811, 13764, 123845, 26212, 120846, 11513, 8867, 17706, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "sentence A, B: 그리고 그가 말했다, \"엄마, 저 왔어요.\" + 그는 한마디도 하지 않았다.\n",
      "tokens: [CLS] 그리고 그가 말했다 , \" 엄마 , 저 왔 ##어 ##요 . \" [SEP] 그는 한마디 ##도 하지 않았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 23289, 57790, 102055, 117, 107, 123845, 117, 9663, 9594, 12965, 48549, 119, 107, 102, 17889, 130761, 12092, 89093, 49137, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../../data11/korpora/kornli/cached_DistilBertTokenizerFast_72_xnli.dev.ko.tsv 0.09119272232055664\n",
      "Creating features from dataset file at ../../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "loading data... LOOKING AT ../../../data11/korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 0.3468155860900879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01804e13a0164aac9438ad9f489693e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: 흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다. + 어떤 방에서도 흡연은 금지됩니다.\n",
      "tokens: [CLS] 흡연 ##자 ##분 ##들은 발코니 ##가 있는 방이 ##면 발코니 ##에서 흡연 ##이 가능 ##합 ##니다 . [SEP] 어떤 방 ##에서 ##도 흡연 ##은 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 129066, 13764, 37712, 22879, 142566, 11287, 13767, 135146, 14867, 142566, 11489, 129066, 10739, 119557, 33188, 48345, 119, 102, 55910, 9328, 11489, 12092, 129066, 10892, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기 불편함이 많았다.\n",
      "tokens: [CLS] 10 ##명이 함께 사용 ##하기 불편 ##함 ##없 ##이 만족 ##했다 . [SEP] 10 ##명이 함께 사용 ##하기 불편 ##함 ##이 많 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10150, 66923, 19653, 119547, 22440, 122835, 48533, 119136, 10739, 120646, 12490, 119, 102, 10150, 66923, 19653, 119547, 22440, 122835, 48533, 10739, 9249, 27303, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../../data11/korpora/klue-nli/cached_DistilBertTokenizerFast_72_klue-nli-v1.1_dev.json 0.10410642623901367\n",
      "Creating features from dataset file at ../../../data11/korpora/gluemnli/glue-mnli-valid.tsv\n",
      "loading data... LOOKING AT ../../../data11/korpora/gluemnli/glue-mnli-valid.tsv\n",
      "tokenize sentences, it could take a lot of time...\n",
      "tokenize sentences [took %.3f s] 1.1749849319458008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fb63e5fcf54df383a2210520632935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9815 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "sentence A, B: The new rights are nice enough + Everyone really likes the newest benefits \n",
      "tokens: [CLS] The new rights are nice enough [SEP] Everyone really like ##s the new ##est benefits [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: neutral\n",
      "features: ClassificationFeatures(input_ids=[101, 10117, 10751, 16691, 10301, 151987, 21408, 102, 157501, 30181, 11850, 10107, 10105, 10751, 13051, 48297, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "sentence A, B: This site includes a list of all award winners and a searchable database of Government Executive articles. + The Government Executive articles housed on the website are not able to be searched.\n",
      "tokens: [CLS] This site includes a list of all award winners and a search ##able database of Government Executive articles . [SEP] The Government Executive articles housed on the website are not able to be searched . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "label: contradiction\n",
      "features: ClassificationFeatures(input_ids=[101, 10747, 11920, 15433, 169, 13416, 10108, 10435, 17725, 33525, 10111, 169, 22419, 13096, 11254, 10108, 14581, 23612, 18416, 119, 102, 10117, 14581, 23612, 18416, 57128, 10135, 10105, 13575, 10301, 10472, 16197, 10114, 10347, 152018, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "Saving features into cached file, it could take a lot of time...\n",
      "Saving features into cached file %s [took %.3f s] ../../../data11/korpora/gluemnli/cached_DistilBertTokenizerFast_72_glue-mnli-valid.tsv 0.3335227966308594\n",
      "train_loader_len: 30246, eval_loader_len: 240\n"
     ]
    }
   ],
   "source": [
    "# 학습 data loader 생성\n",
    "sys.path.append('..')\n",
    "from myutils import ClassificationDataset, KorNLICorpus, KlueNLICorpus, GlueMNLICorpus, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_dataset = []\n",
    "\n",
    "# 훈련 NLI dataset 생성\n",
    "if use_kornli == 1:\n",
    "    corpus = KorNLICorpus()\n",
    "    train_dataset += ClassificationDataset(file_fpath=kor_train_file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "if use_kluenli == 1:\n",
    "    corpus = KlueNLICorpus()\n",
    "    train_dataset += ClassificationDataset(file_fpath=klue_train_file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "    \n",
    "if use_gluenli == 1:\n",
    "    corpus = GlueMNLICorpus()\n",
    "    train_dataset += ClassificationDataset(file_fpath=glue_train_file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "    \n",
    "\n",
    "# 훈련 dataloader 생성\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=train_batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "# 평가 dataset 생성\n",
    "eval_dataset = []\n",
    "\n",
    "if use_kornli == 1:\n",
    "    corpus = KorNLICorpus()\n",
    "    eval_dataset += ClassificationDataset(file_fpath=kor_eval_file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "if use_kluenli == 1:\n",
    "    corpus = KlueNLICorpus()\n",
    "    eval_dataset += ClassificationDataset(file_fpath=klue_eval_file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "    \n",
    "if use_gluenli == 1:\n",
    "    corpus = GlueMNLICorpus()\n",
    "    eval_dataset += ClassificationDataset(file_fpath=glue_eval_file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "\n",
    "    \n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                          batch_size=eval_batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "print('train_loader_len: {}, eval_loader_len: {}'.format(len(train_loader), len(eval_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fec919e-d8b7-4b20-a20b-28cf66a49a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159552\n",
      "[101, 9034, 10530, 124729, 11018, 125871, 10739, 69708, 42428, 10459, 10020, 121612, 10892, 133496, 12508, 49137, 102, 9670, 89523, 126553, 76820, 102]\n",
      "명도\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# tokenier 테스트\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.encode(\"눈에 보이는 반전이었지만 영화의 흡인력은 사라지지 않았다\", \"정말 재미있다\"))\n",
    "print(tokenizer.convert_ids_to_tokens(131027))\n",
    "print(tokenizer.convert_tokens_to_ids('정말'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2068553-5650-47a4-bdd6-c9e79e1580cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 14:28:04,498 - distilbertfttrain - INFO - === model: bongsoo/mdistilbertV3.1 ===\n",
      "2022-11-02 14:28:04,499 - distilbertfttrain - INFO - num_parameters: 166050819\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afde5390b2a84240b6bed9daa3396361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c28099b53545bcb51429479dc0f3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98355/3150600645.py:76: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-11-02 14:44:05,239 - distilbertfttrain - INFO - [Epoch 1/3] Iteration 9073 -> Train Loss: 0.8312, Train Accuracy: 0.613\n",
      "2022-11-02 15:00:11,049 - distilbertfttrain - INFO - [Epoch 1/3] Iteration 18146 -> Train Loss: 0.6553, Train Accuracy: 0.727\n",
      "2022-11-02 15:16:08,573 - distilbertfttrain - INFO - [Epoch 1/3] Iteration 27219 -> Train Loss: 0.6018, Train Accuracy: 0.752\n",
      "2022-11-02 15:21:26,840 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b0e9db2bf747f09169a03269592344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98355/3150600645.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-11-02 15:21:38,500 - distilbertfttrain - INFO - [Epoch 1/3] Validatation Accuracy:0.7014701078079059\n",
      "2022-11-02 15:21:38,501 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-11-02 15:21:38,501 - distilbertfttrain - INFO - === 처리시간: 11.662 초 ===\n",
      "2022-11-02 15:21:38,502 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9dd435d2f214cbb8ee2b74e86fe8d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 15:32:19,612 - distilbertfttrain - INFO - [Epoch 2/3] Iteration 36292 -> Train Loss: 0.5328, Train Accuracy: 0.785\n",
      "2022-11-02 15:48:20,391 - distilbertfttrain - INFO - [Epoch 2/3] Iteration 45365 -> Train Loss: 0.5061, Train Accuracy: 0.799\n",
      "2022-11-02 16:04:28,202 - distilbertfttrain - INFO - [Epoch 2/3] Iteration 54438 -> Train Loss: 0.4963, Train Accuracy: 0.803\n",
      "2022-11-02 16:15:06,691 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163fa48927d84bcb80fc9e345909b5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 16:15:18,125 - distilbertfttrain - INFO - [Epoch 2/3] Validatation Accuracy:0.7266906239790918\n",
      "2022-11-02 16:15:18,126 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-11-02 16:15:18,127 - distilbertfttrain - INFO - === 처리시간: 11.436 초 ===\n",
      "2022-11-02 16:15:18,128 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fe4e2c1da1494fa8cee963f278608b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 16:20:39,864 - distilbertfttrain - INFO - [Epoch 3/3] Iteration 63511 -> Train Loss: 0.4618, Train Accuracy: 0.819\n",
      "2022-11-02 16:36:42,149 - distilbertfttrain - INFO - [Epoch 3/3] Iteration 72584 -> Train Loss: 0.4089, Train Accuracy: 0.843\n",
      "2022-11-02 16:52:38,326 - distilbertfttrain - INFO - [Epoch 3/3] Iteration 81657 -> Train Loss: 0.4075, Train Accuracy: 0.843\n",
      "2022-11-02 17:08:33,220 - distilbertfttrain - INFO - [Epoch 3/3] Iteration 90730 -> Train Loss: 0.4014, Train Accuracy: 0.846\n",
      "2022-11-02 17:08:34,495 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d527a57c60964a18a156c6dc4c126208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 17:08:46,094 - distilbertfttrain - INFO - [Epoch 3/3] Validatation Accuracy:0.732701731460307\n",
      "2022-11-02 17:08:46,096 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-11-02 17:08:46,098 - distilbertfttrain - INFO - === 처리시간: 11.602 초 ===\n",
      "2022-11-02 17:08:46,099 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "logger.info(f\"=== model: {model_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")\n",
    "\n",
    "# 학습 시작\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=lr, \n",
    "                 eps=eps) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "num_warmup_steps = total_steps * 0.1\n",
    "p_itr = int(total_steps * 0.1)           # 손실률 보여줄 step 수\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        if model_type == 1:\n",
    "            token_type_ids = data['token_type_ids'].to(device) \n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        if model_type == 0:\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도와 총 손실률 계산\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_len += len(labels)    \n",
    "            total_loss += loss.item()\n",
    "            #print('pred:{}, correct:{}'.format(pred, correct))\n",
    "\n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "                list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "        #if itr > 5:\n",
    "        #    break\n",
    "   \n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    # 평가 시작\n",
    "    \n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "    \n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        if model_type == 1:\n",
    "            token_type_ids = data['token_type_ids'].to(device) \n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    " \n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "              # 모델 실행\n",
    "            if model_type == 0:\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=labels)\n",
    "            else:\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                                token_type_ids=token_type_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=labels)\n",
    "    \n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            #loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            # 총 손실류 구함\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_test_correct += correct.sum().item()\n",
    "            total_test_len += len(labels)\n",
    "    \n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(f'-END-\\n')\n",
    "    ####################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e56cc12-2914-4fea-a6c9-15bec647112f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvRUlEQVR4nO3dd3xV9f3H8dc3NzuEkAnZgw0BAoaVCA70J0NBwAGiJVhEbBWwrThr1Wp/9lfbSq3F4gAHQkVkyVJUCrIkbAjTELKYCSMhZH9/f5wQAgSSwA3njs/z8cjj5p5z7j2fXODNN9/z/X6P0lojhBDC/rmYXYAQQgjrkEAXQggHIYEuhBAOQgJdCCEchAS6EEI4CFezThwUFKRjYmLMOr0QQtilTZs2ndBaB9e2z7RAj4mJITU11azTCyGEXVJKHbrSPulyEUIIByGBLoQQDkICXQghHIQEuhBCOAgJdCGEcBAS6EII4SAk0IUQwkGYNg5dCCHsjtZQXgIVJcZj9VfxFbaVGo/nt50/ps1dEH6T1cuTQBdCNJ7KSshJhZMZoCuNQERf/KgrL99Wva+Wbegrv1f1Pmo/T2UZlNcI2Yqq4D2/7dIAvii8q/ZbQ5PmEuhCCDtQXgoZq2D317B3CRQeNbuiC1zcwNUTXN2rHj3A4mE8nv9yb1L1veeFbZce4+oJFvcrHFPj/Wvd5g5KNcqPJ4EuhLh+pWfhwAojxPcth5LT4OYDre+AdvdAWAIol6ogUxc/KpfLt6FqOZ6r7KvrvVSjhagtkUAXQlybonzYtwx2L4Kfvze6JLwCoP090P5uiLsV3LzMrtKpSKALIervdA7sWQx7FkHGGtAV0DQcuo02QjwqCSwSK2axu09ea83PxwtpFeJrdilCOIcT+41W+J6vIWeTsS2oDdw8CdrdDWFdnaI7wx7YXaC/vWI/U//7M+ueu53AJh5mlyOE49EacrcYAb77azix19ge1g36vWz0iQe3MbdGUSu7C/R7uoQy5bv9zN6Yxa9va2V2OUI4hopyyFxnhPiexXA6C5QFopOg+y+h3SDwizC7SlEHuwv0ViG+9GkdxKfrDjGubxxuFpnsKsQ1KSuG9JVGd8reJXAu3xhm1/J2uPV5aDsAvAPMrlI0gN0FOsCY5BgenZHKsp1HuKdLmNnlCGE/is/A/m+MED+wAkoLwaOpMXOx3d3Q6g7waGJ2leIa2WWg39omhJhAb6avOSiBLkRdCo8ZLfDdXxst8soy8AmBTvcZ/eGxfY1JL8Lu2WWgu7goRifF8OqiNLZlnaJLZDOzSxLCdhQeh9zNkLMZDv4XMtcDGvxjoOfjxjjxiO7gYjG7UmFldhnoAPfdFMFfv9nHjLUZ/P3BBLPLEcIc507B4a1GeOduhpwtcCa7aqeCFvFwy7PGGPHm8TK80MHZbaD7erpx300RzNxwiOcHtCOkqafZJQnRuEqL4Mj2GuG9GfJ/vrDfPxaiekLYeGOIYWgX6Q93MnYb6AApSTF8vC6DmRsyefpOGRcrHEh5KRzbdSG8c7fCsd3GzEwA3zAI7wYJI43wDusqI1KEfQd6TJAPt7cNYeaGQ/zqtpZ4uEqfoLBDlRVwYl+N8N4CR3ZcWKrVK8AI77YDjPAO7wa+LcytWdgkuw50gJTkGB758CcWbz/MsG4y8UHYOK3h5MGq8N5ifB3eZgwfBGPp1tAE4+Ll+fBuFi1936Je6hXoSqn+wBTAAnygtX7zkv1RwMdAs6pjntNaL7FuqbW7uVUQrUKaMH1NBkO7hqPkL76wJWdyjdCu2fo+d9LYZ/GAFp0g4aEL4R3YGlxkspy4NnUGulLKArwL3AlkAxuVUgu11mk1DnsJ+EJrPVUp1QFYAsQ0Qr211UdKUgwvzd/JpkMnSYyRfkTRCCoroeysMTGnpKDqq+b3l2wrPGaEd+ER4/XKAiEdjCGD58M7pANY3Mz9uYRDqU8LvQdwQGudDqCUmg0MAWoGugaaVn3vB+Ras8i6DOsWzv8t28P0tRkS6OJilRW1hO6lYXzmKgFd48u4r9nVuXmDh6/R7x13y4XwbtFJ1gYXja4+gR4OZNV4ng30vOSYV4BvlFJPAT7AHVaprp683V0Z0SOKD388SO6pc4Q1k384TqP4NBzdBUd2wtEdcHyv0aVRUmC0psvO1u993H2NIK755RtqTIuvuc3z/POmlx/v7itrgQtTWetv30hghtb6r0qp3sCnSql4rXVlzYOUUuOAcQBRUVFWOrXhF72j+WB1Op+tP8Tk/u2s+t7CBlRWGhcTj+6sCu+qx9OZF47xCoDmHY2uDA9f8PS7PHRrC2N3X+m3Fg6hPoGeA0TWeB5Rta2mXwL9AbTW65RSnkAQcKzmQVrracA0gMTExHr8/lp/Ef7e/E+HFsz6KZMJ/Vrj6SZDGO1WSSEcSzOG7p0P7mNpF0aCKBcIbAURiZCYAs07GTMifUNlNIhwavUJ9I1Aa6VULEaQjwAeuuSYTKAfMEMp1R7wBI5bs9D6SEmOYdmuI8zfksOIHtb9DUA0Aq2NdberW9xVAZ5/kOr+ag8/o9Wd8JAxdb1FPAS3B3dvU0sXwhbVGeha63Kl1JPAcowhiR9prXcppV4DUrXWC4HfAu8rpZ7G+JeYorW2agu8PnrGBtA+tCkz1mbwYPdIGcJoS8rOGTMda3aZHN1p9IGfFxBnhHfnEUZwN4+HZlHS6hainurVh141pnzJJdtervF9GpBs3dIaTinFmKQYJs/dzrr0PJJaBpldkvPRGgoOX7hIef6CZd5+OH9Jxc3HCO744VWt7k5V/d6y7ogQ18PhLskPTgjjzWV7mLEmQwL9Rjl5CDZ/Atk/GeF9Lv/CPr8oo7XdYciFVrd/rFyEFKIROFyge7pZGNkjkn+t/Jms/CIiA6SvtVFoDYfWwPqpxs0TUBDa2bj3ZItORnA37whezcyuVAin4XCBDvBIrxj+/d90PlmXwYuDOphdjmMpK4adc2HDVOMippc/JE+E7mPlJsJCmMwhA72FnycDOoUye2MWk+5og4+HQ/6YN1bBEdj4IaR+BEUnjJEm90yBTg/IiBMhbITDJl1KUgyLtuXy1eZsHukdY3Y59itnE6x/D3bNg8pyaNMfeo2H2Ftk9IkQNsZhA71bVDO6RPgxY20Go3pG4+Ii4VNvFeWweyFseA+yNhgzKbv/EnqMg8CWZlcnhLgChw10pRQpyTE8/Z9trD5wglvaBJtdku0ryodNM2DjB3AmxxiN0v9NSBhlrGEihLBpDhvoAIM6hfGnJXuYseagBPrVHNttjFbZ/gWUnzO6Uwb9FVr/j9wZXgg74tCB7u7qwqieUby9Yj/pxwuJC5aJK9UqK2H/N7D+X3Dwv+DqCZ0fhJ7jobmMDBLCHjn87I5RPaNxt7jwybpDZpdiG0oKjIuc/7wJZj0IJ/ZDv5fh6TQY/A8JcyHsmEO30AGCfT24u0soc1Kz+M3/tKGpp5PeISY/HTZMgy2fQWkBRPSA21+C9oPlrjlCOAiHD3SAMUmxfLU5hzmp2fzy5lizy7lxtIaDq4z+8X3LwMUVOg41hh2G32R2dUIIK3OKQO8U4UditD8fr80gJSkGi6MPYSw7Z1zg3PCesY64dxD0fcYYeujbwuzqhBCNxCkCHYy10p/8fAs/7DnGHR2am11O4zidYww53DTDWCCreScY8i7E3wdunmZXJ4RoZE4T6Hd1bEGonycz1mY4XqBnbTRGq+xeaCxR23Yg9HoCopNlNqcQTsRpAt3N4sLDvaL5y/K97DtaQJvmvmaXdH3yD0LafNj5FRzZbtzZp+d4Yzanf7TZ1QkhTOA0gQ7wUI8o/vHdfqavyeB/h3Uyu5yGO3nICPFd8yB3i7EtPBEGvgVdRsoNIoRwck4V6P4+7gztGs68Ldk8278tzbzdzS6pbqeyLoR4ziZjW1g3uPOP0PFe4xZtQgiBkwU6GBdHZ2/MYvbGLMbfYqMLTZ3OuRDi2RuNbaEJcMerRoj7x5hXmxDCZjldoLdr0ZTecYF8uu4QY2+OxdViI5Nlz+RC2kIjxLPWG9tadIZ+fzBCPCDO1PKEELbP6QIdjFb6459u4tu0owzoFGpeIQVHLoR45jpAG0MNb/+9MQFIlqoVQjSAUwb6He2bE+HvxfS1GTc+0AuOGsMLd8037smJhpCOcNuLRks8qPWNrUcI4TCcMtAtLorRvWN4Y8luduWepmOYX+OesPB4VYjPM0JcV0JwO7j1eSPEg9s27vmFEE7BKQMd4IHukfx9xT6mr8ngrfu7WP8EZ0/A7kVGiGesNkI8sLUxBb/jUAhpb/1zCiGcmtMGup+XG8O7RfCfjVk8N6AdQU08rv9Ni/IvhPjBVaArIKAl9PltVYh3kJmbQohG47SBDjA6KYZP1x9i1oZMnup3jX3XRfmwZ7ER4ukrjRD3j4WbJxkh3jxeQlwIcUM4daC3CmlC3zbBfLr+EONvbYlbQ4YwZqfCyjch/QeoLDfGhidPMEK8RWcJcSHEDefUgQ4wJimGMTM2smTHYYYkhNfvRWfzYNYIUC7Q61cQP8yY+CMhLoQwkdMH+i1tgokN8mHG2oz6B/rSZ+DcKRi3ElrEN2Z5QghRbzYyTdI8Li6K0b2j2ZJ5iq1Zp+p+QdpC2DkXbpksYS6EsClOH+gA9yVG4uvhyvQ1B69+4Nk8WPwbCO0CNz99Y4oTQoh6kkAHmni4cn9iJIu3H+bomeIrH7jkd0ZXy71T5cbKQgibI4FeZXRSNBVaM3P9odoPSFsAu76CW56F5h1vbHFCCFEPEuhVogN96NcuhJkbMikpr7h459kT8PX5rpZJptQnhBB1kUCvISUplryzpSzadvjiHUt+B8WnpatFCGHTJNBrSG4VSOuQJkxfcxCttbFx13xjFuit0tUihLBt9Qp0pVR/pdRepdQBpdRztez/u1Jqa9XXPqXUKatXegMopUhJjmFX7hlSD500uloW/9aYNJQso1qEELatzkBXSlmAd4EBQAdgpFKqQ81jtNZPa60TtNYJwDvAV41Q6w0xrGsEfl5uxhDGxb+FkjNVXS1OPwdLCGHj6tNC7wEc0Fqna61LgdnAkKscPxKYZY3izODlbmFEj0hc0hYY9/W85Vlo3qHO1wkhhNnqE+jhQFaN59lV2y6jlIoGYoHvr7B/nFIqVSmVevz48YbWesOM7uzDq64fcdinPSRPMrscIYSoF2tfFB0BfKm1rqhtp9Z6mtY6UWudGBwcbOVTW0/Ympdoqop5sugxzlXIgltCCPtQn0DPASJrPI+o2labEdhxdwsAO7+CtAUc6TqRTedaMH/rlX5UIYSwLfUJ9I1Aa6VUrFLKHSO0F156kFKqHeAPrLNuiTdQ4XFjzHlYNyLufo4OoU0vHsIohBA2rM5A11qXA08Cy4HdwBda611KqdeUUoNrHDoCmK3tNf20NhbeKimAe6eiLG6kJMew72gh637OM7s6IYSoU73G4mmtlwBLLtn28iXPX7FeWSbY9RXsXgj9/gAh7QAY3CWMPy/dw0drMkhqFWRygUIIcXUyUxSg8Bgs/h2E3wRJE6o3e7pZeKhnFN/tOUpmXpGJBQohRN0k0M93tZQWwpB/XTaB6OFe0ViU4uN1GebUJ4QQ9SSBvnMu7F4Et71Q3dVSU/OmngzsFMoXG7M4W1JuQoFCCFE/zh3ohcdgyTNGV0vvp654WEpyDAUl5czdnH0DixNCiIZx3kDXGr5+GkrP1rlWS7cof7pENmPGmgwqK+1zEI8QwvE5b6DvnAt7vja6WoLb1nn4mKQY0k+cZdV+212yQAjh3Jwz0AuOGhOIwhMh6cpdLTUN7BRKiK8H09dkNG5tQghxjZwv0KtHtRQZXS0ulnq9zN3VhYd7RfPffcf5+XhhIxcphBAN53yBvuNLo6vl9hchuE2DXjqyRxTuFhc+XpvROLUJIcR1cK5ALzgKS5+BiO7Q+8kGvzzY14N7uoTx5aZsTp8ra4QChRDi2jlPoFePaikyJhDVs6vlUmOSYygqrWBOalbdBwshxA3kPIG+Yw7sXQy3v9Tgrpaa4sP96B7jz8frMqiQIYxCCBviHIFecMSYQBTRA3r/+rrfLiUplqz8c3y/55gVihNCCOtw/EA/39VSXgz3XntXS013dWxOmJ8nb6/Yx/GCEisUKYQQ18/xA337F7B3idHVEtTaKm/panHhpbs7cOBYIf3fXsWKtKNWeV8hhLgejh3oBUdg6WSI7Am9fmXVtx7YKZRFT91M86aejP0klRfm7aCoVBbvEkKYx3EDXWtYNMnoarmOUS1X06a5L/N+ncTjt8Qx66dMBv3jR7ZlnbL6eYQQoj4cN9C3fwH7lsLtv4egVo12Gg9XC88PaM/nY3tRUlbB8Klreee7/ZRXVDbaOYUQojaOGejVXS29oNcTN+SUvVsGsnRiXwZ0CuWv3+5jxLT1ZOXLXY6EEDeO4wX6RV0t7zZKV8uV+Hm78c7IrkwZkcDeIwUMmLKaLzdlY6/3zRZC2BfHC/Tt/zG6Wvq93KhdLVczJCGcpZP60CGsKb+bs41ff76ZU0WlptQihHAejhXoZw5f6GrpOd7UUiL8vZn1WC+e7d+Ob9OOctfbq/hx/wlTaxJCODbHCXSt4etJUF5qtQlE18vionji1pbM+1UyTTxcefjDDfzx6zSKyyrMLk0I4YAcJ9C3zYZ9y4yulsCWZldzkfhwP75+qg+je0fz4Y8HuffdNew5csbssoQQDsYxAv3MYVj2LET1Nr2r5Uq83C28OiSe6WO6c6KwlMHvrOGD1elyj1IhhNXYf6BrDYsmGl0tQ94FF9v+kW5rG8LySX24pW0wry/ezSMfbeDw6XNmlyWEcAC2nX71sW0W7F8Od/zB5rpariSwiQfTHrmJN4d1YvOhU/R/ezWLtx82uywhhJ2z70A/kwtLn4OoJOjxuNnVNIhSihE9olgysQ8xQT78+vPN/PaLbRQUy52QhBDXxn4D/XxXS0UpDPmnzXe1XElskA9fju/NhH6tmbclmwFTVpOakW92WUIIO2SfKQiw9XPY/w3c8YrddLVciZvFhd/c2YY545NwUYoH/r2Ov36zlzJZD0YI0QD2GehncmHZ8xCdDD3GmV2N1dwU7c+SiX0Y3i2Cd74/wPCpa0k/Xmh2WUIIO2F/ga41LJwAlWV23dVyJU08XPnL/V2YOqobmflFDPrHj8zccEjWgxFC1Mn+0nDrTDjwrdHVEhBndjWNZkCnUJZP6ktijD8vztvJY5+kcqJQbncnhLgy+wv0wNaQ8DB0f8zsShpd86aefDymB3+4pwOr9p+g/9ur+H6P3O5OCFE7Zdav8omJiTo1NdWUc9ujvUcKmDh7C3uOFPBwryheHNgBL3fz16sRQtxYSqlNWuvE2vbVq4WulOqvlNqrlDqglHruCsc8oJRKU0rtUkp9fj0Fi8u1beHLgieTGdc3js/WZzLondXsyD5tdllCCBtSZ6ArpSzAu8AAoAMwUinV4ZJjWgPPA8la647AJOuXKjxcLbwwsD2fj+1JUUkFQ/+1hnd/OECFrAcjhKB+LfQewAGtdbrWuhSYDQy55JjHgHe11icBtNbHrFumqCmpVRDLJ/XlrvgW/GX5XlKm/0RhSbnZZQkhTFafQA8Hsmo8z67aVlMboI1Sao1Sar1Sqr+1ChS18/N2458ju/LmsE6s/TmPh95fL6NghHBy1hrl4gq0Bm4FRgLvK6WaXXqQUmqcUipVKZV6/PhxK53aeZ1fD2baIzex72gB97+3Tm5MLYQTq0+g5wCRNZ5HVG2rKRtYqLUu01ofBPZhBPxFtNbTtNaJWuvE4ODga61ZXKJf++bMHNuL/LOlDJu6lrRcuXmGEM6oPoG+EWitlIpVSrkDI4CFlxwzH6N1jlIqCKMLJt16ZYq63BTtz5fje+Pqonjw3+tYn55ndklCiBuszkDXWpcDTwLLgd3AF1rrXUqp15RSg6sOWw7kKaXSgB+AZ7TWkig3WOvmvsx9Ionmfp784qOfWLZT1lgXwpnIxCIHdKqolEdnbGRr1in+eG88o3pGm12SEMJKrntikbAvzbzdmTm2F7e2DeHFeTuZsmK/LO4lhBOQQHdQXu4W/v3ITQzvFsHfV+zj9wt2ygQkIRycq9kFiMbjZnHhrfs7E+zrwXv//Zm8wlL+/mACnm6yBowQjkgC3cEppXhuQDuCmrjz+uLdnCz6iWm/SKSpp5vZpQkhrEy6XJzE2D5xvP1gAqkZJxnx7/UcKyg2uyQhhJVJoDuRe7uG82FKdzLyzjJ86loyTpw1uyQhhBVJoDuZW9oE8/ljvThbUsHwqWtlCV4hHIgEuhNKiGzGnPG98XSzMGLaOn7cf8LskoQQViCB7qRaBjfhq18lERngzZgZP7FoW67ZJQkhrpMEuhNr3tST/zzem66R/kyYvYUZaw6aXZIQ4jpIoDs5Py83PvllD+5s35xXFqXx1vK9MqtUCDslgS7wdLPwr1HdGNkjkn/+cIDn5u6gvKLS7LKEEA0kE4sEAK4WF/40tBPBTTz4x/cHyDtbyj8f6iqzSoWwI9JCF9WUUvzmf9ry2pCOfLfnKI98uIHTRWVmlyWEqCcJdHGZX/SO4Z8ju7Et6zQP/HsdR07LrFIh7IEEuqjVoM6hzBjTnZxT5xg+dS0HjhWaXZIQog4S6OKKkloFMXtcL0rKK7j/vbVsyTxpdklCiKuQQBdXFR/ux9wnkvD1dOOh9zewcu8xs0sSQlyBBLqoU3SgD3OfSCIu2IexH6cyb0u22SUJIWohgS7qJdjXg9njetEjNoCn/7ON91elm12SEOISEuii3nw93Zg+pjsDO7XgjSW7+dOS3VTKbe2EsBkysUg0iIerhXdGdiPQZxfTVqVzorCEPw/vjJtF2gZCmE0CXTSYxUXx2pCOBPt68Ldv93HybCnvjuqGt7v8dRLCTNKsEtdEKcWEfq3532Gd+O++4zz0/gay8ovMLksIpyZNKnFdRvaIIsDHnadmbaHP//1A9xh/BieEM6hTKAE+7maXJ4RTUWYtlZqYmKhTU1NNObewvuyTRSzYmsv8LTnsP1aIq4uiT+sghiSEc2eH5vh4SNtBCGtQSm3SWifWuk8CXViT1po9RwpYsDWXRdtyyTl1Dk83F+7s0IIhXcLo2yYYd1fp6RPiWkmgC1NUVmo2ZZ5kwdYcFm8/zMmiMvy83BjYKZQhCWH0iAnAxUWZXaYQdkUCXZiurKKSH/efYMHWHL5JO0pRaQUtmnoyOCGMwV3C6BjWFKUk3IWoiwS6sClFpeWs2H2MhVtzWLn3OOWVmpbBPgxJCGdwlzBignzMLlEImyWBLmzWybOlLN15hAVbc9hwMB+ALpHNGNIljLs7hxLS1NPkCoWwLRLowi7knjrH19tzWbA1l125Z3BRkNQyiMEJYfSPb0FTTzezSxTCdBLowu4cOFbAwq25LNiWy6G8ItxdXbi9bQhDEsK4rV2I3OtUOC0JdGG3tNZsyz7Ngq05LNp2mBOFJfh6uHJXfAuGJITROy4QV1lHRjgRCXThEMorKlmfns+CrTks23mEgpJygpp4cHdnYxhkQmQzGSkjHJ4EunA4xWUVrNx7jAVbc/luzzFKyyuJCvDmni6hdAj1IzLAi6gAb/y83CTkhUO5WqDXaz62Uqo/MAWwAB9ord+8ZH8K8Bcgp2rTP7XWH1xzxULUwdPNQv/4UPrHh3KmuIzlO4+wcFsuU1f+TM0l2n09XIkI8CbS34vIqseoQG8i/b2J8PfGy1364oXjqLOFrpSyAPuAO4FsYCMwUmudVuOYFCBRa/1kfU8sLXTRGApLysnMKyLrZBFZ+VVfJ89VPRZRXFZ50fFBTTyIDPAi0t+bqADv6u8jA7wJ9fOU/nlhc663hd4DOKC1Tq96s9nAECDtqq8SwgRNPFzpENaUDmFNL9unteZEYSmZ+UVkVwf+ObJOFrEl6ySLdxymokbz3uKiCPXzNILevyrsA4yWfWSAF8FNPKQ7R9iU+gR6OJBV43k20LOW44YrpfpitOaf1lpnXXqAUmocMA4gKiqq4dUKcR2UUgT7ehDs68FN0f6X7S+vqOTw6eLq1vz5sM/ML+K7Pcc4UVhy0fFebhYianTlnA/7qABvogO9ZYVJccNZ62/cImCW1rpEKfU48DFw+6UHaa2nAdPA6HKx0rmFsApXi4sRzgHete4/V1phtOyrwj6zRpfOxoP5FJSUX3gvF8XghDAe6xNH+9DLf1sQojHUJ9BzgMgazyO4cPETAK11Xo2nHwD/d/2lCWFbvNwttG7uS+vmvpft01pz+lxZddBvzMjni9QsvtqcQ982wYzrE0dyq0DpohGNqj4XRV0xulH6YQT5RuAhrfWuGseEaq0PV30/FHhWa93rau8rF0WFoztVVMrMDZnMWJvB8YIS2oc2ZVzfWO7uHCY31RbX7LrHoSulBgJvYwxb/Ehr/YZS6jUgVWu9UCn1v8BgoBzIB57QWu+52ntKoAtnUVJewYKtuby/Kp39xwoJ9fNkTHIMI3pEyfo0osFkYpEQNqCyUvPf/cd5f1U6a3/Oo4mHKw/1jCIlKYawZl5mlyfshAS6EDZmR/Zp3l+dzuIdh1HAPV3CGNsnlo5hfmaXJmycBLoQNir7ZBHT12Qw+6dMzpZWcHOrIB7rG0ff1kFyAVXUSgJdCBt3+lwZn2/IZPqagxwrKKFdC1/G9oljcJcwuam2uIgEuhB2orS8koXbjAuoe48W0LypB2OSYxnZIwo/L7mAKiTQhbA7WmtW7T/B+6vS+fHACXzcLYzoEcWY5Bgi/Guf+CScgwS6EHZsV+5pPlh9kEXbctHA3Z1DeaxPHPHhcgHVGUmgC+EAck+dY/qag8z6KYvCknKSWgbyWN84bm0TLBdQnYgEuhAO5ExxGbN/yuSjHzM4cqaYNs2bMLZPHEMSwvBwlfXdHZ0EuhAOqLS8ksU7cpm26iC7D58hxNeDlOQYRvWIxs9bLqA6Kgl0IRyY1pofD5xg2qp0Vu8/gbe7hQe7R/JocuwVV44U9stuAr2srIzs7GyKi4tNqUk0jKenJxEREbi5SWvQVuw+fIb3V6ezcGsulVpzb0I4L93dgQAfd7NLE1ZiN4F+8OBBfH19CQyUZUZtndaavLw8CgoKiI2NNbsccYnDp88xfU0GM9Zk4Oftxv/d15nb2oaYXZawgqsFuk1NQSsuLpYwtxNKKQIDA+W3KRsV6ufFCwPbs+DJZAJ93BkzfSMvzd9BUWl53S8WdsumAh2QMLcj8mdl+9qHNmX+r5MZ1zeOmRsyGfSPH9mSedLsskQjsblAF0JYl6ebhRcGtufzsb0oLa/kvvfW8fdv91FWUWl2acLKJNBryMvLIyEhgYSEBFq0aEF4eHj189LS0qu+NjU1lQkTJjTofDExMZw4ceJ6Shai3nq3DGTppD4MSQhjynf7uW/qWn4+Xmh2WcKK5LbkNQQGBrJ161YAXnnlFZo0acLvfve76v3l5eW4utb+kSUmJpKYWOt1CiFsRlNPN/72QAJ3tG/OC/N2MOgfq3lxYHse7hUtXWgOwGYD/dVFu0jLPWPV9+wQ1pQ/3NOxQa9JSUnB09OTLVu2kJyczIgRI5g4cSLFxcV4eXkxffp02rZty8qVK3nrrbf4+uuveeWVV8jMzCQ9PZ3MzEwmTZpU79Z7RkYGjz76KCdOnCA4OJjp06cTFRXFnDlzePXVV7FYLPj5+bFq1Sp27drFmDFjKC0tpbKykrlz59K6detr+WiEkxnYKZTEaH+e+XI7v1+wi293H+Mv93WmeVNPs0sT18FmA92WZGdns3btWiwWC2fOnGH16tW4urqyYsUKXnjhBebOnXvZa/bs2cMPP/xAQUEBbdu25YknnqjXeO2nnnqK0aNHM3r0aD766CMmTJjA/Pnzee2111i+fDnh4eGcOnUKgPfee4+JEycyatQoSktLqaiosPaPLhxYSFNPZozpzmcbMnljcRp3vb2KN+7txKDOoWaXJq6RzQZ6Q1vSjen+++/HYjHWyDh9+jSjR49m//79KKUoKyur9TWDBg3Cw8MDDw8PQkJCOHr0KBEREXWea926dXz11VcAPPLII0yePBmA5ORkUlJSeOCBBxg2bBgAvXv35o033iA7O5thw4ZJ61w0mFKKR3pFk9wykKf/s5Vff76ZFbvDeWVwR1l/3Q7JRdF68PHxqf7+97//Pbfddhs7d+5k0aJFVxyH7eHhUf29xWKhvPz6xv++9957vP7662RlZXHTTTeRl5fHQw89xMKFC/Hy8mLgwIF8//3313UO4bzigpvw5RNJTLqjNQu35TLg7VWs+znP7LJEA0mgN9Dp06cJDw8HYMaMGVZ//6SkJGbPng3AzJkz6dOnDwA///wzPXv25LXXXiM4OJisrCzS09OJi4tjwoQJDBkyhO3bt1u9HuE83CwuTLqjDXOfSMLDzcJDH6znjcVpFJdJV569kEBvoMmTJ/P888/TtWvX6251A3Tu3JmIiAgiIiL4zW9+wzvvvMP06dPp3Lkzn376KVOmTAHgmWeeoVOnTsTHx5OUlESXLl344osviI+PJyEhgZ07d/KLX/ziuusRIiGyGYsn3MzDPaN5f/VBhvxzjdUHKIjGYVNruezevZv27dubUo+4NvJn5th+2HuMyV9u51RRKb+5sy3j+sZhcZHhjWaym7VchBC25ba2ISyf1Jc72jfnz8v2MGLaOrLyi8wuS1yBBLoQ4qoCfNz516hu/O2BLuw5XED/t1fxRWoWZv12L65MAl0IUSelFMO6RbB0Uh/iw/2Y/OV2xn+2ibzCErNLEzVIoAsh6i3C35tZj/XixYHt+WHPce56ezXf7T5qdlmiigS6EKJBXFwUj/WNY+FTyQQ1ceeXH6fy/Fc7OFsia62bTQJdCHFN2rVoyoInk3n8ljhmb8xk4D9Ws+mQrLVuJgn0Gm708rkAW7duRSnFsmXLrrVsIUzj4Wrh+QHtmf1YL8orNPe/t5a/frNX1lo3iYxDv4KGLp97rZ599lnWrl1LXFwcH3/8sVXfu6aKiorq9WisyZb+zIS5CorLeHVRGl9uyqZTuB9/fzCBViFNzC7L4VxtHLrNLs7F0ufgyA7rvmeLTjDgzQa9pDGXz9VaM2fOHL799lv69OlDcXExnp7G8qV//vOf+eyzz3BxcWHAgAG8+eabHDhwgPHjx3P8+HEsFgtz5swhKyur+rwATz75JImJiaSkpBATE8ODDz7It99+y+TJkykoKGDatGmUlpbSqlUrPv30U7y9vTl69Cjjx48nPT0dgKlTp7Js2TICAgKYNGkSAC+++CIhISFMnDjxOv4AhCPz9XTjrfu7cEf7EJ7/ylhr/fkB7fhF7xhcZDLSDWG7gW5DGmv53LVr1xIbG0vLli259dZbWbx4McOHD2fp0qUsWLCADRs24O3tTX5+PgCjRo3iueeeY+jQoRQXF1NZWUlWVtZVaw8MDGTz5s2A0aX02GOPAfDSSy/x4Ycf8tRTTzFhwgRuueUW5s2bR0VFBYWFhYSFhTFs2DAmTZpEZWUls2fP5qeffrLGxykcXP/4ULpF+/Psl9t5ZVEa3+05xqQ72uDtbsHNonB1ccHVonCzuODqonC1uFRvd7MoudHGdbDdQG9gS7oxNdbyubNmzWLEiBEAjBgxgk8++YThw4ezYsUKxowZg7e3NwABAQEUFBSQk5PD0KFDAapb8nV58MEHq7/fuXMnL730EqdOnaKwsJC77roLgO+//55PPvkEoPoGGn5+fgQGBrJlyxaOHj1K165dCQwMrO9HJpxciK8nH6V05/OfMnn9690Mn7q23q+1uCgsLgq3S8K+9v8ELv8PwdXFBYul9tf7eroR6OOOv4+78ejtTmAT49Hd1f4vKdYr0JVS/YEpgAX4QGtda9oqpYYDXwLdtdaptR1jj2pbPnfevHlkZGRw66231vqaupbPraioYO7cuSxYsIA33ngDrTV5eXkUFBQ0qDZXV1cqKy9cgLp0Od+ataekpDB//ny6dOnCjBkzWLly5VXfe+zYscyYMYMjR47w6KOPNqguIZRSjOoZzW1tQ9iVe4byikrKKjXlFZWUV2jKKqseKyopr9peVqEpr95ufF9Woak4v63W4yopLqukvKL84u2XvE95habwKkMrfT1cCagK90AfdwJqfFX/B+BzYV8TD1eb+22izkBXSlmAd4E7gWxgo1JqodY67ZLjfIGJwIbGKNRWWGv53O+++47OnTuzfPny6m2jR49m3rx53Hnnnbz22muMGjWqusslICCAiIgI5s+fz7333ktJSQkVFRVER0eTlpZGSUkJ586d47vvvuPmm2+u9ZwFBQWEhoZSVlbGzJkzq3+Ofv36MXXqVCZNmlTd5eLn58fQoUN5+eWXKSsr4/PPP7/mn1U4t7BmXoQ18zK7DADKKyo5da6Mk2dLyTtbWv2YX+PrZFEph08Xk3b4DHlnSyktr33EjrvFBX8ft+pWfoCPBwHebsajj/Ho7+NGoI+H8Z+Ctxuulsb9LaA+LfQewAGtdTqAUmo2MARIu+S4PwJ/Bp6xaoU2ZvLkyYwePZrXX3+dQYMGXfP7zJo1q7r75Lzhw4czdepUli5dytatW0lMTMTd3Z2BAwfypz/9iU8//ZTHH3+cl19+GTc3N+bMmUNcXBwPPPAA8fHxxMbG0rVr1yue849//CM9e/YkODiYnj17Vv82MGXKFMaNG8eHH36IxWJh6tSp9O7dG3d3d2677TaaNWvWKCNkhLjRXC0uBDXxIKiJB/W5v5fWmqLSCvIv+Q/gsseiUnbmnCavsIQzxVf+LcDPy40AH3eevrMNg7uEWe8Hq1LnsEWl1H1Af6312KrnjwA9tdZP1jimG/Ci1nq4Umol8Lu6ulxsfdiigMrKSrp168acOXOueHs7+TMT4mJlFZWcLCrl5Nky8s6WcPJsGflnS8g//1hUxoOJkdzcOuia3r9Rhy0qpVyAvwEp9Th2HDAOICoq6npPLRpRWload999N0OHDpV7lQrRAG4WF0J8PQnx9QR8b+i56xPoOUBkjecRVdvO8wXigZVVFwhaAAuVUoMvbaVrracB08BooV9H3aKRdejQoXpcuhDCPtSnh34j0FopFauUcgdGAAvP79Ran9ZaB2mtY7TWMcB64LIwry9ZY9l+yJ+VELalzkDXWpcDTwLLgd3AF1rrXUqp15RSg61ZjKenJ3l5eRIUduD8MMv6jocXQjQ+m1rLpaysjOzs7MvGUgvb5OnpSURExGUzYIUQjcdu1nJxc3MjNjbW7DKEEMIu2f9cVyGEEIAEuhBCOAwJdCGEcBCmXRRVSh0HDl3jy4OAE1Ysx97J53Ex+TwukM/iYo7weURrrYNr22FaoF8PpVTqla7yOiP5PC4mn8cF8llczNE/D+lyEUIIByGBLoQQDsJeA32a2QXYGPk8LiafxwXyWVzMoT8Pu+xDF0IIcTl7baELIYS4hAS6EEI4CLsLdKVUf6XUXqXUAaXUc2bXYxalVKRS6gelVJpSapdSaqLZNdkCpZRFKbVFKfW12bWYTSnVTCn1pVJqj1Jqt1Kqt9k1mUUp9XTVv5OdSqlZSimHXCbUrgK9xg2rBwAdgJFKqQ7mVmWacuC3WusOQC/g1078WdQ0EWOZZwFTgGVa63ZAF5z0c1FKhQMTgEStdTxgwbivg8Oxq0Cnxg2rtdalwPkbVjsdrfVhrfXmqu8LMP6xhptblbmUUhHAIOADs2sxm1LKD+gLfAigtS7VWp8ytShzuQJeSilXwBvINbmeRmFvgR4OZNV4no2ThxiAUioG6ApsMLkUs70NTAYqTa7DFsQCx4HpVV1QHyilfMwuygxa6xzgLSATOAyc1lp/Y25VjcPeAl1cQinVBJgLTNJanzG7HrMope4GjmmtN5ldi41wBboBU7XWXYGzgFNec1JK+WP8Jh8LhAE+SqmHza2qcdhboNd1w2qnopRywwjzmVrrr8yux2TJwGClVAZGV9ztSqnPzC3JVNlAttb6/G9tX2IEvDO6AziotT6utS4DvgKSTK6pUdhboF/1htXORCmlMPpHd2ut/2Z2PWbTWj+vtY6oulH5COB7rbVDtsLqQ2t9BMhSSrWt2tQPSDOxJDNlAr2UUt5V/2764aAXiG3qFnR10VqXK6XO37DaAnyktd5lcllmSQYeAXYopbZWbXtBa73EvJKEjXkKmFnV+EkHxphcjym01huUUl8CmzFGh23BQZcAkKn/QgjhIOyty0UIIcQVSKALIYSDkEAXQggHIYEuhBAOQgJdCCEchAS6EEI4CAl0IYRwEP8PXnvBhlO6jVcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86681fcb-84e3-428c-82ea-fb08f5ea4a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986e2215-3eda-471e-be76-bace043b0770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../../data11/model/NLI/mdistilbertV3.1-nli/tokenizer_config.json',\n",
       " '../../../data11/model/NLI/mdistilbertV3.1-nli/special_tokens_map.json',\n",
       " '../../../data11/model/NLI/mdistilbertV3.1-nli/vocab.txt',\n",
       " '../../../data11/model/NLI/mdistilbertV3.1-nli/added_tokens.json',\n",
       " '../../../data11/model/NLI/mdistilbertV3.1-nli/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체모델 저장\n",
    "#OUTPATH = '../model/distilbert/distilbert-model-0317-distillation-best-nli'\n",
    "\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장\n",
    "VOCAB_PATH = OUTPATH\n",
    "os.makedirs(VOCAB_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d4a3a-a213-4018-bc1f-3d6e34a76887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
