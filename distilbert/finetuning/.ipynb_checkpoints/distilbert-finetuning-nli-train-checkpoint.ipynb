{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159bc54e-0c70-4cc7-b3ff-cb2b3683cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-30.log\n",
      "logfilepath:qnadataset_2022-03-30.log\n",
      "logfilepath:distilbertftmultitrain_2022-03-30.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "# NLI(Natural Language Interference:자연어 추론) 훈련 예제\n",
    "#\n",
    "# => input_ids : [CLS]senetence1(전제)[SEP]sentence2(가설)\n",
    "# => attention_mask : 1111111111(전체,가설)0000000(그외)\n",
    "# => token_type_ids : 0000000(전제)1111111(가설)00000000(그외)\n",
    "# => laels : 참(수반:entailment), 거짓(모순:contradiction), 모름(중립:neutral)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig, DistilBertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "logger = mlogging(loggername=\"distilbertfttrain\", logfilename=\"distilbertftmultitrain\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731f6745-a2f4-4b2d-9a19-1fa2efa6f3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../model/distilbert/distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ../../model/distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################################################\n",
    "# 변수들 설정\n",
    "# - model_path : from_pretrained() 로 호출하는 경우에는 모델파일이 있는 폴더 경로나 \n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')\n",
    "#          torch.load(model)로 로딩하는 경우에는 모델 파일 풀 경로\n",
    "#\n",
    "# - vocab_path : from_pretrained() 호출하는 경우에는 모델파일이 있는 폴더 경로나\n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')   \n",
    "#          BertTokenizer() 로 호출하는 경우에는 vocab.txt 파일 풀 경로,\n",
    "#\n",
    "# - OUTPATH : 출력 모델, vocab 저장할 폴더 경로\n",
    "#############################################################################################\n",
    "\n",
    "model_path = '../../model/distilbert/distilbert-base-multilingual-cased'\n",
    "vocab_path = '../../model/distilbert/distilbert-base-multilingual-cased'\n",
    "OUTPATH = '../../model/distilbert/distilbert-base-multilingual-cased-nli-0330'\n",
    "\n",
    "# tokeniaer 및 model 설정\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# strip_accents=False : True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "# do_lower_case=False : # 소문자 입력 사용 안함(한국어에서는 반드시 False)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(vocab_path, strip_accents=False, do_lower_case=False) \n",
    "                        \n",
    "# NLI 모델에서 레벨은 3개지(참,거짓,모름) 이므로, num_labels=3을 입력함\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=6)\n",
    "\n",
    "# 레벨을 멀티로 선택해야 하는 경우\n",
    "#model = BertForSequenceClassification.from_pretrained(model_path, problem_type=\"multi_label_classification\",num_labels=6)\n",
    "                   \n",
    "#기존 모델 파일을 로딩하는 경우    \n",
    "#model = torch.load(model_path) \n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918a05d8-aee5-42df-9a1c-1dc2443f655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135326979"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d611fe-b906-45ee-afc2-dcbfcd760751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:27:00,247 - bwpdataset - INFO - Creating features from dataset file at ../../korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "2022-03-30 14:27:00,248 - bwpdataset - INFO - loading data... LOOKING AT ../../korpora/klue-nli/klue-nli-v1.1_train.json\n",
      "2022-03-30 14:27:00,484 - bwpdataset - INFO - tokenize sentences, it could take a lot of time...\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "2022-03-30 14:27:07,297 - bwpdataset - INFO - tokenize sentences [took 6.812 s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680fd0f145434bcda81292dcef6f557d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:27:07,489 - bwpdataset - INFO - *** Example ***\n",
      "2022-03-30 14:27:07,490 - bwpdataset - INFO - sentence A, B: 힛걸 진심 최고다 그 어떤 히어로보다 멋지다 + 힛걸 진심 최고로 멋지다.\n",
      "2022-03-30 14:27:07,491 - bwpdataset - INFO - tokens: [CLS] [UNK] 진 ##심 최고 ##다 그 어떤 히 ##어로 ##보다 멋 ##지 ##다 [SEP] [UNK] 진 ##심 최고 ##로 멋 ##지 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-30 14:27:07,491 - bwpdataset - INFO - label: entailment\n",
      "2022-03-30 14:27:07,492 - bwpdataset - INFO - features: ClassificationFeatures(input_ids=[101, 100, 9708, 71013, 83491, 11903, 8924, 55910, 10025, 81483, 80001, 9270, 12508, 11903, 102, 100, 9708, 71013, 83491, 11261, 9270, 12508, 11903, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n",
      "2022-03-30 14:27:07,492 - bwpdataset - INFO - *** Example ***\n",
      "2022-03-30 14:27:07,493 - bwpdataset - INFO - sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n",
      "2022-03-30 14:27:07,494 - bwpdataset - INFO - tokens: [CLS] 100 ##분 ##간 [UNK] 그 ##래 ##도 [UNK] 2 ##점 ##준 ##다 [SEP] 100 ##분 ##간 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-30 14:27:07,494 - bwpdataset - INFO - label: contradiction\n",
      "2022-03-30 14:27:07,495 - bwpdataset - INFO - features: ClassificationFeatures(input_ids=[101, 10407, 37712, 18784, 100, 8924, 37388, 12092, 100, 123, 34907, 54867, 11903, 102, 10407, 37712, 18784, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "2022-03-30 14:27:07,495 - bwpdataset - INFO - Saving features into cached file, it could take a lot of time...\n",
      "2022-03-30 14:27:08,671 - bwpdataset - INFO - Saving features into cached file ../../korpora/klue-nli/cached_DistilBertTokenizer_128_klue-nli-v1.1_train.json [took 1.176 s]\n",
      "2022-03-30 14:27:08,685 - bwpdataset - INFO - Creating features from dataset file at ../../korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "2022-03-30 14:27:08,686 - bwpdataset - INFO - loading data... LOOKING AT ../../korpora/klue-nli/klue-nli-v1.1_dev.json\n",
      "2022-03-30 14:27:08,728 - bwpdataset - INFO - tokenize sentences, it could take a lot of time...\n",
      "2022-03-30 14:27:09,605 - bwpdataset - INFO - tokenize sentences [took 0.876 s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd64b44157f4b25acdeccb43d05169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:27:09,628 - bwpdataset - INFO - *** Example ***\n",
      "2022-03-30 14:27:09,629 - bwpdataset - INFO - sentence A, B: 흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다. + 어떤 방에서도 흡연은 금지됩니다.\n",
      "2022-03-30 14:27:09,630 - bwpdataset - INFO - tokens: [CLS] 흡 ##연 ##자 ##분 ##들은 발 ##코 ##니 ##가 있는 방 ##이 ##면 발 ##코 ##니 ##에서 흡 ##연 ##이 가 ##능 ##합 ##니다 . [SEP] 어떤 방 ##에서 ##도 흡 ##연 ##은 [UNK] . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-30 14:27:09,630 - bwpdataset - INFO - label: contradiction\n",
      "2022-03-30 14:27:09,631 - bwpdataset - INFO - features: ClassificationFeatures(input_ids=[101, 10020, 25486, 13764, 37712, 22879, 9323, 25517, 25503, 11287, 13767, 9328, 10739, 14867, 9323, 25517, 25503, 11489, 10020, 25486, 10739, 8843, 74986, 33188, 48345, 119, 102, 55910, 9328, 11489, 12092, 10020, 25486, 10892, 100, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "2022-03-30 14:27:09,631 - bwpdataset - INFO - *** Example ***\n",
      "2022-03-30 14:27:09,632 - bwpdataset - INFO - sentence A, B: 10명이 함께 사용하기 불편함없이 만족했다. + 10명이 함께 사용하기 불편함이 많았다.\n",
      "2022-03-30 14:27:09,632 - bwpdataset - INFO - tokens: [CLS] 10 ##명이 함께 사 ##용 ##하기 불 ##편 ##함 ##없 ##이 만 ##족 ##했다 . [SEP] 10 ##명이 함께 사 ##용 ##하기 불 ##편 ##함 ##이 많 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-30 14:27:09,633 - bwpdataset - INFO - label: contradiction\n",
      "2022-03-30 14:27:09,633 - bwpdataset - INFO - features: ClassificationFeatures(input_ids=[101, 10150, 66923, 19653, 9405, 24974, 22440, 9368, 50450, 48533, 119136, 10739, 9248, 52560, 12490, 119, 102, 10150, 66923, 19653, 9405, 24974, 22440, 9368, 50450, 48533, 10739, 9249, 27303, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "2022-03-30 14:27:09,634 - bwpdataset - INFO - Saving features into cached file, it could take a lot of time...\n",
      "2022-03-30 14:27:09,773 - bwpdataset - INFO - Saving features into cached file ../../korpora/klue-nli/cached_DistilBertTokenizer_128_klue-nli-v1.1_dev.json [took 0.139 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader_len: 1563, eval_loader_len: 188\n"
     ]
    }
   ],
   "source": [
    "# 학습 data loader 생성\n",
    "sys.path.append('..')\n",
    "from myutils import ClassificationDataset, KlueNLICorpus, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = 128   # 글자 최대 토큰 길이 해당 토큰 길이 이상은 잘린다.\n",
    "batch_size = 16        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "\n",
    "# 훈련할 csv 파일\n",
    "file_fpath = '../../korpora/klue-nli/klue-nli-v1.1_train.json'\n",
    "#file_fpath = 'Korpora/nsmc/ratings_train.txt'\n",
    "cache = True   # 캐쉬파일 생성할거면 True로 (True이면 loding할때 캐쉬파일있어도 이용안함)\n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 설정\n",
    "corpus = KlueNLICorpus()\n",
    "\n",
    "# 학습 dataset 생성\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath,max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "train_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "# 평가 dataset 생성\n",
    "file_fpath = '../../korpora/klue-nli/klue-nli-v1.1_dev.json'\n",
    "dataset = ClassificationDataset(file_fpath=file_fpath, max_seq_length=max_seq_len, tokenizer=tokenizer, corpus=corpus, overwrite_cache=cache)\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "eval_loader = DataLoader(dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "print('train_loader_len: {}, eval_loader_len: {}'.format(len(train_loader), len(eval_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fec919e-d8b7-4b20-a20b-28cf66a49a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n",
      "[101, 9034, 10530, 9356, 31728, 9321, 16617, 10739, 69708, 42428, 10459, 10020, 12030, 28143, 10892, 9405, 17342, 12508, 12508, 49137, 102, 9670, 89523, 9659, 22458, 76820, 102]\n",
      "[UNK]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# tokenier 테스트\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.encode(\"눈에 보이는 반전이었지만 영화의 흡인력은 사라지지 않았다\", \"정말 재미있다\"))\n",
    "print(tokenizer.convert_ids_to_tokens(131027))\n",
    "print(tokenizer.convert_tokens_to_ids('정말'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2068553-5650-47a4-bdd6-c9e79e1580cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:27:09,794 - distilbertfttrain - INFO - === model: ../../model/distilbert/distilbert-base-multilingual-cased ===\n",
      "2022-03-30 14:27:09,796 - distilbertfttrain - INFO - num_parameters: 135326979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b54ca72c0446e4915548f6501b02e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd936ff474d64021a4548ad39ad1e1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105035/2786573272.py:75: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-03-30 14:27:20,787 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 200 -> Train Loss: 1.1029, Train Accuracy: 0.332\n",
      "2022-03-30 14:27:30,462 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 400 -> Train Loss: 1.0981, Train Accuracy: 0.347\n",
      "2022-03-30 14:27:40,146 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 600 -> Train Loss: 1.0570, Train Accuracy: 0.443\n",
      "2022-03-30 14:27:49,700 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 800 -> Train Loss: 0.8936, Train Accuracy: 0.598\n",
      "2022-03-30 14:27:59,264 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 1000 -> Train Loss: 0.8474, Train Accuracy: 0.632\n",
      "2022-03-30 14:28:08,754 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 1200 -> Train Loss: 0.8418, Train Accuracy: 0.638\n",
      "2022-03-30 14:28:18,284 - distilbertfttrain - INFO - [Epoch 1/10] Iteration 1400 -> Train Loss: 0.8151, Train Accuracy: 0.648\n",
      "2022-03-30 14:28:26,155 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ec07990fc94f979958dbeca8b6dcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105035/2786573272.py:131: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n",
      "2022-03-30 14:28:28,630 - distilbertfttrain - INFO - [Epoch 1/10] Validatation Accuracy:0.6023333333333334\n",
      "2022-03-30 14:28:28,631 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:28:28,632 - distilbertfttrain - INFO - === 처리시간: 2.477 초 ===\n",
      "2022-03-30 14:28:28,633 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66eb955eedf43388e5b56fea9b21bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:28:30,749 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 1600 -> Train Loss: 0.8022, Train Accuracy: 0.661\n",
      "2022-03-30 14:28:40,738 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 1800 -> Train Loss: 0.7643, Train Accuracy: 0.677\n",
      "2022-03-30 14:28:50,140 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 2000 -> Train Loss: 0.7520, Train Accuracy: 0.695\n",
      "2022-03-30 14:28:59,617 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 2200 -> Train Loss: 0.7564, Train Accuracy: 0.678\n",
      "2022-03-30 14:29:08,928 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 2400 -> Train Loss: 0.7276, Train Accuracy: 0.700\n",
      "2022-03-30 14:29:18,245 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 2600 -> Train Loss: 0.7247, Train Accuracy: 0.689\n",
      "2022-03-30 14:29:27,475 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 2800 -> Train Loss: 0.7268, Train Accuracy: 0.694\n",
      "2022-03-30 14:29:36,688 - distilbertfttrain - INFO - [Epoch 2/10] Iteration 3000 -> Train Loss: 0.6930, Train Accuracy: 0.712\n",
      "2022-03-30 14:29:42,698 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65329ccf9c14637b29d14a44b20671c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:29:45,125 - distilbertfttrain - INFO - [Epoch 2/10] Validatation Accuracy:0.6353333333333333\n",
      "2022-03-30 14:29:45,126 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:29:45,128 - distilbertfttrain - INFO - === 처리시간: 2.429 초 ===\n",
      "2022-03-30 14:29:45,128 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea684121bef8431c97b3858a82f27430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:29:49,109 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 3200 -> Train Loss: 0.6660, Train Accuracy: 0.726\n",
      "2022-03-30 14:29:58,387 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 3400 -> Train Loss: 0.5939, Train Accuracy: 0.757\n",
      "2022-03-30 14:30:07,652 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 3600 -> Train Loss: 0.5799, Train Accuracy: 0.766\n",
      "2022-03-30 14:30:19,469 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 3800 -> Train Loss: 0.5616, Train Accuracy: 0.783\n",
      "2022-03-30 14:30:31,455 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 4000 -> Train Loss: 0.6302, Train Accuracy: 0.748\n",
      "2022-03-30 14:30:41,707 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 4200 -> Train Loss: 0.5910, Train Accuracy: 0.761\n",
      "2022-03-30 14:30:51,037 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 4400 -> Train Loss: 0.5823, Train Accuracy: 0.759\n",
      "2022-03-30 14:31:00,426 - distilbertfttrain - INFO - [Epoch 3/10] Iteration 4600 -> Train Loss: 0.6088, Train Accuracy: 0.758\n",
      "2022-03-30 14:31:04,558 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1986f247fb5a458fbf9896b58737b4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:31:06,954 - distilbertfttrain - INFO - [Epoch 3/10] Validatation Accuracy:0.6513333333333333\n",
      "2022-03-30 14:31:06,956 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:31:06,957 - distilbertfttrain - INFO - === 처리시간: 2.400 초 ===\n",
      "2022-03-30 14:31:06,959 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb0fbebe55d4223a2900c833bfddbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:31:12,586 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 4800 -> Train Loss: 0.5206, Train Accuracy: 0.798\n",
      "2022-03-30 14:31:22,455 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 5000 -> Train Loss: 0.4728, Train Accuracy: 0.813\n",
      "2022-03-30 14:31:31,732 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 5200 -> Train Loss: 0.4768, Train Accuracy: 0.822\n",
      "2022-03-30 14:31:41,130 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 5400 -> Train Loss: 0.4718, Train Accuracy: 0.818\n",
      "2022-03-30 14:31:50,195 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 5600 -> Train Loss: 0.4672, Train Accuracy: 0.816\n",
      "2022-03-30 14:31:59,269 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 5800 -> Train Loss: 0.4608, Train Accuracy: 0.828\n",
      "2022-03-30 14:32:08,217 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 6000 -> Train Loss: 0.4845, Train Accuracy: 0.812\n",
      "2022-03-30 14:32:17,202 - distilbertfttrain - INFO - [Epoch 4/10] Iteration 6200 -> Train Loss: 0.4883, Train Accuracy: 0.806\n",
      "2022-03-30 14:32:19,664 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13475ce1e514613996ee1cec4dc0f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:32:22,189 - distilbertfttrain - INFO - [Epoch 4/10] Validatation Accuracy:0.665\n",
      "2022-03-30 14:32:22,191 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:32:22,193 - distilbertfttrain - INFO - === 처리시간: 2.529 초 ===\n",
      "2022-03-30 14:32:22,194 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195c20ae1641485aa6f69cdc183f66ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:32:29,610 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 6400 -> Train Loss: 0.3981, Train Accuracy: 0.851\n",
      "2022-03-30 14:32:38,798 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 6600 -> Train Loss: 0.3409, Train Accuracy: 0.875\n",
      "2022-03-30 14:32:47,739 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 6800 -> Train Loss: 0.3646, Train Accuracy: 0.865\n",
      "2022-03-30 14:32:56,657 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 7000 -> Train Loss: 0.3843, Train Accuracy: 0.856\n",
      "2022-03-30 14:33:05,592 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 7200 -> Train Loss: 0.3821, Train Accuracy: 0.853\n",
      "2022-03-30 14:33:15,664 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 7400 -> Train Loss: 0.3833, Train Accuracy: 0.854\n",
      "2022-03-30 14:33:26,446 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 7600 -> Train Loss: 0.3789, Train Accuracy: 0.854\n",
      "2022-03-30 14:33:37,763 - distilbertfttrain - INFO - [Epoch 5/10] Iteration 7800 -> Train Loss: 0.3663, Train Accuracy: 0.864\n",
      "2022-03-30 14:33:38,589 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea66958b8aa4976a290dcc3cd876e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:33:41,040 - distilbertfttrain - INFO - [Epoch 5/10] Validatation Accuracy:0.6563333333333333\n",
      "2022-03-30 14:33:41,041 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:33:41,042 - distilbertfttrain - INFO - === 처리시간: 2.453 초 ===\n",
      "2022-03-30 14:33:41,043 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538c2f21baac4bac86e4a122e86e6b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:33:51,944 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 8000 -> Train Loss: 0.2791, Train Accuracy: 0.892\n",
      "2022-03-30 14:34:03,101 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 8200 -> Train Loss: 0.2972, Train Accuracy: 0.893\n",
      "2022-03-30 14:34:12,065 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 8400 -> Train Loss: 0.2825, Train Accuracy: 0.897\n",
      "2022-03-30 14:34:21,489 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 8600 -> Train Loss: 0.3078, Train Accuracy: 0.891\n",
      "2022-03-30 14:34:30,942 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 8800 -> Train Loss: 0.3038, Train Accuracy: 0.892\n",
      "2022-03-30 14:34:40,467 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 9000 -> Train Loss: 0.2908, Train Accuracy: 0.890\n",
      "2022-03-30 14:34:49,891 - distilbertfttrain - INFO - [Epoch 6/10] Iteration 9200 -> Train Loss: 0.2853, Train Accuracy: 0.898\n",
      "2022-03-30 14:34:58,313 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91870160eda4a27b9713871b4e50602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:35:00,802 - distilbertfttrain - INFO - [Epoch 6/10] Validatation Accuracy:0.655\n",
      "2022-03-30 14:35:00,804 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:35:00,805 - distilbertfttrain - INFO - === 처리시간: 2.492 초 ===\n",
      "2022-03-30 14:35:00,806 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3465033a642d48799223be2097957221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:35:02,366 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 9400 -> Train Loss: 0.2887, Train Accuracy: 0.899\n",
      "2022-03-30 14:35:11,547 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 9600 -> Train Loss: 0.2347, Train Accuracy: 0.924\n",
      "2022-03-30 14:35:20,821 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 9800 -> Train Loss: 0.2235, Train Accuracy: 0.926\n",
      "2022-03-30 14:35:30,502 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 10000 -> Train Loss: 0.2141, Train Accuracy: 0.918\n",
      "2022-03-30 14:35:39,785 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 10200 -> Train Loss: 0.2162, Train Accuracy: 0.925\n",
      "2022-03-30 14:35:49,010 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 10400 -> Train Loss: 0.2325, Train Accuracy: 0.923\n",
      "2022-03-30 14:35:58,163 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 10600 -> Train Loss: 0.2382, Train Accuracy: 0.921\n",
      "2022-03-30 14:36:07,168 - distilbertfttrain - INFO - [Epoch 7/10] Iteration 10800 -> Train Loss: 0.2190, Train Accuracy: 0.924\n",
      "2022-03-30 14:36:13,887 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd164e421c64b869bc0a3ec7016c734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:36:16,316 - distilbertfttrain - INFO - [Epoch 7/10] Validatation Accuracy:0.6566666666666666\n",
      "2022-03-30 14:36:16,318 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:36:16,319 - distilbertfttrain - INFO - === 처리시간: 2.433 초 ===\n",
      "2022-03-30 14:36:16,320 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8b2f7ed3214b3f9a1363fbcd6b5e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:36:19,730 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 11000 -> Train Loss: 0.2093, Train Accuracy: 0.930\n",
      "2022-03-30 14:36:29,572 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 11200 -> Train Loss: 0.1818, Train Accuracy: 0.943\n",
      "2022-03-30 14:36:39,080 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 11400 -> Train Loss: 0.1823, Train Accuracy: 0.941\n",
      "2022-03-30 14:36:48,622 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 11600 -> Train Loss: 0.1992, Train Accuracy: 0.933\n",
      "2022-03-30 14:36:57,937 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 11800 -> Train Loss: 0.1895, Train Accuracy: 0.938\n",
      "2022-03-30 14:37:07,791 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 12000 -> Train Loss: 0.1957, Train Accuracy: 0.936\n",
      "2022-03-30 14:37:17,725 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 12200 -> Train Loss: 0.1960, Train Accuracy: 0.941\n",
      "2022-03-30 14:37:27,879 - distilbertfttrain - INFO - [Epoch 8/10] Iteration 12400 -> Train Loss: 0.1950, Train Accuracy: 0.941\n",
      "2022-03-30 14:37:33,386 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb60fbe01e5544d6a76421b01e58f912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:37:35,845 - distilbertfttrain - INFO - [Epoch 8/10] Validatation Accuracy:0.649\n",
      "2022-03-30 14:37:35,847 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:37:35,848 - distilbertfttrain - INFO - === 처리시간: 2.463 초 ===\n",
      "2022-03-30 14:37:35,849 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace7b530ca714abe99e5d39bf86aa348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:37:40,893 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 12600 -> Train Loss: 0.1673, Train Accuracy: 0.951\n",
      "2022-03-30 14:37:50,391 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 12800 -> Train Loss: 0.1683, Train Accuracy: 0.950\n",
      "2022-03-30 14:37:59,670 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 13000 -> Train Loss: 0.1548, Train Accuracy: 0.954\n",
      "2022-03-30 14:38:09,064 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 13200 -> Train Loss: 0.1648, Train Accuracy: 0.949\n",
      "2022-03-30 14:38:18,345 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 13400 -> Train Loss: 0.1464, Train Accuracy: 0.954\n",
      "2022-03-30 14:38:27,640 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 13600 -> Train Loss: 0.1666, Train Accuracy: 0.952\n",
      "2022-03-30 14:38:37,063 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 13800 -> Train Loss: 0.1317, Train Accuracy: 0.959\n",
      "2022-03-30 14:38:46,709 - distilbertfttrain - INFO - [Epoch 9/10] Iteration 14000 -> Train Loss: 0.1417, Train Accuracy: 0.956\n",
      "2022-03-30 14:38:50,191 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642f210cbb9f44a7922e7a043c8903d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:38:52,756 - distilbertfttrain - INFO - [Epoch 9/10] Validatation Accuracy:0.653\n",
      "2022-03-30 14:38:52,758 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:38:52,760 - distilbertfttrain - INFO - === 처리시간: 2.568 초 ===\n",
      "2022-03-30 14:38:52,761 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37eead7a90b14217b4e326c7136cf250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:38:59,673 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 14200 -> Train Loss: 0.1397, Train Accuracy: 0.959\n",
      "2022-03-30 14:39:09,337 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 14400 -> Train Loss: 0.1273, Train Accuracy: 0.963\n",
      "2022-03-30 14:39:19,028 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 14600 -> Train Loss: 0.1147, Train Accuracy: 0.965\n",
      "2022-03-30 14:39:28,871 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 14800 -> Train Loss: 0.1185, Train Accuracy: 0.964\n",
      "2022-03-30 14:39:39,017 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 15000 -> Train Loss: 0.1303, Train Accuracy: 0.966\n",
      "2022-03-30 14:39:48,886 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 15200 -> Train Loss: 0.1182, Train Accuracy: 0.968\n",
      "2022-03-30 14:39:58,453 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 15400 -> Train Loss: 0.1150, Train Accuracy: 0.966\n",
      "2022-03-30 14:40:08,081 - distilbertfttrain - INFO - [Epoch 10/10] Iteration 15600 -> Train Loss: 0.1263, Train Accuracy: 0.968\n",
      "2022-03-30 14:40:09,686 - distilbertfttrain - INFO - ---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b532fa37885b49c38ca99a6329abd4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:40:12,188 - distilbertfttrain - INFO - [Epoch 10/10] Validatation Accuracy:0.6496666666666666\n",
      "2022-03-30 14:40:12,190 - distilbertfttrain - INFO - ---------------------------------------------------------\n",
      "2022-03-30 14:40:12,191 - distilbertfttrain - INFO - === 처리시간: 2.505 초 ===\n",
      "2022-03-30 14:40:12,191 - distilbertfttrain - INFO - -END-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "logger.info(f\"=== model: {model_path} ===\")\n",
    "logger.info(f\"num_parameters: {model.num_parameters()}\")\n",
    "\n",
    "# 학습 시작\n",
    "\n",
    "##################################################\n",
    "# 변수 설정\n",
    "##################################################\n",
    "epochs = 10            # epochs\n",
    "learning_rate = 2e-5  # 학습률\n",
    "p_itr = 200           # 손실률 보여줄 step 수\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "num_warmup_steps = total_steps * 0.1\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=num_warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        #print('Labels:{}'.format(labels))\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        #print('Loss:{}, logits:{}'.format(loss, logits))\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 정확도와 총 손실률 계산\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_len += len(labels)    \n",
    "            total_loss += loss.item()\n",
    "            #print('pred:{}, correct:{}'.format(pred, correct))\n",
    "\n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "                list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "        #if itr > 5:\n",
    "        #    break\n",
    "   \n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    # 평가 시작\n",
    "    \n",
    "    start = time.time()\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "    \n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    " \n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "    \n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            #loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            # 총 손실류 구함\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_test_correct += correct.sum().item()\n",
    "            total_test_len += len(labels)\n",
    "    \n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    logger.info(f'---------------------------------------------------------')\n",
    "    logger.info(f'=== 처리시간: {time.time() - start:.3f} 초 ===')\n",
    "    logger.info(f'-END-\\n')\n",
    "    ####################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56cc12-2914-4fea-a6c9-15bec647112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86681fcb-84e3-428c-82ea-fb08f5ea4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986e2215-3eda-471e-be76-bace043b0770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../model/distilbert/distilbert-base-multilingual-cased-nli-0330/tokenizer_config.json',\n",
       " '../../model/distilbert/distilbert-base-multilingual-cased-nli-0330/special_tokens_map.json',\n",
       " '../../model/distilbert/distilbert-base-multilingual-cased-nli-0330/vocab.txt',\n",
       " '../../model/distilbert/distilbert-base-multilingual-cased-nli-0330/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체모델 저장\n",
    "#OUTPATH = '../model/distilbert/distilbert-model-0317-distillation-best-nli'\n",
    "\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장\n",
    "VOCAB_PATH = OUTPATH\n",
    "os.makedirs(VOCAB_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d4a3a-a213-4018-bc1f-3d6e34a76887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
