{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159bc54e-0c70-4cc7-b3ff-cb2b3683cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-16.log\n",
      "logfilepath:qnadataset_2022-03-16.log\n",
      "logfilepath:bertQAtest_2022-03-16.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "# Question & Answer 테스트 예제\n",
    "#\n",
    "# => input_ids : [CLS]질문[SEP]지문[SEP]\n",
    "# => attention_mask : 1111111111(질문, 지문 모두 1)\n",
    "# => token_type_ids : 0000000(질문)1111111(지문)\n",
    "# => start_positions : 45 (질문에 대한 지문에서의 답변 시작 위치)\n",
    "# => end_positions : 60 (질문에 대한 지문에서의 답변 끝 위치)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging, QADataset\n",
    "\n",
    "logger = mlogging(loggername=\"bertQAtest\", logfilname=\"bertQAtest\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731f6745-a2f4-4b2d-9a19-1fa2efa6f3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(143772, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################################################\n",
    "# 변수들 설정\n",
    "# - model_path : from_pretrained() 로 호출하는 경우에는 모델파일이 있는 폴더 경로나 \n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')\n",
    "#          torch.load(model)로 로딩하는 경우에는 모델 파일 풀 경로\n",
    "#\n",
    "# - vocab_path : from_pretrained() 호출하는 경우에는 모델파일이 있는 폴더 경로나\n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')   \n",
    "#          BertTokenizer() 로 호출하는 경우에는 vocab.txt 파일 풀 경로,\n",
    "#\n",
    "# - OUTPATH : 출력 모델, vocab 저장할 폴더 경로\n",
    "#############################################################################################\n",
    "\n",
    "model_path = '../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315'\n",
    "vocab_path = '../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315'\n",
    "\n",
    "# tokeniaer 및 model 설정\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# strip_accents=False : True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "# do_lower_case=False : # 소문자 입력 사용 안함(한국어에서는 반드시 False)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(vocab_path, strip_accents=False, do_lower_case=False) \n",
    "   \n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918a05d8-aee5-42df-9a1c-1dc2443f655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153340418"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec919e-d8b7-4b20-a20b-28cf66a49a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143772\n",
      "[101, 9034, 10530, 119728, 11018, 128441, 10739, 69708, 42428, 10459, 10020, 12030, 28143, 10892, 124227, 12508, 49137, 102, 122108, 131027, 11903, 102]\n",
      "재미있\n",
      "122108\n"
     ]
    }
   ],
   "source": [
    "# tokenier 테스트\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.encode(\"눈에 보이는 반전이었지만 영화의 흡인력은 사라지지 않았다\", \"정말 재미있다\"))\n",
    "print(tokenizer.convert_ids_to_tokens(131027))\n",
    "print(tokenizer.convert_tokens_to_ids('정말'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "68abb782-4e86-437f-b1dd-a76ac20c8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "text = \"\"\"서울특별시는 대한민국의 수도이자 최대 도시이다. \n",
    "삼국시대 백제의 첫 수도인 위례성이었고, 고려의 남경이었으며, 조선의 수도가 된 이후로 현재까지 대한민국 정치·경제·사회·문화의 중심지이다. \n",
    "중앙으로 한강이 흐르고, 이를 기준으로 강북과 강남 지역으로 구분한다. \n",
    "북한산, 관악산, 도봉산, 불암산, 인릉산, 청계산, 아차산 등의 여러 산들로 둘러싸인 분지 지형의 도시이다.\n",
    "서울의 면적은 605.2 km2로 대한민국 면적의 0.6%이고, 인구는 약 950만 명으로 대한민국 인구의 17%를 차지한다. \n",
    "시청 소재지는 중구이며, 25개의 자치구가 있다. 1986년 아시안 게임, 1988년 하계 올림픽, 2010년 서울 G20 정상회의 등을 개최하였다. \n",
    "2018년 서울의 지역내총생산은 422조원이었다.'\n",
    "\"서울\" 어원에 관해 여러 가지 설이 존재하나, 학계에서는 일반적으로 수도를 뜻하는 신라 계통의 고유어인 서라벌에서 유래했다는 설을 유력하게 받아들이고 있다.\n",
    "이때 한자 가차 표기인 서라벌 원래 의미에 관해서도 여러 학설이 존재한다.\"\"\"\n",
    "\n",
    "#query='서울의 인구수는?'\n",
    "query = '서울에는 몇명이 살고 있을까?'\n",
    "#query ='서울은 어느나라 수도인가?'\n",
    "'''\n",
    "\n",
    "text = \"\"\"\n",
    "알베르트 아인슈타인은 독일 태생의 이론물리학자로서 역사상 가장 위대한 물리학자 중의 한명으로 널리 인정되고 있다. \n",
    "상대성 이론을 개발한 것으로 가장 잘 알려져 있지만 양자역학 이론의 발전에도 중요한 공헌을 했다. \n",
    "상대성 이론과 양자역학은 함께 현대 물리학의 두 기둥이다.\n",
    "그는 1921년 양자 이론 발전의 중추적인 단계에 대한 공로로 노벨 물리학상을 받았다.\n",
    "그는 1916년 일반 상대성이론에 대한 논문을 발표하여, 자신의 중력 이론을 소개했다. \n",
    "그는 1917년 일반상대성 이론을 적용하여 우주의 구조를 모델링했다.\n",
    "아인슈타인은 독일 제국에서 태어났지만 1895년 스위스로 이주하여 이듬해 독일 시민권을 포기했다. \n",
    "1897년, 17세의 나이로 취리히 연방 공과대학교의 수학 맟 물리학 교직 과정에 입학하여 1900년에 졸업했다. \n",
    "1901년에 스위스 시민권을 취득하여 평생 유지했으며, 1903년 그는 베른에 있는 스위스 특허국 사무소에서 정규직을 확보했다. \n",
    "1905년에, 그는 취리히 대학교에서 박사학위를 받았다. 1914년에, 아인슈타인은 프로이센 과학 아카데미와 훔볼트 대학교에 합류하기 위해 베를린으로 이주했다. \n",
    "1917년, 아인슈타인은 카이저 빌헬름 물리학 연구소의 소장이 되어, 이번에는 프로이센인으로 다시 독일 시민이 되었다.\n",
    "\"\"\"\n",
    "\n",
    "#query = '아인슈타인이 노벨 물리학상을 받은 년도는?'\n",
    "query = '아인슈타인이 태어난 나라는?'\n",
    "\n",
    "print(len(text))\n",
    "print(len(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8493556e-d438-4ddb-a3ae-b15aa66ab24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 502\n",
    "tokenized_input = tokenizer(query, text, return_tensors='pt',\n",
    "                   max_length=max_length, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "95ea8126-2eb6-45f2-84a6-8a53e91aabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', '아인슈타인', '##이', '태어난', '나라', '##는', '?', '[SEP]', '알베르트', '아인슈타인', '##은', '독일', '태생', '##의', '이론', '##물', '##리학', '##자로', '##서', '역사', '##상', '가장', '위대', '##한', '물리학자', '중', '##의', '한명', '##으로', '널리', '인정', '##되고', '있다', '.', '상대', '##성', '이론', '##을', '개발', '##한', '것으로', '가장', '잘', '알려져', '있지만', '양자', '##역학', '이론', '##의', '발전', '##에도', '중요한', '공헌', '##을', '했다', '.', '상대', '##성', '이론', '##과', '양자', '##역학', '##은', '함께', '현대', '물리학', '##의', '두', '기둥', '##이다', '.', '그는', '1921', '##년', '양자', '이론', '발전', '##의', '중추', '##적인', '단계', '##에', '대한', '공로', '##로', '노벨', '물리학상', '##을', '받았다', '.', '그는', '1916', '##년', '일반', '상대', '##성이', '##론', '##에', '대한', '논문', '##을', '발표', '##하여', ',', '자신의', '중력', '이론', '##을', '소개', '##했다', '.', '그는', '1917', '##년', '일반', '##상', '##대성', '이론', '##을', '적용', '##하여', '우주', '##의', '구조', '##를', '모델링', '##했다', '.', '아인슈타인', '##은', '독일', '제국', '##에서', '태어났', '##지만', '1895', '##년', '스위스', '##로', '이주하', '##여', '이듬해', '독일', '시민', '##권을', '포기', '##했다', '.', '1897', '##년', ',', '17', '##세의', '나이', '##로', '취리히', '연방', '공과', '##대학교', '##의', '수학', '[UNK]', '물리학', '교직', '과정', '##에', '입학', '##하여', '1900', '##년에', '졸업', '##했다', '.', '1901', '##년에', '스위스', '시민', '##권을', '취득', '##하여', '평생', '유지', '##했으며', ',', '1903', '##년', '그는', '베른', '##에', '있는', '스위스', '특허', '##국', '사무소', '##에서', '정규', '##직', '##을', '확보', '##했다', '.', '1905', '##년에', ',', '그는', '취리히', '대학교', '##에서', '박사', '##학', '##위를', '받았다', '.', '1914', '##년에', ',', '아인슈타인', '##은', '프로이센', '과학', '아카데미', '##와', '훔', '##볼트', '대학교', '##에', '합류', '##하기', '위해', '베를린', '##으로', '이주', '##했다', '.', '1917', '##년', ',', '아인슈타인', '##은', '카이저', '빌헬름', '물리학', '연구소', '##의', '소장', '##이', '되어', ',', '이번', '##에는', '프로이센', '##인', '##으로', '다시', '독일', '시민', '##이', '되었다', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
      "{'input_ids': tensor([[   101, 128844,  10739, 122066, 119690,  11018,    136,    102, 137518,\n",
      "         128844,  10892,  51175, 126597,  10459, 119945,  29364, 123792,  57713,\n",
      "          12424, 119598,  14871,  22224, 122503,  11102, 126046,   9694,  10459,\n",
      "         133459,  11467, 107323, 119857,  29208,  11506,    119, 119699,  17138,\n",
      "         119945,  10622, 110176,  11102,  23925,  22224,   9654,  62350,  76123,\n",
      "         121391, 129715, 119945,  10459, 119800,  35979,  63552, 123265,  10622,\n",
      "          23622,    119, 119699,  17138, 119945,  11882, 121391, 129715,  10892,\n",
      "          19653, 104518, 122518,  10459,   9102, 123363,  11925,    119,  17889,\n",
      "          11466,  10954, 121391, 119945, 119800,  10459, 123444,  15387, 120207,\n",
      "          10530,  18154, 122526,  11261, 122157, 136992,  10622,  46530,    119,\n",
      "          17889,  11785,  10954, 119640, 119699,  53371,  42769,  10530,  18154,\n",
      "         121009,  10622, 119696,  13374,    117,  31956, 123369, 119945,  10622,\n",
      "         120304,  12490,    119,  17889,  11368,  10954, 119640,  14871, 135102,\n",
      "         119945,  10622, 120029,  13374, 119896,  10459, 119719,  11513, 135563,\n",
      "          12490,    119, 128844,  10892,  51175, 119733,  11489, 120111,  28578,\n",
      "          12599,  10954, 121132,  11261, 132279,  29935, 120897,  51175, 120054,\n",
      "          57952, 121697,  12490,    119,  12549,  10954,    117,  10273,  65443,\n",
      "         119952,  11261, 131402, 120144, 124900,  30461,  10459, 120510,    100,\n",
      "         122518, 129827, 119692,  10530, 120473,  13374,  11568,  27056,  45004,\n",
      "          12490,    119,  12255,  27056, 121132, 120054,  57952, 122086,  13374,\n",
      "         123677, 119819,  51491,    117,  12175,  10954,  17889, 128749,  10530,\n",
      "          13767, 121132, 124375,  20479, 121436,  11489, 120161,  33077,  10622,\n",
      "         121291,  12490,    119,  11812,  27056,    117,  17889, 131402,  72085,\n",
      "          11489, 120228,  23321,  31166,  46530,    119,  11306,  27056,    117,\n",
      "         128844,  10892, 123911, 119703, 121484,  12638,  10006, 138939,  72085,\n",
      "          10530, 121316,  22440,  19905, 122216,  11467, 121059,  12490,    119,\n",
      "          11368,  10954,    117, 128844,  10892, 134999, 125979, 122518, 120279,\n",
      "          10459, 120761,  10739,  37909,    117, 120709,  15303, 123911,  12030,\n",
      "          11467,  25805,  51175, 120054,  10739,  17737,    119,    102,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "token_str = [[tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0]]]\n",
    "print(token_str)\n",
    "print(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "24d3c20c-3f27-4b2b-a690-cc2a79ca864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*시작 token idx:tensor([11]), 끝 token idx:tensor([11])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model(**tokenized_input)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "#print(start_scores)\n",
    "#print(end_scores)\n",
    "\n",
    "start_pred = torch.argmax(start_scores, dim=1)\n",
    "end_pred = torch.argmax(end_scores, dim=1)\n",
    "    \n",
    "print(f'*시작 token idx:{start_pred}, 끝 token idx:{end_pred}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "354cab37-a7e3-4a63-a0a4-d56aa42873e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Q: 아인슈타인이 태어난 나라는?\n",
      "*A: 독일 \n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰 inddex과 끝 토큰 index+1 위치에 토큰 id 값을 토큰으로 변환함\n",
    "start_token = int(start_pred)\n",
    "end_token = int(end_pred)+1\n",
    "\n",
    "token_list = [tokenizer.convert_ids_to_tokens(s) for s in tokenized_input['input_ids'].tolist()[0][start_token:end_token]]\n",
    "\n",
    "# 출력한 토큰값을 join\n",
    "#answer = ' '.join(token_list)\n",
    "\n",
    "# token list에서 token들을 얻어와서, 앞에 '##' 가 있으면 없엠\n",
    "answer = ''\n",
    "for token in token_list:\n",
    "    #print(token[0:2])\n",
    "    if token[0:2] == \"##\":\n",
    "        answer += token[2:]\n",
    "    else:\n",
    "        answer += token + ' '\n",
    "        \n",
    "\n",
    "print(f'*Q: {query}')\n",
    "print(f'*A: {answer}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11bea0-37d4-42c0-8e91-afd7aa7bd949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c50be2-fd58-45bd-9cb9-51305ccf8e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
