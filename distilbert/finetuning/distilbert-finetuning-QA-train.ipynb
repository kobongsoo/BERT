{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159bc54e-0c70-4cc7-b3ff-cb2b3683cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logfilepath:bwdataset_2022-03-16.log\n",
      "logfilepath:qnadataset_2022-03-16.log\n",
      "logfilepath:bertQAtrain_2022-03-16.log\n",
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "#==================================================================================\n",
    "# Question & Answer 훈련 예제\n",
    "#\n",
    "# => input_ids : [CLS]질문[SEP]지문[SEP]\n",
    "# => attention_mask : 1111111111(질문, 지문 모두 1)\n",
    "# => token_type_ids : 0000000(질문)1111111(지문)\n",
    "# => start_positions : 45 (질문에 대한 지문에서의 답변 시작 위치)\n",
    "# => end_positions : 60 (질문에 대한 지문에서의 답변 끝 위치)\n",
    "#==================================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging, QADataset\n",
    "\n",
    "logger = mlogging(loggername=\"bertQAtrain\", logfilname=\"bertQAtrain\")\n",
    "device = GPU_info()\n",
    "seed_everything(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "731f6745-a2f4-4b2d-9a19-1fa2efa6f3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313 were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at ../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(143772, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################################################\n",
    "# 변수들 설정\n",
    "# - model_path : from_pretrained() 로 호출하는 경우에는 모델파일이 있는 폴더 경로나 \n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')\n",
    "#          torch.load(model)로 로딩하는 경우에는 모델 파일 풀 경로\n",
    "#\n",
    "# - vocab_path : from_pretrained() 호출하는 경우에는 모델파일이 있는 폴더 경로나\n",
    "#          huggingface에 등록된 모델명(예:'bert-base-multilingual-cased')   \n",
    "#          BertTokenizer() 로 호출하는 경우에는 vocab.txt 파일 풀 경로,\n",
    "#\n",
    "# - OUTPATH : 출력 모델, vocab 저장할 폴더 경로\n",
    "#############################################################################################\n",
    "\n",
    "model_path = '../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313'\n",
    "vocab_path = '../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313'\n",
    "OUTPATH = '../model/distilbert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315/'\n",
    "\n",
    "# tokeniaer 및 model 설정\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# strip_accents=False : True로 하면, 가자 => ㄱ ㅏ ㅈ ㅏ 식으로 토큰화 되어 버림(*따라서 한국어에서는 반드시 False)\n",
    "# do_lower_case=False : # 소문자 입력 사용 안함(한국어에서는 반드시 False)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(vocab_path, strip_accents=False, do_lower_case=False) \n",
    "   \n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918a05d8-aee5-42df-9a1c-1dc2443f655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153340418"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d611fe-b906-45ee-afc2-dcbfcd760751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:30:25,327 - qnadataset - INFO - Creating features from dataset file at ../korpora/korQuAD/KorQuAD_v1.0_train.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create train_loader===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1420/1420 [00:00<00:00, 19789.84it/s]\n",
      "convert squad examples to features: 100%|█| 57688/57688 [01:13<00:00, 779.91it/s\n",
      "2022-03-16 09:31:40,534 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,536 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 괴테 ##의 파우스트 ##를 읽 ##고 무엇 ##을 쓰 ##고 ##자 했 ##는 ##가 ? [SEP] 1839 ##년 바그너 ##는 괴테 ##의 파우스트 ##을 처음 읽 ##고 그 내용 ##에 마음 ##이 끌려 이를 소재 ##로 해서 하나의 교향곡 ##을 쓰 ##려 ##는 뜻 ##을 갖 ##는다 . 이 시기 바그너 ##는 1838 ##년에 빛 독 ##촉 ##으로 산전 ##수 ##전을 다 [UNK] 상황 ##이 ##라 좌절 ##과 실망 ##에 가득 ##했으며 메 ##피스 ##토 ##펠 ##레스 ##를 만나 ##는 파우스트 ##의 심경 ##에 공감 ##했다 ##고 한다 . 또한 파리 ##에서 아브 ##네 ##크 ##의 지휘 ##로 파리 음악원 관현악단 ##이 연주 ##하는 베토벤 ##의 교향곡 9 ##번 ##을 듣 ##고 깊 ##은 감명 ##을 받 ##았 ##는데 , 이것 ##이 이듬해 1월 [SEP]\n",
      "2022-03-16 09:31:40,537 - qnadataset - INFO - answer: 교향곡\n",
      "2022-03-16 09:31:40,539 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 143664, 10459, 141463, 11513, 9642, 11664, 120950, 10622, 9511, 11664, 13764, 9965, 11018, 11287, 136, 102, 16221, 10954, 131787, 11018, 143664, 10459, 141463, 10622, 62849, 9642, 11664, 8924, 119632, 10530, 119916, 10739, 126940, 35756, 120286, 11261, 119687, 90387, 124556, 10622, 9511, 26737, 11018, 9153, 10622, 8854, 40410, 119, 9638, 119876, 131787, 11018, 16347, 27056, 9387, 9088, 119267, 11467, 142025, 15891, 54918, 9056, 100, 119803, 10739, 17342, 126431, 11882, 126981, 10530, 124809, 51491, 9272, 121657, 26444, 119394, 100929, 11513, 120312, 11018, 141463, 10459, 141726, 10530, 124618, 12490, 11664, 16139, 119, 19789, 120425, 11489, 130625, 77884, 20308, 10459, 120154, 11261, 120425, 137413, 133390, 10739, 120441, 12178, 128268, 10459, 124556, 130, 35465, 10622, 9116, 11664, 8938, 10892, 139954, 10622, 9322, 119118, 41850, 117, 119757, 10739, 120897, 17206, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=41, end_positions=41)\n",
      "2022-03-16 09:31:40,540 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,542 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 괴테 ##의 파우스트 ##를 읽 ##고 무엇 ##을 쓰 ##고 ##자 했 ##는 ##가 ? [SEP] 파우스트 ##의 심경 ##에 공감 ##했다 ##고 한다 . 또한 파리 ##에서 아브 ##네 ##크 ##의 지휘 ##로 파리 음악원 관현악단 ##이 연주 ##하는 베토벤 ##의 교향곡 9 ##번 ##을 듣 ##고 깊 ##은 감명 ##을 받 ##았 ##는데 , 이것 ##이 이듬해 1월 ##에 파우스트 ##의 서곡 ##으로 쓰여진 이 작품 ##에 조금 ##이 ##라도 영향을 끼쳤 ##으 ##리 ##라는 것은 의심 ##할 여지 ##가 없다 . 여기 ##의 라 ##단조 조성 ##의 경우 ##에도 그의 전기 ##에 적혀 있는 것 ##처럼 단순 ##한 정신 ##적 피로 ##나 실 ##의 ##가 반영 ##된 것이 아니라 베토벤 ##의 합창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 [SEP]\n",
      "2022-03-16 09:31:40,543 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,545 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 143664, 10459, 141463, 11513, 9642, 11664, 120950, 10622, 9511, 11664, 13764, 9965, 11018, 11287, 136, 102, 141463, 10459, 141726, 10530, 124618, 12490, 11664, 16139, 119, 19789, 120425, 11489, 130625, 77884, 20308, 10459, 120154, 11261, 120425, 137413, 133390, 10739, 120441, 12178, 128268, 10459, 124556, 130, 35465, 10622, 9116, 11664, 8938, 10892, 139954, 10622, 9322, 119118, 41850, 117, 119757, 10739, 120897, 17206, 10530, 141463, 10459, 137473, 11467, 131367, 9638, 119626, 10530, 120893, 10739, 120365, 58088, 130331, 119185, 12692, 60362, 30050, 121471, 14843, 124889, 11287, 39218, 119, 119777, 10459, 9157, 136161, 120668, 10459, 28467, 35979, 21555, 119942, 10530, 127086, 13767, 8870, 92383, 120543, 11102, 120174, 14801, 128478, 16439, 9489, 10459, 11287, 122102, 13441, 27487, 45021, 128268, 10459, 124616, 25242, 79544, 55670, 120668, 10459, 58088, 74141, 21371, 9359, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,548 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,550 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 괴테 ##의 파우스트 ##를 읽 ##고 무엇 ##을 쓰 ##고 ##자 했 ##는 ##가 ? [SEP] 여지 ##가 없다 . 여기 ##의 라 ##단조 조성 ##의 경우 ##에도 그의 전기 ##에 적혀 있는 것 ##처럼 단순 ##한 정신 ##적 피로 ##나 실 ##의 ##가 반영 ##된 것이 아니라 베토벤 ##의 합창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교향곡 작곡 ##을 1839 ##년부터 40 ##년에 걸쳐 파리 ##에서 착수 ##했 ##으나 1 ##악 ##장을 쓴 뒤 ##에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서곡 ( 1 ##악 ##장 ) 을 파리 음악원 ##의 연주회 ##에서 연주 ##할 파트 ##보 ##까지 준비 ##하 ##였으나 , 실제로 ##는 이루어지 ##지는 않았다 . 결국 초연 ##은 [SEP]\n",
      "2022-03-16 09:31:40,551 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,552 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 143664, 10459, 141463, 11513, 9642, 11664, 120950, 10622, 9511, 11664, 13764, 9965, 11018, 11287, 136, 102, 124889, 11287, 39218, 119, 119777, 10459, 9157, 136161, 120668, 10459, 28467, 35979, 21555, 119942, 10530, 127086, 13767, 8870, 92383, 120543, 11102, 120174, 14801, 128478, 16439, 9489, 10459, 11287, 122102, 13441, 27487, 45021, 128268, 10459, 124616, 25242, 79544, 55670, 120668, 10459, 58088, 74141, 21371, 9359, 9460, 11506, 119, 121608, 124556, 76512, 10622, 16221, 87188, 10533, 27056, 92210, 120425, 11489, 125385, 119424, 35466, 122, 119110, 35963, 9512, 9109, 10530, 120604, 12490, 119, 19789, 119626, 10459, 120899, 11882, 58248, 17889, 9638, 137473, 113, 122, 119110, 13890, 114, 9633, 120425, 137413, 10459, 132695, 11489, 120441, 14843, 121425, 30005, 18382, 120339, 35506, 74519, 117, 120603, 11018, 121661, 32815, 49137, 119, 50342, 127134, 10892, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,553 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,555 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 괴테 ##의 파우스트 ##를 읽 ##고 무엇 ##을 쓰 ##고 ##자 했 ##는 ##가 ? [SEP] 쓴 뒤 ##에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서곡 ( 1 ##악 ##장 ) 을 파리 음악원 ##의 연주회 ##에서 연주 ##할 파트 ##보 ##까지 준비 ##하 ##였으나 , 실제로 ##는 이루어지 ##지는 않았다 . 결국 초연 ##은 4 ##년 반 ##이 지난 후에 드레스 ##덴 ##에서 연주 ##되었고 재연 ##도 이루 ##어졌 ##지만 , 이후 ##에 그대로 방치 ##되고 말 ##았다 . 그 사이에 그는 리엔 ##치 ##와 방황 ##하는 네덜란드인 ##을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착수 ##하는 등 분 ##주 ##한 시간 ##을 보냈 ##는데 , 그런 바 ##쁜 생활 ##이 이 곡 ##을 잊 ##게 한 [SEP]\n",
      "2022-03-16 09:31:40,556 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,557 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 143664, 10459, 141463, 11513, 9642, 11664, 120950, 10622, 9511, 11664, 13764, 9965, 11018, 11287, 136, 102, 9512, 9109, 10530, 120604, 12490, 119, 19789, 119626, 10459, 120899, 11882, 58248, 17889, 9638, 137473, 113, 122, 119110, 13890, 114, 9633, 120425, 137413, 10459, 132695, 11489, 120441, 14843, 121425, 30005, 18382, 120339, 35506, 74519, 117, 120603, 11018, 121661, 32815, 49137, 119, 50342, 127134, 10892, 125, 10954, 9321, 10739, 120600, 56528, 126715, 118790, 11489, 120441, 49953, 142552, 12092, 119661, 121022, 28578, 117, 18347, 10530, 110589, 128194, 29208, 9251, 27303, 119, 8924, 64932, 17889, 143091, 18622, 12638, 135821, 12178, 140872, 10622, 120899, 12453, 9847, 20309, 10739, 48387, 35979, 125385, 12178, 9121, 9367, 16323, 11102, 119610, 10622, 121357, 41850, 117, 119989, 9318, 119023, 119782, 10739, 9638, 8889, 10622, 9649, 14153, 9954, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,562 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,563 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 괴테 ##의 파우스트 ##를 읽 ##고 무엇 ##을 쓰 ##고 ##자 했 ##는 ##가 ? [SEP] 방치 ##되고 말 ##았다 . 그 사이에 그는 리엔 ##치 ##와 방황 ##하는 네덜란드인 ##을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착수 ##하는 등 분 ##주 ##한 시간 ##을 보냈 ##는데 , 그런 바 ##쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-16 09:31:40,564 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,566 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 143664, 10459, 141463, 11513, 9642, 11664, 120950, 10622, 9511, 11664, 13764, 9965, 11018, 11287, 136, 102, 128194, 29208, 9251, 27303, 119, 8924, 64932, 17889, 143091, 18622, 12638, 135821, 12178, 140872, 10622, 120899, 12453, 9847, 20309, 10739, 48387, 35979, 125385, 12178, 9121, 9367, 16323, 11102, 119610, 10622, 121357, 41850, 117, 119989, 9318, 119023, 119782, 10739, 9638, 8889, 10622, 9649, 14153, 9954, 27487, 127780, 23969, 119743, 12092, 11506, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,567 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,568 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 교향곡 작곡 ##을 어디 ##까지 쓴 뒤 ##에 중단 ##했 ##는 ##가 ? [SEP] 1839 ##년 바그너 ##는 괴테 ##의 파우스트 ##을 처음 읽 ##고 그 내용 ##에 마음 ##이 끌려 이를 소재 ##로 해서 하나의 교향곡 ##을 쓰 ##려 ##는 뜻 ##을 갖 ##는다 . 이 시기 바그너 ##는 1838 ##년에 빛 독 ##촉 ##으로 산전 ##수 ##전을 다 [UNK] 상황 ##이 ##라 좌절 ##과 실망 ##에 가득 ##했으며 메 ##피스 ##토 ##펠 ##레스 ##를 만나 ##는 파우스트 ##의 심경 ##에 공감 ##했다 ##고 한다 . 또한 파리 ##에서 아브 ##네 ##크 ##의 지휘 ##로 파리 음악원 관현악단 ##이 연주 ##하는 베토벤 ##의 교향곡 9 ##번 ##을 듣 ##고 깊 ##은 감명 ##을 받 ##았 ##는데 , 이것 ##이 이듬해 1월 ##에 파우스트 [SEP]\n",
      "2022-03-16 09:31:40,569 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,570 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 124556, 76512, 10622, 121328, 18382, 9512, 9109, 10530, 120604, 119424, 11018, 11287, 136, 102, 16221, 10954, 131787, 11018, 143664, 10459, 141463, 10622, 62849, 9642, 11664, 8924, 119632, 10530, 119916, 10739, 126940, 35756, 120286, 11261, 119687, 90387, 124556, 10622, 9511, 26737, 11018, 9153, 10622, 8854, 40410, 119, 9638, 119876, 131787, 11018, 16347, 27056, 9387, 9088, 119267, 11467, 142025, 15891, 54918, 9056, 100, 119803, 10739, 17342, 126431, 11882, 126981, 10530, 124809, 51491, 9272, 121657, 26444, 119394, 100929, 11513, 120312, 11018, 141463, 10459, 141726, 10530, 124618, 12490, 11664, 16139, 119, 19789, 120425, 11489, 130625, 77884, 20308, 10459, 120154, 11261, 120425, 137413, 133390, 10739, 120441, 12178, 128268, 10459, 124556, 130, 35465, 10622, 9116, 11664, 8938, 10892, 139954, 10622, 9322, 119118, 41850, 117, 119757, 10739, 120897, 17206, 10530, 141463, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,572 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,573 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 교향곡 작곡 ##을 어디 ##까지 쓴 뒤 ##에 중단 ##했 ##는 ##가 ? [SEP] 파우스트 ##의 심경 ##에 공감 ##했다 ##고 한다 . 또한 파리 ##에서 아브 ##네 ##크 ##의 지휘 ##로 파리 음악원 관현악단 ##이 연주 ##하는 베토벤 ##의 교향곡 9 ##번 ##을 듣 ##고 깊 ##은 감명 ##을 받 ##았 ##는데 , 이것 ##이 이듬해 1월 ##에 파우스트 ##의 서곡 ##으로 쓰여진 이 작품 ##에 조금 ##이 ##라도 영향을 끼쳤 ##으 ##리 ##라는 것은 의심 ##할 여지 ##가 없다 . 여기 ##의 라 ##단조 조성 ##의 경우 ##에도 그의 전기 ##에 적혀 있는 것 ##처럼 단순 ##한 정신 ##적 피로 ##나 실 ##의 ##가 반영 ##된 것이 아니라 베토벤 ##의 합창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 [SEP]\n",
      "2022-03-16 09:31:40,577 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,578 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 124556, 76512, 10622, 121328, 18382, 9512, 9109, 10530, 120604, 119424, 11018, 11287, 136, 102, 141463, 10459, 141726, 10530, 124618, 12490, 11664, 16139, 119, 19789, 120425, 11489, 130625, 77884, 20308, 10459, 120154, 11261, 120425, 137413, 133390, 10739, 120441, 12178, 128268, 10459, 124556, 130, 35465, 10622, 9116, 11664, 8938, 10892, 139954, 10622, 9322, 119118, 41850, 117, 119757, 10739, 120897, 17206, 10530, 141463, 10459, 137473, 11467, 131367, 9638, 119626, 10530, 120893, 10739, 120365, 58088, 130331, 119185, 12692, 60362, 30050, 121471, 14843, 124889, 11287, 39218, 119, 119777, 10459, 9157, 136161, 120668, 10459, 28467, 35979, 21555, 119942, 10530, 127086, 13767, 8870, 92383, 120543, 11102, 120174, 14801, 128478, 16439, 9489, 10459, 11287, 122102, 13441, 27487, 45021, 128268, 10459, 124616, 25242, 79544, 55670, 120668, 10459, 58088, 74141, 21371, 9359, 9460, 11506, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,579 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,580 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 교향곡 작곡 ##을 어디 ##까지 쓴 뒤 ##에 중단 ##했 ##는 ##가 ? [SEP] 여지 ##가 없다 . 여기 ##의 라 ##단조 조성 ##의 경우 ##에도 그의 전기 ##에 적혀 있는 것 ##처럼 단순 ##한 정신 ##적 피로 ##나 실 ##의 ##가 반영 ##된 것이 아니라 베토벤 ##의 합창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교향곡 작곡 ##을 1839 ##년부터 40 ##년에 걸쳐 파리 ##에서 착수 ##했 ##으나 1 ##악 ##장을 쓴 뒤 ##에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서곡 ( 1 ##악 ##장 ) 을 파리 음악원 ##의 연주회 ##에서 연주 ##할 파트 ##보 ##까지 준비 ##하 ##였으나 , 실제로 ##는 이루어지 ##지는 않았다 . 결국 초연 ##은 4 ##년 [SEP]\n",
      "2022-03-16 09:31:40,581 - qnadataset - INFO - answer: 1 ##악 ##장을\n",
      "2022-03-16 09:31:40,583 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 124556, 76512, 10622, 121328, 18382, 9512, 9109, 10530, 120604, 119424, 11018, 11287, 136, 102, 124889, 11287, 39218, 119, 119777, 10459, 9157, 136161, 120668, 10459, 28467, 35979, 21555, 119942, 10530, 127086, 13767, 8870, 92383, 120543, 11102, 120174, 14801, 128478, 16439, 9489, 10459, 11287, 122102, 13441, 27487, 45021, 128268, 10459, 124616, 25242, 79544, 55670, 120668, 10459, 58088, 74141, 21371, 9359, 9460, 11506, 119, 121608, 124556, 76512, 10622, 16221, 87188, 10533, 27056, 92210, 120425, 11489, 125385, 119424, 35466, 122, 119110, 35963, 9512, 9109, 10530, 120604, 12490, 119, 19789, 119626, 10459, 120899, 11882, 58248, 17889, 9638, 137473, 113, 122, 119110, 13890, 114, 9633, 120425, 137413, 10459, 132695, 11489, 120441, 14843, 121425, 30005, 18382, 120339, 35506, 74519, 117, 120603, 11018, 121661, 32815, 49137, 119, 50342, 127134, 10892, 125, 10954, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=78, end_positions=80)\n",
      "2022-03-16 09:31:40,584 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,585 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 교향곡 작곡 ##을 어디 ##까지 쓴 뒤 ##에 중단 ##했 ##는 ##가 ? [SEP] 쓴 뒤 ##에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서곡 ( 1 ##악 ##장 ) 을 파리 음악원 ##의 연주회 ##에서 연주 ##할 파트 ##보 ##까지 준비 ##하 ##였으나 , 실제로 ##는 이루어지 ##지는 않았다 . 결국 초연 ##은 4 ##년 반 ##이 지난 후에 드레스 ##덴 ##에서 연주 ##되었고 재연 ##도 이루 ##어졌 ##지만 , 이후 ##에 그대로 방치 ##되고 말 ##았다 . 그 사이에 그는 리엔 ##치 ##와 방황 ##하는 네덜란드인 ##을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착수 ##하는 등 분 ##주 ##한 시간 ##을 보냈 ##는데 , 그런 바 ##쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 [SEP]\n",
      "2022-03-16 09:31:40,586 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,587 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 124556, 76512, 10622, 121328, 18382, 9512, 9109, 10530, 120604, 119424, 11018, 11287, 136, 102, 9512, 9109, 10530, 120604, 12490, 119, 19789, 119626, 10459, 120899, 11882, 58248, 17889, 9638, 137473, 113, 122, 119110, 13890, 114, 9633, 120425, 137413, 10459, 132695, 11489, 120441, 14843, 121425, 30005, 18382, 120339, 35506, 74519, 117, 120603, 11018, 121661, 32815, 49137, 119, 50342, 127134, 10892, 125, 10954, 9321, 10739, 120600, 56528, 126715, 118790, 11489, 120441, 49953, 142552, 12092, 119661, 121022, 28578, 117, 18347, 10530, 110589, 128194, 29208, 9251, 27303, 119, 8924, 64932, 17889, 143091, 18622, 12638, 135821, 12178, 140872, 10622, 120899, 12453, 9847, 20309, 10739, 48387, 35979, 125385, 12178, 9121, 9367, 16323, 11102, 119610, 10622, 121357, 41850, 117, 119989, 9318, 119023, 119782, 10739, 9638, 8889, 10622, 9649, 14153, 9954, 27487, 127780, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,588 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:31:40,589 - qnadataset - INFO - question & context: [CLS] 바그너 ##는 교향곡 작곡 ##을 어디 ##까지 쓴 뒤 ##에 중단 ##했 ##는 ##가 ? [SEP] 방치 ##되고 말 ##았다 . 그 사이에 그는 리엔 ##치 ##와 방황 ##하는 네덜란드인 ##을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착수 ##하는 등 분 ##주 ##한 시간 ##을 보냈 ##는데 , 그런 바 ##쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-16 09:31:40,590 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:31:40,592 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 131787, 11018, 124556, 76512, 10622, 121328, 18382, 9512, 9109, 10530, 120604, 119424, 11018, 11287, 136, 102, 128194, 29208, 9251, 27303, 119, 8924, 64932, 17889, 143091, 18622, 12638, 135821, 12178, 140872, 10622, 120899, 12453, 9847, 20309, 10739, 48387, 35979, 125385, 12178, 9121, 9367, 16323, 11102, 119610, 10622, 121357, 41850, 117, 119989, 9318, 119023, 119782, 10739, 9638, 8889, 10622, 9649, 14153, 9954, 27487, 127780, 23969, 119743, 12092, 11506, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:31:40,597 - qnadataset - INFO - Saving features into cached file, it could take a lot of time...\n",
      "2022-03-16 09:31:55,828 - qnadataset - INFO - Saving features into cached file ../korpora/korQuAD/cached_DistilBertTokenizer_128_32_64_KorQuAD_v1.0_train.json [took 15.231 s]\n",
      "2022-03-16 09:31:57,000 - qnadataset - INFO - Creating features from dataset file at ../korpora/korQuAD/KorQuAD_v1.0_dev.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end train_loader===========================================================\n",
      "create eval_loader===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 140/140 [00:00<00:00, 20909.54it/s]\n",
      "convert squad examples to features: 100%|██| 5533/5533 [00:06<00:00, 796.54it/s]\n",
      "2022-03-16 09:32:04,838 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,840 - qnadataset - INFO - question & context: [CLS] 1989년 6월 30일 평양 ##축 ##전에 대표 ##로 파견 된 인물 ##은 ? [SEP] 1989년 2월 15일 여의도 농민 폭력 시위 ##를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처 ##벌 ##에 ##관 ##한 ##법 ##률 ##위 ##반 ) 으로 지명 ##수 ##배 ##되었다 . 1989년 3월 12일 서울 ##지 ##방 ##검 ##찰청 공안 ##부는 임종 ##석 ##의 사전 ##구 ##속 ##영 ##장을 발부 ##받 ##았다 . 같은 해 6월 30일 평양 ##축 ##전에 임수 ##경 ##을 대표 ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 오전 7 ##시 40 ##분 경 가스 [SEP]\n",
      "2022-03-16 09:32:04,842 - qnadataset - INFO - answer: 임수 ##경\n",
      "2022-03-16 09:32:04,843 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 76485, 17253, 40636, 121340, 70122, 68767, 119597, 11261, 120811, 9099, 119664, 10892, 136, 102, 76485, 17520, 37912, 127473, 122083, 121735, 120889, 11513, 120495, 11102, 121207, 113, 121735, 125686, 101322, 60469, 68773, 10530, 20595, 11102, 33768, 88350, 19855, 30134, 114, 29805, 120160, 15891, 76036, 13628, 119, 76485, 15361, 46026, 48253, 12508, 42337, 118625, 137233, 129249, 58904, 130960, 40958, 10459, 120455, 17196, 43962, 30858, 35963, 140525, 118965, 27303, 119, 18589, 9960, 17253, 40636, 121340, 70122, 68767, 135306, 31720, 10622, 119597, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 121408, 128, 14040, 10533, 37712, 8885, 121206, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=75, end_positions=76)\n",
      "2022-03-16 09:32:04,844 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,849 - qnadataset - INFO - question & context: [CLS] 1989년 6월 30일 평양 ##축 ##전에 대표 ##로 파견 된 인물 ##은 ? [SEP] ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 오전 7 ##시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특공 ##조 및 대공 ##과 직원 12 ##명 등 22 ##명의 사복 경찰 ##을 승용차 8 ##대에 나누 ##어 경희대 ##학교 ##에 투입 ##했다 . 1989년 12월 18일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경희대 ##학교 학생회 ##관 건물 계단 ##을 내려오 ##는 임종 ##석 [SEP]\n",
      "2022-03-16 09:32:04,851 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:32:04,852 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 76485, 17253, 40636, 121340, 70122, 68767, 119597, 11261, 120811, 9099, 119664, 10892, 136, 102, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 121408, 128, 14040, 10533, 37712, 8885, 121206, 119270, 11882, 119970, 118989, 11467, 120937, 11102, 128019, 20626, 9316, 122739, 11882, 120801, 10186, 16758, 9121, 10306, 45441, 140897, 119772, 10622, 132797, 129, 81980, 121118, 12965, 127764, 46599, 10530, 121627, 12490, 119, 76485, 16367, 45972, 121408, 129, 14040, 10208, 37712, 8885, 48253, 40311, 44321, 12692, 132021, 37321, 125967, 119843, 126, 16758, 11882, 19653, 127764, 46599, 129899, 20595, 120007, 125235, 10622, 128359, 11018, 130960, 40958, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:32:04,853 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,854 - qnadataset - INFO - question & context: [CLS] 1989년 6월 30일 평양 ##축 ##전에 대표 ##로 파견 된 인물 ##은 ? [SEP] 22 ##명의 사복 경찰 ##을 승용차 8 ##대에 나누 ##어 경희대 ##학교 ##에 투입 ##했다 . 1989년 12월 18일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경희대 ##학교 학생회 ##관 건물 계단 ##을 내려오 ##는 임종 ##석 ##을 발견 , 검거 ##해 구속 ##을 집행 ##했다 . 임종 ##석 ##은 청량리 ##경찰 ##서 ##에서 약 1 ##시간 동안 조사 ##를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장안 ##동 ##의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실 ##로 인계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-16 09:32:04,855 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:32:04,856 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 76485, 17253, 40636, 121340, 70122, 68767, 119597, 11261, 120811, 9099, 119664, 10892, 136, 102, 10306, 45441, 140897, 119772, 10622, 132797, 129, 81980, 121118, 12965, 127764, 46599, 10530, 121627, 12490, 119, 76485, 16367, 45972, 121408, 129, 14040, 10208, 37712, 8885, 48253, 40311, 44321, 12692, 132021, 37321, 125967, 119843, 126, 16758, 11882, 19653, 127764, 46599, 129899, 20595, 120007, 125235, 10622, 128359, 11018, 130960, 40958, 10622, 119729, 117, 129189, 14523, 121675, 10622, 121158, 12490, 119, 130960, 40958, 10892, 134328, 132021, 12424, 11489, 9539, 122, 100699, 41886, 119781, 11513, 74141, 9109, 121408, 130, 14040, 10462, 37712, 8885, 48253, 126157, 18778, 10459, 48253, 12508, 42337, 132021, 40311, 129249, 37712, 31503, 11261, 135140, 13628, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:32:04,857 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,859 - qnadataset - INFO - question & context: [CLS] 임종 ##석 ##을 검거 ##한 장소 ##는 경희대 내 어디 ##인 ##가 ? [SEP] 1989년 2월 15일 여의도 농민 폭력 시위 ##를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처 ##벌 ##에 ##관 ##한 ##법 ##률 ##위 ##반 ) 으로 지명 ##수 ##배 ##되었다 . 1989년 3월 12일 서울 ##지 ##방 ##검 ##찰청 공안 ##부는 임종 ##석 ##의 사전 ##구 ##속 ##영 ##장을 발부 ##받 ##았다 . 같은 해 6월 30일 평양 ##축 ##전에 임수 ##경 ##을 대표 ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 오전 7 ##시 40 ##분 경 가스 [SEP]\n",
      "2022-03-16 09:32:04,860 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:32:04,861 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 130960, 40958, 10622, 129189, 11102, 120319, 11018, 127764, 8996, 121328, 12030, 11287, 136, 102, 76485, 17520, 37912, 127473, 122083, 121735, 120889, 11513, 120495, 11102, 121207, 113, 121735, 125686, 101322, 60469, 68773, 10530, 20595, 11102, 33768, 88350, 19855, 30134, 114, 29805, 120160, 15891, 76036, 13628, 119, 76485, 15361, 46026, 48253, 12508, 42337, 118625, 137233, 129249, 58904, 130960, 40958, 10459, 120455, 17196, 43962, 30858, 35963, 140525, 118965, 27303, 119, 18589, 9960, 17253, 40636, 121340, 70122, 68767, 135306, 31720, 10622, 119597, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 121408, 128, 14040, 10533, 37712, 8885, 121206, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:32:04,862 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,863 - qnadataset - INFO - question & context: [CLS] 임종 ##석 ##을 검거 ##한 장소 ##는 경희대 내 어디 ##인 ##가 ? [SEP] ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 오전 7 ##시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특공 ##조 및 대공 ##과 직원 12 ##명 등 22 ##명의 사복 경찰 ##을 승용차 8 ##대에 나누 ##어 경희대 ##학교 ##에 투입 ##했다 . 1989년 12월 18일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경희대 ##학교 학생회 ##관 건물 계단 ##을 내려오 ##는 임종 ##석 [SEP]\n",
      "2022-03-16 09:32:04,864 - qnadataset - INFO - answer: 학생회 ##관 건물 계단\n",
      "2022-03-16 09:32:04,865 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 130960, 40958, 10622, 129189, 11102, 120319, 11018, 127764, 8996, 121328, 12030, 11287, 136, 102, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 121408, 128, 14040, 10533, 37712, 8885, 121206, 119270, 11882, 119970, 118989, 11467, 120937, 11102, 128019, 20626, 9316, 122739, 11882, 120801, 10186, 16758, 9121, 10306, 45441, 140897, 119772, 10622, 132797, 129, 81980, 121118, 12965, 127764, 46599, 10530, 121627, 12490, 119, 76485, 16367, 45972, 121408, 129, 14040, 10208, 37712, 8885, 48253, 40311, 44321, 12692, 132021, 37321, 125967, 119843, 126, 16758, 11882, 19653, 127764, 46599, 129899, 20595, 120007, 125235, 10622, 128359, 11018, 130960, 40958, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=118, end_positions=121)\n",
      "2022-03-16 09:32:04,866 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,867 - qnadataset - INFO - question & context: [CLS] 임종 ##석 ##을 검거 ##한 장소 ##는 경희대 내 어디 ##인 ##가 ? [SEP] 22 ##명의 사복 경찰 ##을 승용차 8 ##대에 나누 ##어 경희대 ##학교 ##에 투입 ##했다 . 1989년 12월 18일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경희대 ##학교 학생회 ##관 건물 계단 ##을 내려오 ##는 임종 ##석 ##을 발견 , 검거 ##해 구속 ##을 집행 ##했다 . 임종 ##석 ##은 청량리 ##경찰 ##서 ##에서 약 1 ##시간 동안 조사 ##를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장안 ##동 ##의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실 ##로 인계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-16 09:32:04,869 - qnadataset - INFO - answer: 학생회 ##관 건물 계단\n",
      "2022-03-16 09:32:04,870 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 130960, 40958, 10622, 129189, 11102, 120319, 11018, 127764, 8996, 121328, 12030, 11287, 136, 102, 10306, 45441, 140897, 119772, 10622, 132797, 129, 81980, 121118, 12965, 127764, 46599, 10530, 121627, 12490, 119, 76485, 16367, 45972, 121408, 129, 14040, 10208, 37712, 8885, 48253, 40311, 44321, 12692, 132021, 37321, 125967, 119843, 126, 16758, 11882, 19653, 127764, 46599, 129899, 20595, 120007, 125235, 10622, 128359, 11018, 130960, 40958, 10622, 119729, 117, 129189, 14523, 121675, 10622, 121158, 12490, 119, 130960, 40958, 10892, 134328, 132021, 12424, 11489, 9539, 122, 100699, 41886, 119781, 11513, 74141, 9109, 121408, 130, 14040, 10462, 37712, 8885, 48253, 126157, 18778, 10459, 48253, 12508, 42337, 132021, 40311, 129249, 37712, 31503, 11261, 135140, 13628, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=54, end_positions=57)\n",
      "2022-03-16 09:32:04,871 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,872 - qnadataset - INFO - question & context: [CLS] 임종 ##석 ##이 조사 ##를 받은 뒤 인계 ##된 곳 ##은 어딘가 ? [SEP] 1989년 2월 15일 여의도 농민 폭력 시위 ##를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처 ##벌 ##에 ##관 ##한 ##법 ##률 ##위 ##반 ) 으로 지명 ##수 ##배 ##되었다 . 1989년 3월 12일 서울 ##지 ##방 ##검 ##찰청 공안 ##부는 임종 ##석 ##의 사전 ##구 ##속 ##영 ##장을 발부 ##받 ##았다 . 같은 해 6월 30일 평양 ##축 ##전에 임수 ##경 ##을 대표 ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 오전 7 ##시 40 ##분 경 가스 [SEP]\n",
      "2022-03-16 09:32:04,873 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:32:04,874 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 130960, 40958, 10739, 119781, 11513, 74141, 9109, 135140, 13441, 8895, 10892, 135834, 136, 102, 76485, 17520, 37912, 127473, 122083, 121735, 120889, 11513, 120495, 11102, 121207, 113, 121735, 125686, 101322, 60469, 68773, 10530, 20595, 11102, 33768, 88350, 19855, 30134, 114, 29805, 120160, 15891, 76036, 13628, 119, 76485, 15361, 46026, 48253, 12508, 42337, 118625, 137233, 129249, 58904, 130960, 40958, 10459, 120455, 17196, 43962, 30858, 35963, 140525, 118965, 27303, 119, 18589, 9960, 17253, 40636, 121340, 70122, 68767, 135306, 31720, 10622, 119597, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 121408, 128, 14040, 10533, 37712, 8885, 121206, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:32:04,875 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,876 - qnadataset - INFO - question & context: [CLS] 임종 ##석 ##이 조사 ##를 받은 뒤 인계 ##된 곳 ##은 어딘가 ? [SEP] ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 오전 7 ##시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특공 ##조 및 대공 ##과 직원 12 ##명 등 22 ##명의 사복 경찰 ##을 승용차 8 ##대에 나누 ##어 경희대 ##학교 ##에 투입 ##했다 . 1989년 12월 18일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경희대 ##학교 학생회 ##관 건물 계단 ##을 내려오 ##는 임종 ##석 [SEP]\n",
      "2022-03-16 09:32:04,877 - qnadataset - INFO - answer: [CLS]\n",
      "2022-03-16 09:32:04,878 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 130960, 40958, 10739, 119781, 11513, 74141, 9109, 135140, 13441, 8895, 10892, 135834, 136, 102, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 121408, 128, 14040, 10533, 37712, 8885, 121206, 119270, 11882, 119970, 118989, 11467, 120937, 11102, 128019, 20626, 9316, 122739, 11882, 120801, 10186, 16758, 9121, 10306, 45441, 140897, 119772, 10622, 132797, 129, 81980, 121118, 12965, 127764, 46599, 10530, 121627, 12490, 119, 76485, 16367, 45972, 121408, 129, 14040, 10208, 37712, 8885, 48253, 40311, 44321, 12692, 132021, 37321, 125967, 119843, 126, 16758, 11882, 19653, 127764, 46599, 129899, 20595, 120007, 125235, 10622, 128359, 11018, 130960, 40958, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n",
      "2022-03-16 09:32:04,879 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,880 - qnadataset - INFO - question & context: [CLS] 임종 ##석 ##이 조사 ##를 받은 뒤 인계 ##된 곳 ##은 어딘가 ? [SEP] 22 ##명의 사복 경찰 ##을 승용차 8 ##대에 나누 ##어 경희대 ##학교 ##에 투입 ##했다 . 1989년 12월 18일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경희대 ##학교 학생회 ##관 건물 계단 ##을 내려오 ##는 임종 ##석 ##을 발견 , 검거 ##해 구속 ##을 집행 ##했다 . 임종 ##석 ##은 청량리 ##경찰 ##서 ##에서 약 1 ##시간 동안 조사 ##를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장안 ##동 ##의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실 ##로 인계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "2022-03-16 09:32:04,881 - qnadataset - INFO - answer: 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실\n",
      "2022-03-16 09:32:04,881 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 130960, 40958, 10739, 119781, 11513, 74141, 9109, 135140, 13441, 8895, 10892, 135834, 136, 102, 10306, 45441, 140897, 119772, 10622, 132797, 129, 81980, 121118, 12965, 127764, 46599, 10530, 121627, 12490, 119, 76485, 16367, 45972, 121408, 129, 14040, 10208, 37712, 8885, 48253, 40311, 44321, 12692, 132021, 37321, 125967, 119843, 126, 16758, 11882, 19653, 127764, 46599, 129899, 20595, 120007, 125235, 10622, 128359, 11018, 130960, 40958, 10622, 119729, 117, 129189, 14523, 121675, 10622, 121158, 12490, 119, 130960, 40958, 10892, 134328, 132021, 12424, 11489, 9539, 122, 100699, 41886, 119781, 11513, 74141, 9109, 121408, 130, 14040, 10462, 37712, 8885, 48253, 126157, 18778, 10459, 48253, 12508, 42337, 132021, 40311, 129249, 37712, 31503, 11261, 135140, 13628, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=98, end_positions=105)\n",
      "2022-03-16 09:32:04,883 - qnadataset - INFO - *** Example ***\n",
      "2022-03-16 09:32:04,884 - qnadataset - INFO - question & context: [CLS] 1989년 2월 15일 여의도 농민 폭력 시위 ##를 주도 ##한 혐의 ##로 지명 ##수 ##배 ##된 사람 ##의 이름은 ? [SEP] 1989년 2월 15일 여의도 농민 폭력 시위 ##를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처 ##벌 ##에 ##관 ##한 ##법 ##률 ##위 ##반 ) 으로 지명 ##수 ##배 ##되었다 . 1989년 3월 12일 서울 ##지 ##방 ##검 ##찰청 공안 ##부는 임종 ##석 ##의 사전 ##구 ##속 ##영 ##장을 발부 ##받 ##았다 . 같은 해 6월 30일 평양 ##축 ##전에 임수 ##경 ##을 대표 ##로 파견 ##하여 국가 ##보 ##안 ##법 ##위 ##반 혐의 ##가 추가 ##되었다 . 경찰 ##은 12월 18일 ~ 20일 사이 서울 경희대 ##학교 ##에서 임종 ##석 ##이 성명 발표 ##를 추진 ##하고 있다는 첩보 ##를 입수 ##했고 , 12월 18일 [SEP]\n",
      "2022-03-16 09:32:04,885 - qnadataset - INFO - answer: 임종 ##석\n",
      "2022-03-16 09:32:04,886 - qnadataset - INFO - features: QAFeatures(input_ids=[101, 76485, 17520, 37912, 127473, 122083, 121735, 120889, 11513, 120495, 11102, 121207, 11261, 120160, 15891, 76036, 13441, 119578, 10459, 78199, 136, 102, 76485, 17520, 37912, 127473, 122083, 121735, 120889, 11513, 120495, 11102, 121207, 113, 121735, 125686, 101322, 60469, 68773, 10530, 20595, 11102, 33768, 88350, 19855, 30134, 114, 29805, 120160, 15891, 76036, 13628, 119, 76485, 15361, 46026, 48253, 12508, 42337, 118625, 137233, 129249, 58904, 130960, 40958, 10459, 120455, 17196, 43962, 30858, 35963, 140525, 118965, 27303, 119, 18589, 9960, 17253, 40636, 121340, 70122, 68767, 135306, 31720, 10622, 119597, 11261, 120811, 13374, 93222, 30005, 34951, 33768, 19855, 30134, 121207, 11287, 119720, 13628, 119, 119772, 10892, 16367, 45972, 198, 41518, 119580, 48253, 127764, 46599, 11489, 130960, 40958, 10739, 122767, 119696, 11513, 120363, 12453, 77324, 132685, 11513, 128273, 38181, 117, 16367, 45972, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=63, end_positions=64)\n",
      "2022-03-16 09:32:04,887 - qnadataset - INFO - Saving features into cached file, it could take a lot of time...\n",
      "2022-03-16 09:32:06,484 - qnadataset - INFO - Saving features into cached file ../korpora/korQuAD/cached_DistilBertTokenizer_128_32_64_KorQuAD_v1.0_dev.json [took 1.597 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end eval_loader===========================================================\n",
      "train_loader_len: 6478, eval_loader_len: 653\n"
     ]
    }
   ],
   "source": [
    "# 학습 data loader 생성\n",
    "sys.path.append('..')\n",
    "from myutils import KorQuADCorpus, QADataset, data_collator\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "#############################################################################\n",
    "# 변수 설정\n",
    "#############################################################################\n",
    "max_seq_len = 128   # 질문 + 지문 최대 크기\n",
    "max_query_length = 32  # 질문 최대 크기\n",
    "doc_stride = 64     # 지문이 128을 넘을 경우, 얼만큼씩 다음 지문으로 대체할지\n",
    "\n",
    "batch_size = 32        # 배치 사이즈(64면 GUP Memory 오류 나므로, 32 이하로 설정할것=>max_seq_length 를 줄이면, 64도 가능함)\n",
    "cache = True   # 캐쉬파일 생성할거면 True로 \n",
    "#############################################################################\n",
    "\n",
    "# corpus 파일 설정\n",
    "corpus = KorQuADCorpus()\n",
    "\n",
    "\n",
    "# 학습 dataset 생성\n",
    "print('create train_loader===========================================================')\n",
    "train_file_fpath = '../korpora/korQuAD/KorQuAD_v1.0_train.json'\n",
    "train_dataset = QADataset(file_fpath=train_file_fpath, tokenizer=tokenizer, corpus=corpus, max_seq_length=max_seq_len, max_query_length = max_query_length, doc_stride= doc_stride, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 학습 dataloader 생성\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(train_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "\n",
    "print('end train_loader===========================================================')\n",
    "\n",
    "print('create eval_loader===========================================================')\n",
    "eval_file_fpath = '../korpora/korQuAD/KorQuAD_v1.0_dev.json'\n",
    "eval_dataset = QADataset(file_fpath=eval_file_fpath, tokenizer=tokenizer, corpus=corpus, max_seq_length=max_seq_len, max_query_length = max_query_length, doc_stride= doc_stride, overwrite_cache=cache)\n",
    "\n",
    "\n",
    "# 평가 dataloader 생성\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          #shuffle=True, # dataset을 섞음\n",
    "                          sampler=RandomSampler(eval_dataset, replacement=False), #dataset을 랜덤하게 샘플링함\n",
    "                          collate_fn=data_collator, # dataset을 tensor로 변환(예시 {'input_ids':tensor[0,1,2,3,1,], 'token_type_id:tensor[0,0,0,0,0], 'attention_mask:tensor[1,1,1,1,1], 'labels':tensor[5]}\n",
    "                          num_workers=4)\n",
    "print('end eval_loader===========================================================')\n",
    "\n",
    "print('train_loader_len: {}, eval_loader_len: {}'.format(len(train_loader), len(eval_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fec919e-d8b7-4b20-a20b-28cf66a49a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143772\n",
      "[101, 9034, 10530, 119728, 11018, 128441, 10739, 69708, 42428, 10459, 10020, 12030, 28143, 10892, 124227, 12508, 49137, 102, 122108, 131027, 11903, 102]\n",
      "재미있\n",
      "122108\n"
     ]
    }
   ],
   "source": [
    "# tokenier 테스트\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.encode(\"눈에 보이는 반전이었지만 영화의 흡인력은 사라지지 않았다\", \"정말 재미있다\"))\n",
    "print(tokenizer.convert_ids_to_tokens(131027))\n",
    "print(tokenizer.convert_tokens_to_ids('정말'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2068553-5650-47a4-bdd6-c9e79e1580cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps:64780, warmup_steps: 6478.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb460a235af452ebd703edf5ffdb3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98356e8b6c894deb8e37a98d5c627640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38225/4184282464.py:81: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  start_pred = torch.argmax(F.softmax(start_scores), dim=1)\n",
      "/tmp/ipykernel_38225/4184282464.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  end_pred = torch.argmax(F.softmax(end_scores), dim=1)\n",
      "2022-03-16 09:32:24,410 - bertQAtrain - INFO - [Epoch 1/10] Iteration 200 -> Train Loss: 4.6490, Train Accuracy: 0.226\n",
      "2022-03-16 09:32:40,120 - bertQAtrain - INFO - [Epoch 1/10] Iteration 400 -> Train Loss: 3.3410, Train Accuracy: 0.610\n",
      "2022-03-16 09:32:55,849 - bertQAtrain - INFO - [Epoch 1/10] Iteration 600 -> Train Loss: 2.2812, Train Accuracy: 0.614\n",
      "2022-03-16 09:33:11,634 - bertQAtrain - INFO - [Epoch 1/10] Iteration 800 -> Train Loss: 2.0419, Train Accuracy: 0.616\n",
      "2022-03-16 09:33:27,440 - bertQAtrain - INFO - [Epoch 1/10] Iteration 1000 -> Train Loss: 1.7868, Train Accuracy: 0.626\n",
      "2022-03-16 09:33:43,230 - bertQAtrain - INFO - [Epoch 1/10] Iteration 1200 -> Train Loss: 1.5883, Train Accuracy: 0.617\n",
      "2022-03-16 09:33:59,009 - bertQAtrain - INFO - [Epoch 1/10] Iteration 1400 -> Train Loss: 1.3464, Train Accuracy: 0.639\n",
      "2022-03-16 09:34:14,998 - bertQAtrain - INFO - [Epoch 1/10] Iteration 1600 -> Train Loss: 1.2516, Train Accuracy: 0.640\n",
      "2022-03-16 09:34:30,801 - bertQAtrain - INFO - [Epoch 1/10] Iteration 1800 -> Train Loss: 1.1527, Train Accuracy: 0.662\n",
      "2022-03-16 09:34:48,045 - bertQAtrain - INFO - [Epoch 1/10] Iteration 2000 -> Train Loss: 1.0355, Train Accuracy: 0.678\n",
      "2022-03-16 09:35:04,689 - bertQAtrain - INFO - [Epoch 1/10] Iteration 2200 -> Train Loss: 1.0151, Train Accuracy: 0.675\n",
      "2022-03-16 09:35:22,205 - bertQAtrain - INFO - [Epoch 1/10] Iteration 2400 -> Train Loss: 0.9947, Train Accuracy: 0.675\n",
      "2022-03-16 09:35:40,351 - bertQAtrain - INFO - [Epoch 1/10] Iteration 2600 -> Train Loss: 0.9148, Train Accuracy: 0.687\n",
      "2022-03-16 09:35:57,437 - bertQAtrain - INFO - [Epoch 1/10] Iteration 2800 -> Train Loss: 0.8937, Train Accuracy: 0.699\n",
      "2022-03-16 09:36:13,273 - bertQAtrain - INFO - [Epoch 1/10] Iteration 3000 -> Train Loss: 0.8785, Train Accuracy: 0.692\n",
      "2022-03-16 09:36:29,110 - bertQAtrain - INFO - [Epoch 1/10] Iteration 3200 -> Train Loss: 0.8886, Train Accuracy: 0.688\n",
      "2022-03-16 09:36:44,973 - bertQAtrain - INFO - [Epoch 1/10] Iteration 3400 -> Train Loss: 0.8399, Train Accuracy: 0.700\n",
      "2022-03-16 09:37:00,784 - bertQAtrain - INFO - [Epoch 1/10] Iteration 3600 -> Train Loss: 0.8085, Train Accuracy: 0.701\n",
      "2022-03-16 09:37:16,529 - bertQAtrain - INFO - [Epoch 1/10] Iteration 3800 -> Train Loss: 0.7805, Train Accuracy: 0.705\n",
      "2022-03-16 09:37:32,213 - bertQAtrain - INFO - [Epoch 1/10] Iteration 4000 -> Train Loss: 0.7744, Train Accuracy: 0.705\n",
      "2022-03-16 09:37:47,954 - bertQAtrain - INFO - [Epoch 1/10] Iteration 4200 -> Train Loss: 0.7245, Train Accuracy: 0.719\n",
      "2022-03-16 09:38:03,682 - bertQAtrain - INFO - [Epoch 1/10] Iteration 4400 -> Train Loss: 0.6892, Train Accuracy: 0.716\n",
      "2022-03-16 09:38:19,443 - bertQAtrain - INFO - [Epoch 1/10] Iteration 4600 -> Train Loss: 0.6808, Train Accuracy: 0.718\n",
      "2022-03-16 09:38:35,156 - bertQAtrain - INFO - [Epoch 1/10] Iteration 4800 -> Train Loss: 0.6614, Train Accuracy: 0.725\n",
      "2022-03-16 09:38:50,891 - bertQAtrain - INFO - [Epoch 1/10] Iteration 5000 -> Train Loss: 0.6223, Train Accuracy: 0.729\n",
      "2022-03-16 09:39:06,624 - bertQAtrain - INFO - [Epoch 1/10] Iteration 5200 -> Train Loss: 0.6249, Train Accuracy: 0.725\n",
      "2022-03-16 09:39:22,250 - bertQAtrain - INFO - [Epoch 1/10] Iteration 5400 -> Train Loss: 0.6226, Train Accuracy: 0.730\n",
      "2022-03-16 09:39:37,897 - bertQAtrain - INFO - [Epoch 1/10] Iteration 5600 -> Train Loss: 0.5809, Train Accuracy: 0.740\n",
      "2022-03-16 09:39:53,570 - bertQAtrain - INFO - [Epoch 1/10] Iteration 5800 -> Train Loss: 0.5752, Train Accuracy: 0.738\n",
      "2022-03-16 09:40:09,251 - bertQAtrain - INFO - [Epoch 1/10] Iteration 6000 -> Train Loss: 0.5719, Train Accuracy: 0.742\n",
      "2022-03-16 09:40:24,871 - bertQAtrain - INFO - [Epoch 1/10] Iteration 6200 -> Train Loss: 0.5561, Train Accuracy: 0.741\n",
      "2022-03-16 09:40:40,549 - bertQAtrain - INFO - [Epoch 1/10] Iteration 6400 -> Train Loss: 0.5511, Train Accuracy: 0.740\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23f856bdf6c4871bbc9893b950965aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38225/4184282464.py:146: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  start_pred = torch.argmax(F.softmax(start_scores), dim=1)\n",
      "/tmp/ipykernel_38225/4184282464.py:150: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  end_pred = torch.argmax(F.softmax(end_scores), dim=1)\n",
      "2022-03-16 09:40:59,909 - bertQAtrain - INFO - [Epoch 1/10] Validatation Accuracy:0.7463837532330683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b070464a49234e84a31a74d3b68a31e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:41:09,644 - bertQAtrain - INFO - [Epoch 2/10] Iteration 6600 -> Train Loss: 0.4876, Train Accuracy: 0.751\n",
      "2022-03-16 09:41:25,545 - bertQAtrain - INFO - [Epoch 2/10] Iteration 6800 -> Train Loss: 0.4636, Train Accuracy: 0.752\n",
      "2022-03-16 09:41:41,428 - bertQAtrain - INFO - [Epoch 2/10] Iteration 7000 -> Train Loss: 0.4654, Train Accuracy: 0.754\n",
      "2022-03-16 09:41:58,326 - bertQAtrain - INFO - [Epoch 2/10] Iteration 7200 -> Train Loss: 0.4871, Train Accuracy: 0.750\n",
      "2022-03-16 09:42:16,552 - bertQAtrain - INFO - [Epoch 2/10] Iteration 7400 -> Train Loss: 0.4458, Train Accuracy: 0.752\n",
      "2022-03-16 09:42:32,665 - bertQAtrain - INFO - [Epoch 2/10] Iteration 7600 -> Train Loss: 0.4422, Train Accuracy: 0.758\n",
      "2022-03-16 09:42:48,471 - bertQAtrain - INFO - [Epoch 2/10] Iteration 7800 -> Train Loss: 0.4533, Train Accuracy: 0.756\n",
      "2022-03-16 09:43:04,309 - bertQAtrain - INFO - [Epoch 2/10] Iteration 8000 -> Train Loss: 0.4457, Train Accuracy: 0.762\n",
      "2022-03-16 09:43:20,158 - bertQAtrain - INFO - [Epoch 2/10] Iteration 8200 -> Train Loss: 0.4698, Train Accuracy: 0.753\n",
      "2022-03-16 09:43:36,014 - bertQAtrain - INFO - [Epoch 2/10] Iteration 8400 -> Train Loss: 0.4413, Train Accuracy: 0.765\n",
      "2022-03-16 09:43:51,851 - bertQAtrain - INFO - [Epoch 2/10] Iteration 8600 -> Train Loss: 0.4542, Train Accuracy: 0.761\n",
      "2022-03-16 09:44:07,697 - bertQAtrain - INFO - [Epoch 2/10] Iteration 8800 -> Train Loss: 0.4406, Train Accuracy: 0.759\n",
      "2022-03-16 09:44:23,556 - bertQAtrain - INFO - [Epoch 2/10] Iteration 9000 -> Train Loss: 0.4423, Train Accuracy: 0.763\n",
      "2022-03-16 09:44:39,407 - bertQAtrain - INFO - [Epoch 2/10] Iteration 9200 -> Train Loss: 0.4343, Train Accuracy: 0.760\n",
      "2022-03-16 09:44:55,255 - bertQAtrain - INFO - [Epoch 2/10] Iteration 9400 -> Train Loss: 0.4221, Train Accuracy: 0.765\n",
      "2022-03-16 09:45:12,628 - bertQAtrain - INFO - [Epoch 2/10] Iteration 9600 -> Train Loss: 0.4339, Train Accuracy: 0.760\n",
      "2022-03-16 09:45:30,741 - bertQAtrain - INFO - [Epoch 2/10] Iteration 9800 -> Train Loss: 0.4235, Train Accuracy: 0.768\n",
      "2022-03-16 09:45:46,532 - bertQAtrain - INFO - [Epoch 2/10] Iteration 10000 -> Train Loss: 0.4209, Train Accuracy: 0.757\n",
      "2022-03-16 09:46:02,303 - bertQAtrain - INFO - [Epoch 2/10] Iteration 10200 -> Train Loss: 0.4061, Train Accuracy: 0.765\n",
      "2022-03-16 09:46:18,088 - bertQAtrain - INFO - [Epoch 2/10] Iteration 10400 -> Train Loss: 0.4271, Train Accuracy: 0.756\n",
      "2022-03-16 09:46:33,924 - bertQAtrain - INFO - [Epoch 2/10] Iteration 10600 -> Train Loss: 0.4097, Train Accuracy: 0.765\n",
      "2022-03-16 09:46:49,695 - bertQAtrain - INFO - [Epoch 2/10] Iteration 10800 -> Train Loss: 0.4028, Train Accuracy: 0.770\n",
      "2022-03-16 09:47:05,446 - bertQAtrain - INFO - [Epoch 2/10] Iteration 11000 -> Train Loss: 0.3988, Train Accuracy: 0.764\n",
      "2022-03-16 09:47:21,193 - bertQAtrain - INFO - [Epoch 2/10] Iteration 11200 -> Train Loss: 0.3922, Train Accuracy: 0.770\n",
      "2022-03-16 09:47:36,939 - bertQAtrain - INFO - [Epoch 2/10] Iteration 11400 -> Train Loss: 0.3939, Train Accuracy: 0.770\n",
      "2022-03-16 09:47:52,356 - bertQAtrain - INFO - [Epoch 2/10] Iteration 11600 -> Train Loss: 0.3842, Train Accuracy: 0.768\n",
      "2022-03-16 09:48:08,071 - bertQAtrain - INFO - [Epoch 2/10] Iteration 11800 -> Train Loss: 0.4004, Train Accuracy: 0.764\n",
      "2022-03-16 09:48:23,821 - bertQAtrain - INFO - [Epoch 2/10] Iteration 12000 -> Train Loss: 0.3991, Train Accuracy: 0.766\n",
      "2022-03-16 09:48:39,578 - bertQAtrain - INFO - [Epoch 2/10] Iteration 12200 -> Train Loss: 0.3699, Train Accuracy: 0.779\n",
      "2022-03-16 09:48:55,271 - bertQAtrain - INFO - [Epoch 2/10] Iteration 12400 -> Train Loss: 0.3957, Train Accuracy: 0.771\n",
      "2022-03-16 09:49:10,561 - bertQAtrain - INFO - [Epoch 2/10] Iteration 12600 -> Train Loss: 0.3901, Train Accuracy: 0.771\n",
      "2022-03-16 09:49:26,346 - bertQAtrain - INFO - [Epoch 2/10] Iteration 12800 -> Train Loss: 0.3783, Train Accuracy: 0.774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e915bb2cf47468bbc41c707c8188e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:49:51,922 - bertQAtrain - INFO - [Epoch 2/10] Validatation Accuracy:0.7596273589424274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d51b91044e54c1e86bc1cb8b11b584f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:49:56,026 - bertQAtrain - INFO - [Epoch 3/10] Iteration 13000 -> Train Loss: 0.3488, Train Accuracy: 0.774\n",
      "2022-03-16 09:50:10,299 - bertQAtrain - INFO - [Epoch 3/10] Iteration 13200 -> Train Loss: 0.2558, Train Accuracy: 0.794\n",
      "2022-03-16 09:50:26,386 - bertQAtrain - INFO - [Epoch 3/10] Iteration 13400 -> Train Loss: 0.2598, Train Accuracy: 0.795\n",
      "2022-03-16 09:50:42,148 - bertQAtrain - INFO - [Epoch 3/10] Iteration 13600 -> Train Loss: 0.2575, Train Accuracy: 0.796\n",
      "2022-03-16 09:50:57,901 - bertQAtrain - INFO - [Epoch 3/10] Iteration 13800 -> Train Loss: 0.2768, Train Accuracy: 0.783\n",
      "2022-03-16 09:51:13,671 - bertQAtrain - INFO - [Epoch 3/10] Iteration 14000 -> Train Loss: 0.2813, Train Accuracy: 0.793\n",
      "2022-03-16 09:51:29,452 - bertQAtrain - INFO - [Epoch 3/10] Iteration 14200 -> Train Loss: 0.2551, Train Accuracy: 0.795\n",
      "2022-03-16 09:51:45,241 - bertQAtrain - INFO - [Epoch 3/10] Iteration 14400 -> Train Loss: 0.2636, Train Accuracy: 0.797\n",
      "2022-03-16 09:52:01,010 - bertQAtrain - INFO - [Epoch 3/10] Iteration 14600 -> Train Loss: 0.2395, Train Accuracy: 0.796\n",
      "2022-03-16 09:52:16,795 - bertQAtrain - INFO - [Epoch 3/10] Iteration 14800 -> Train Loss: 0.2604, Train Accuracy: 0.794\n",
      "2022-03-16 09:52:32,572 - bertQAtrain - INFO - [Epoch 3/10] Iteration 15000 -> Train Loss: 0.2574, Train Accuracy: 0.795\n",
      "2022-03-16 09:52:48,341 - bertQAtrain - INFO - [Epoch 3/10] Iteration 15200 -> Train Loss: 0.2445, Train Accuracy: 0.796\n",
      "2022-03-16 09:53:05,802 - bertQAtrain - INFO - [Epoch 3/10] Iteration 15400 -> Train Loss: 0.2621, Train Accuracy: 0.795\n",
      "2022-03-16 09:53:23,832 - bertQAtrain - INFO - [Epoch 3/10] Iteration 15600 -> Train Loss: 0.2720, Train Accuracy: 0.794\n",
      "2022-03-16 09:53:39,728 - bertQAtrain - INFO - [Epoch 3/10] Iteration 15800 -> Train Loss: 0.2768, Train Accuracy: 0.791\n",
      "2022-03-16 09:53:55,558 - bertQAtrain - INFO - [Epoch 3/10] Iteration 16000 -> Train Loss: 0.2767, Train Accuracy: 0.796\n",
      "2022-03-16 09:54:11,372 - bertQAtrain - INFO - [Epoch 3/10] Iteration 16200 -> Train Loss: 0.2532, Train Accuracy: 0.796\n",
      "2022-03-16 09:54:27,032 - bertQAtrain - INFO - [Epoch 3/10] Iteration 16400 -> Train Loss: 0.2600, Train Accuracy: 0.792\n",
      "2022-03-16 09:54:42,843 - bertQAtrain - INFO - [Epoch 3/10] Iteration 16600 -> Train Loss: 0.2524, Train Accuracy: 0.795\n",
      "2022-03-16 09:54:58,637 - bertQAtrain - INFO - [Epoch 3/10] Iteration 16800 -> Train Loss: 0.2548, Train Accuracy: 0.794\n",
      "2022-03-16 09:55:14,357 - bertQAtrain - INFO - [Epoch 3/10] Iteration 17000 -> Train Loss: 0.2614, Train Accuracy: 0.796\n",
      "2022-03-16 09:55:30,124 - bertQAtrain - INFO - [Epoch 3/10] Iteration 17200 -> Train Loss: 0.2490, Train Accuracy: 0.800\n",
      "2022-03-16 09:55:45,921 - bertQAtrain - INFO - [Epoch 3/10] Iteration 17400 -> Train Loss: 0.2632, Train Accuracy: 0.793\n",
      "2022-03-16 09:56:01,709 - bertQAtrain - INFO - [Epoch 3/10] Iteration 17600 -> Train Loss: 0.2634, Train Accuracy: 0.792\n",
      "2022-03-16 09:56:17,904 - bertQAtrain - INFO - [Epoch 3/10] Iteration 17800 -> Train Loss: 0.2674, Train Accuracy: 0.792\n",
      "2022-03-16 09:56:33,760 - bertQAtrain - INFO - [Epoch 3/10] Iteration 18000 -> Train Loss: 0.2369, Train Accuracy: 0.798\n",
      "2022-03-16 09:56:49,572 - bertQAtrain - INFO - [Epoch 3/10] Iteration 18200 -> Train Loss: 0.2613, Train Accuracy: 0.796\n",
      "2022-03-16 09:57:05,231 - bertQAtrain - INFO - [Epoch 3/10] Iteration 18400 -> Train Loss: 0.2569, Train Accuracy: 0.795\n",
      "2022-03-16 09:57:20,532 - bertQAtrain - INFO - [Epoch 3/10] Iteration 18600 -> Train Loss: 0.2637, Train Accuracy: 0.793\n",
      "2022-03-16 09:57:36,303 - bertQAtrain - INFO - [Epoch 3/10] Iteration 18800 -> Train Loss: 0.2679, Train Accuracy: 0.797\n",
      "2022-03-16 09:57:52,092 - bertQAtrain - INFO - [Epoch 3/10] Iteration 19000 -> Train Loss: 0.2605, Train Accuracy: 0.791\n",
      "2022-03-16 09:58:07,871 - bertQAtrain - INFO - [Epoch 3/10] Iteration 19200 -> Train Loss: 0.2421, Train Accuracy: 0.795\n",
      "2022-03-16 09:58:23,659 - bertQAtrain - INFO - [Epoch 3/10] Iteration 19400 -> Train Loss: 0.2392, Train Accuracy: 0.796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0661dbebe94ee29a5a2ba1dd80f3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:58:39,748 - bertQAtrain - INFO - [Epoch 3/10] Validatation Accuracy:0.7636028355206438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7ad289313f47038bf0aa4594c0778f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 09:58:55,510 - bertQAtrain - INFO - [Epoch 4/10] Iteration 19600 -> Train Loss: 0.1854, Train Accuracy: 0.807\n",
      "2022-03-16 09:59:12,207 - bertQAtrain - INFO - [Epoch 4/10] Iteration 19800 -> Train Loss: 0.1519, Train Accuracy: 0.820\n",
      "2022-03-16 09:59:28,900 - bertQAtrain - INFO - [Epoch 4/10] Iteration 20000 -> Train Loss: 0.1651, Train Accuracy: 0.815\n",
      "2022-03-16 09:59:44,823 - bertQAtrain - INFO - [Epoch 4/10] Iteration 20200 -> Train Loss: 0.1685, Train Accuracy: 0.815\n",
      "2022-03-16 10:00:00,714 - bertQAtrain - INFO - [Epoch 4/10] Iteration 20400 -> Train Loss: 0.1602, Train Accuracy: 0.816\n",
      "2022-03-16 10:00:16,606 - bertQAtrain - INFO - [Epoch 4/10] Iteration 20600 -> Train Loss: 0.1463, Train Accuracy: 0.820\n",
      "2022-03-16 10:00:32,521 - bertQAtrain - INFO - [Epoch 4/10] Iteration 20800 -> Train Loss: 0.1765, Train Accuracy: 0.811\n",
      "2022-03-16 10:00:48,430 - bertQAtrain - INFO - [Epoch 4/10] Iteration 21000 -> Train Loss: 0.1788, Train Accuracy: 0.809\n",
      "2022-03-16 10:01:04,336 - bertQAtrain - INFO - [Epoch 4/10] Iteration 21200 -> Train Loss: 0.1684, Train Accuracy: 0.808\n",
      "2022-03-16 10:01:20,475 - bertQAtrain - INFO - [Epoch 4/10] Iteration 21400 -> Train Loss: 0.1691, Train Accuracy: 0.816\n",
      "2022-03-16 10:01:36,856 - bertQAtrain - INFO - [Epoch 4/10] Iteration 21600 -> Train Loss: 0.1854, Train Accuracy: 0.804\n",
      "2022-03-16 10:01:52,804 - bertQAtrain - INFO - [Epoch 4/10] Iteration 21800 -> Train Loss: 0.1691, Train Accuracy: 0.814\n",
      "2022-03-16 10:02:08,715 - bertQAtrain - INFO - [Epoch 4/10] Iteration 22000 -> Train Loss: 0.1698, Train Accuracy: 0.812\n",
      "2022-03-16 10:02:24,630 - bertQAtrain - INFO - [Epoch 4/10] Iteration 22200 -> Train Loss: 0.1630, Train Accuracy: 0.813\n",
      "2022-03-16 10:02:41,104 - bertQAtrain - INFO - [Epoch 4/10] Iteration 22400 -> Train Loss: 0.1741, Train Accuracy: 0.812\n",
      "2022-03-16 10:02:59,379 - bertQAtrain - INFO - [Epoch 4/10] Iteration 22600 -> Train Loss: 0.1627, Train Accuracy: 0.817\n",
      "2022-03-16 10:03:17,594 - bertQAtrain - INFO - [Epoch 4/10] Iteration 22800 -> Train Loss: 0.1625, Train Accuracy: 0.816\n",
      "2022-03-16 10:03:35,288 - bertQAtrain - INFO - [Epoch 4/10] Iteration 23000 -> Train Loss: 0.1712, Train Accuracy: 0.811\n",
      "2022-03-16 10:03:51,632 - bertQAtrain - INFO - [Epoch 4/10] Iteration 23200 -> Train Loss: 0.1561, Train Accuracy: 0.815\n",
      "2022-03-16 10:04:07,576 - bertQAtrain - INFO - [Epoch 4/10] Iteration 23400 -> Train Loss: 0.1682, Train Accuracy: 0.812\n",
      "2022-03-16 10:04:23,495 - bertQAtrain - INFO - [Epoch 4/10] Iteration 23600 -> Train Loss: 0.1622, Train Accuracy: 0.814\n",
      "2022-03-16 10:04:39,439 - bertQAtrain - INFO - [Epoch 4/10] Iteration 23800 -> Train Loss: 0.1636, Train Accuracy: 0.805\n",
      "2022-03-16 10:04:55,365 - bertQAtrain - INFO - [Epoch 4/10] Iteration 24000 -> Train Loss: 0.1744, Train Accuracy: 0.805\n",
      "2022-03-16 10:05:11,420 - bertQAtrain - INFO - [Epoch 4/10] Iteration 24200 -> Train Loss: 0.1681, Train Accuracy: 0.812\n",
      "2022-03-16 10:05:27,768 - bertQAtrain - INFO - [Epoch 4/10] Iteration 24400 -> Train Loss: 0.1755, Train Accuracy: 0.819\n",
      "2022-03-16 10:05:43,636 - bertQAtrain - INFO - [Epoch 4/10] Iteration 24600 -> Train Loss: 0.1568, Train Accuracy: 0.820\n",
      "2022-03-16 10:05:59,550 - bertQAtrain - INFO - [Epoch 4/10] Iteration 24800 -> Train Loss: 0.1839, Train Accuracy: 0.803\n",
      "2022-03-16 10:06:15,482 - bertQAtrain - INFO - [Epoch 4/10] Iteration 25000 -> Train Loss: 0.1686, Train Accuracy: 0.817\n",
      "2022-03-16 10:06:31,415 - bertQAtrain - INFO - [Epoch 4/10] Iteration 25200 -> Train Loss: 0.1715, Train Accuracy: 0.812\n",
      "2022-03-16 10:06:47,341 - bertQAtrain - INFO - [Epoch 4/10] Iteration 25400 -> Train Loss: 0.1600, Train Accuracy: 0.819\n",
      "2022-03-16 10:07:04,827 - bertQAtrain - INFO - [Epoch 4/10] Iteration 25600 -> Train Loss: 0.1708, Train Accuracy: 0.808\n",
      "2022-03-16 10:07:20,641 - bertQAtrain - INFO - [Epoch 4/10] Iteration 25800 -> Train Loss: 0.1589, Train Accuracy: 0.814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f95b67c58b47f19175f6b6efb705dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:07:42,797 - bertQAtrain - INFO - [Epoch 4/10] Validatation Accuracy:0.7645368330299838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf05fa14a4541a1a6836403d063c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:07:50,550 - bertQAtrain - INFO - [Epoch 5/10] Iteration 26000 -> Train Loss: 0.1522, Train Accuracy: 0.818\n",
      "2022-03-16 10:08:06,271 - bertQAtrain - INFO - [Epoch 5/10] Iteration 26200 -> Train Loss: 0.1084, Train Accuracy: 0.831\n",
      "2022-03-16 10:08:22,168 - bertQAtrain - INFO - [Epoch 5/10] Iteration 26400 -> Train Loss: 0.1025, Train Accuracy: 0.829\n",
      "2022-03-16 10:08:37,873 - bertQAtrain - INFO - [Epoch 5/10] Iteration 26600 -> Train Loss: 0.1071, Train Accuracy: 0.826\n",
      "2022-03-16 10:08:53,522 - bertQAtrain - INFO - [Epoch 5/10] Iteration 26800 -> Train Loss: 0.1027, Train Accuracy: 0.820\n",
      "2022-03-16 10:09:09,203 - bertQAtrain - INFO - [Epoch 5/10] Iteration 27000 -> Train Loss: 0.1012, Train Accuracy: 0.824\n",
      "2022-03-16 10:09:24,910 - bertQAtrain - INFO - [Epoch 5/10] Iteration 27200 -> Train Loss: 0.1190, Train Accuracy: 0.822\n",
      "2022-03-16 10:09:40,471 - bertQAtrain - INFO - [Epoch 5/10] Iteration 27400 -> Train Loss: 0.1089, Train Accuracy: 0.829\n",
      "2022-03-16 10:09:56,189 - bertQAtrain - INFO - [Epoch 5/10] Iteration 27600 -> Train Loss: 0.1139, Train Accuracy: 0.823\n",
      "2022-03-16 10:10:11,950 - bertQAtrain - INFO - [Epoch 5/10] Iteration 27800 -> Train Loss: 0.1129, Train Accuracy: 0.823\n",
      "2022-03-16 10:10:27,712 - bertQAtrain - INFO - [Epoch 5/10] Iteration 28000 -> Train Loss: 0.1134, Train Accuracy: 0.824\n",
      "2022-03-16 10:10:43,348 - bertQAtrain - INFO - [Epoch 5/10] Iteration 28200 -> Train Loss: 0.1150, Train Accuracy: 0.828\n",
      "2022-03-16 10:10:58,916 - bertQAtrain - INFO - [Epoch 5/10] Iteration 28400 -> Train Loss: 0.1208, Train Accuracy: 0.821\n",
      "2022-03-16 10:11:14,462 - bertQAtrain - INFO - [Epoch 5/10] Iteration 28600 -> Train Loss: 0.1062, Train Accuracy: 0.824\n",
      "2022-03-16 10:11:30,216 - bertQAtrain - INFO - [Epoch 5/10] Iteration 28800 -> Train Loss: 0.1157, Train Accuracy: 0.821\n",
      "2022-03-16 10:11:45,976 - bertQAtrain - INFO - [Epoch 5/10] Iteration 29000 -> Train Loss: 0.1115, Train Accuracy: 0.826\n",
      "2022-03-16 10:12:01,703 - bertQAtrain - INFO - [Epoch 5/10] Iteration 29200 -> Train Loss: 0.1211, Train Accuracy: 0.827\n",
      "2022-03-16 10:12:17,524 - bertQAtrain - INFO - [Epoch 5/10] Iteration 29400 -> Train Loss: 0.1093, Train Accuracy: 0.826\n",
      "2022-03-16 10:12:33,260 - bertQAtrain - INFO - [Epoch 5/10] Iteration 29600 -> Train Loss: 0.1230, Train Accuracy: 0.820\n",
      "2022-03-16 10:12:47,831 - bertQAtrain - INFO - [Epoch 5/10] Iteration 29800 -> Train Loss: 0.1155, Train Accuracy: 0.824\n",
      "2022-03-16 10:13:03,626 - bertQAtrain - INFO - [Epoch 5/10] Iteration 30000 -> Train Loss: 0.1072, Train Accuracy: 0.829\n",
      "2022-03-16 10:13:19,342 - bertQAtrain - INFO - [Epoch 5/10] Iteration 30200 -> Train Loss: 0.1172, Train Accuracy: 0.825\n",
      "2022-03-16 10:13:36,723 - bertQAtrain - INFO - [Epoch 5/10] Iteration 30400 -> Train Loss: 0.1122, Train Accuracy: 0.824\n",
      "2022-03-16 10:13:54,002 - bertQAtrain - INFO - [Epoch 5/10] Iteration 30600 -> Train Loss: 0.1125, Train Accuracy: 0.827\n",
      "2022-03-16 10:14:09,806 - bertQAtrain - INFO - [Epoch 5/10] Iteration 30800 -> Train Loss: 0.1170, Train Accuracy: 0.826\n",
      "2022-03-16 10:14:25,633 - bertQAtrain - INFO - [Epoch 5/10] Iteration 31000 -> Train Loss: 0.1186, Train Accuracy: 0.822\n",
      "2022-03-16 10:14:41,587 - bertQAtrain - INFO - [Epoch 5/10] Iteration 31200 -> Train Loss: 0.1144, Train Accuracy: 0.819\n",
      "2022-03-16 10:14:57,574 - bertQAtrain - INFO - [Epoch 5/10] Iteration 31400 -> Train Loss: 0.1011, Train Accuracy: 0.824\n",
      "2022-03-16 10:15:13,508 - bertQAtrain - INFO - [Epoch 5/10] Iteration 31600 -> Train Loss: 0.1157, Train Accuracy: 0.824\n",
      "2022-03-16 10:15:29,400 - bertQAtrain - INFO - [Epoch 5/10] Iteration 31800 -> Train Loss: 0.1054, Train Accuracy: 0.826\n",
      "2022-03-16 10:15:45,367 - bertQAtrain - INFO - [Epoch 5/10] Iteration 32000 -> Train Loss: 0.1204, Train Accuracy: 0.817\n",
      "2022-03-16 10:16:01,328 - bertQAtrain - INFO - [Epoch 5/10] Iteration 32200 -> Train Loss: 0.1271, Train Accuracy: 0.820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2491c7b1a50c4f3d91ae947e38b6a451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:16:29,866 - bertQAtrain - INFO - [Epoch 5/10] Validatation Accuracy:0.764249449180956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bc3ceb1535449b97e69e1cb0a12062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:16:31,327 - bertQAtrain - INFO - [Epoch 6/10] Iteration 32400 -> Train Loss: 0.1253, Train Accuracy: 0.817\n",
      "2022-03-16 10:16:47,262 - bertQAtrain - INFO - [Epoch 6/10] Iteration 32600 -> Train Loss: 0.0617, Train Accuracy: 0.833\n",
      "2022-03-16 10:17:03,147 - bertQAtrain - INFO - [Epoch 6/10] Iteration 32800 -> Train Loss: 0.0891, Train Accuracy: 0.825\n",
      "2022-03-16 10:17:18,947 - bertQAtrain - INFO - [Epoch 6/10] Iteration 33000 -> Train Loss: 0.0780, Train Accuracy: 0.831\n",
      "2022-03-16 10:17:34,644 - bertQAtrain - INFO - [Epoch 6/10] Iteration 33200 -> Train Loss: 0.0887, Train Accuracy: 0.831\n",
      "2022-03-16 10:17:50,432 - bertQAtrain - INFO - [Epoch 6/10] Iteration 33400 -> Train Loss: 0.0838, Train Accuracy: 0.829\n",
      "2022-03-16 10:18:06,163 - bertQAtrain - INFO - [Epoch 6/10] Iteration 33600 -> Train Loss: 0.0768, Train Accuracy: 0.831\n",
      "2022-03-16 10:18:21,908 - bertQAtrain - INFO - [Epoch 6/10] Iteration 33800 -> Train Loss: 0.0706, Train Accuracy: 0.833\n",
      "2022-03-16 10:18:37,735 - bertQAtrain - INFO - [Epoch 6/10] Iteration 34000 -> Train Loss: 0.0789, Train Accuracy: 0.837\n",
      "2022-03-16 10:18:53,492 - bertQAtrain - INFO - [Epoch 6/10] Iteration 34200 -> Train Loss: 0.0752, Train Accuracy: 0.833\n",
      "2022-03-16 10:19:09,206 - bertQAtrain - INFO - [Epoch 6/10] Iteration 34400 -> Train Loss: 0.0808, Train Accuracy: 0.834\n",
      "2022-03-16 10:19:25,008 - bertQAtrain - INFO - [Epoch 6/10] Iteration 34600 -> Train Loss: 0.0701, Train Accuracy: 0.834\n",
      "2022-03-16 10:19:40,826 - bertQAtrain - INFO - [Epoch 6/10] Iteration 34800 -> Train Loss: 0.0765, Train Accuracy: 0.834\n",
      "2022-03-16 10:19:56,604 - bertQAtrain - INFO - [Epoch 6/10] Iteration 35000 -> Train Loss: 0.0769, Train Accuracy: 0.830\n",
      "2022-03-16 10:20:12,233 - bertQAtrain - INFO - [Epoch 6/10] Iteration 35200 -> Train Loss: 0.0781, Train Accuracy: 0.828\n",
      "2022-03-16 10:20:27,873 - bertQAtrain - INFO - [Epoch 6/10] Iteration 35400 -> Train Loss: 0.0915, Train Accuracy: 0.834\n",
      "2022-03-16 10:20:43,698 - bertQAtrain - INFO - [Epoch 6/10] Iteration 35600 -> Train Loss: 0.0715, Train Accuracy: 0.828\n",
      "2022-03-16 10:20:59,303 - bertQAtrain - INFO - [Epoch 6/10] Iteration 35800 -> Train Loss: 0.0824, Train Accuracy: 0.829\n",
      "2022-03-16 10:21:14,976 - bertQAtrain - INFO - [Epoch 6/10] Iteration 36000 -> Train Loss: 0.0721, Train Accuracy: 0.827\n",
      "2022-03-16 10:21:30,768 - bertQAtrain - INFO - [Epoch 6/10] Iteration 36200 -> Train Loss: 0.0830, Train Accuracy: 0.832\n",
      "2022-03-16 10:21:46,534 - bertQAtrain - INFO - [Epoch 6/10] Iteration 36400 -> Train Loss: 0.0871, Train Accuracy: 0.834\n",
      "2022-03-16 10:22:03,995 - bertQAtrain - INFO - [Epoch 6/10] Iteration 36600 -> Train Loss: 0.0716, Train Accuracy: 0.831\n",
      "2022-03-16 10:22:19,851 - bertQAtrain - INFO - [Epoch 6/10] Iteration 36800 -> Train Loss: 0.0739, Train Accuracy: 0.834\n",
      "2022-03-16 10:22:35,778 - bertQAtrain - INFO - [Epoch 6/10] Iteration 37000 -> Train Loss: 0.0797, Train Accuracy: 0.828\n",
      "2022-03-16 10:22:51,746 - bertQAtrain - INFO - [Epoch 6/10] Iteration 37200 -> Train Loss: 0.0806, Train Accuracy: 0.834\n",
      "2022-03-16 10:23:07,673 - bertQAtrain - INFO - [Epoch 6/10] Iteration 37400 -> Train Loss: 0.0798, Train Accuracy: 0.828\n",
      "2022-03-16 10:23:24,848 - bertQAtrain - INFO - [Epoch 6/10] Iteration 37600 -> Train Loss: 0.0763, Train Accuracy: 0.834\n",
      "2022-03-16 10:23:40,767 - bertQAtrain - INFO - [Epoch 6/10] Iteration 37800 -> Train Loss: 0.0918, Train Accuracy: 0.831\n",
      "2022-03-16 10:23:56,685 - bertQAtrain - INFO - [Epoch 6/10] Iteration 38000 -> Train Loss: 0.0806, Train Accuracy: 0.826\n",
      "2022-03-16 10:24:12,583 - bertQAtrain - INFO - [Epoch 6/10] Iteration 38200 -> Train Loss: 0.0856, Train Accuracy: 0.830\n",
      "2022-03-16 10:24:28,441 - bertQAtrain - INFO - [Epoch 6/10] Iteration 38400 -> Train Loss: 0.0868, Train Accuracy: 0.826\n",
      "2022-03-16 10:24:44,336 - bertQAtrain - INFO - [Epoch 6/10] Iteration 38600 -> Train Loss: 0.0788, Train Accuracy: 0.831\n",
      "2022-03-16 10:25:00,230 - bertQAtrain - INFO - [Epoch 6/10] Iteration 38800 -> Train Loss: 0.0806, Train Accuracy: 0.828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19725a69357345f481fd46a04e35392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:25:18,997 - bertQAtrain - INFO - [Epoch 6/10] Validatation Accuracy:0.7599147427914551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d23ae371c64ca8856f068ec9e040d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:25:30,195 - bertQAtrain - INFO - [Epoch 7/10] Iteration 39000 -> Train Loss: 0.0579, Train Accuracy: 0.834\n",
      "2022-03-16 10:25:45,880 - bertQAtrain - INFO - [Epoch 7/10] Iteration 39200 -> Train Loss: 0.0579, Train Accuracy: 0.839\n",
      "2022-03-16 10:26:01,559 - bertQAtrain - INFO - [Epoch 7/10] Iteration 39400 -> Train Loss: 0.0524, Train Accuracy: 0.838\n",
      "2022-03-16 10:26:17,330 - bertQAtrain - INFO - [Epoch 7/10] Iteration 39600 -> Train Loss: 0.0544, Train Accuracy: 0.833\n",
      "2022-03-16 10:26:32,761 - bertQAtrain - INFO - [Epoch 7/10] Iteration 39800 -> Train Loss: 0.0541, Train Accuracy: 0.833\n",
      "2022-03-16 10:26:48,679 - bertQAtrain - INFO - [Epoch 7/10] Iteration 40000 -> Train Loss: 0.0569, Train Accuracy: 0.830\n",
      "2022-03-16 10:27:04,393 - bertQAtrain - INFO - [Epoch 7/10] Iteration 40200 -> Train Loss: 0.0549, Train Accuracy: 0.834\n",
      "2022-03-16 10:27:20,092 - bertQAtrain - INFO - [Epoch 7/10] Iteration 40400 -> Train Loss: 0.0625, Train Accuracy: 0.833\n",
      "2022-03-16 10:27:35,880 - bertQAtrain - INFO - [Epoch 7/10] Iteration 40600 -> Train Loss: 0.0541, Train Accuracy: 0.835\n",
      "2022-03-16 10:27:53,289 - bertQAtrain - INFO - [Epoch 7/10] Iteration 40800 -> Train Loss: 0.0471, Train Accuracy: 0.837\n",
      "2022-03-16 10:28:10,485 - bertQAtrain - INFO - [Epoch 7/10] Iteration 41000 -> Train Loss: 0.0606, Train Accuracy: 0.832\n",
      "2022-03-16 10:28:26,361 - bertQAtrain - INFO - [Epoch 7/10] Iteration 41200 -> Train Loss: 0.0614, Train Accuracy: 0.837\n",
      "2022-03-16 10:28:42,230 - bertQAtrain - INFO - [Epoch 7/10] Iteration 41400 -> Train Loss: 0.0635, Train Accuracy: 0.835\n",
      "2022-03-16 10:28:58,104 - bertQAtrain - INFO - [Epoch 7/10] Iteration 41600 -> Train Loss: 0.0528, Train Accuracy: 0.842\n",
      "2022-03-16 10:29:13,986 - bertQAtrain - INFO - [Epoch 7/10] Iteration 41800 -> Train Loss: 0.0660, Train Accuracy: 0.841\n",
      "2022-03-16 10:29:29,856 - bertQAtrain - INFO - [Epoch 7/10] Iteration 42000 -> Train Loss: 0.0581, Train Accuracy: 0.835\n",
      "2022-03-16 10:29:45,744 - bertQAtrain - INFO - [Epoch 7/10] Iteration 42200 -> Train Loss: 0.0481, Train Accuracy: 0.830\n",
      "2022-03-16 10:30:02,025 - bertQAtrain - INFO - [Epoch 7/10] Iteration 42400 -> Train Loss: 0.0576, Train Accuracy: 0.835\n",
      "2022-03-16 10:30:18,145 - bertQAtrain - INFO - [Epoch 7/10] Iteration 42600 -> Train Loss: 0.0654, Train Accuracy: 0.828\n",
      "2022-03-16 10:30:34,329 - bertQAtrain - INFO - [Epoch 7/10] Iteration 42800 -> Train Loss: 0.0612, Train Accuracy: 0.836\n",
      "2022-03-16 10:30:50,212 - bertQAtrain - INFO - [Epoch 7/10] Iteration 43000 -> Train Loss: 0.0581, Train Accuracy: 0.836\n",
      "2022-03-16 10:31:08,323 - bertQAtrain - INFO - [Epoch 7/10] Iteration 43200 -> Train Loss: 0.0589, Train Accuracy: 0.834\n",
      "2022-03-16 10:31:26,099 - bertQAtrain - INFO - [Epoch 7/10] Iteration 43400 -> Train Loss: 0.0556, Train Accuracy: 0.835\n",
      "2022-03-16 10:31:43,467 - bertQAtrain - INFO - [Epoch 7/10] Iteration 43600 -> Train Loss: 0.0516, Train Accuracy: 0.838\n",
      "2022-03-16 10:32:01,500 - bertQAtrain - INFO - [Epoch 7/10] Iteration 43800 -> Train Loss: 0.0564, Train Accuracy: 0.834\n",
      "2022-03-16 10:32:17,352 - bertQAtrain - INFO - [Epoch 7/10] Iteration 44000 -> Train Loss: 0.0590, Train Accuracy: 0.834\n",
      "2022-03-16 10:32:33,222 - bertQAtrain - INFO - [Epoch 7/10] Iteration 44200 -> Train Loss: 0.0587, Train Accuracy: 0.835\n",
      "2022-03-16 10:32:50,727 - bertQAtrain - INFO - [Epoch 7/10] Iteration 44400 -> Train Loss: 0.0638, Train Accuracy: 0.836\n",
      "2022-03-16 10:33:06,599 - bertQAtrain - INFO - [Epoch 7/10] Iteration 44600 -> Train Loss: 0.0617, Train Accuracy: 0.833\n",
      "2022-03-16 10:33:22,459 - bertQAtrain - INFO - [Epoch 7/10] Iteration 44800 -> Train Loss: 0.0591, Train Accuracy: 0.837\n",
      "2022-03-16 10:33:38,333 - bertQAtrain - INFO - [Epoch 7/10] Iteration 45000 -> Train Loss: 0.0648, Train Accuracy: 0.834\n",
      "2022-03-16 10:33:54,194 - bertQAtrain - INFO - [Epoch 7/10] Iteration 45200 -> Train Loss: 0.0568, Train Accuracy: 0.839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c84334842304ddc940fb0e70004fe09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:34:19,189 - bertQAtrain - INFO - [Epoch 7/10] Validatation Accuracy:0.7608487403007951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075b7885750d4936bbc1c4c7110c51ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:34:24,191 - bertQAtrain - INFO - [Epoch 8/10] Iteration 45400 -> Train Loss: 0.0566, Train Accuracy: 0.833\n",
      "2022-03-16 10:34:40,192 - bertQAtrain - INFO - [Epoch 8/10] Iteration 45600 -> Train Loss: 0.0401, Train Accuracy: 0.835\n",
      "2022-03-16 10:34:56,082 - bertQAtrain - INFO - [Epoch 8/10] Iteration 45800 -> Train Loss: 0.0459, Train Accuracy: 0.838\n",
      "2022-03-16 10:35:12,043 - bertQAtrain - INFO - [Epoch 8/10] Iteration 46000 -> Train Loss: 0.0349, Train Accuracy: 0.836\n",
      "2022-03-16 10:35:27,999 - bertQAtrain - INFO - [Epoch 8/10] Iteration 46200 -> Train Loss: 0.0457, Train Accuracy: 0.831\n",
      "2022-03-16 10:35:43,902 - bertQAtrain - INFO - [Epoch 8/10] Iteration 46400 -> Train Loss: 0.0411, Train Accuracy: 0.834\n",
      "2022-03-16 10:35:59,849 - bertQAtrain - INFO - [Epoch 8/10] Iteration 46600 -> Train Loss: 0.0433, Train Accuracy: 0.835\n",
      "2022-03-16 10:36:15,807 - bertQAtrain - INFO - [Epoch 8/10] Iteration 46800 -> Train Loss: 0.0396, Train Accuracy: 0.840\n",
      "2022-03-16 10:36:31,743 - bertQAtrain - INFO - [Epoch 8/10] Iteration 47000 -> Train Loss: 0.0375, Train Accuracy: 0.846\n",
      "2022-03-16 10:36:47,675 - bertQAtrain - INFO - [Epoch 8/10] Iteration 47200 -> Train Loss: 0.0384, Train Accuracy: 0.839\n",
      "2022-03-16 10:37:03,607 - bertQAtrain - INFO - [Epoch 8/10] Iteration 47400 -> Train Loss: 0.0424, Train Accuracy: 0.841\n",
      "2022-03-16 10:37:19,510 - bertQAtrain - INFO - [Epoch 8/10] Iteration 47600 -> Train Loss: 0.0381, Train Accuracy: 0.838\n",
      "2022-03-16 10:37:35,450 - bertQAtrain - INFO - [Epoch 8/10] Iteration 47800 -> Train Loss: 0.0392, Train Accuracy: 0.842\n",
      "2022-03-16 10:37:51,379 - bertQAtrain - INFO - [Epoch 8/10] Iteration 48000 -> Train Loss: 0.0444, Train Accuracy: 0.834\n",
      "2022-03-16 10:38:07,553 - bertQAtrain - INFO - [Epoch 8/10] Iteration 48200 -> Train Loss: 0.0393, Train Accuracy: 0.841\n",
      "2022-03-16 10:38:24,079 - bertQAtrain - INFO - [Epoch 8/10] Iteration 48400 -> Train Loss: 0.0372, Train Accuracy: 0.832\n",
      "2022-03-16 10:38:40,017 - bertQAtrain - INFO - [Epoch 8/10] Iteration 48600 -> Train Loss: 0.0394, Train Accuracy: 0.838\n",
      "2022-03-16 10:38:55,896 - bertQAtrain - INFO - [Epoch 8/10] Iteration 48800 -> Train Loss: 0.0397, Train Accuracy: 0.839\n",
      "2022-03-16 10:39:11,794 - bertQAtrain - INFO - [Epoch 8/10] Iteration 49000 -> Train Loss: 0.0444, Train Accuracy: 0.836\n",
      "2022-03-16 10:39:27,744 - bertQAtrain - INFO - [Epoch 8/10] Iteration 49200 -> Train Loss: 0.0427, Train Accuracy: 0.836\n",
      "2022-03-16 10:39:43,689 - bertQAtrain - INFO - [Epoch 8/10] Iteration 49400 -> Train Loss: 0.0459, Train Accuracy: 0.839\n",
      "2022-03-16 10:39:59,643 - bertQAtrain - INFO - [Epoch 8/10] Iteration 49600 -> Train Loss: 0.0414, Train Accuracy: 0.840\n",
      "2022-03-16 10:40:15,570 - bertQAtrain - INFO - [Epoch 8/10] Iteration 49800 -> Train Loss: 0.0418, Train Accuracy: 0.837\n",
      "2022-03-16 10:40:32,034 - bertQAtrain - INFO - [Epoch 8/10] Iteration 50000 -> Train Loss: 0.0373, Train Accuracy: 0.838\n",
      "2022-03-16 10:40:47,955 - bertQAtrain - INFO - [Epoch 8/10] Iteration 50200 -> Train Loss: 0.0417, Train Accuracy: 0.841\n",
      "2022-03-16 10:41:03,868 - bertQAtrain - INFO - [Epoch 8/10] Iteration 50400 -> Train Loss: 0.0406, Train Accuracy: 0.838\n",
      "2022-03-16 10:41:19,781 - bertQAtrain - INFO - [Epoch 8/10] Iteration 50600 -> Train Loss: 0.0487, Train Accuracy: 0.836\n",
      "2022-03-16 10:41:35,697 - bertQAtrain - INFO - [Epoch 8/10] Iteration 50800 -> Train Loss: 0.0409, Train Accuracy: 0.840\n",
      "2022-03-16 10:41:51,688 - bertQAtrain - INFO - [Epoch 8/10] Iteration 51000 -> Train Loss: 0.0452, Train Accuracy: 0.840\n",
      "2022-03-16 10:42:07,638 - bertQAtrain - INFO - [Epoch 8/10] Iteration 51200 -> Train Loss: 0.0512, Train Accuracy: 0.839\n",
      "2022-03-16 10:42:23,581 - bertQAtrain - INFO - [Epoch 8/10] Iteration 51400 -> Train Loss: 0.0342, Train Accuracy: 0.839\n",
      "2022-03-16 10:42:39,516 - bertQAtrain - INFO - [Epoch 8/10] Iteration 51600 -> Train Loss: 0.0449, Train Accuracy: 0.840\n",
      "2022-03-16 10:42:55,434 - bertQAtrain - INFO - [Epoch 8/10] Iteration 51800 -> Train Loss: 0.0387, Train Accuracy: 0.839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f8b2b43bc400680d29763a520e7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:43:10,756 - bertQAtrain - INFO - [Epoch 8/10] Validatation Accuracy:0.7628364785899032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996c2f39a54a45548214ae3d2c50836a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:43:24,870 - bertQAtrain - INFO - [Epoch 9/10] Iteration 52000 -> Train Loss: 0.0310, Train Accuracy: 0.833\n",
      "2022-03-16 10:43:40,502 - bertQAtrain - INFO - [Epoch 9/10] Iteration 52200 -> Train Loss: 0.0266, Train Accuracy: 0.841\n",
      "2022-03-16 10:43:55,871 - bertQAtrain - INFO - [Epoch 9/10] Iteration 52400 -> Train Loss: 0.0323, Train Accuracy: 0.840\n",
      "2022-03-16 10:44:11,539 - bertQAtrain - INFO - [Epoch 9/10] Iteration 52600 -> Train Loss: 0.0264, Train Accuracy: 0.837\n",
      "2022-03-16 10:44:25,511 - bertQAtrain - INFO - [Epoch 9/10] Iteration 52800 -> Train Loss: 0.0351, Train Accuracy: 0.840\n",
      "2022-03-16 10:44:41,207 - bertQAtrain - INFO - [Epoch 9/10] Iteration 53000 -> Train Loss: 0.0322, Train Accuracy: 0.844\n",
      "2022-03-16 10:44:56,596 - bertQAtrain - INFO - [Epoch 9/10] Iteration 53200 -> Train Loss: 0.0326, Train Accuracy: 0.835\n",
      "2022-03-16 10:45:11,842 - bertQAtrain - INFO - [Epoch 9/10] Iteration 53400 -> Train Loss: 0.0394, Train Accuracy: 0.840\n",
      "2022-03-16 10:45:27,504 - bertQAtrain - INFO - [Epoch 9/10] Iteration 53600 -> Train Loss: 0.0320, Train Accuracy: 0.840\n",
      "2022-03-16 10:45:43,168 - bertQAtrain - INFO - [Epoch 9/10] Iteration 53800 -> Train Loss: 0.0351, Train Accuracy: 0.843\n",
      "2022-03-16 10:45:58,906 - bertQAtrain - INFO - [Epoch 9/10] Iteration 54000 -> Train Loss: 0.0302, Train Accuracy: 0.842\n",
      "2022-03-16 10:46:14,625 - bertQAtrain - INFO - [Epoch 9/10] Iteration 54200 -> Train Loss: 0.0267, Train Accuracy: 0.838\n",
      "2022-03-16 10:46:30,394 - bertQAtrain - INFO - [Epoch 9/10] Iteration 54400 -> Train Loss: 0.0336, Train Accuracy: 0.841\n",
      "2022-03-16 10:46:46,120 - bertQAtrain - INFO - [Epoch 9/10] Iteration 54600 -> Train Loss: 0.0278, Train Accuracy: 0.841\n",
      "2022-03-16 10:47:01,861 - bertQAtrain - INFO - [Epoch 9/10] Iteration 54800 -> Train Loss: 0.0323, Train Accuracy: 0.839\n",
      "2022-03-16 10:47:18,022 - bertQAtrain - INFO - [Epoch 9/10] Iteration 55000 -> Train Loss: 0.0283, Train Accuracy: 0.839\n",
      "2022-03-16 10:47:34,072 - bertQAtrain - INFO - [Epoch 9/10] Iteration 55200 -> Train Loss: 0.0340, Train Accuracy: 0.838\n",
      "2022-03-16 10:47:49,888 - bertQAtrain - INFO - [Epoch 9/10] Iteration 55400 -> Train Loss: 0.0247, Train Accuracy: 0.842\n",
      "2022-03-16 10:48:05,778 - bertQAtrain - INFO - [Epoch 9/10] Iteration 55600 -> Train Loss: 0.0303, Train Accuracy: 0.847\n",
      "2022-03-16 10:48:22,517 - bertQAtrain - INFO - [Epoch 9/10] Iteration 55800 -> Train Loss: 0.0327, Train Accuracy: 0.837\n",
      "2022-03-16 10:48:38,252 - bertQAtrain - INFO - [Epoch 9/10] Iteration 56000 -> Train Loss: 0.0238, Train Accuracy: 0.845\n",
      "2022-03-16 10:48:54,084 - bertQAtrain - INFO - [Epoch 9/10] Iteration 56200 -> Train Loss: 0.0361, Train Accuracy: 0.841\n",
      "2022-03-16 10:49:09,864 - bertQAtrain - INFO - [Epoch 9/10] Iteration 56400 -> Train Loss: 0.0237, Train Accuracy: 0.843\n",
      "2022-03-16 10:49:25,668 - bertQAtrain - INFO - [Epoch 9/10] Iteration 56600 -> Train Loss: 0.0304, Train Accuracy: 0.838\n",
      "2022-03-16 10:49:41,461 - bertQAtrain - INFO - [Epoch 9/10] Iteration 56800 -> Train Loss: 0.0284, Train Accuracy: 0.841\n",
      "2022-03-16 10:49:57,237 - bertQAtrain - INFO - [Epoch 9/10] Iteration 57000 -> Train Loss: 0.0258, Train Accuracy: 0.840\n",
      "2022-03-16 10:50:14,644 - bertQAtrain - INFO - [Epoch 9/10] Iteration 57200 -> Train Loss: 0.0285, Train Accuracy: 0.835\n",
      "2022-03-16 10:50:30,451 - bertQAtrain - INFO - [Epoch 9/10] Iteration 57400 -> Train Loss: 0.0333, Train Accuracy: 0.838\n",
      "2022-03-16 10:50:46,342 - bertQAtrain - INFO - [Epoch 9/10] Iteration 57600 -> Train Loss: 0.0251, Train Accuracy: 0.846\n",
      "2022-03-16 10:51:02,212 - bertQAtrain - INFO - [Epoch 9/10] Iteration 57800 -> Train Loss: 0.0325, Train Accuracy: 0.841\n",
      "2022-03-16 10:51:18,079 - bertQAtrain - INFO - [Epoch 9/10] Iteration 58000 -> Train Loss: 0.0300, Train Accuracy: 0.841\n",
      "2022-03-16 10:51:33,947 - bertQAtrain - INFO - [Epoch 9/10] Iteration 58200 -> Train Loss: 0.0330, Train Accuracy: 0.838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3f7f65f9d14a38ae32c74f1b964046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:51:55,414 - bertQAtrain - INFO - [Epoch 9/10] Validatation Accuracy:0.7643691924513842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d079bfc6af24bb4a00bc14c3d28304a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:52:03,764 - bertQAtrain - INFO - [Epoch 10/10] Iteration 58400 -> Train Loss: 0.0249, Train Accuracy: 0.840\n",
      "2022-03-16 10:52:19,583 - bertQAtrain - INFO - [Epoch 10/10] Iteration 58600 -> Train Loss: 0.0206, Train Accuracy: 0.842\n",
      "2022-03-16 10:52:35,369 - bertQAtrain - INFO - [Epoch 10/10] Iteration 58800 -> Train Loss: 0.0175, Train Accuracy: 0.845\n",
      "2022-03-16 10:52:51,167 - bertQAtrain - INFO - [Epoch 10/10] Iteration 59000 -> Train Loss: 0.0269, Train Accuracy: 0.843\n",
      "2022-03-16 10:53:07,591 - bertQAtrain - INFO - [Epoch 10/10] Iteration 59200 -> Train Loss: 0.0242, Train Accuracy: 0.838\n",
      "2022-03-16 10:53:25,783 - bertQAtrain - INFO - [Epoch 10/10] Iteration 59400 -> Train Loss: 0.0222, Train Accuracy: 0.843\n",
      "2022-03-16 10:53:43,956 - bertQAtrain - INFO - [Epoch 10/10] Iteration 59600 -> Train Loss: 0.0211, Train Accuracy: 0.840\n",
      "2022-03-16 10:54:02,121 - bertQAtrain - INFO - [Epoch 10/10] Iteration 59800 -> Train Loss: 0.0267, Train Accuracy: 0.834\n",
      "2022-03-16 10:54:18,871 - bertQAtrain - INFO - [Epoch 10/10] Iteration 60000 -> Train Loss: 0.0244, Train Accuracy: 0.843\n",
      "2022-03-16 10:54:34,710 - bertQAtrain - INFO - [Epoch 10/10] Iteration 60200 -> Train Loss: 0.0221, Train Accuracy: 0.845\n",
      "2022-03-16 10:54:51,172 - bertQAtrain - INFO - [Epoch 10/10] Iteration 60400 -> Train Loss: 0.0278, Train Accuracy: 0.843\n",
      "2022-03-16 10:55:07,052 - bertQAtrain - INFO - [Epoch 10/10] Iteration 60600 -> Train Loss: 0.0250, Train Accuracy: 0.841\n",
      "2022-03-16 10:55:22,913 - bertQAtrain - INFO - [Epoch 10/10] Iteration 60800 -> Train Loss: 0.0235, Train Accuracy: 0.838\n",
      "2022-03-16 10:55:38,763 - bertQAtrain - INFO - [Epoch 10/10] Iteration 61000 -> Train Loss: 0.0254, Train Accuracy: 0.844\n",
      "2022-03-16 10:55:54,628 - bertQAtrain - INFO - [Epoch 10/10] Iteration 61200 -> Train Loss: 0.0269, Train Accuracy: 0.841\n",
      "2022-03-16 10:56:10,508 - bertQAtrain - INFO - [Epoch 10/10] Iteration 61400 -> Train Loss: 0.0258, Train Accuracy: 0.839\n",
      "2022-03-16 10:56:27,280 - bertQAtrain - INFO - [Epoch 10/10] Iteration 61600 -> Train Loss: 0.0261, Train Accuracy: 0.846\n",
      "2022-03-16 10:56:44,826 - bertQAtrain - INFO - [Epoch 10/10] Iteration 61800 -> Train Loss: 0.0198, Train Accuracy: 0.842\n",
      "2022-03-16 10:57:01,203 - bertQAtrain - INFO - [Epoch 10/10] Iteration 62000 -> Train Loss: 0.0231, Train Accuracy: 0.850\n",
      "2022-03-16 10:57:17,029 - bertQAtrain - INFO - [Epoch 10/10] Iteration 62200 -> Train Loss: 0.0235, Train Accuracy: 0.844\n",
      "2022-03-16 10:57:32,856 - bertQAtrain - INFO - [Epoch 10/10] Iteration 62400 -> Train Loss: 0.0218, Train Accuracy: 0.839\n",
      "2022-03-16 10:57:48,699 - bertQAtrain - INFO - [Epoch 10/10] Iteration 62600 -> Train Loss: 0.0221, Train Accuracy: 0.838\n",
      "2022-03-16 10:58:04,502 - bertQAtrain - INFO - [Epoch 10/10] Iteration 62800 -> Train Loss: 0.0198, Train Accuracy: 0.842\n",
      "2022-03-16 10:58:20,335 - bertQAtrain - INFO - [Epoch 10/10] Iteration 63000 -> Train Loss: 0.0181, Train Accuracy: 0.839\n",
      "2022-03-16 10:58:38,446 - bertQAtrain - INFO - [Epoch 10/10] Iteration 63200 -> Train Loss: 0.0214, Train Accuracy: 0.842\n",
      "2022-03-16 10:58:54,966 - bertQAtrain - INFO - [Epoch 10/10] Iteration 63400 -> Train Loss: 0.0187, Train Accuracy: 0.845\n",
      "2022-03-16 10:59:10,773 - bertQAtrain - INFO - [Epoch 10/10] Iteration 63600 -> Train Loss: 0.0240, Train Accuracy: 0.838\n",
      "2022-03-16 10:59:26,583 - bertQAtrain - INFO - [Epoch 10/10] Iteration 63800 -> Train Loss: 0.0232, Train Accuracy: 0.846\n",
      "2022-03-16 10:59:42,405 - bertQAtrain - INFO - [Epoch 10/10] Iteration 64000 -> Train Loss: 0.0226, Train Accuracy: 0.842\n",
      "2022-03-16 10:59:58,236 - bertQAtrain - INFO - [Epoch 10/10] Iteration 64200 -> Train Loss: 0.0241, Train Accuracy: 0.840\n",
      "2022-03-16 11:00:14,114 - bertQAtrain - INFO - [Epoch 10/10] Iteration 64400 -> Train Loss: 0.0221, Train Accuracy: 0.839\n",
      "2022-03-16 11:00:30,026 - bertQAtrain - INFO - [Epoch 10/10] Iteration 64600 -> Train Loss: 0.0195, Train Accuracy: 0.844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a528ff89e1cc4a92b6dc78cd4b3b9db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/653 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 11:00:57,703 - bertQAtrain - INFO - [Epoch 10/10] Validatation Accuracy:0.7665964172813488\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "\n",
    "##################################################\n",
    "# 변수 설정\n",
    "##################################################\n",
    "epochs = 10            # epochs\n",
    "learning_rate = 2e-5  # 학습률\n",
    "p_itr = 500           # 손실률 보여줄 step 수\n",
    "##################################################\n",
    "\n",
    "# optimizer 적용\n",
    "optimizer = AdamW(model.parameters(), \n",
    "                 lr=learning_rate, \n",
    "                 eps=1e-8) # 0으로 나누는 것을 방지하기 위한 epsilon 값(10^-6 ~ 10^-8 사이 이값 입력합)\n",
    "\n",
    "# 총 훈련과정에서 반복할 스탭\n",
    "total_steps = len(train_loader)*epochs\n",
    "\n",
    "warmup_steps = total_steps * 0.1 #10% of train data for warm-up\n",
    "\n",
    "print(f'total_steps:{total_steps}, warmup_steps: {warmup_steps}')\n",
    "\n",
    "# 스캐줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "itr = 1\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "list_training_loss = []\n",
    "list_acc_loss = []\n",
    "list_validation_acc_loss = []\n",
    "\n",
    "\n",
    "model.zero_grad()# 그래디언트 초기화\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    count1 = 0\n",
    "    model.train() # 훈련모드로 변환\n",
    "    for data in tqdm(train_loader):\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        model.zero_grad()# 그래디언트 초기화\n",
    "        \n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        #token_type_ids = data['token_type_ids'].to(device)  # distilbert에는 token_type_id가 없음\n",
    "        start_positions = data['start_positions'].to(device)\n",
    "        end_positions = data['end_positions'].to(device)\n",
    "        \n",
    "        # 모델 실행\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        #token_type_id=token_type_id,      # distilbert에는 token_type_id가 없음\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        \n",
    "        # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "        loss = outputs.loss\n",
    "        start_scores = outputs.start_logits\n",
    "        end_scores = outputs.end_logits\n",
    "        \n",
    "        # optimizer 과 scheduler 업데이트 시킴\n",
    "        loss.backward()   # backward 구함\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # 그래디언트 클리핑 (gradient vanishing이나 gradient exploding 방지하기 위한 기법)\n",
    "        optimizer.step()  # 가중치 파라미터 업데이트(optimizer 이동)\n",
    "        scheduler.step()  # 학습률 감소\n",
    "        \n",
    "        # 정확도와 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "           \n",
    "            # 정확도와 총 손실률 계산\n",
    "            \n",
    "            # start 포지션 정확도 구함\n",
    "            # argmax = 최대 인덱스값 리턴함\n",
    "            start_pred = torch.argmax(F.softmax(start_scores), dim=1)\n",
    "            start_correct = start_pred.eq(start_positions)\n",
    "            \n",
    "            # end 포지션 정확도 구함\n",
    "            end_pred = torch.argmax(F.softmax(end_scores), dim=1)\n",
    "            end_correct = start_pred.eq(end_positions)\n",
    "            \n",
    "            # start 포지션과 end 포지션 정확도를 더하고 2로 나줌\n",
    "            total_correct += (start_correct.sum().item() + end_correct.sum().item()) / 2\n",
    "                \n",
    "            total_len += len(start_positions)    \n",
    "            total_loss += loss.item()\n",
    "            #print('pred:{}, correct:{}'.format(pred, correct))\n",
    "\n",
    "            # 주기마다 test(validataion) 데이터로 평가하여 손실류 계산함.\n",
    "            if itr % p_itr == 0:\n",
    "\n",
    "                logger.info('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Train Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss/p_itr, total_correct/total_len))\n",
    "\n",
    "                list_training_loss.append(total_loss/p_itr)\n",
    "                list_acc_loss.append(total_correct/total_len)\n",
    "\n",
    "                total_loss = 0\n",
    "                total_len = 0\n",
    "                total_correct = 0\n",
    "\n",
    "        itr+=1\n",
    "        \n",
    "        #if itr > 5:\n",
    "        #    break\n",
    "\n",
    "    ####################################################################\n",
    "    # 1epochs 마다 실제 test(validattion)데이터로 평가 해봄\n",
    "    # 평가 시작\n",
    "    model.eval()\n",
    "    \n",
    "    total_test_correct = 0\n",
    "    total_test_len = 0\n",
    "    \n",
    "    for data in tqdm(eval_loader):\n",
    "        # 입력 값 설정\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        #token_type_ids = data['token_type_ids'].to(device)  \n",
    "        start_positions = data['start_positions'].to(device)\n",
    "        end_positions = data['end_positions'].to(device)\n",
    "    \n",
    "        # 손실률 계산하는 부분은 no_grade 시켜서, 계산량을 줄임.\n",
    "        # => torch.no_grad()는 gradient을 계산하는 autograd engine를 비활성화 하여 \n",
    "        # 필요한 메모리를 줄이고, 연산속도를 증가시키는 역활을 함\n",
    "        with torch.no_grad():\n",
    "            # 모델 실행\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            #token_type_id=token_type_id,      # distilbert에는 token_type_id가 없음\n",
    "                            start_positions=start_positions,\n",
    "                            end_positions=end_positions)\n",
    "    \n",
    "            # 출력값 loss,logits를 outputs에서 얻어옴\n",
    "            loss = outputs.loss\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "    \n",
    "            # 총 손실류 구함\n",
    "            # start 포지션 정확도 구함\n",
    "            start_pred = torch.argmax(F.softmax(start_scores), dim=1)\n",
    "            start_correct = start_pred.eq(start_positions)\n",
    "            \n",
    "            # end 포지션 정확도 구함\n",
    "            end_pred = torch.argmax(F.softmax(end_scores), dim=1)\n",
    "            end_correct = start_pred.eq(end_positions)\n",
    "            \n",
    "            # start 포지션과 end 포지션 정확도를 더하고 2로 나줌\n",
    "            total_test_correct += (start_correct.sum().item() + end_correct.sum().item()) / 2\n",
    "            total_test_len += len(start_positions)\n",
    "    \n",
    "    list_validation_acc_loss.append(total_test_correct/total_test_len)\n",
    "    logger.info(\"[Epoch {}/{}] Validatation Accuracy:{}\".format(epoch+1, epochs, total_test_correct / total_test_len))\n",
    "    \n",
    "    ####################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e56cc12-2914-4fea-a6c9-15bec647112f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 loss 표기\n",
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_acc_loss, label='Train Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86681fcb-84e3-428c-82ea-fb08f5ea4a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train loss와 Validatiaon acc 출력\n",
    "plt.plot(list_training_loss, label='Train Loss')\n",
    "plt.plot(list_validation_acc_loss, label='Validatiaon Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986e2215-3eda-471e-be76-bace043b0770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/distibert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315/tokenizer_config.json',\n",
       " '../model/distibert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315/special_tokens_map.json',\n",
       " '../model/distibert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315/vocab.txt',\n",
       " '../model/distibert/distilbert-fpt-wiki_20190620-mecab-model-0313-QA-0315/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 전체모델 저장\n",
    "os.makedirs(OUTPATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "model.save_pretrained(OUTPATH)  # save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = OUTPATH\n",
    "os.makedirs(VOCAB_PATH, exist_ok=True)\n",
    "tokenizer.save_pretrained(VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3736e0-3ee5-4963-8a7c-fa478633ccd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
